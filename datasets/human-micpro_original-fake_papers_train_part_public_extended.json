[
    {
        "abs": "A low-cost robot sensor system based on light sensitive tiles and an associated microcomputer has been developed to investigate the use of sensory feedback in the control of a robot manipulator. The construction of the sensor tile is described together with an outline of the microcomputer-system, which is based on the Hitachi 64180 microprocessor. Object location and recognition software has been written to enable the performance of the sensor tile to be assessed. The operation of this software is described to illustrate one possible approach to the analysis of the sensor surface images. Some results are presented which show the use of the sensor tile and image analysis software to locate and recognize conduit couplings.",
        "title": "50"
    },
    {
        "abs": "Nowadays, employing the worst case analysis is the most common approach to provide unified static task mapping–scheduling plans on MPSoCs. Since the whole design space nor a subset of design space are not explored in the worst case methods, these approaches may fail to achieve efficient performance yield. In this paper, we present a temperature-aware quasi-static task mapping–scheduling framework under process variation for hard real-time and periodic systems on MPSoCs. By employing the stochastic optimization and scenario-based approaches, we explore a few representative scenarios in the whole design space of the chip using the probability density function of the problem random variables. Then, we obtain a compact set of near optimal mapping–scheduling of real-time tasks which targets performance-yield maximization and minimization of the expected values of peak temperature. Consequently, considering different chip parameter configurations, we construct the plan set as the solutions that attain the best variation-aware task mapping–scheduling that satisfy the deadline and minimize the temperature. This plan set can readily look up at run time by the system scheduler of the chip to find the proper plan of the tasks based on the run-time parameters. The experimental results demonstrate significant improvements in performance-yield and peak temperature for almost all of the test cases off homogenous and heterogeneous MPSoCs.",
        "title": "51"
    },
    {
        "abs": "The transputer is useful not only as a parallel processing element, but also as an embedded controller. The paper describes a digital waveform generator based on the transputer. By executing a program written in a high-level language, a transputer aided by a single-chip counter generates complex digital signals with precise state times ranging from 200 ns to infinity in 50 ns increments. The duration of each output state is determined by data rather than by program execution speed: this simplifies programming. Repetition and concatenation of sequences of output states may also be encoded into data structures, which are interpreted by an invariant program. The transputer is controlled with a single serial link, and is easily integrated into a network of transputers or any system capable of communicating with a byte serial protocol. The control of focal plane arrays for astronomy is presented as a demonstration of the flexibility of the device. Fabrication cost, size and power consumption are greatly reduced relative to traditional bit-slice designs.",
        "title": "66"
    },
    {
        "abs": "In this paper we propose a low-error approximation of the sigmoid function and hyperbolic tangent, which are mainly used to activate the artificial neuron, based on the piecewise linear method. Here, the hyperbolic tangent is alternatively approximated by exploiting its mathematical relationship with the sigmoid function, showing better results. Special attention has been paid to study the minimum number of precision bits to achieve the convergence of a multi-layer perceptron network in finite arithmetic machine. All the approximation results show lower mean relative and absolute error than those reported in the state-of-the-art. Finally, the sigmoid digital implementation is discussed and assessed in terms of work frequency, complexity and error in comparison with the state-of-the-art.",
        "title": "74"
    },
    {
        "abs": "On-chip instruction cache is a potential power hungry component in embedded systems due to its large chip area and high access-frequency. Aiming at reducing power consumption of the on-chip cache, we propose a Reduced One-Bit Tag Instruction Cache ( ROBTIC ), where the cache size is judiciously reduced and the cache tag field only contains the least significant bit of the full-tag. We develop a cache operational control scheme for ROBTIC so that with the one-bit cache tag, the program locality can still be efficiently exploited. For applications where most of the memory accesses are localized, our cache can achieve similar performance as a traditional full-tag cache; however, the power consumption of the cache can be significantly reduced due to the much smaller cache size, narrower tag array (just one bit), and tinier tag comparison circuit being used. Experiments on a set of benchmarks implemented in CMOS 180 nm process technology demonstrate that our proposed design can reduce up to 27.3% dynamic power consumption and 30.9% area of the traditional cache when the cache size is fixed at 32 instructions, which outperforms the existing partial-tag based cache design. With the cache size customization, a further 47.8% power saving can be achieved. Our experimental results also show that when implemented in the deep sub-micron technologies where the leakage power is not ignorable, our design is still efficient – a coherent power saving trend (about 22%) has been observed for technologies from 130 nm down to 65 nm.",
        "title": "111"
    },
    {
        "abs": "Studying and understanding human brain is one of the main challenges of 21st century scientists. The Human Brain Project was conceived for addressing this challenge in an innovative way, enabling collaborations between 112 partners spread in 24 European countries. The project is funded by the European Commission and will last until 2023. This paper describes the ongoing activity at one of the Italian units focused on innovative brain simulation through high performance computing technologies. Simulations concern realistic models of neurons belonging to the cerebellar cortex. Due to the level of biological realism, the computational complexity of this model is high, requiring suitable technologies. In this work, simulations have been conducted on high-end Graphical Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The first technology is used during model tuning and validation phases, while the latter allows to achieve real time elaboration, aiming at a possible development of embedded implantable systems. Simulations performance evaluations are discussed in the result section.",
        "title": "130"
    },
    {
        "abs": "Over the years many efficient algorithms for the multiplierless design of multiple constant multiplications (MCMs) have been introduced. These algorithms primarily focus on finding the fewest number of addition/subtraction operations that generate the MCM. Although the complexity of an MCM design is decreased by reducing the number of operations, their solutions may not lead to an MCM design with optimal area at gate-level since they do not consider the implementation costs of the operations in hardware. This article introduces two approximate algorithms that aim to optimize the area of the MCM operation by taking into account the gate-level implementation of each addition and subtraction operation which realizes a constant multiplication. To find the optimal tradeoff between area and delay, the proposed algorithms are further extended to find an MCM design with optimal area under a delay constraint. Experimental results clearly indicate that the solutions of the proposed algorithms lead to significantly better MCM designs at gate-level when compared to those obtained by the solutions of algorithms designed for the optimization of the number of operations.",
        "title": "136"
    },
    {
        "abs": "As part of its continuing research into open systems and ISDN systems the Computer Science Department at University College London has built a network layer gateway between primary rate ISDN and Ethernet. The gateway is built from a 68000-based front-end, with a primary rate ISDN interface, installed in a Sun workstation; it is designed to have a minimal impact on the native Sun operating system software. The gatewaying task is shared between the Sun and the front-end, with the aim of relieving the front-end of the more complex gatewaying tasks such as fragmentation and the exchange of routing information with other gateways. The gateway operates as a connectionless network relay and thus reflects the dominant mode of working on LANs. The paper describes the design decisions that were taken in the construction of the gateway and outlines the eventual hardware and software design. Consideration is given to the overall system and protocol architecture, memory access, buffering strategies, circuit management, and the facilities and protocols used for network management.",
        "title": "146"
    },
    {
        "abs": "The paper analyses the possibility to replace the hydraulic and mechanical parts of a hydraulic position servo with a real-time simulation model. The aim is to test the controller hardware against the simulation model that includes the dynamics of the servo system as well as the feedback sensors and amplifier card. The requirements caused by this hardware-in-the-loop (HIL) approach are analyzed. A special attention is paid to the correct modeling of incremental encoders that are widely used as a feedback sensor. Experimental results show that there is no remarkable difference between the responses of the real system and HIL model and so the HIL model can replace the real system.",
        "title": "173"
    },
    {
        "abs": "The process of successfully creating an embedded system is highly challenging and complex; engineers typically operate under tight financial, technical and time-to-market constraints. To achieve the desired objective, the design team need to utilise effectively the most advanced software tools available, in order that the task may be completed to specification in a timely and cost-effective manner. This paper discusses the use of a CASE-tool in an embedded systems design, and reviews issues pertaining to the integration of such a tool into an embedded systems development environment. The paper focuses on the application of this high level approach in embedded systems design and concludes by describing the use of the CASE-tool in the design of a simple demonstrator.",
        "title": "229"
    },
    {
        "abs": "This paper presents a technique using a genetic algorithm to compute an efficient routing for an application-specific NoC (Network-on-Chip). The main goal of this paper is to introduce multi-objective optimization techniques to address the NoC routing. Thus, Pareto optimization has been used to determine non-dominated solutions according to two fixed objectives: (i) avoiding the reuse of same links as far as possible to reduce congestion; (ii) reducing the number of loops to limit the risk of deadlocks. The proposed method called MORGA (Multi-Objective Routing based on Genetic Algorithm) uses two steps: (i) an off-line process consisting at selecting a non-dominated solution among a pre-calculated population of solutions; (ii) an on-line process allowing the data transmission based on the built solution by the use of routing tables. MORGA is also applicable in the presence of permanent faulty links by calculating fault-free solutions. A reconfiguration of routing tables is performed when a new application is loaded on the system. Results show how a selection of the most appropriate solution can provide considerable improvement in performance.",
        "title": "343"
    },
    {
        "abs": "Structural features of plastics such as orientation, degree of crystallinity and spherulite dimension are influenced by basic extrusion conditions of temperature, screw speed, haul off speed, mass outflow and the nature of casting. These induced structural features in turn influence the post-extrusion processes which are an essential part of film-fibre production. A correlation between structural features and mechanical properties of extruded film has previously been established. A sensor, based on small-angle light scattering, using a laser source and microprocessor-based pattern processing has been designed for online control. Principles involved are described and circuit details included.",
        "title": "376"
    },
    {
        "abs": "In the last 15 years we have seen, as a response to power and thermal limits for current chip technologies, an explosion in the use of multiple and even many computer cores on a single chip. But now, to further improve performance and energy efficiency, when there are potentially hundreds of computing cores on a chip, we see a need for a specialization of individual cores and the development of heterogeneous manycore computer architectures. However, developing such heterogeneous architectures is a significant challenge. Therefore, we propose a design method to generate domain specific manycore architectures based on RISC-V instruction set architecture and automate the main steps of this method with software tools. The design method allows generation of manycore architectures with different configurations including core augmentation through instruction extensions and custom accelerators. The method starts from developing applications in a high-level dataflow language and ends by generating synthesizable Verilog code and cycle accurate emulator for the generated architecture. We evaluate the design method and the software tools by generating several architectures specialized for two different applications and measure their performance and hardware resource usages. Our results show that the design method can be used to generate specialized manycore architectures targeting applications from different domains. The specialized architectures show at least 3 to 4 times better performance than the general purpose counterparts. In certain cases, replacing general purpose components with specialized components saves hardware resources. Automating the method increases the speed of architecture development and facilitates the design space exploration of manycore architectures.",
        "title": "501"
    },
    {
        "abs": "Processor vendors have been expanding Single Instruction Multiple Data (SIMD) extensions to exploit data-level-parallelism in their General Purpose Processors (GPPs). Each SIMD technology such as Streaming SIMD Extensions (SSE) and Advanced Vector eXtensions (AVX) has its own Instruction Set Architecture (ISA) which equipped with Special Purpose Instructions (SPIs). In order to exploit these features, many programming approaches have been developed. Intrinsic Programming Model (IPM) is a low-level concept for explicit SIMDization. Besides, Compiler’s Automatic Vectorization (CAV) has been embedded in modern compilers such as Intel C++ compiler (ICC), GNU Compiler Collections (GCC) and LLVM for implicit vectorization. Each SIMDization shows different improvements because of different SIMD ISAs, vector register width, and programming approaches. Our goal in this paper is to evaluate the performance of explicit and implicit vectorization. Our experimental results show that the behavior of explicit vectorization on different compilers is almost the same compared to implicit vectorization. IPM improves the performance more than CAVs. In general, ICC and GCC compilers can more efficiently vectorize kernels and use SPI compared to LLVM. In addition, AVX2 technology is more useful for small matrices and compute-intensive kernels compared to large matrices and data-intensive kernels because of memory bottlenecks. Furthermore, CAVs fail to vectorize kernels which have overlapping and non-consecutive memory access patterns. The way of implementation of a kernel impacts the vectorization. In order to understand what kind of scalar implementations of an algorithm is suitable for vectorization, an approach based on code modification technique is proposed. Our experimental results show that scalar implementations that have either loop collapsing, loop unrolling, software pipelining, or loop exchange techniques can be efficiently vectorized compared to straightforward implementations.",
        "title": "599"
    },
    {
        "abs": "We propose a new genetics-based approach to scheduling parallel program tasks on multiprocessors. In the presence of communication delays, it is shown that task duplication is a useful technique for shortening the length of schedules. Though some genetic algorithms (GAs) for multiprocessor scheduling have been proposed so far, none of them allows task duplication. To overcome this deficiency, we develop a new GA, incorporating new genetic operators to control the degree of replication of tasks. Through simulation studies, we show that the proposed GA works effectively, especially when communication delays are relatively small.",
        "title": "688"
    },
    {
        "abs": "Some aspects of the PLEIADES project at the University of Kent are described. Work at many levels is involved in determining how effectively typical knowledge manipulation systems may be implemented on multimicroprocessor hardware. The background and motivations for this work are discussed together with descriptions of aspects of the hardware and software of the prototype Pleiades system.",
        "title": "779"
    },
    {
        "abs": "The paper proposes an analog hardware solution for the implementation of two-dimensional gradient-based algorithm. The algorithm employs the discrete cosine transform and performs missing samples reconstruction by using the compressive sensing principles. Although there is a number of algorithms for solving two-dimensional compressive sensing problems, for many of them a real-time application is a challenging task. Therefore, this paper observes an algorithm whose real-time application has relatively low complexity. Also, the reconstruction accuracy is comparable to the commonly used compressive sensing algorithms. The algorithm is observed within the several parts. The implementation of each part is considered in details, with provided discussion on the computational complexity of each part.",
        "title": "782"
    },
    {
        "abs": "Most existing wormhole networks do not provide support for prioritized traffic at the link level. Conventional demand multiplexing does not allow flexibility for fast movement of high priority messages such as for synchronization and control information. In this paper, an approach to prioritized physical channel scheduling is proposed. The motivation behind the proposed approach and its descripton are presented. The approach is evaluated and compared against the conventional demand multiplexing for a wide range of system parameters. The results demonstrate significant potential for designing high performance wormhole systems to support prioritized traffic.",
        "title": "837"
    },
    {
        "abs": "The fast Fourier transform (FFT) algorithm is widely used in digital signal processing systems (DSPs); hence, the development of a high-performance and resource-efficient FFT processor that conforms to the processing and precision requirements of real-time signal processing is highly desirable. We propose an FFT processor for field programmable gate array (FPGA) devices, based on the radix-2-decimation-in-frequency (R2DIF) algorithm. An appropriately modified parallel double-path delay commutator (DDC) architecture for radix-2 with continuous dual-input and dual-output streams (CoDIDOS) is proposed to increase throughput and reduce latency in FFT computation. The chip-area of the proposed design is reduced by decreasing the memory footprint of the complex twiddle factor multipliers. A multiplication scheme based on a combination of the unrolled coordinate rotation digital computer (CORDIC) and the canonical signed digit-based binary expression (CSDBE) is used to multiply the complex twiddle factors without requiring memory blocks for their storage. The CSDBE technique is proposed to optimize the multiplication of constants in the architecture. The proposed FFT processor is implemented as an intellectual property (IP) core and tested on a Xilinx Virtex-7 FPGA. Experimental results confirm that the proposed design improves the speed, latency, throughput, accuracy, and resource utilization of computation on FPGA devices over existing designs.",
        "title": "879"
    },
    {
        "abs": "The importance of microprocessor education when applied to physics students and the need for incorporation of software in new automated equipment is realized. Experience of developing and applying a programme, which is based on practical knowledge gained from lecture courses, for such an education is reported. Much of the programme has been directed towards the use of microprocessor-based equipment, since such a need is being realized with new equipment arriving in this part of the world. The programme facilities, course contents and laboratory experiments are also explained.",
        "title": "974"
    },
    {
        "abs": "Many broadcast algorithms have been proposed for the mesh in the literature. However, most of these algorithms do not exhibit good scalability properties as the network size increases. As a consequence, most existing broadcast algorithms cannot support real-world parallel applications that require large-scale system sizes due to their high computational demands. Motivated by this observation, this paper makes two contributions. Firstly, in an effort to minimise the effects of network size on communication performance, this study proposes a new routing approach that enables the development of efficient broadcast algorithms that can maintain good performance levels for various mesh sizes. Secondly, based on the new routing approach, we propose a new adaptive broadcast algorithm for the mesh. The main feature of the proposed algorithm is its ability to handle broadcast operations with a fixed number of message-passing steps irrespective of the network size. Results from extensive comparative analysis reveal that our algorithm exhibits superior performance characteristics over those of the well-known Recursive Doubling and Extending Dominating Node algorithms.",
        "title": "982"
    },
    {
        "abs": "This research discusses hardware architectures, script-based automation and software and hardware methodologies for developing customized System-on-Chip scalar/vector processors within the example application domain of telephony codes. The approaches researched include Register-Transfer-Level methodologies resulting in an SIMD-enhanced processor known as the ITU-VE1, and Electronic System Level methodologies resulting in a multi-parallel vector processor known as the SS_SPARC. The example applications were the ITU-T G.729A and G.723.1 speech codecs chosen for their abundant data-level parallelism and availability for research purposes. Results indicate the proposed scalar/vector accelerators achieve a maximum speed-up of 4.27 and 4.62 for the G729.A and G723.1 encoders respectively for 512-bit wide SIMD configurations. Both vector processors resulting from the proposed methodologies were implemented as VLSI macros and compared at the silicon level. Compared to the Register-Transfer-Level flow, the Electronic System Level flow implementing the same datapath results in increased power consumption of 3–15% however delivers an area reduction of 2–18% and substantially shortens design and verification time making it a viable alternative to established RTL methodologies.",
        "title": "1039"
    },
    {
        "abs": "The need for scalable and efficient on-chip communication in future many-core architecture has resulted in the network-on-chip (NoC) design emerging as a popular solution. It is a common belief that packet-based NoC can provide high efficiency, high throughput, and low latency for future applications instead of conventional transaction-based bus. However, these superior features of NoCs are only applied to unicast (one-to-one) latency non-critical traffic. Their multi-hop feature and inefficient multicast (one-to-many) or broadcast (one-to-all) support have made it awkward when performing some kinds of communications including cache coherence protocol, global timing and control signals, and some latency critical communications. This paper presents VBON, a new architecture of incorporating buses into NoCs in order to take advantage of both NOCs and buses in a hierarchical way. The point-to-point links of conventional NoC designs can be used as bus transaction link dynamically for bus request. This can achieve low latency while sustain high throughput for both unicast and multicast communications at low cost. To reduce the latency of physical layout for the bus organization, the hierarchical redundant buses are used. Detailed network latency simulation and hardware characterization demonstrate that VBON can provide the ideal interconnect for a broad spectrum of unicast and multicast scenarios and achieve these benefits with inexpensive extensions to current NoC router.",
        "title": "1046"
    },
    {
        "abs": "The benefits and deficiencies of shared and private caches have been identified by researchers. The performance impact of privatizing or sharing caches on homogeneous multi-core architectures is less understood. This paper investigates the performance impact of cache sharing on a homogeneous same-ISA 16-core processor with private first-level (L1) caches by considering 3 cache models which vary the sharing property of second-level (L2) and third-level (L3) cache banks. It is observed that across many scenarios, the cache privatization’s average memory access time improved as the L1 cache miss rate increased and/or the cross-partition interconnect latencies increased. Under uniform memory address distribution, and when the L3 cache miss rate is close to 0, privatizing both L2s and L3s performs best among the 3 cache models. Furthermore, we mathematically demonstrate that when the interconnect’s bridge latency is below 264 cycles, privatizing L2 caches beats privatizing both L2 and L3 caches, while the reverse is true for large bridge latencies representing high-traffic and heavy workload applications. For large interconnect delays, the private L2 and L3 model is best. For low to moderate interconnect latencies, and when the L3 miss rate is not close to 0, sharing both L2 and L3 banks among all cores performs best followed by privatizing L2s, while privatizing both L2s and L3s ranks last. Under worst case address distributions, cache privatizing benefits generally increase, and with large bridge latencies, privatizing L2 and L3 banks outperforms the other cache models. This reveals that as application workloads become heavier with time, resulting in large cache miss rates and long bridge and interconnect delays, privatizing L2 and L3 caches may prove beneficial. Under less stressful workloads, sharing both L2 and L3 caches have the upper hand. This study confirms the desired configurability and flexibility of the cache memory’s sharing degree based on the running workload.",
        "title": "1082"
    },
    {
        "abs": "We answer the question on how much memory a packet switch/router needs; more specifically, we propose a systematic method that is simple, rigorous and general for determining the absolute lower bound of packet buffering required by practical switching systems. Accordingly, we introduce a deterministic traffic scenario that stresses the global stability property of finite output queues and demonstrate its usefulness by dimensioning the internal buffer capacity of two popular CIOQ switches.",
        "title": "1127"
    },
    {
        "abs": "Previous task mapping assumes that applications directly fetch data on remote nodes and build up their energy efficient mapping based on pattern of the NoC traffic. However, the data movement is actually managed by a cache coherence mechanism that executes a much more complicated protocol under application layer to guarantee data correction. Thus, we propose an energy efficient task mapping referring the protocol layer, rather than application layer, to precisely model protocol activities and energy dissipation. By a probabilistic description of cache accessing, we find a efficient method to convert application activities to energy consumption, generating an energy evaluation as a global optimization goal. We also propose a task mapping algorithm to minimize the energy consumption by referring activity intensity of the protocol. The experimental results show that the proposed energy model achieves a precision with less than 2% error and provides a credible quantitative criterion for energy optimization of cache coherence protocols. Comparing to application-layer optimization, our task mapping can obtain 20% energy saving and 15% latency reduction on average.",
        "title": "1133"
    },
    {
        "abs": "The performance analysis and comparison of 2 × 4 network on chip (NoC) topology are mainly presented in this paper. Firstly, three common 2 × 4 topologies, 2D Mesh topology, 2D Torus topology and hierarchical Mesh topology are designed. Secondly, the performances of three topologies are analyzed and compared in detail by using NoC performance evaluation standard. Finally, the occupying resources of three topologies are also compared. The result shows that 2D Torus topology can achieve higher throughput and lower average network latency in occupying fewer resources.",
        "title": "1220"
    },
    {
        "abs": "Recent embedded applications are widely used in several industrial domains such as automotive and multimedia systems. These applications are critical and complex, involving more computing resources and therefore increasing the power consumption of the system. Although performance still remains an important design metric, power consumption has become a critical factor for several systems, particularly after the increasing complexity of recent System-on-Chip (SoC) designs. Consequently, the whole computing domain is being forced to switch from a focus on high performance computation to energy-efficient computation. In addition to the time-to-market challenge, designers need to estimate, rapidly and accurately, both area occupation and power consumption of complex and diverse applications. High-Level Synthesis (HLS) has been emerged as an attractive solution for designers to address this challenge in order to explore a large number of design points at a high-level of abstraction. In this paper, we target FPGA-based accelerators. We propose HAPE, a high-level framework based on analytic models for area and power estimation without requiring register-transfer level (RTL) implementations. This technique allows to estimate the required FPGA resources and the power consumption at the source code level. The proposed models also enable a fast design space exploration (DSE) with different trade-offs through HLS optimization pragmas, including loop unrolling, pipelining, array partitioning, etc. The accuracy of our proposed models is evaluated by using a variety of synthetic benchmarks. Estimated power results are compared to real board measurements. The area and power estimation results are less than 5% of error compared to RTL implementations.",
        "title": "1247"
    },
    {
        "abs": "Asynchronous systems are attracting the interest of the designer community because of several useful features for sub-micron technologies: process-variation tolerant, low-power, removal of the clock tree generation, etc. One of the main problems for the simulation of these systems is the variable computation delays of their modules, that compute as fast as possible under the actual conditions of the system. This behavior complicates the high-level simulation of such systems and it is the main reason for the lack of simulation tools devoted to asynchronous microarchitectures. In this paper we present a modeling method useful for this kind of systems that describes the variable computation delay of an asynchronous circuit by using probability distribution functions. This method is deployed in an architectural simulator of a 64-bit superscalar asynchronous microarchitecture where the computation delay of each one of the modules of the microarchitecture was characterized through a probability distribution function. The experimental results show that the asynchronous behavior is successfully modeled, and the architectural simulations of standard benchmarks is affordable in terms of wall-clock simulation time.",
        "title": "1284"
    },
    {
        "abs": "This paper describes a Single Event Transient (SET) suppression design technique for hardening combinational circuits against SETs in non-volatile Field Programmable Gate Arrays (FPGAs). The proposed method adds a SET suppressor circuit that is insensitive to SETs, to each primary output of a combinational circuit. The SET suppressor circuit consists of three components; an AND gate to suppress an SET reaching the primary output, when the primary output is logic ‘0’, and an OR gate when the primary output is logic ‘1’. The third component is a simple two input multiplexer with its output connected to its own select line such that it will select the AND gate output when the combinational circuit primary output is logic ‘0’ and the OR gate output when the primary output is logic ‘1’. A delay element is used to split each primary output of the combinational circuit into two signals. The two signals, one being the original primary output and the other a delayed copy of it, is sent to input one and input two of the SET suppressor. An alternative embodiment of the SET suppressor circuit is to use Double Modular Redundancy (DMR) instead of the delay element implementation. The SET Suppressor method is thoroughly tested on MCNC’91 benchmarks using the ModelSim simulator. The SET Suppressor circuit provides total immunity against SETs, however it does so with an area savings of 11.6–62.2% with respect to TMR when the delay element technique is use. When the DMR SET Suppressor technique is used, the area savings with respect TMR is between 16.1% and 31.9%.",
        "title": "1397"
    },
    {
        "abs": "The paper presents a decomposition method dedicated for PAL based CPLDs. Non-standard usage of decomposition, which leads to the minimization of area in an implemented circuit and the reduction of used logic blocks in a programmable structure, is the aim of the proposed method. Each decomposition step (bound set selection, graph colouring, column pattern coding, etc.) is oriented for implementation in a PAL-based structure that is characterized by a PAL-based logic block. The proposed decomposition method is an extension of the classical approach, commonly thought to be adequately efficient. Experiments carried out on typical benchmarks show significant area reduction.",
        "title": "1527"
    },
    {
        "abs": "An FPGA-based fully hardware Kalman filter has been designed and presented and a reconfigurable Kalman filter-based coprocessor in FPGAs has been proposed. High-speed arithmetic function implementations and pipelining have been used and a substantial improvement in performance has been gained. The cycle time (one iteration) for computing Kalman filter is reduced from 1.8274 μs in our previous design to 0.4013 μs. The performance gained in our approach includes two to four orders of magnitude higher speed than other implementations. The high-speed, recongifuration and easy-to-develop characteristics of the FPGA-based Kalman filter will largely broaden the real-time application area of Kalman filter.",
        "title": "1571"
    },
    {
        "abs": "An effective development environment has been designed for single-board microprocessor products, providing many of the advantages of conventional development systems but without the high cost of special purpose development hardware, such as in-circuit emulation. The use of a low-cost personal computer combined with the poly FORTH programming language running on the single-board computer solves the problems of cheap access to mass storage and the wish to interact with the hardware during development.",
        "title": "1610"
    },
    {
        "abs": "The ignition control requirements of an internal combustion petrol engine are reviewed and the benefits of accurate ignition control are discussed. A design for a microprocessor-based open-loop ignition controller is described and the experimental results obtained with this controller presented. Various means of achieving further improvements are suggested, including a closed-loop controller for optimum ignition timing under all conditions of engine wear. This strategy uses peak cylinder pressure angle as the feedback signal on which the adaptive control is based.",
        "title": "1611"
    },
    {
        "abs": "Modern SoC architectures use NoCs for high-speed inter-IP communication. For NoC architectures, high-performance efficient routing algorithms with low power consumption are essential for real-time applications. NoCs with mesh and torus interconnection topologies are now popular due to their simple structures. A torus NoC is very similar to the mesh NoC, but has rather smaller diameter. For a routing algorithm to be deadlock-free in a torus, at least two virtual channels per physical channel must be used to avoid cyclic channel dependencies due to the warp-around links; however, in a mesh network deadlock freedom can be insured using only one virtual channel. The employed number of virtual channels is important since it has a direct effect on the power consumption of NoCs. In this paper, we propose a novel systematic approach for designing deadlock-free routing algorithms for torus NoCs. Using this method a new deterministic routing algorithm (called TRANC) is proposed that uses only one virtual channel per physical channel in torus NoCs. We also propose an algorithmic mapping that enables extracting TRANC-based routing algorithms from existing routing algorithms, which can be both deterministic and adaptive. The simulation results show power consumption and performance improvements when using the proposed algorithms.",
        "title": "1617"
    },
    {
        "abs": "Within this article an adaptive approach for parallel simulation of SystemC RTL models on future many-core architectures like the Single-chip Cloud Computer (SCC) from Intel is presented. It is based on a configurable parallel SystemC kernel that preserves the partial order defined by the SystemC delta cycles while avoiding global synchronization as far as possible. The underlying algorithm relies on a classification of existing communication relations between parallel processes. The type and topology of communication relations determines the type and number of causality conditions that need to be fulfilled during runtime. The parallel kernel is complemented by an automated tool flow that allows detecting relevant model-specific properties, performing a fine-grained model partitioning, classifying communication relations and configuring the kernel. Experiments by means of a MPSoC model show that pure local synchronization can provide significant performance gains compared to global synchronization. Furthermore, the combination of local synchronization with fine-grained partitioning provides additional degrees of freedom for optimization.",
        "title": "1723"
    },
    {
        "abs": "The GFLOPS project's aim is to develop a parallel architecture as well as its software environment to implement scientific applications efficiently. This goal can be achieved only with a real collaboration among the architecture, the compiler and the programming language. This paper investigates the C// Parallel language and the underlying programming model on a global address space architecture. The main advantage of our paradigm is that it allows a unique framework to express and optimize both data and control parallelism at user-level. The evaluation of C// has been conducted on a cluster of PCs.",
        "title": "1778"
    },
    {
        "abs": "The paper considers the design of a dynamic test bed for the master processor of a realtime, distributed, fault-tolerant computer system. Requirements for a response time of the order of 20 μs, for synchronized operation with the realtime system during mission time, for handling tasks of different periodicity, for storage and retrieval of large amounts of data, and for fault simulation and display, all add to system complexity. The system described uses a dedicated system approach and an I/O-intensive multi-processing architecture in the iRMX operating system environment. The choice of language, mode of communication, buffer size selection, synchronization scheme, etc. are important aspects considered in the realization of the system.",
        "title": "1834"
    },
    {
        "abs": "In this work, we extend our previous manuscript regarding a systematic study of data remanence effects on an intrinsic Static Random Access Memory Physical Unclonable Function (SRAM PUF) implemented on a Commercial Off-The-Shelf (COTS) device in the temperature range between [formula omitted]C and [formula omitted]C. As the experimental results of our previous work show, an attack against intrinsic SRAM PUFs, which takes advantage of data remanence effects exhibited due to low temperatures, is possible, resulting in the attacker being able to know the PUF response, with high probability. As demonstrated in our previous work, this attack is highly resistant to memory erasure techniques and can be used to manipulate the cryptographic keys produced by the SRAM PUF. In this work, we examine and discuss potential countermeasures against this attack in more detail, and investigate whether this attack can be performed using an experimental setup that does not guarantee a high degree of thermal isolation. Additionally, we also examine and discuss whether very low temperatures can be used to perform another relevant type of attack against SRAM PUFs, based on whether very low temperature can prevent the SRAM from being overwritten. Finally, we also discuss related works and the generalisation of our results in more detail.",
        "title": "2018"
    },
    {
        "abs": "This paper presents an energy efficient architecture to provide on-demand fault tolerance to multiple traffic classes, running simultaneously on single network on chip (NoC) platform. Today, NoCs host multiple traffic classes with potentially different reliability needs. Providing platform-wide worst-case (maximum) protection to all the classes is neither optimal nor desirable. To reduce the overheads incurred by fault tolerance, various adaptive strategies have been proposed. The proposed techniques rely on individual packet fields and operating conditions to adjust the intensity and hence the overhead of fault tolerance. Presence of multiple traffic classes undermines the effectiveness of these methods. To complement the existing adaptive strategies, we propose on-demand fault tolerance, capable of providing required reliability, while significantly reducing the energy overhead. Our solution relies on a hierarchical agent based control layer and a reconfigurable fault tolerance data path. The control layer identifies the traffic class and directs the packet to the path providing the needed reliability. Simulation results using representative applications (matrix multiplication, FFT, wavefront, and HiperLAN) showed up to 95% decrease in energy consumption compared to traditional worst case methods. Synthesis results have confirmed a negligible additional overhead, for providing on-demand protection (up to 5.3% area), compared to the overall fault tolerance circuitry.",
        "title": "2055"
    },
    {
        "abs": "This paper reports on research conducted by Smiths Industries (SI) Aerospace within the Control Technology Programme (CTP), to ascertain the feasibility of hardware fault tolerance via dynamic software reconfiguration and to demonstrate its viability in the context of a typical real-time avionic application. Hardware fault-tolerant (FT) systems require the physical replication of hardware components, with the component being the smallest configurable unit. The research approach adopted here is to segregate fully the software (functionality) from the hardware, and regard the configurable units as the software functions themselves. Failure of a component within a computing module would therefore require dynamically reconfiguring the affected software functions elsewhere within the module. Furthermore, it would be possible to reconfigure individual functions not only over different processors but also to currently active processors if spare processing capacity was available in those processors. The computing platform for conducting the research comprised a message-based multiprocessor module, on which was developed a distributed Operating System layer to support both the initial configuration of the application functions and their reconfiguration as a result of user-instigated failure of the module hardware. Software reconfiguration from both module-local memory and module-external backing store was successfully demonstrated for critical and non-critical functions respectively. Based on the research/development system, a self-contained FT module variant was constructed for integration within the System Digital Control Laboratory (SDCL) at BAe Airbus. This module additionally demonstrated the periodic and aperiodic communication capability of the ARINC 629 Combined Mode Protocol (CP) Databus in supporting both the module's functional operation and configuration/reconfiguration process.",
        "title": "2065"
    },
    {
        "abs": "The development of front-end converters for power factor correction and DC link voltage control of power electronics converters such as, UPS, Inverters, and Switched Power Supplies, has been attracting great interest from the scientific community that works toward the achievements of cost reduction, high efficiency, and reliability. In this context, this paper proposes a microprocessed control technique for sinusoidal input line current imposition in front-end ac–dc converters. This gave rise to an innovative sensorless boost converter, named in this work as PFC-Boost-CSL. The proposed method is based on experimental acquisition of gate-drive signal sequences for different load conditions. These signals correspond to a complete cycle of the AC input voltage and are recorded in the microcontroller memory in order to be reproduced when used in a boost converter without current sensor. In the operation of PFC-Boost-CSL, a suitable switching sequence is sent to drive the power switch in order to minimize output voltage error and maintain a sinusoidal input current. Aiming to prove the proposed control concept, a 600 W PFC-Boost-CSL prototype was built and analyzed in laboratory and the main experimental results are presented herein.",
        "title": "2134"
    },
    {
        "abs": "This paper summarizes the workplan of the COBRA project. Thereby it emphasizes the work concerning our prototyping environment with special benefit for hardware/software codesign which we use as target architecture in COBRA. This architecture is very flexible, easily extensible, and provides a high gate capacity. It supports standard processor integration as well as processor emulation.",
        "title": "2142"
    },
    {
        "abs": "This paper presents a model-driven framework that provides a tool-supported design flow for fault-tolerant embedded systems. Its system models comprise abstract descriptions of the application and the underlying execution platform. They provide the input to our analysis and optimization techniques that enable the automated exploration of design alternatives for applications with reliability requirements. The automated generation of source code and platform configuration files speeds up the development process. Our contribution is to advance reliability-aware design further into practice by providing an integrated tool framework and removing unrealistic assumptions in the analyzes. The case studies demonstrate the effectiveness of our approach.",
        "title": "2202"
    },
    {
        "abs": "We describe the parallel implementation of fully recurrent neural networks (RRN) on a transputer-based multiprocessor system. To train the RNN, the real-time recurrent learning (RTRL) algorithm was used. The computationally intensive sequential RTRL algorithm has been transformed to an equivalent parallel algorithm, realized in a ring topology that can be matched to a variety of target architectures, ranging from application specific VLSI arrays to general purpose multiprocessor systems. A ring array of up to 19 T800 transputers was programmed to efficiently perform the parallel RTRL algorithm. The speedup of the transputer implementation was estimated both analytically and through simulations, and the effect of the communication overhead is discussed. It is shown that as more neuron units are allocated to the same processor the efficiency is increased.",
        "title": "2292"
    },
    {
        "abs": "This presentation covers mainly five sections of ARINC 629 physical layer technology. The first three will give an introduction with an historical overview, leading from aeronautical data bus systems in general to the ARINC 629 development. The impact of new avionic architectures such as IMA (integrated modular avionic), ACR (avionic computing resource) and RDC (remote data concentrator), which are creating new data bus requirements will be discussed—considering the ARINC 629 solution in particular. European activities in this area are also mentioned here. In the fourth section, the current status of the B 777 technology is mentioned briefly and the European view on ARINC 629 CP technology is covered in more detail. The fifth section covers physical layer development with a special view on current mode coupling methods. Also, possible alternatives to the present solution will be investigated from technical and economical viewpoints, and the problem areas and advantages of individual solutions will be shown. This will lead to directions for probable solutions in the near future.",
        "title": "2294"
    },
    {
        "abs": "This paper presents the design and implementation of an RSA cryptosystem using multiple TMS320E15 DSP chips. The system consists of a stand-alone unit containing the DSP hardware and a high-level PC user interface. The system is flexible and allows for additional DSP chips to be inserted in allocated slots to improve its performance. It also allows for trade-off between speed of encryption and level of security. The system was found to be 70 times faster than the same RSA algorithm implemented using C-language at PC level.",
        "title": "2320"
    },
    {
        "abs": "The paper considers a microcomputer implementation of an algorithm for generating sine and cosine functions at high speed and with moderate precision. In essence, the trigonometric functions are derived by interpolating between function values held in a look-up table: the method uses simple integer arithmetic and it does not require the services of an arithmetic coprocessor device. From a detailed examination of interpolation errors, a design rule is established for determining the minimum size of table for a particular function magnitude, and for realizing errors below 1/2 LSB. For those applications requiring increased accuracy, a modification to the basic interpolation algorithm is offered to allow estimation and cancellation of the interpolation errors. As an illustration of the design method, a subroutine that produces sine and cosine function values to 13-bit 2's complement precision on an 80286 microprocessor system is listed. Testing the subroutine's accuracy confirms the validity of the error analysis, the design rule and the interpolation error correcting scheme. Further tests, performed on a range of IBM PC compatible machines, show that the subroutine calculates the function faster than one that relies on the PC's attendant coprocessor device.",
        "title": "2352"
    },
    {
        "abs": "The paper presents a new testing method applicable to VLSI arrays made up of microcomputers as processing elements. A system with single instruction multiple data (SIMD) processing is assumed. In this system, computing elements are connected by a regular interconnection network. A new fault model for the array is presented. Faults are defined at a functional level and allow a systematic test generation procedure to be derived. This procedure is independent of array implementation details and still retains a SIMD characterization. Testing is performed by sequences of instructions. Test sequences are defined by using two ordering criteria. The first criterion establishes the external observability and controllability of the instructions. The second criterion uses instruction cardinality as metric for evaluation of inspection complexity. Algorithms and procedures for a correct execution of functional testing are presented. An example of the application of the proposed technique to an existing parallel scheme made of complex microprocessors is described. The criteria for structuring the test procedure lead to an optimization of fault coverage and a reduction of ambiguity.",
        "title": "2365"
    },
    {
        "abs": "ARINC 653 is currently one year from the completion of its first phase. Phase 1 is intended to meet the requirements of the most critical systems which require strictly deterministic periodic processing. The original hardware platform envisaged for software conforming to ARINC 653 was cabinet-based line replaceable modules communicating with each other over the ARINC 659 backplane databus. However, ARINC 653 conformant software must be compatible with other hardware configurations such as the proposed Avionics Computing Resource (RTCA SC-182) and LRMs with direct ARINC 629 connections and ARINC 429 based systems. This paper describes the challenges facing the ARINC 653 working group in defining a sufficiently rigorous specification of services without constraining the developer to a particular system architecture. The external communications mechanisms and configuration information necessary to realize the aims of portability and re-use are considered. The achievement of ‘time partitioning’ on a shared processor resource is discussed. The paper concludes with a discussion of future extensions to ARINC 653 to enable reconfiguration of applications both statically and dynamic- ally, and to accommodate aperiodic processes.",
        "title": "2435"
    },
    {
        "abs": "The ability to tolerate one failing link in communication networks is sufficient for many practical purposes. One-fault tolerance can also be achieved at much lower cost than methods that can guarantee tolerance of multiple faults. We consider wormhole-routed meshes with two different routing algorithms (dimension order arid positive first) arid for each of these we propose two simple methods that guarantee to tolerate one failing link. Then we study how well these methods work in the presence of multiple faults. Through extensive experiments, we demonstrate that even if these simple methods are not guaranteed to be able to handle more than one fault, there is a high probability of them successfully handling a modest number of faults.",
        "title": "2526"
    },
    {
        "abs": "The M3 backplane bus was designed at the beginning of the 1980s with the goal of providing the backbone for development of a new family of multiprocessor systems. Being used by companies which are interested in the system market rather than in selling just boards, M3 did not receive widespread publicity. It did, however, apply some design solutions later incorporated in other better known buses. The paper focusses on the bus design process, and presents the environment and constraints of M3, its key features, current status and position with respect to other standards.",
        "title": "2559"
    },
    {
        "abs": "The paper presents an actuator programming language IPL in brief. This elementary language is optimized to control independent actuators in a distributed environment, where actuators communicate with a machine controller via an intra-machine communication network. In this environment the role of the higher control level is to command independent actuators. The language IPL is found to be suitable for elementary machines, but it is clear that a higher-level programming language is needed for robotics.",
        "title": "2617"
    },
    {
        "abs": "The mechanism of multiprocessing (MMP) has been developed and implemented as an enhancement of a standard operating system (OS ES) to support efficient execution of fine-grained parallel activities. The MMP mechanism is presented through the description of its primitives, associated data structures and its interface with the OS ES. Further efficiency enhancement of the MMP mechanism has been achieved by the hardware implementation of the MMP primitives in the specially designed processor, named the MMP processor. The details of its architecture and organisation are given in the paper. Special emphasis is put on the design of the efficient pipelined control, which resulted from the precise timing analysis of the considered design choices.",
        "title": "2694"
    },
    {
        "abs": "This paper compares gallium arsenide and silicon technologies for high-speed digital applications. The bases of comparison are physics, engineering and economics. Gallium arsenide is shown to be limited by engineering and economic considerations; true like-for-like comparisons on speed are almost impossible to obtain. Where complexity is low and performance at a premium, gallium arsenide is probably the best choice, but for even modest complexity silicon bipolar will remain dominant.",
        "title": "2801"
    },
    {
        "abs": "By exploring the scalability of memory controllers (MCs) and ranks in scalable memory systems, larger degrees of memory bandwidth are offered when scaling cores in traditional multicores and embedded systems, and the ratio computation versus memory width - expressed as ratio between the number of cores and MCs - favors the former in detriment to the latter. In scalable memory systems, this ratio tends to balance the number of cores and MCs. Furthermore, since each core has their Last Level Cache (LLC) strongly subject to the number of Miss Status Holding Registers (MSHRs) present, which retain information on all outstanding misses of a specific cache line, it is fundamental to evaluate the impact of these elements in scalable memory systems. Experimental results show that, as reducing the number of MSHRs, memory bandwidth levels are reduced by about 64% and rank energy-per-bit levels are increased of about 36% for different patterns.",
        "title": "2899"
    },
    {
        "abs": "This paper presents a new design that implements the data-driven (i.e. dataflow) computation paradigm with intelligent memories. Also, a relevant prototype that employs FPGAs is presented for the support of intelligent memory structures. Instead of giving the CPU the privileged right to decide what instructions to fetch in each cycle (as is the case for control-flow CPUs), instructions in dataflow computers enter the execution unit on their own when they are ready to execute. This way, the application-knowledgeable algorithm, rather than the application-ignorant CPU, is in control. This approach could eventually result in outstanding performance and elimination of large numbers of redundant operations that plague current control-flow designs. Control-flow and dataflow machines are two extreme computation paradigms. In their pure form, the former machines follow an inherently sequential execution process while the latter are parallel in nature. The sequential nature of control-flow machines makes them relatively easy to implement compared to dataflow machines, which have to address a number of issues that are easily solved in the realm of the control-flow paradigm. Our dataflow design solves these issues at the intelligent memory level, separating the processor from dataflow maintenance tasks. It is shown that using intelligent memories with basic components similar to those of FPGAs produces a feasible approach. Expected improvements within the next few years in underlying intelligent memory and FPGA technologies will have the potential to make the effect of our approach even more dramatic.",
        "title": "3076"
    },
    {
        "abs": "We propose pixel pipeline architecture with a selective z-test scheme that focuses on reducing the data processed in the pixel pipeline by employing preprocessing. Reduction of data can reduce the data transmission between the 3D graphics processor and the memory and also reduce the power consumption of memory access, which is a critical point in the case of mobile devices. In 3D graphics processor, most of the memory transmissions are occurred in rasterization stage, especially in pixel pipelines. To reduce memory transmission, the proposed architecture exploits the coherency among pixel fragments to predict the visibility of each pixel fragment. Through this, the proposed architecture eliminates invisible fragments before texture mapping using a single z-test, which would require two z-tests in the mid-texturing architecture. According to the simulations, the proposed architecture reduces data transmission by 19.9–22.6% as compared to the mid-texturing architecture at the expense of a 5% reduction in performance. Further, the proposed architecture also reduces the cell area of the depth cache by 26.4% and the area of overall architecture by 6% as compared to that in the mid-texturing architecture.",
        "title": "3131"
    },
    {
        "abs": "In 2016, Renes et al. were the first to propose complete addition formulas for Elliptic Curve Cryptography (ECC) on Weierstrass curves. With these formulas, the same set of equations can be used for point addition and point doubling, which makes software and hardware implementations less vulnerable to side-channel (SCA) attacks. Further, all inputs are valid, so there is no need for conditional statements handling special cases such as the point at infinity. This paper presents the first ASIC design of the complete addition formulas of Renes et al. Each computation layer in the design is balanced, from the field arithmetic to the point multiplication. The design explores two datapaths: a full-width Montgomery Multiplier ALU (MMALU) with a built-in adder and a serialized version of the MMALU. The interface sizes of the MMALU are optimized through an exploration of the design parameters. The register file size is minimized through an optimal scheduling of the modular operations. The top-level point multiplication is implemented using the Montgomery ladder algorithm, with the additional option of randomizing the execution order of the point operations as a countermeasure against SCA attacks. The implementation results after synthesis are generated using the open source NANGATE45 library.",
        "title": "3157"
    },
    {
        "abs": "Energy reduction in embedded processors is a must since most embedded systems run on batteries and processor energy reduction helps increase usage time before needing a recharge. Register files are among the most power consuming parts of a processor core. Register file power consumption mainly depends on its size (height as well as width), especially in newer technologies where leakage power is increasing. We provide a register file architecture that, depending on the application behavior, dynamically (i) adapts the width of individual registers, and (ii) puts partitions of temporarily unused registers into low-power mode, so as to save both static and dynamic power. We show that our scheme increases register file area by 3.6% and imposes 2.85% performance overhead on average. Our experimental results on OpenRISC 1200 processor and with selected MiBench benchmark suite show up to 29%, and 54% (24% and 49% on average) reduction in dynamic and static energy consumption of the register file, respectively.",
        "title": "3164"
    },
    {
        "abs": "A food recording electronic device to simplify the collection of dietary data is described. The need for a subject to maintain a quantitative account of food intake, and for a dietitian to subsequently code this record prior to computer analysis, are both eliminated using this approach. In combination with an electronic kitchen balance, the recorder, which is based upon a single-board computer (SBC) and special keyboard, enables the weight, food type and time of consumption of each food to be recorded automatically in response to simple key entry. The resident program, transparent to the user, allows data from surveys lasting up to three weeks to be stored for subsequent serial downloading to a host computer.",
        "title": "3207"
    },
    {
        "abs": "The paper presents the results of investigations into techniques for automatically generating concurrent implementations of visual object recognition systems from high-level visual domain descriptions. Visual knowledge can be represented using ‘schemata’-object-centred recognition models that intentionally represent classes of related objects. Schemata form hierarchies based on relationships such as composition and function. Schema-based representations contain implicit concurrencies that can be exploited by a recognition system called a ‘recognition network’. The paper presents the design of a simple schema representation language, and shows how it can be automatically transformed into concurrent code. This is done by building a compiler that takes schemata as input and produces a recognition network, in occam , as output. The design of the compiler involves several novel features. The existence of large transputer arrays makes it possible to execute the resulting code with high physical, as well as conceptual, parallelism.",
        "title": "3256"
    },
    {
        "abs": "Traditional memory design aims to improve bandwidth and reduce power by trading off memory width and frequency scaling (FS). In this context, we propose [formula omitted] a hardware scheduling mechanism that, for the first time, performs FS on ranks in scalable memory systems which employ Double Data Rate (DDR) synchronous dynamic random access memories (SDRAM). [formula omitted] is able to utilize different rank frequencies via controlling FS intensity - defined as the ratio between the amount of time FS is applied and the total selected scheduled cycle. We propose a design space exploration of [formula omitted] with different FS intensities aiming to determine the behavior of system implications such as bandwidth, rank temperature, and power utilization. Our findings show that for 100% of FS intensity, bandwidth increases proportionally while rank temperature is increased of about +23.7°C%, and energy-per-bit magnitude is decreased in up to 67%.",
        "title": "3320"
    },
    {
        "abs": "Due to the current proliferation of GPU devices in HPC environments, scientist and engineers spend much of their time optimizing codes for these platforms. At the same time, manufactures produce new versions of their devices every few years, each one more powerful than the last. The question that arises is: is it optimization effort worthwhile? In this paper, we present a review of the different CUDA architectures, including Fermi, and optimize a set of algorithms for each using widely-known optimization techniques. This work would require a tremendous coding effort if done manually. However, using our fast prototyping tool, this is an effortless process. The result of our analysis will guide developers on the right path towards efficient code optimization. Preliminary results show that some optimizations recommended for older CUDA architectures may not be useful for the newer ones.",
        "title": "3357"
    },
    {
        "abs": "In the digitized world, there is an increased demand for high-speed Time to Digital converters (TDC) in many fields like Nuclear physics, Time of Flight measurements (TOF), Space sciences, medical diagnosis, and imaging. This paper majorly focuses on establishing a Field Programmable Gate Array (FPGA) based Area Efficient and high-performance TDC architectures using KINTEX-7(28 nm) FPGA. Here, we have proposed an area-efficient shift register-based TDC whose output is a thermometer code. Later, this code is passed to an encoder to produce the relative binary code as an output. We have also proposed two encoding techniques, namely Multiplexer (MUX) based encoder with fourth-order bubble error correction and Scalable encoder to generate the output binary code. Our test results using Vivado Design Suite Software have shown that single channel TDC consumes less logical resources with decreased critical path delay and it will be suitable for multichannel architectures. Kintex-7 FPGA has a maximum clock frequency of 450 MHZ, with which we can achieve a resolution of 2.2 ns. To obtain the picoseconds resolution, we can use our TDC in multiple stages, connected in a ring configuration, which can provide fine interpolation and wider dynamic range. In this paper, finally we implemented all the encoders in 180 nm CMOS technologies to estimate the chip layout area and power.",
        "title": "3490"
    },
    {
        "abs": "The mechanical structure and control system architecture for pneumomechanical single-axis modular units are described. The units were designed as subsystems to allow the construction of robots with kinematics suitable for a wide range of workhandling tasks. The controls also demonstrate modularity with regard to both software and hardware. They have been designed to reduce the systems engineering required when user-defined manipulators are being constructed and to provide a user-friendly interface for operators.",
        "title": "3505"
    },
    {
        "abs": "Long-range dependence is a property of stochastic processes that has an important impact on network performance, especially on the buffer usage in routers. We analyze the presence of long-range dependence in on-chip processor traffic and we study the impact of long-range dependence on networks-on-chip. long-range dependence in communication traces of processor ip s at the cycle-accurate level . We also study the impact of long-range dependence on a real network-on-chip using the SocLib simulation environment and traffic generators of our own. Our experiments show that long-range dependence is not an ubiquitous property of on-chip processor traffic and that its impact on the network-on-chip is highly correlated with the low level communication protocol used.",
        "title": "3567"
    },
    {
        "abs": "In this paper, an on-chip interconnection scheme called Heterogeneous IP Block Interconnection (HIBI) is presented. HIBI offers a scaleable and easy to use architecture for system-on-a-chip designs. Its most distinguishing feature is the lack of a central arbiter and specialized arbitration signals. The arbitration is distributed among the connected agents that are made aware of each other's communication requirements. This enables data transmissions with very low latencies and also minimizes the amount of needed signal lines. These features make the scheme a good fit to continuous-media systems transmitting large amounts of streaming data. With the presented interconnection scheme, bus efficiencies greater than 90% have been achieved in several test cases. With the streaming burst transfers and time slot based accesses of HIBI, throughputs of over one data transmission per clock cycle are possible.",
        "title": "3632"
    },
    {
        "abs": "Helmet Mounted Sight and Display systems are now in service in military aircraft. Although these systems share some common features with ground based `virtual reality' systems, the military cockpit environment and the application they are used for, dictate a different approach in the design, development and qualification for military use. The primary function of the Aircrew helmet is to protect the pilot. The advent of helmet mounted displays places additional constraints on the helmet which is now an important element of the cockpit displays system, providing weapon aiming, and other information to the pilot. The design of HMDs for the military cockpit environment is therefore a demanding task if the operational benefits are to be realised without affecting pilot safety. Many designs now use the helmet visor as the display surface rather than eyepieces in front of the pilots eyes. This has resulted in a family of visor projected displays which cater for a range of requirements from simple day helmet mounted sights to wide field of view, 24 h displays. This paper provides an overview of the visor projected HMD design and technology, discusses the building blocks within a typical HMD system and describes practical implementation for several different applications.",
        "title": "3655"
    },
    {
        "abs": "The merits of microprocessor development on a host mini-computer system are described, using crossassemblers and crosscompilers. Aspects of cost are developed particularly and a crossdevelopment system is proposed as being far lower in cost than stand-alone microprocessor development systems, providing that there is already a computer available. For multiple users, crossdevelopment systems may be more economical than individual systems, even if the host computer is purchased specifically for that purpose. The second section describes the author's development system. A PDP-11/60 is used for writing and generating object programs for the MC6800. A commercial crossassembler, a crossassembler written by the author and a crosscompiler written by the author are used. Construction details for the system are provided and examples of typical development sessions are given.",
        "title": "3710"
    },
    {
        "abs": "In recent years, Phase Shift Keying (PSK) has been often used for digital modulation of satellite link because of its good noise resistance. The PSK demodulation requires complex hardware which becomes a constraint for some applications. In this paper a new scheme is proposed for the demodulation. When compared to conventional schemes such as costas loop and square loop, the proposed scheme uses a very simple technique and requires very less hardware for the realization. But the scheme is more suitable for low noise environment applications such as the one discussed in the paper. Performance analysis of the proposed scheme is also briefly presented here.",
        "title": "3739"
    },
    {
        "abs": "A DS-link is a high-speed point-to-point serial interconnect, originally developed for interprocessor communication, which can also be applied in communication systems. Electrical (twisted pair cable) and fibre-optic technologies for link signal transmission have been studied and their performance in terms of link speed and length has been measured. In order to extend the reach of DS links, a fibre-optic interface employing a new line encoding scheme has been designed and characterized. The DS-link interconnect is specified in the IEEE standard 1355.",
        "title": "3745"
    },
    {
        "abs": "Industrial process control requires acquisition of data, calculation of the appropriate control action and output of a signal to effect that control action. The advent of microcomputers has resulted in a trend towards distributed, rather than centralized, control. This trend is in part due to the development of interfaces for industrial input and output. After, an introduction to the requirements for data acquisition and control, the development of microcomputer systems is traced, illustrated by consideration of particular units.",
        "title": "3905"
    },
    {
        "abs": "This paper presents an approach for incorporating the effect of various logic synthesis options and logic level implementations into the custom instruction (CI) selection for extensible processors. This effect translates into the availability of a piecewise continuous spectrum of delay versus area choices for each CI, which in turn influences the selection of the CI set that maximizes the speedup per area cost (SPA) metric. The effectiveness of the proposed approach is evaluated by applying it to several benchmarks and comparing the results with those of a conventional technique. We also apply the methodology to the existing serialization algorithms aimed at relaxing register file constraints in multi-cycle custom instruction design. The comparison shows considerable improvements in the speedup per area compared to the custom instruction selection algorithms under the same area-budget constraint.",
        "title": "4027"
    },
    {
        "abs": "Networks−on−chip (NoC) are an alternative to alleviate the problems of legacy interconnect fabrics. However, many emerging technology NoC are developed and are now seen as their potential substitutes. In this context, this work introduces how the NoC industry is involved in the NoC technology design−trends and promotion. Secondly, an extensive discussion on the outstanding research problems and challenges for conventional and emerging technology NoC is developed. The related security concerns are particularly investigated.",
        "title": "4083"
    },
    {
        "abs": "Interconnection architectures for hierarchical monitoring communication in parallel System-on-Chip (SoC) platforms are explored. Hierarchical agent monitoring design paradigm is an efficient and scalable approach for the design of parallel embedded systems. Between distributed agents on different levels, monitoring communication is required to exchange information, which forms a prioritized traffic class over data traffic. The paper explains the common monitoring operations in SoCs, and categorizes them into different types of functionality and various granularities. Requirements for on-chip interconnections to support the monitoring communication are outlined. Baseline architecture with best-effort service, time division multiple access (TDMA) and two types of physically separate interconnections are discussed and compared, both theoretically and quantitatively on a Network-on-Chip (NoC)-based platform. The simulation uses power estimation of 65 nm technology and NoC microbenchmarks as traffic traces. The evaluation points out the benefits and issues of each interconnection alternative. In particular, hierarchical monitoring networks are the most suitable alternative, which decouple the monitoring communication from data traffic, provide the highest energy efficiency with simple switching, and enable flexible reconfiguration to tradeoff power and performance.",
        "title": "4092"
    },
    {
        "abs": "In comparing two signed operands, it may be shown theoretically that, if operand 1 is greater than operand 2, then flag bit X5 is reset; otherwise X5 is set. In this paper, JX5 and JNX5 instructions are used for checking the set and reset conditions, respectively, of X5 in proper branching within a program. The above criteria are verified experimentally using the SDK-85 design kit.",
        "title": "4130"
    },
    {
        "abs": "Wireless Sensor Network (WSN) requires Signal Processing and control applications in order to use technology effectively. Digital Signal Processing presents a wide range of applications in various fields with the exception of WSN due to the power constraint factor. The Spectrum holes can be effectively utilized by means of spectrum sensing technique in WSN. Multiply and Accumulate unit (MAC) is the computation Kernel in DSP Systems which in turn computes the overall speed of the system. Also, MAC is the major power consumption block due to its complex operation. Developing low power and high speed MAC and utilizing it in spectrum sensing for WSN is a challenging task. In this research, Low Latency Column Bit Compressed (LLCBC) MAC architecture is used for spectrum sensing by examining the hardware complexities. The Architecture is synthesized with 90 nm standard CMOS library using cadence SoC encounter.",
        "title": "4145"
    },
    {
        "abs": "According to the ITU-T, the quality of a multimedia service is defined by a set of user-related parameters: delay, delay variation, and information loss. To provide multimedia applications with end-to-end QoS guarantees, an efficient resource reservation and management strategy has to be adopted. This paper presents a schema for satisfying multimedia QoS parameters over a real-time operating system, which adopts the rate monotonic as a scheduling algorithm. Such a schema is implemented in a real-time based architecture for QoS multimedia provisioning. This architecture allows to define classes of services with different quality attributes concerning the multimedia data delivery.",
        "title": "4190"
    },
    {
        "abs": "A new generation of software-based protection systems for high-performance trains has been designed during the last 10 years. The first application is now in operation on the Paris heavy-rail RER, a line with a peak capacity of about 800 000 passengers per day. In this project, the new protection system performs a full overlay of all train protection functions, with the ability to reduce the headway between trains while permanently monitoring train speed. The software is wholly written in modula-2 and represents 20k source lines for the application. This software is safety critical and required (including the development of specific software tools) 1000 man-months for specification and implementation. The paper illustrates the use of modula-2 for the development of safety-critical applications.",
        "title": "4220"
    },
    {
        "abs": "Developing a safety critical real-time application raises high challenge: “failure is not an option”. The code has to be readable, reliable and efficient. For doing so, VERILOG has developed an environment based on formal approach. This is the only way to be in a position to prove that the code is doing what it is supposed to do, always.",
        "title": "4230"
    },
    {
        "abs": "A range of commercially available processors is investigated to evaluate their potential in image processing systems. The examination covers a selection of the more important instructions in use in image processing algorithms and a selection of the algorithms themselves as a simple benchmark. It also includes a survey of multiprocessor systems implemented with some of these processors. It is concluded that digital signal processors (DSPs) are better suited to embedded systems but the transputer may offer advantages in a research environment, of general usefulness for supercomputing power.",
        "title": "4244"
    },
    {
        "abs": "The paper describes a form of content addressable memory based on a dynamically configured state machine. This device has particular application to serial data systems, such as local area networks. The system described provides a cost-effective solution to the problem of address recognition in packet based network technologies such as Ethernet, and has applications in network bridges, routers and ATM switching nodes. A particular feature is that address matching can be performed on the serial data stream with no additional time overhead.",
        "title": "4250"
    },
    {
        "abs": "The design and application of a DSP-based Parallel Image Processing system is presented. A scaleable system based around a TMS320C40 DSP Master–Slave architecture is shown to be a suitable vehicle for industrial inspection problems. Custom vision bus and parallel communication channels are used to pass data between local and shared memory in the image processing system. The target application of SMT solder bond inspection required the fast processing of 2-D FFTs. Analysis of the system's suitability to this task is presented followed by actual results from a three-processor system. The target inspection rate of 100 SMT solder joints per second can be met.",
        "title": "4292"
    },
    {
        "abs": "Railway signalling systems are based on fault-tolerant and fail-safe techniques to provide high reliability and safety. Transputers offer multiple communication links and provide a high degree of parallelism and are being used in safety-critical and fault-tolerant applications. We propose a fault-tolerant and fail-safe node using transputers for a local area network with dual ring topology to be used in distributed railway signalling systems. The proposed scheme uses a new technique of leadership based on rotation amongst the transputers of the node. Aspects of safe shutdown, recovery, reconfiguration and clock synchronization of the fault-tolerant and fail-safe node are addressed. Formal specifications of the fail-safe node and the simulation of the proposed technique are also given.",
        "title": "4297"
    },
    {
        "abs": "This paper provides an overview of the concepts of ‘risk’ and ‘safety integrity’ in relation to safety-related electrical/electronic/programmable electronic systems. The paper is an abridged version of Annex A of the emerging International Electrotechnical Commission (IEC) Standard: ‘Functional safety of electrical/electronic/programmable electronic systems’. Although based on Annex A, the authors have deviated in a few instances from its strict wording in order to more properly represent their own views. Where this occurs, a note in the text has been added to alert the reader to the deviation. The concepts of risk, including tolerable risk, safety integrity, safety-related systems, system and software integrity levels, are discussed.",
        "title": "4344"
    },
    {
        "abs": "Many applications in mobile and embedded systems like signal processing, machine learning, kinematics, dynamics, and control depend on computationally expensive matrix operations. However, such systems underlie tight constraints regarding power consumption and physical space, which prohibits the usage of powerful multicore systems. In this paper, we propose a novel scalable and power-efficient architecture for matrix algebra in FPGA-based Systems-on-Chip. The architecture is based on a linear systolic array and has been developed with a focus on flexibility in order to be adapted to different applications. We evaluate the performance, resource utilization and power consumption of different configurations and show that it provides significant speed-ups over a mobile processor and is significantly more power efficient than a standard PC.",
        "title": "4435"
    },
    {
        "abs": "A microsequencer lies at the heart of any microprogrammed control unit, being responsible for stepping through the microinstructions of a microprogram. Until recently, microprogrammed control units were implemented either by large quantities of TTL logic and a ROM or by more modest amounts of TTL logic, a ROM and a few bit-slice devices. The Altera EPS444/448 standalone microsequencer contains all the logic needed to implement a microsequencer on a single chip. Inside the device there is a microprogram sequencer, branch control logic, a microprogram return address stack, a microprogram memory and a user programmable microcode EPROM that holds the microprogram to be executed. This single device, when programmed, provides the functionality of a custom IC without the time or expense of designing one. The EPS444 and EPS448 have relatively short microinstruction outputs (12 bit and 16 bit respectively) that are intended for high-performance controller designs, but they may also be used for simple microcoded control units. The EPS444/448 offers an ideal solution to the design of a synchronous state machine. Synchronous state machines are attractive alternatives to microprocessors where simplicity, speed, performance and economics are more important than the versatility of microprocessors. This application note, taken from the Altera EPS444/448 data sheet, describes the operation of this sequencer and its instruction set.",
        "title": "4457"
    },
    {
        "abs": "There are two main ways in which aids are designed to allow those with impaired sight access to information technology. Adaptations of existing equipment for sighted users can be made. An alternative approach is to replace sight-oriented systems with systems designed specifically for sound and touch. Currently used techniques for this purpose are described.",
        "title": "4467"
    },
    {
        "abs": "Optical feedback or self-mixing interferometry technique has been widely used for sensing vibration, displacement, velocity, distance and flow applications. Such applications require an accurate and consistent output for real-time target measurements. Furthermore, array based sensing, as opposed to point sensing, is being increasingly pursued which requires multiple-input, parallel computing platform. In this article, we have proposed and developed a real-time interferometric sensor applications based multi-core system architecture and programming toolkit. To show that our system is useful in a variety of situations, we applied three algorithms of varying complexity and accuracy for displacement and vibration measurement. In order to prove the efficiency of proposed sensor processing system, we compared it performance and power consumption with a state of the art NXP LPCX54102 sensor processing and motion system architecture. When compared with the baseline multi-core system, the results show that our system improves the system performance upto 7.55 times, draws 15.6% less dynamic power and consumes 8.9 times less energy.",
        "title": "4497"
    },
    {
        "abs": "The introduction of SRAM-based field programmable gate arrays (FPGAs) has opened up a new dimension to parallel computing architectures. This paper describes an alternative approach to parallel computing — reconfigurable or virtual parallel processing (VPP). Rather than mapping an application onto a given parallel machine, the WP approach synthesizes the appropriate type and number of processing elements, as well as the interconnection topology, that is optimal for the application. For each application, configuration data is downloaded to the machine that personalizes the hardware for the task at hand. The paper provides a brief description of the authors reconfigurable computer, Archimedes . The benefits of the VPP approach are highlighted by two example applications — the 2-D FFT and a narrow-band digital filter. A novel parallel implementation of a polynomial transform based 2-D transform is described and compared with results for distributed memory parallel machines that have been reported in the literature. The comparison highlights the computational advantage provided by reconfigurable computing. The digital filter implementation employs sigma–delta modulation encoding to reduce the arithmetic workload. The application of this technology to a communications receiver is explained.",
        "title": "4601"
    },
    {
        "abs": "In order to fulfill the ever-increasing demand for high-speed and high-bandwidth, wireless-based MCSoC is presented based on a NoC communication infrastructure. Inspiring the separation between the communication and the computation demands as well as providing the flexible topology configurations, makes wireless-based NoC a promising future MCSoC architecture. However, congestion occurrence in wireless routers reduces the benefit of high-speed wireless links and significantly increases the network latency. Therefore, in this paper, a congestion-aware platform, named CAP-W, is introduced for wireless-based NoC in order to reduce congestion in the network and especially over wireless routers. The triple-layer platform of CAP-W is composed of mapping, migration, and routing layers. In order to minimize the congestion probability, the mapping layer is responsible for selecting the suitable free core as the first candidate, finding the suitable first task to be mapped onto the selected core, and allocating other tasks with respect to contiguity. Considering dynamic variation of application behaviors, the migration layer modifies the primary task mapping to improve congestion situation. Furthermore, the routing layer balances utilization of wired and wireless networks by separating short-distance and long-distance communications. Experimental results show meaningful gain in congestion control of wireless-based NoC compared to state-of-the-art works.",
        "title": "4682"
    },
    {
        "abs": "The 8-bit microprocessor has not been rendered obsolete by newer and more powerful 16- and 32-bit microprocessors. Many applications are more than adequately served by 8-bit microprocessors, with their lower-cost address and data buses. However, there is a ‘grey area’ in which 8-bit chips are not sufficient but 16-bit chips represent an uneconomic ‘overkill’. A significant limitation of 8-bit microprocessors with 16-bit address buses is their inability to access more than 64 kbyte of data. This application shows how the SN54/74LS610-3 series of memory mapping units enables an 8-bit microprocessor to access memories much larger than 64 kbyte without significantly increasing the chip count of the system. Note that a memory mapping unit cannot simply be engineered into a microprocessor system without adding the appropriate software, as the 8-bit microprocessor can still address only 64 kbyte of logical address space at any instant. Software is usually included in the operating system to map 64 kbyte of the physical memory onto the logical address space.",
        "title": "4704"
    },
    {
        "abs": "The paper describes a programmable digital audio time delay system which uses the 6809 microprocessor to store and retrieve 12-bit samples in RAM. A/D and D/A conversions were accomplished by a 12-bit analogue I/O board for the Apple II microcomputer. A 6809 machine code time delay program, which was run on a 6809 coprocessor board for the Apple II, was able to attain an audio frequency response of over 14 kHz.",
        "title": "4761"
    },
    {
        "abs": "The unusual class of multiprocessors which employ a global memory and private process access environments presents intriguing possibilities for machines which retain high efficiency even when process interactions are relatively frequent. Because all writable data is private to a process, such machines are free from the cache consistency concerns of shared-variable architectures, while, in contrast to distributed memory machines, a global memory is available for code and data which is physically (not logically) shared. An example of this class of multiprocessors, the virtual port memory (VPM) architecture, is apparently scalable to at least 256 processors using only a simple bus-based interconnection network. A prototype VPM machine is discussed, including a comparison of its scalability with that of the Butterfly multiprocessor. VRAM global memory and second-generation RISC processors are discussed as enhancements for this architecture.",
        "title": "4788"
    },
    {
        "abs": "This article presents an integrated self-aware computing model in a Heterogeneous Multicore Architecture (HMA) to mitigate the power dissipation of an Orthogonal Frequency-Division Multiplexing (OFDM) receiver. The proposed platform consists of template-based Coarse-Grained Reconfigurable Array (CGRA) devices connected through a Network-on-Chip (NoC) around a few Reduced Instruction-Set Computing (RISC) cores. The self-aware computing model exploits Feedback Control System (FCS) which constantly monitors the execution-time of each core and dynamically scales the operating frequency of each node of the NoC depending on the worst execution-time. Therefore, the performance of the overall system is equalized towards a desired level besides mitigating the power dissipation. Measurement results obtained from Field-Programmable Gate Array (FPGA) synthesis show up to 20.2% dynamic power dissipation and 16.8% total power dissipation savings. Since FCS technique can be employed for scaling the frequency and the voltage and on the other hand, voltage supply cannot be scaled on the FPGA-based prototyped platform, the implementation is also estimated in 28nm Ultra-Thin Body and Buried oxide (UTBB) Fully-Depleted Silicon-On-Insulator (FD-SOI) Application-Specific Integrated Circuit (ASIC) technology to scale voltage in addition to frequency and get more benefits in terms of dynamic power dissipation reduction. Subsequent to synthesizing the whole platform on ASIC and scaling the voltage and frequency simultaneously as a Dynamic Voltage and Frequency Scaling (DVFS) method, significant dynamic power dissipation savings by 5.97X against Dynamic Frequency Scaling (DFS) method were obtained.",
        "title": "4825"
    },
    {
        "abs": "Cyber-physical systems (CPS) are expected to continuously monitor the physical components to autonomously calculate appropriate runtime reactions to deal with the uncertain environmental conditions. Self-adaptation, as a promising concept to fulfill a set of provable rules, majorly needs runtime quantitative verification (RQV). Taking a few probabilistic variables into account to represent the uncertainties, the system configuration will be extremely large. Thus, efficient approaches are needed to reduce the model state-space, preferably with certain bounds on the approximation error. In this paper, we propose an approximation framework to efficiently approximate the entire model of a self-adaptive system. We split up the large model into strongly-connected components (SCCs), apply the approximation algorithm separately on each SCC, and integrate the result of each part using a centralized algorithm. Due to a number of changes in probabilistic variables, it is not possible to use static models. Addressing this issue, we have deployed parametric Markov decision process. In order to apply approximation on the model, the notion of ε-approximate probabilistic bisimulation is utilized that introduces the approximation level ε. We show that our approximation framework offers a certain error bound on each level of approximation. Then, we denote that the approximation framework is appropriate to be applied in decision-making process of self-adaptive systems where the models are relatively large. The results reveal that we can achieve up to 50% size reduction in the approximate model while maintaining the accuracy about 95%. In addition, we discuss about the trade-off between efficiency and accuracy of our approximation framework.",
        "title": "4860"
    },
    {
        "abs": "A cost-effective design of an ultrasonic interface to a graphics tablet is accomplished by means of PIC microcontrollers. This paper analyses the design and implementation from requirements capture through high-level design to circuitry. Ultrasonic transducers enable direct placement of the tablet mechanism above an LCD so that the image is viewed as it is created. Results show that the prototype tablet has up to 0.8 mm resolution.",
        "title": "4919"
    },
    {
        "abs": "This paper describes a novel codesign tool for rapid prototyping FPGA's in hybrid systems (HW+SW+analog+RF+electromechanical+user interface). This tool uses Simulink™ from The Mathworks as a high level description language, as well as a flexible simulation environment. After functional simulation and parameter tuning, the user partitions the system into digital HW, SW and analog HW. A performance/cost analysis of the partitioned system can then be made and architectural parameters can be optimized. After the simulation, the proposed codesign tool automatically compiles the digital HW (respectively, SW) subsystem for any user-defined FPGA (respectively, DSP/PC/microcontroller/softCore), in a rather transparent way. Analog subsystems can only be simulated but not yet compiled for analog FPGA's. The paper also shows the many advantages of the proposed codesign flow, among which, a short time-to-market, an improved flexibility and reusability, a more reliable design, a better final cost/performance ratio. The tool simulates and compiles all integer, fixed-point and floating-point data formats and all scalar, vector and matrix data which are supported by Simulink, both for HW and SW, therefore it is suited to virtually all Signal Processing algorithms. A few practical cases are described at the end of this paper.",
        "title": "4924"
    },
    {
        "abs": "The selection method of resource adjustment strategy is a key step of multi-VM (Virtual Machine) resource adjustment in a single physical machine (PM). The traditional genetic algorithm (GA) do not evaluate and filter the initial population, and not make full use of decision of historical data as to increase the optimal solution time either. Based on the conventional research, this paper establishes the relation model between the service performance and the amount of resources consumption (P-R model), which is used to evaluate and filter the initial population, and presents design method of the revenue function and the termination conditions. It also presents the way which turns the empirical data into historical decision and uses it in the next cycle. The experiment results indicate the method is able to maintain high resource utilization and meets the demands of response time.",
        "title": "4934"
    },
    {
        "abs": "In modern signal processing, there are increasing demands for large-volume and high-speed computations. At the same time, VLSI has had a noticeable effect on signal processing by offering almost unlimited computing hardware at low cost. These factors combined have affected markedly the rapid upgrading of current signal processors. We review the influence of the basic VLSI device technology and layout design on VLSI processor architectures. The array processors in which we take special interest are those for the common primitives needed in signal processing algorithms such as convolution, fast Fourier transforms and matrix operations. Regarding VLSI devices, special emphasis is placed on alleviating the burden of global interconnection and global synchronization. For cost-effective design, programmable processor modules are adopted. On the basis of these guidelines, we establish the algorithmic and architectural footing for the evolution of the design of VLSI array processors. We note that the systolic and wavefront arrays elegantly avoid global interconnection by effectively managing local data movements. Moreover, the asynchronous data-driven nature of the wavefront array offers a natural solution to get around the global synchronization problem. The wavefront notion lends itself to a wavefront language (matrix dataflow language (MDFL)) which simplifies the description of parallel algorithms.",
        "title": "4952"
    },
    {
        "abs": "Flash memory offers the same non-volatile storage as EPROM but also provides in-system write capability. Using Intel's 28F001BX for BIOS storage, code updates may be carried out quickly in the factory during test and debug, while allowing cost-effective field updates to end users via floppy discs or modem BBS. This application note describes various methods of implementing a flash memory BIOS using the 28F001BX. Design targets are both laptop and desktop systems. The primary emphasis is on application of flash memory for BIOS and ROM executable software applications.",
        "title": "5111"
    },
    {
        "abs": "The logic cell array (LCA) is a programmable logic circuit that can require a high level of design capability from digital systems designers intent on using it. System support is available in the form of CAE software, much of it PC based, which makes the task of logic design for the LCA device a relatively straightforward process. The OrCAD schematic capture program and interface conversion software allows the designer to use schematic entry on a PC-AT. This makes logic design within the LCA less product specific because the designer does not need to know much detail of the LCA architecture. This is especially useful to those designers who do not have a background in designing with programmable logic devices and are unfamiliar with software packages that support Boolean and state machine design entry. Also, the schematic capture package provides an output that can be used for documentation, giving a range of standard symbols for gates and register logic that varies in degrees of complexity up to counter and shift register circuits. The application note describes the design of a video controller circuit in the M2064 LCA device using the OrCAD SDT/111 schematic capture package, support libraries and interface software.",
        "title": "5197"
    },
    {
        "abs": "The primary goal of first generation RISC processors was to achieve an average execution rate of one instruction per clock cycle (CPI or clocks per instruction). Once this goal is achieved, the same architecture can be implemented in technologies that offer very high clock rate and achieve high performance. The paper discusses two basic aspects of implementing an architecture in ECL, using a case study of MIPS R3000 and R6000 processors. The first concerns the architectural elements that make R3000 easy to implement in ECL, and the second looks at how to resolve the problems raised by a wide gap between processor clock speed and main memory speed.",
        "title": "5217"
    },
    {
        "abs": "The goal of designing a special-purpose computer is to offer a highly efficient computer-aided environment for circuit simulation using hardware description languages. The suggestion is to have a machine that supports bitstring operations. A bitstring comprises a sequence of bits and must have at least one bit, and is found to be the predominant operation in hardware description languages, yet not a common operation in Pascal-like languages. A special-purpose machine is proposed and its hardware architecture reflects the characteristics of a hardware description language. The main part of this paper deals with the design concepts of the hardware machine, which is derived directly from the run-time requirements of hardware description programs. We start by examining examples of these programs and their operators, then identifying areas which can be improved through the use of hardware. Next, we present the architecture of the machine and bitstring operators that are supported. The instruction format is also presented and some comparisons are made to the reduced instruction set computers. Finally, we study the related works that have been conducted elsewhere.",
        "title": "5312"
    },
    {
        "abs": "Data processing on a continuously growing volume of data and the increasing power restrictions have become an ubiquitous challenge in our world today. Besides parallel computing, a promising approach to improve the energy efficiency of current systems is to integrate specialized hardware. This paper presents two application-specific architectures to accelerate basic database operators frequently used in modern database systems: an extended instruction set based on a given Cadence Tensilica processor (ASIP) and a comparable application-specific integrated circuit (ASIC). The ASIP is implemented in a system-on-chip and manufactured in a 28 nm CMOS technology to realize measurements of performance and power consumption. Furthermore, the comparison with the ASIC blocks allows to quantify the results with the ASIP approach in terms of throughput, area, and energy efficiency as well as to discuss the capabilities and limitations when accelerating selected database operators.",
        "title": "5376"
    },
    {
        "abs": "The problem of lifetime maximization of PCM has been well studied. The arrival of non-volatile memory devices has replaced the traditional DRAM. Still the DRAM has many limitations on endurance and high power write operations. Similarly, number of designs has been discussed earlier to maximize the lifetime of PCM by catching the main memory at available DRAM. Still they could not achieve the performance on power consumption reduction and increasing memory utilization. To improve the performance in power consumption reduction and lifetime maximization, and categorical model is presented in this paper. The proposed method categorizes the processes according to their memory access activity. The categorized process has been allocated to respective part of hybrid memory which encourages maximum read and minimum write in PCM. The proposed method increases the lifetime of PCM than other methods.",
        "title": "5451"
    },
    {
        "abs": "With the growth of the embedded devices consumer market, power efficient hardware is needed. Therefore power-aware architectural exploration is one of the most crucial design steps. For such an exploration procedure, it is important to accurately model the power consumption of all main components of the embedded system. Registers and register files are one of the highest power consumers of any programmable processor, but there is a lack of accurate and publicly available models. This paper provides such a power model for standard cell based register files for 130 and 90 nm technologies. The proposed model provides dynamic power, leakage power, area and timing information for register files in terms of key parameters like width, depth, activity, ports, and capacitive loading. It is shown that current models capture neither correct absolute nor relative trends present in register files. It is shown that some key, but often neglected parameters like switching activity, load have a larger influence in some particular sizes of the register files than others. Therefore, using the Empire model, accurate architectural exploration is possible.",
        "title": "5461"
    },
    {
        "abs": "Correlation is a mathematical technique normally found in texts on analogue signal processing, control theory or pattern recognition. However, correlation is a technique that can be applied to digital signals just as well as to analogue signals. Indeed, it is now possible to obtain ready-made digital correlators. What then is correlation? In the simplest of terms, correlation involves ‘sliding’ two sequences of signals against each other and then attempting to determine how closely the sequences resemble each other as they move with respect to each other. This application note provides an overview of digital correlation and demonstrates how correlation techniques can be applied to the detection of particular patterns in a data stream, to the synchronization of detectors in demodulation processes and to the determination of the time delay between two digital sequences.",
        "title": "5528"
    },
    {
        "abs": "Mil-Std-1553B is a well established serial, digital, multiplex data bus standard used in military realtime system applications. The standard is now finding applications in the commercial and industrial sector because of its inherent flexibility and durability in harsh environments. The history and main features of the standard are presented, together with an indication of why it was developed and how it is applied in systems design. A brief comparison is made with three commercial counterparts, and possible future developments are discussed.",
        "title": "5609"
    },
    {
        "abs": "HARP (the Hatfield RISC processor) is a reduced instruction set processor being developed at Hatfield Polytechnic, UK. The major aim of the HARP project is to develop a RISC processor capable of a sustained instruction execution rate in excess of one instruction per cycle. Investigations to date support the hypothesis that this goal can be achieved by the development of an integrated processor-compiler pair in which the processor is specifically designed to support low-level parallelism identified by the compiler. This paper describes the HARP architectural model and discusses those features which support parallel instruction execution. Parallelism is provided in the hardware by multiple instruction pipelines which execute independent RISC-like instructions simultaneously. The principal techniques employed to exploit the available parallelism are efficient pipelining, register bypassing, optional register writeback and conditional execution of instructions. Examples are given which illustrate the effectiveness of these techniques in increasing the performance of HARP.",
        "title": "5629"
    },
    {
        "abs": "A study has been carried out to specify the undefined flag bits, D3 and D5, of the Z80 microprocessor. The set-reset conditions for D3 and D5 were observed experimentally with 50 different data sets under various instructions. Conclusions are drawn from the results of these tests about the set-reset conditions of the unspecified flag bits.",
        "title": "5666"
    },
    {
        "abs": "This paper proposes techniques for face detection using Haar-like features as weak classifiers and gives the implementation details for an FPGA development board. We analyze and discuss the relation between the system computation cost and selection of the image scaling factor. Based on the empirical results of our previous work, we give a new method to select the stop threshold for the image reduction process, which reduces the total computation by half. We present and implement an improved integral image pipeline calculation design. We also provide a color image output mode to let our system enjoy more human-oriented design. Test results show that the system achieves real-time face detection speed (100 fps ) and a high face detection rate (87.2%) for an SVGA (600 × 800) video source. The low power consumption (3.5 W) is another advantage over previous work.",
        "title": "5687"
    },
    {
        "abs": "This paper aims for accurate diagnosis of arrhythmia beats in real time to enhance the health care service for cardiovascular diseases. The proposed methodology for the diagnosis involves the integration of the R-peak detection algorithm, FFT (fast fourier transform) based discrete wavelet transform for feature extraction and feedforward based Neural Network Architecture to classify generic cardiac beat classes into eight categories namely Right Bundled Block, Left Bundled Block, Preventricular Contraction (PVC), Atrial Premature Contraction (APC), Ventricular Flutter wave (VF), Paced Beat, Ventricular Escape (VE) and Normal beat. The paper contributes the development, prototyping and analysis of proposed methodology on ARM (Advanced RISC Machine) based SoC (System-on-Chip) in laboratory setup. This system is validated by generating real-time ECG signals using MIT-BIH database while the output of the system is monitored on the displaying device. The performance analysis of the proposed methodology implemented on the microcontroller based system is computed by performing the experiment which achieves a high overall accuracy of 97.4% with average sensitivity ([formula omitted]) of 97.57%, specificity ([formula omitted]) of 99.59% and positive predictivity ([formula omitted]) of 97.93%. The system provides an assistive diagnostic solution to the users to lead a healthy lifestyle. Moreover, the ARM-based system can be fabricated into a handheld device for reliable automatic monitoring of the condition of heart by patients.",
        "title": "5690"
    },
    {
        "abs": "Promoting energy efficiency to a first class system design goal is an important research challenge. Although more energy-efficient hardware can be designed, it is software that controls the hardware; for a given system the potential for energy savings is likely to be much greater at the higher levels of abstraction in the system stack. Thus the greatest savings are expected from energy-aware software development, which is the vision of the EU ENTRA project. This article presents the concept of energy transparency as a foundation for energy-aware software development. We show how energy modelling of hardware is combined with static analysis to allow the programmer to understand the energy consumption of a program without executing it, thus enabling exploration of the design space taking energy into consideration. The paper concludes by summarising the current and future challenges identified in the ENTRA project.",
        "title": "5790"
    },
    {
        "abs": "Estimation of circuit testability is an important issue when evaluating the circuit design. A testability measure indicates how easy or difficult it would be to generate tests for the circuit. STAFAN (Statistical Fault Analysis) is a well known gate-level testability analysis program which predicts the fault coverage of a digital circuit under the stuck-at fault model, without actually performing fault simulation. STAFAN offers speed advantage over other testability analysis programs such as SCOAP; further, it explicitly predicts the fault coverage for a given test set, unlike other testability measures which are harder to interpret. We show how a STAFAN-like testability analysis program can be constructed for circuits built out of register-level modules such as adders, multipliers, multiplexers, and busses. Our tool, which we call FSTAFAN, is useful in a testability-driven high-level synthesis environment. We have implemented FSTAFAN on a Sun/SPARC workstation and describe its performance on some register-level circuits.",
        "title": "5859"
    },
    {
        "abs": "Demand Side Management (DSM) is one of the most important parts of future smart power grid. With the rise in global energy awareness, smart grids enhance the potency and peak levelling of power systems. DSM is the controlling scheme in such grids and it aims to optimize several characteristics of load demand. This smart grids comprises energy storage (battery) and distributed solar photovoltaic generation storage. In this proposed methodology the combination of Glow-worm Swarm Optimization (GSO) and Support Vector Machine (SVM) is used for decision making process in battery storage to reduce the electricity tariff. GSO is a powerful technique to obtain near optimal solution which is used for this load rescheduling problem for a sample test system to minimize the cost of end user. Especially, the electricity expenditures of the end user can be reduced by responding to pricing which changes with different hours of a day.  Then optimized range of the battery's energy storage is extracted from the GSO. Here, the SVM is trained based on the optimized data from the GSO. This combination is used for finding the amount of energy is transferred in/out the battery which aims the minimal electricity bill value. The electricity tariff of the proposed methodology of Average gosc is 2.27 for residential load, while considering it is less when compared to the existing method of 2.3 at the consumed load of 8.2 kWh/day. The proposed GSO-SVM method reduces 11.2% of energy cost which helps decision makers to take best demand-side actions for balancing the stability.",
        "title": "5863"
    },
    {
        "abs": "Memory race recording is a key technology to replay multithreaded programming deterministically. Modern computers supply efficient communication mechanism and memory races occur frequently. So it is significant to develop an efficient memory race recording algorithm with low log growth rate and rapid replay speed. This paper proposes a new efficient point-to-point memory race recording algorithm, called CCTR, which writes a small race log with small hardware state, operates well as the number of cores per system scales, and can replay multithreaded programs at production run speed. CCTR uses a new relative indirect dependency to present each memory race instead of its precise dependency. In this dependency, CCTR need not store any timestamp for each memory block and detects memory races in chunks. Through simulation on 4-core chip multiprocessor (CMP), a good result is achieved which includes smallest log growth rate (∼5 bytes per thousand memory instructions), small hardware state (∼504 bytes per core), low runtime overhead (less than 2%), low bandwidth overhead (∼7%) and good scalability.",
        "title": "5970"
    },
    {
        "abs": "Designs targeted for FPGAs are becoming increasingly large and more complex. The need for I/O often surpasses the number of pads that can be provided at the perimeter of the FPGA chip. As a result, these designs have to be implemented in FPGAs the sizes of which are fixed by the number of I/O pads and not by the logic needed. This results in larger delays and more unused logic. Providing FPGA chips with I/O pads that are spread out across the whole chip area drastically reduces this problem. In this paper, an analytical model is derived to show the impact of area-I/O on FPGA delays. In contrast with the analysis in Ref. [1], we take the effect of the growing FPGA size—due to the I/O limitations—into account. Experimental data is provided to substantiate the theoretical claims.",
        "title": "5976"
    },
    {
        "abs": "This report describes the design of a modular, massive-parallel, neural-network (NN)-based vector quantizer for real-time video coding. The NN is a self-organizing map (SOM) that works only in the training phase for codebook generation, only at the recall phase for real-time image coding, or in both phases for adaptive applications. The neural net can be learned using batch or adaptive training and is controlled by an inside circuit, finite-state machine-based hard controller. The SOM is described in VHDL and implemented on electrically (FPGA) and mask (standard-cell) programmable devices.",
        "title": "5999"
    },
    {
        "abs": "Unmanned ground vehicles need accurate sensors to detect obstacles and map their surroundings. Laser-based distance sensors offers precise results, but 3D off-the-shelf sensors may be too expensive. This paper presents a 3D sensing system using a 2D laser sensor with a rotation system. Point cloud density analyses are presented in order to achieve the optimal rotation speed depending on the vehicle speed, distance to obstacles, etc. The proposed system is able to generate real-time point clouds, detect obstacles and produce maps, with high accuracy and a reasonable price (less than 5, 000 USD).",
        "title": "6007"
    },
    {
        "abs": "Certification of avionics software is an increasingly important subject, since more and more avionics systems in future aircraft will be software equipped. The DO-17813 standard provides guidelines for software certification. Re-use of software is emerging, partly enabled by the integrated modular avionics concept, and imposed by a reduction of life-cycle costs. Re-use, however, requires re-certification or certification of software that was not developed according to DO-17813. The DO-178B standard is specially developed to provide a certification basis for avionics software, without going into details of the software development process. Other standards focus on software engineering aspects. We have used the DO-178B standard as a common basis for comparison with DOD-STD2167A (military), ESA PSS-05-0 (space), and IEC65A(Secretariat)122 (industry). Comparison topics include: &amp;#x02022; • life cycles; &amp;#x02022; • prescribed documentation; &amp;#x02022; • configuration management; &amp;#x02022; • verification and validation; &amp;#x02022; • quality assurance. All standards prescribe the software development process, emphasizing specific aspects in a certain area of interest. The results of our investigation will assist in understanding the rationale behind several standards, and can be used for: &amp;#x02022; • certification according to DO-17813 of software that was developed using another standard; &amp;#x02022; • certification of software using DO-17813, in concert with another standard.",
        "title": "6078"
    },
    {
        "abs": "The Motorola MC68020 is one of the first 32-bit microprocessors. The 68020 is part of the 68000 family, which has a register-based architecture. In the 68020 a number of instructions and addressing modes, and some data types, have been implemented, to increase input and to help in the implementation of modular high-level languages and their associated constructs and data structures. The 68020 is made with the HCMOS process.",
        "title": "6109"
    },
    {
        "abs": "Dynamic programming algorithms are widely used to find the optimal sequence alignment between any two DNA sequences. This manuscript presents a new, flexible and scalable hardware accelerator architecture to speedup the implementation of the frequently used Smith–Waterman algorithm. When integrated with a general purpose processor, the developed accelerator significantly reduces the computation time and memory space requirements of alignment tasks. Such efficiency mainly comes from two innovative techniques that are proposed. First, the usage of the maximum score cell coordinates, gathered during the computation of the alignment scores in the matrix-fill phase, in order to significantly reduce the time and memory requirements of the traceback phase. Second, the exploitation of an additional level of parallelism in order to simultaneously align several query sequences with the same reference sequence, targeting the processing of short-read DNA sequences. The results obtained from the implementation of a complete alignment system based on the new accelerator architecture in a Virtex-4 FPGA showed that the proposed techniques are feasible and the developed accelerator is able to provide speedups as high as 16 for the considered test sequences. Moreover, it was also shown that the proposed approach allows the processing of larger DNA sequences in memory restricted environments.",
        "title": "6147"
    },
    {
        "abs": "We discuss VThreads, a novel VLIW CMP with hardware-assisted shared-memory Thread support. VThreads supports Instruction Level Parallelism via static multiple-issue and Thread Level Parallelism via hardware-assisted POSIX Threads along with extensive customization. It allows the instantiation of tightly-coupled streaming accelerators and supports up to 7-address Multiple-Input, Multiple-Output instruction extensions. VThreads is designed in technology-independent Register-Transfer-Level VHDL and prototyped on 40  nm and 28  nm Field-Programmable gate arrays. It was evaluated against a PThreads-based multiprocessor based on the Sparc-V8 ISA. On a 65  nm ASIC implementation VThreads achieves up to x7.2 performance increase on synthetic benchmarks, x5 on a parallel Mandelbrot implementation, 66% better on a threaded JPEG implementation, 79% better on an edge-detection benchmark and ∼13% improvement on DES compared to the Leon3MP CMP. In the range of 2 to 8 cores, VThreads demonstrates a post-route (statistical) power reduction between 65% and 57% at an area increase of 1.2%–10% for 1–8 cores, compared to a similarly-configured Leon3MP CMP. This combination of micro-architectural features, scalability, extensibility, hardware support for low-latency PThreads, power efficiency and area make the processor an attractive proposition for low-power, deeply-embedded applications requiring minimum OS support.",
        "title": "6173"
    },
    {
        "abs": "High-level programming paradigms are examined to determine their appropriateness for describing systems, which are amenable to automated compilation onto a reconfigurable computing platform. We aim to find a set of language features to act as a basis for future language development which: provide a concise description of the system to be realised; provide clear and intuitive semantics; abstract the underlying technology and are appropriate to the underlying technology. Clearly language design is a subjective process, but we have adopted a systematic approach by assessing the efficacy of existing programming and hardware description languages. We examine the languages Java, VHDL, Standard ML and Circal. Our method also bases the comparison on a set of properties drawn from an archetypal example which we know maps well onto reconfigurable computing platforms. We conclude that none of these established languages has all the properties required for describing reconfigurable computation. This approach leads to insight into the capabilities of existing languages and allows us to determine the essential characteristics of future languages oriented to reconfigurable computing.",
        "title": "6259"
    },
    {
        "abs": "The genetic algorithm is a general purpose optimization metaheuristic for solving complex optimization problems. Because the algorithm usually requires a large number of iterations to evolve a population of solutions to good final solutions, it normally exhibits long execution times, especially if running on low-performance conventional processors. In this work, we present a scalable computing array to parallelize and accelerate the execution of cellular GAs (cGAs). This is a variant of genetic algorithms which can conveniently exploit the coarse-grain parallelism afforded by custom parallel processing. The proposed architecture targets Xilinx FPGAs and was implemented as an auxiliary processor of an embedded soft-core CPU (MicroBlaze). To facilitate the customization for different optimization problems, a high-level synthesis design flow is proposed where the problem-dependent operations are specified in C++ and synthesised to custom hardware, thus demanding of the programmer only minimal knowledge of low-level digital design for FPGAs. To demonstrate the efficiency of the array processor architecture and the effectiveness of the design methodology, the development of a hardware solver for the minimum energy broadcast problem in wireless ad hoc networks is employed as a use case. Implementation results for a Virtex-6 FPGA show significant speedups, especially when comparing to embedded processors used in current FPGA devices.",
        "title": "6283"
    },
    {
        "abs": "A SuperH™ embedded processor core, SH-X2, implemented in a 90-nm CMOS process running at 800 MHz achieved 1440 Dhrystone MIPS, 5.6 GFLOPS, and 73M polygons/s. It has a dual-issue eight-stage pipeline architecture, but maintains the 1.8 MIPS/MHz of the previous seven-stage processor core SH-X. The processor meets the requirements of a wide range of applications, and is suitable for digital appliances aimed at the consumer market, such as cellular phones, digital still/video cameras, and car navigation systems. This paper focuses on the implementation of floating-point units in the SH-X2 and its resulting performance, and considers ways of enhancing this performance in future.",
        "title": "6291"
    },
    {
        "abs": "Programming multiprocessor systems is not a simple task and is often error-prone. In particular, programming a message-passing parallel computer usually demands large amounts of work in development and debugging to avoid deadlock. It is, therefore, essential to have software to support the task of parallel programming, or more desirably to have automatic parallel program generators to greatly reduce the labour. In this paper, we present a novel method of automatically generating Occam programs; or more precisely, we present an automatic FP-to-Occam translator called AFO. Occam is a parallel, message-passing programming language executed on a network of processors called transputers, but is basically a sequential language extended with primitive parallel and communication constructs. In contrast, FP is a functional language that contains no communication details and supports parallel thinking naturally with higher-level parallel constructs. FP has been used to derive many systolic algorithms. Systolic algorithms can be implemented in Occam as soft-systolic algorithms. With this knowledge, we developed AFO to automatically generate Occam programs from FP programs that have corresponding systolic algorithms, in the hope of generating parallel programs without needing to know the details of low-level communications, and thus greatly improving the productivity of parallel programming.",
        "title": "6322"
    },
    {
        "abs": "This paper details an implementation of a supervisor processor to monitor the performance of an adaptive signal processing system. The system is based on the use of the Inmos transputer. The supervisor is part of a threetransputer network which has been programmed as an adaptive smoothing filter. The system employs task-level parallelism with a transputer dedicated to parameter estimation, using the extended least squares algorithm, digital smoothing and system supervision. An example is considered of the adaptive smoother dealing with a random signal which is subject to various distortions. By performing a range of different checks on the adaptive signal processor the supervisor is able to ensure the stable operation of the smoother.",
        "title": "6376"
    },
    {
        "abs": "As multi-core processors continue to scale, more and more multiple distributed applications with precedence-constrained tasks simultaneously and widely exist in multi-functional embedded systems. Scheduling multiple DAGs-based applications on heterogeneous multi-core processors faces conflicting high-performance and real-time requirements. This study presents a multiple DAGs-based applications scheduling optimization with respect to high performance and timing constraint. We first present the fairness and the whole priority scheduling algorithms from high performance and timing constraint perspectives, respectively. Thereafter, we mix these two algorithms to present the partial priority scheduling algorithm to meet the deadlines of more high-priority applications and reduce the overall makespan of the system. The partial priority scheduling is implemented by preferentially scheduling the partial tasks of high-priority applications, and then fairly scheduling their remaining tasks with all the tasks of low-priority applications. Both example and experimental evaluation demonstrate the significant optimization of the partial priority scheduling algorithm.",
        "title": "6384"
    },
    {
        "abs": "Most recent graphics workstations implement graphics accelerators based on dedicated VLSI chips. The paper proposes a modular architecture using transputers and flexible software routines. A set of transputers with a crossbar connection scheme prepares the drawing. Another set using video RAMs controls a high-resolution colour monitor, each transputer handling a strip on the screen and a colour. Performance using straightforward C routines is 900 trapezoids per second per transputer. The flexible architecture allows the number of transputers to be adapted to the performance required.",
        "title": "6421"
    },
    {
        "abs": "The paper analyses the impact of technology on the design and performance of small cache memories. Technology impacts are modelled through choice of optimal pipeline structures. Emphasis is placed on both silicon and gallium arsenide technologies and on variations thereof. Numerical results are presented for a number of relevant perfomance parameters, and for two representative benchmarks.",
        "title": "6435"
    },
    {
        "abs": "Despite the bandwidth limitations of a bus, a design is presented for a parallel computer (GRIP) based on Futurebus, which limits bus bandwidth requirements by using intelligent memories. Such a machine offers higher performance than a uniprocessor and lower cost than a more extensible multiprocessor, as well as serving as a vehicle for research into parallel architectures.",
        "title": "6458"
    },
    {
        "abs": "A PIC17C44 microcontroller development board and a real-time algorithm were developed with the capability of monitoring the fetal heart rate (FHR) as well as recording both the FHR and maternal heart rate for long-term. This paper describes the development of the microcontroller board and implementation of a real-time algorithm which were used to develop the portable recorder. Two set-up were developed. One setup was used to implement the algorithm in the PIC17C44 microcontroller board. Second set-up was developed for the assessment of the reliability of processed heart rates. A large number of clinical tests have shown the very good performance of the developed monitor in comparison with FHR curves simultaneously recorded with IFM-500 Doppler ultrasound fetal monitor. Statistical comparison was done and showed nonsignificant difference in mean ( p =0.05), correlation coefficient ( r =0.8–0.92) and low PRD (5–15).",
        "title": "6478"
    },
    {
        "abs": "The Allied Standard Avionics Architecture Council (ASAAC) Phase II programme is sponsored by the Ministry of Defence of the UK, Germany and France through a Memorandum of Understanding (MOU), which provides for a two stage programme over five years to establish a complete set of standards for military core avionics for the end of this century. The standards will cover systems, software, networks, packaging and common functional modules, and will be validated by the construction of a set of demonstrators. The contract was let on 18th November 1997, and project teams were formed in the UK, France and Germany to perform the work. The current contract concerns Stage 1, i.e. the first 15 months of the ASAAC Phase II programme. Stage 1 is required to build on the results of the earlier ASAAC Phase I programme by firstly assessing and refining the architecture concepts and secondly by producing detailed specifications for Stage 2 demonstrations and outline standards for the architecture. The main objectives of this paper are, one year after the beginning of the ASAAC Phase II programme, to present as: • ASAAC Phase II programme objectives; • ASAAC Phase II contractual and industrial organisation; • Architecture concepts definition results of the Stage 1 work; • Demonstrations planned for Stage 2.",
        "title": "6497"
    },
    {
        "abs": "Geometric algorithms play an important role in computer graphics and digital image analysis. This paper makes an improvement of computing geometric properties of labelled images using arrays with reconfigurable optical buses (AROB). For an N × N image, we first design two constant time algorithms for computing the leftmost one problem and for the component reallocation problem. Then, several geometric algorithms for the labelled images are derived. These include finding the border pixels, computing the perimeter and area, computing the convex hull, determining whether two figures are linearly separable, computing the smallest enclosing box, solving the nearest neighbor problems, and computing the width and diameter. The major contribution of this work is to design both time and cost optimal algorithms for these problems. To the best of our knowledge, the proposed algorithms are the first constant time algorithms for these problems on the labelled images.",
        "title": "6508"
    },
    {
        "abs": "The throughput of a string-matching engine can be multiplied up by inspecting multiple characters in parallel. However, the space that is required to implement a matching engine that can process multiple characters in every cycle grows dramatically with the number of characters to be processed in parallel. This paper presents a hybrid finite automaton (FA) that has deterministic and nondeterministic finite automaton (NFA and DFA) parts and is based on the Aho-Corasick algorithm, for inspecting multiple characters in parallel while maintaining favorable space utilization. In the presented approach, the number of multi-character transitions increases almost linearly with respect to the number of characters to be inspected in parallel. This paper also proposes a multi-stage architecture for implementing the hybrid FA. Since this multi-stage architecture has deterministic stages, configurable features can be introduced into it for processing various keyword sets by simply updating the configuration. The experimental results of the implementation of the multi-stage architecture on FPGAs for 8-character transitions reveal a 4.3 Gbps throughput with a 67 MHz clock, and the results obtained when the configurable architecture with two-stage pipelines was implemented in ASICs reveal a 7.9 Gbps throughput with a 123 MHz clock.",
        "title": "6584"
    },
    {
        "abs": "The on-line computation of special algorithms, such as those required for controlling multidetectors in heavy-ion nuclear physics experiments, is a fundamental problem. It requires real-time computational resources capable of satisfying the associated time constraints. The work presented describes general considerations on real-time architectures, and tests performed employing, as a benchmark, the computation of a power law, used for particle identification in nuclear physics experiments.",
        "title": "6586"
    },
    {
        "abs": "This paper presents an optimized fault diagnosing procedure applicable in Built-in Self-Test environments. Instead of the known approach based on a simple bisection of patterns in pseudorandom test sequences, we propose a novel bisection procedure where the diagnostic weight of test patterns is taken into account. Another novelty is the sequential nature of the procedure which allows pruning the search space. Opposite to the classical approach which targets all failing patterns, in the proposed method not all of such patterns are needed to be used for diagnosis. This allows to trade-off the speed of diagnosis with diagnostic resolution. To improve the diagnostic resolution multiple signature analyzers are used. A method is proposed to partition a single signature analyzer into a set of multiple independent analyzers, and the algorithms are given to synthesize an optimal interface between the outputs of the circuit under test and the analyzers. The proposed method is compared with three known fault diagnosis methods: classical Binary Search based on patterns bisection, Doubling and Jumping. Experimental results demonstrate the advantages of the proposed method compared to the previous ones.",
        "title": "6618"
    },
    {
        "abs": "The earth is bombarded by a nearly isotropic flux of energetic charged particles called cosmic rays which interact with air nuclei to generate a cascade of secondary particles building up to a maximum intensity at 60 000 feet. At normal cruising altitudes, the radiation is still several hundred times the ground level intensity. These particles are sufficiently energetic and ionising that they can deposit enough charge in a small volume of semiconductor to change the state of a memory cell, while certain devices can be triggered into a state of high current drain, leading to burn-out and hardware failure. These deleterious interactions of individual particles are referred to as single event effects. The authors have flown Cosmic Radiation Effects detectors in a variety of spacecraft and aircraft and illustrative results will be presented together with a review of published instances of such phenomena in flight systems. In the future there is likely to be increased susceptibility due to growing reliance on high performance computers using smaller devices operated at lower voltages and flying at higher altitudes. The influence of cosmic rays will have to be properly considered in the assessment of reliability.",
        "title": "6671"
    },
    {
        "abs": "Matrix algorithms are an important part of many digital signal processing applications as they are core kernels that are usually required to be applied many times while computing different tasks. Hardware assisted implementations using FPGAs provide a good compromise between performance, cost and power consumption, specially when high level synthesis techniques are employed for deriving co-processors. In this paper a high level synthesis approach to generate embedded processor arrays for matrix algorithms based on the polytope model is presented. The proposed approach provides a solution for efficient data memory accesses and data transferring for feeding the processor array, as well as support for solving problems independently of their size and limited only by the FPGA available resources. The proposed approach has been validated by generating processor arrays for three different matrix algorithms used in digital signal processing applications; more precisely matrix–matrix multiplication, Cholesky and LU decomposition algorithms. These algorithms were targeted for a Spartan-6 device and compared against their sequential implementations targeted for a MicroBlaze processor in order to provide a general view of the gain achieved by the processor arrays when the arrays and sequential processors are implemented in the same technology. Results show that the implemented arrays outperforms hardware and software implementations considering an embedded platforms scenario with a Spartan-6 device.",
        "title": "6740"
    },
    {
        "abs": "With the aggressive scaling of the VLSI technology, Networks-on-Chip (NoCs) are becoming more susceptible to faults. Therefore, designing reliable and efficient routing methods is of significant importance. Most of the existing fault-tolerant techniques rely on rerouting solutions which may degrade the network performance drastically not only by taking unnecessary longer paths, but also by creating hotspots around the faults. Moreover, such off-line techniques cannot adapt to the dynamic traffic distribution in the network. In this paper, a reconfigurable and deadlock-free routing method is proposed based on the Abacus Turn Model (AbTM) to tolerate single and double switch or link failures. The required resources are kept to a minimum by avoiding to use virtual channels and routing tables. The proposed method is able to dynamically adjust the availability of the healthy paths according to the location of failures and congestion in the network to minimize rerouting. Moreover, it can grant a high degree of adaptiveness to the packets. This efficiency makes the proposed method a powerful asset for reliable routing in NoCs. The experimental results demonstrate that an 8 × 8 mesh network remains 100% reliable against single faults, and 99.8% and 99.94% reliable against double switch and link failures, respectively.",
        "title": "6770"
    },
    {
        "abs": "The second of a series of papers on modern high-level programming languages for microprocessors introduces PASCAL, which was the first language to implement strong data typing and support for structured programming. The major language constructs — program structures, control structures, simple and structured data types etc. — are discussed with respect to their support for good software engineering practice. The development of a program to simulate the behaviour of a simple computer is outlined as a programming example.",
        "title": "6782"
    },
    {
        "abs": "Multiple mobile robot systems working together to achieve a task have many advantages over single robot systems. However, the planning and execution of a task which is to be undertaken by multiple robots is extremely difficult. To date no tools exist which allow such systems to be engineered. One of the key questions that arises when developing such systems is: does communication between the robots aid the completion of the task, and if so what information should be communicated? This paper presents the results of an investigation undertaken to address the above question. The approach adopted is to utilize genetic programming (GP) with the aim of evolving a controller, and letting the evolution process determine what information should be communicated and how best to use this information. A number of experiments were performed with the aim of determining the communication requirements. The results of these experiments are presented in this paper. It is shown that the GP system evolved controllers whose performance benefitted as a result of the communication process.",
        "title": "6820"
    },
    {
        "abs": "An approach to the design of operating systems for multiple processor systems with shared memory is presented. It is based on the use of a commercial uniprocessor real-time kernel together with the development of a minimal global kernel, a minimal local and a remote I/O system. For a number of applications, this approach provides an optimal ratio between the cost of developing the operating system, on the one hand, and the real-time characteristics of the system, on the other hand. Predictability is supported by preallocation of low cost processors and memory, and by primitives that provide minimal queuing. A new implementation of interprocessor message passing, which provides satisfactory performance of the distributed operating system, is proposed. Also, an I/O system optimally structured for embedded real-time multiple processor systems with shared memory is suggested.",
        "title": "6866"
    },
    {
        "abs": "Sound field analysis is complicated and computationally intensive. In this paper, a two-dimensional sound field solver based on the Digital Huygens’ Model (DHM) is designed and implemented by a Field Programmable Gate Array (FPGA). In this sound field solver, the original DHM and its boundary condition are extended to reduce operations and hardware resource consumption. The computation is completed locally, and external memory access is avoided. In a two-dimensional space with both length and width being 1.28 m, when boundaries are rigid walls, the FPGA-based analysis system enhances performance from 44 to 217 times, and from 37 to 179 times against the software simulations based on the original DHM and Standard Leapfrog Finite-difference Time-domain (SLF-FDTD), respectively. Compared with the General-purpose Graphic Processing Unit (GPGPU) Tesla C1060, it speeds up by 1223 times in computation and by 114 times in overall performance in the case of time steps being 20,000. When the node scales are different and the calculated time steps are 32,000, the FPGA-based sound field solver achieves about 1795 and 1190 times faster in computation, 218 and 179 times enhancement in final performance over the software simulations based on the original DHM and SLF-FDTD, respectively. Furthermore, the proposed system provides high data throughput, and is easily applied in real-time applications.",
        "title": "6991"
    },
    {
        "abs": "The security of cryptographic protocols depends on the security of key sequences consisting of random numbers. In this paper, we design a Cryptographically Secure Pseudo-Random Number Generator (CSPRNG), which consists of a hash based Deterministic Random Bit Generator (DRBG) and a Get Entropy module. SRAM Physical Unclonable Functions (PUFs) are regarded as entropy sources, providing entropy data with enough entropy for CSPRNG. The construction of Get Entropy module is proposed to verify the availability of SRAM PUFs and compress the entropy data into truly random seeds that are fed into DRBG. This CSPRNG can reseed itself dynamically and can monitor the entropy of entropy sources in real time. The system is implemented and tested on Altera DE2. The test results show that, the pseudo-random numbers generated by this system can pass all random tests of National Institute of Standards and Technology (NIST) SP800-22 Test Suite and the throughput is up to 598.1 Mbps. Through the security discussion, this CSPRNG is theoretically confirmed that it can be securely applied to cryptographic protocols.",
        "title": "7107"
    },
    {
        "abs": "The number of small embedded devices is constantly growing and it is expected that there will be 50 billion of Internet connected devices in the 2020. One of the open challenges is the way of powering these devices. At the beginning they were battery powered, but now the energy is usually harvested from the environment and then accumulated. At the same time various adaptation methods have been introduced to manage the power consumption and adaptation appropriately to the energy remaining in the batteries in order to extend battery lifetime. We think that in the future, the next generation of embedded devices will take advantage of the Internet connectivity and accessibility to the weather forecasting models in order to enhance adaptability methods by analyzing the forthcoming harvested power levels. Unfortunately, these devices will use low power and lossy network protocols which may influence the quality of adaptation. In the paper, we propose the concept of the two-stage predictive power adaptation method for embedded devices that uses the weather forecast services and is designed to handle unavailability of network connection.",
        "title": "7112"
    },
    {
        "abs": "With the rapid development of semiconductor IC technology, particularly in the field of very large scale integration (VLSI), high-speed monolithic parallel multipliers are functional blocks that may one day find wide acceptance. The authors describe the implementation of an iterative division algorithm using a 16-bit monolithic multiplier. The basis of the division algorithm is the Newton-Raphson iterative method, which is slightly modified here in order to adapt to the multiplier performance characteristics and to further simplify the hardware complexity. The article also shows that by applying a suitably chosen initial iterate, the desired result is obtained in a single iteration.",
        "title": "7238"
    },
    {
        "abs": "With each passing day, Internet of Things (IoT), has the potential to transform our society to a more digital way. In this paper, a cryptographic system is proposed, which has been designed and implemented, following the IoT optimized technologies. As the benefits of IoT are numerous, the need for a privacy platform is more than necessary to be developed. This work aims to demonstrate this by, firstly, implementing efficient and flexible, the fundamentals primitives of cryptography and privacy. Secondly, this is achieved, by introducing applied cryptography, in a more interactive and flexible approach. The proposed system and the incorporation of this platform is scrutinized. In the context of this work, an application of symmetric cryptography is introduced, based on the Advanced Encryption Standard (AES) in Electronic Code Book (ECB), Cipher Block Chaining (CBC) and Counter (CTR) modes of operation, for both encryption and decryption of texts, images and electronic data applications. In addition two other security schemes are supported by the proposed system: AES Galois/Counter Mode (GCM) and AES Galois Message Authentication Code (GMAC). The GCM proposed integration, in an authentication scheme, designed to provide authenticity and confidentiality, at the same time. On the other hand, GMAC, can be applied as message authentication code. Both operations, are optimized in sense of implementation resources, since the major cost is targeted to AES core. In addition, based on the integrated hardware modules, user registration and validation is proposed and implemented, with no additional cost, and with no performance penalty. Furthermore, two factor authentication has been designed and proposed, based on One Time Passwords (OTP), which can been produced with a random procedure. After these, a reference to the security levels, as regards to the communication between the IoT layers of the architecture, is presented. IoT hardware platforms are facing lack of security level and this brings the opportunity to use advanced security mechanisms. Implementation comparison results emphasize the importance of testing and measuring the performance of the alternative encryption algorithms, supported by hardware platforms.",
        "title": "7363"
    },
    {
        "abs": "Excessive test power utilization is one of the major obstacles which the chip industry is facing at present. In SOC plan, test data volume is reduced extensively by using Test data compression strategies. In this paper, a variable-to-variable length compression method in light of encoding with perfect examples is presented. Initially, the don't care bits in the test vector are loaded with a proposed X -filling algorithm which is then encoded using the proposed Modified Equal Run Length Coding (MERLC) based encoding scheme. In relationship with the proposed X-filling and encoding scheme, an efficient decoder is designed and implemented with low area overhead. To assess the effectiveness of the proposed approach, it is tested on the ISCAS89 benchmark circuits. The tests results demonstrate that the proposed algorithm gets a higher compression ratio, when compared with the existing schemes. The Percentage compression of this scheme is 4.28%, 8.72%, 2.19%, 14.42% and 1.15% higher than those of ERLC, FDR, EFDR, Golomb and 9C coding respectively.",
        "title": "7364"
    },
    {
        "abs": "This paper describes the architecture and hardware generation concept of a parameterized MAC unit for use in a scalable embedded DSP core. The MAC unit supports a broad set of instructions for integer and fractional datatypes. Its generation is controlled by architectural as well as implementation and placement parameters. Including structured physical placement in the generation process ensures fast and predictable performance estimation. Especially for modern technologies, where wire effects dominate the achievable performance of a circuit, tight control of cell placement makes a predictable quantitative analysis and optimization possible. In the context of early-stage design space exploration, which is used to determine an optimal DSP core architecture, the presented methodology allows a fast and consistent estimation of the MAC unit’s performance characteristics for various “what if” scenarios. Also implementation bottlenecks can be identified in an early project phase. In the context of the subsequent implementation phase, it enables local, detailed, and predictable quantitative design optimizations.",
        "title": "7431"
    },
    {
        "abs": "Multithreading is a well known technique to hide latency in a non-blocking cache architecture. By switching execution from one thread to another, the CPU can perform useful work, while waiting for pending requests to be processed by the main memory. In this paper we examine the effects of varying the associativity and block size on cache performance in a reduced locality of reference environment, due to multithreading. We find that for associativity equal to the number of threads, the cache produces very low miss rate even for small sizes. Also by taking into account the increase in cycle time due to larger cache size or associativity we find that the optimum cache configuration for best processor performance is 16Kbytes direct mapped. Finally, with a constant main memory bandwidth, increasing the block size to more than 32 bytes, reduces the miss rate, but degrades processor performance.",
        "title": "7463"
    },
    {
        "abs": "Military system design poses few problems that cannot also be found in the civil field. Perhaps the only distinctly military problem is the need for many systems to adjust the trade-off between safety and performance according to circumstances. The paper (which represents the personal opinions of the author, and should not be taken as a statement of MoD policy) considers what the objective of a safe design actually is, and what technical and administrative steps need to be taken if the objective is to be met.",
        "title": "7474"
    },
    {
        "abs": "This article describes the implementation of a two-dimensional heat transfer simulation system using a Splash-2 configurable computing machine (CCM). This application was implemented as a proof-of-concept system illustrating the computational benefits of CCMs in simulating physical systems. Configurable computing machines are an emerging class of computing platform characterized by providing the computational performance of application-specific processors, yet retaining the flexibility and rapid reconfigurability attributed to general-purpose processors over a diversity of tasks. In addition, this paper discusses the process of discretizing the physical domain of such systems, along with the techniques used for mathematical simulation, to illustrate the development process for a CCM application. Also presented are the fundamental properties of CCMS and why they are well suited for the application class presented. A description of the approach and implementation is included, along with an analysis of the performance on the target system.",
        "title": "7489"
    },
    {
        "abs": "An interconnection network with n nodes is four-pancyclic if it contains a cycle of length l for each integer l with 4≤ l ≤ n . An interconnection network is fault-tolerant four-pancyclic if the surviving network is four-pancyclic in the presence of faults. The fault-tolerant four-pancyclicity of interconnection networks is a desired property because many classical parallel algorithms can be mapped onto such networks in a communication-efficient fashion, even in the presence of failing nodes or edges. Due to some attractive properties as compared with its hypercube counterpart of the same size, the Möbius cube has been proposed as a promising candidate for interconnection topology. Hsieh and Chen [S.Y. Hsieh, C.H. Chen, Pancyclicity on Möbius cubes with maximal edge faults, Parallel Computing, 30(3) (2004) 407–421.] showed that an n -dimensional Möbius cube is four-pancyclic in the presence of up to n −2 faulty edges. In this paper, we show that an n -dimensional Möbius cube is four-pancyclic in the presence of up to n -2 faulty nodes. The obtained result is optimal in that, if n −1 nodes are removed, the surviving network may not be four-pancyclic.",
        "title": "7502"
    },
    {
        "abs": "With the rapid growth of deep learning and neural network algorithms, various fields such as communication, Industrial automation, computer vision system and medical applications have seen the drastic improvements in recent years. However, deep learning and neural network models are increasing day by day, while model parameters are used for representing the models. Although the existing models use efficient GPU for accommodating these models, their implementation in the dedicated embedded devices needs more optimization which remains a real challenge for researchers. Thus paper, carries an investigation of deep learning frameworks, more particularly as review of adders implemented in the deep learning framework. A new pipelined hybrid merged adders (PHMAC) optimized for FPGA architecture which has more efficient in terms of area and power is presented. The proposed adders represent the integration of the principle of carry select and carry look ahead principle of adders in which LUT is re-used for the different inputs which consume less power and provide effective area utilization. The proposed adders were investigated on different FPGA architectures in which the power and area were analyzed. Comparison of the proposed adders with the other adders such as carry select adders (CSA), carry look ahead adder (CLA), Carry skip adders and Koggle Stone adders has been made and results have proved to be highly vital into a 50% reduction in the area, power and 45% when compared with above mentioned traditional adders.",
        "title": "7516"
    },
    {
        "abs": "A System-on-a-Chip (SoC) is the most successful example of how the evolution of the chip integration technology allows the manufacture of complex embedded systems. However, the bulk of the design effort, to efficiently combine the HW and SW components in a SoC, still resides in the HW/SW interfacing architecture. A good HW/SW integration strategy has a positive impact either in performance, efficiency, development times, productivity or reutilization of platforms for future designs. In this paper, we present an object-oriented approach to cope with the HW/SW integration problem in SoCs. The Object-Oriented Communication Engine (OOCE) is a system-level middleware particularly designed for SoCs which provides a high-level and homogeneous view of the system components based on the Distributed Object paradigm. Communication between components is abstracted by means of a HW implementation of the Remote Method Invocation semantics and all the SW and HW adapters are automatically generated from functional descriptions of the components interface. The resulting communication infrastructure simplifies the integration effort required and makes the embedded software more resilient to changes in the HW platform. To prove the viability and efficiency of our proposal a prototype implementation on the Xilinx ML505 evaluation platform has been performed.",
        "title": "7552"
    },
    {
        "abs": "This paper presents a System on Programmable Chip (SoPC) design of a digitizer to determine particle features in nuclear physics covering arrival time and energy also for pileup events. The preamplified pulses from the radiation detector are digitized with a rate of 125 Ms/s. Pulse triggering and arrival time is measured by analysis of the pulse output after CR-RC filtering. Trapezoidal pulse shaping is applied for pulse-height energy measure and noise suppression. A new method is presented for trapezoidal flat top height analysis to ease calibration of the trapezoidal pulse shaping filter. The presented method also improves pulse analysis in terms of pileup identification and false pulse rejection. Experimental results obtain a repetitive pulse rate of 50 kHz. The digitizer is able to detect pileup events with a delay between pulses down to few micro seconds.",
        "title": "7615"
    },
    {
        "abs": "The automotive industry has not been excepted from the rapid development of microcomputer applications. Since the advent of the trip computer some four or five years ago, the functional possibilities for automotive instruments have mushroomed. To clocks and trip computers have been added printers, security systems, cruise controls, environmental controls, vehicle condition monitoring and multiple displays for dashboards. The dashboard itself will host more functions than could once be imagined without filling the dash with gauges, and many that were once inconceivable such as speech synthesis. As well as increasing the information content of automotive instruments, it is believed at Smiths Industries these advances will provide new sales features which could prove to be invaluable in the marketplace.",
        "title": "7625"
    },
    {
        "abs": "Task assignment is one of the core steps to effectively exploit the capabilities of distributed or parallel computing systems. The task assignment problem is an NP-complete problem. In this paper, we present a new task assignment algorithm that is based on the principles of particle swarm optimization (PSO). PSO follows a collaborative population-based search, which models over the social behavior of bird flocking and fish schooling. PSO system combines local search methods (through self experience) with global search methods (through neighboring experience), attempting to balance exploration and exploitation. We discuss the adaptation and implementation of the PSO search strategy to the task assignment problem. The effectiveness of the proposed PSO-based algorithm is demonstrated by comparing it with the genetic algorithm, which is well-known population-based probabilistic heuristic, on randomly generated task interaction graphs. Simulation results indicate that PSO-based algorithm is a viable approach for the task assignment problem.",
        "title": "7758"
    },
    {
        "abs": "We define a model checking technique that applies to a finite state representation of sequential programs. This representation is built by means of an abstraction method which cuts the state explosion by introducing a special symbol, ⊥, to model ‘unknown’ variable values. Program properties are expressed by means of a temporal logic, which allow a further abstraction on the basis of the structure of the formulae. The satisfaction of the formulae is checked through a sort of symbolic execution of the programs which may produce a number of false results depending on the number of ⊥ values associated to the variables. Each abstraction produces a different level of incompleteness of the verification result.",
        "title": "7795"
    },
    {
        "abs": "A major obstacle in designing superscalar processors is the size and port requirement of the register file. Multiple register files of a scalar processor can be used in a superscalar processor if results are renamed when they are written to the register file. Consequently, a scalable register file architecture can be implemented without performance degradation. Another benefit is that the cycle time of the register file is significantly shortened, potentially producing an increase in the speed of the processor.",
        "title": "7870"
    },
    {
        "abs": "As we enter the deep submicron era, the number of transistors integrated on die is exponentially increased. While the additional transistors largely boost the processor performance, a repugnant side effect caused by the evolution is the ever-rising power consumption and chip temperature. It is widely acknowledged that the shortage of power supplied to a processor will be a major hazard to sustain the generational performance scaling, if the processor design is to follow the conventional approach. To utilize the on-chip resources in an efficient manner, computer architects need to consider new design paradigms that effectively leverage the advantages of modern semiconductor technology. In this paper, we address this issue by exploiting the device-heterogeneity and two-fold asymmetry in the processor manufacturing. We conduct a thorough investigation on these design patterns from different evaluation perspectives including performance, energy-efficiency, and cost-efficiency. Our observations can provide insightful guidance to the design of future processors.",
        "title": "7910"
    },
    {
        "abs": "Due to their unique path property, Banyan multistage interconnection networks (MINs) can be self-routed using control tags. This paper introduces a number of routing control classes of MINs and studies their structure. These include the D-controllable networks where the control tags are the destination labels, the FD-controllable networks where the control tags are functions of the destination labels, and the doubly D- or FD-controllable networks which are D- or FD-controllable forward and backward. The paper shows that all D- and FD-controllable networks have a recursive structure, and that all doubly D-controllable (resp., FD-controllable) networks are strictly (resp., widely) functionally equivalent to the baseline network. The subclass of MINs where the interconnections are operations on bits or digits is also studied and shown to be doubly FD-controllable and hence equivalent to the baseline. Finally, the paper presents an efficient, parallel algorithm that relabels the terminals of the baseline to simulate any network in that subclass.",
        "title": "8003"
    },
    {
        "abs": "Use of the architectural features of transputers to provide realtime matching of speech recognition is considered. The dynamic time warping algorithm has been implemented in occam and tested initially for matching a single word against a single template. The implementation has been extended to the multitemplate case and preliminary results for this are given. Logical extension to a multitransputer system is also discussed. The single-transputer implementation has been undertaken using a T414 transputer resident in an IBM XT personal computer on an Inmos B004 evaluation board with 2 Mbyte of RAM.",
        "title": "8036"
    },
    {
        "abs": "This paper presents an optimal method for topology synthesis by taking into account factors related to power, performance, and contention in an application-specific Network-on-Chip (NoC) architecture. A Tabu search based approach is used for topology generation with an automated design technique, incorporating floorplan information to attain accurate values for power consumption of the routers and physical links. The Tabu search method incorporates multiple objectives and is able to generate optimal NoC topologies which account for both power and performance. The contention analysis technique assesses performance and relieves any potential bottlenecks using virtual channel insertion after considering its effect on power consumption and performance improvement within the NoC. The contention analyzer uses a Layered Queuing Network approach to model the rendezvous interactions among system components. Several experiments are conducted using various SoC benchmark applications to compare the power and performance outcomes of the proposed technique.",
        "title": "8155"
    },
    {
        "abs": "This paper presents a microprocessor-based FPGA system for lossy image compression. The system implements a widely known wavelet-based compression method, i.e. the Set Partitioning In Hierarchical Trees algorithm (SPIHT). The computationally intensive 2D wavelet-transform is performed by means of custom circuits, whereas an embedded microprocessor is used to execute a purpose-build SPIHT encoding process. The aim of this work is to demonstrate and verify the feasibility of a compact and programmable image compression sub-system that uses just one low-cost FPGA device. The entire system consumes just 1637 slices of an XC2V chip, it runs at 100 MHz clock frequency and reaches a speed performance suitable for several real-time applications.",
        "title": "8205"
    },
    {
        "abs": "Concurrent and distributed processing systems based on multiple microprocessors are both feasible and desirable. Processes residing on different processors execute in parallel while processes allocated on the same processor execute in a multiprogramming environment. These processes normally have to communicate and synchronize in order to achieve a common goal. Communication and synchronization are usually achieved by calling primitives supplied by a kernel. The paper describes a model of implementation for message-passing primitives which must be added to a single-processor kernel for communication and synchronization purposes in multimicroprocessor systems.",
        "title": "8239"
    },
    {
        "abs": "Single-hop non-blocking networks have the advantage of providing uniform latency and throughput, which is important for cache-coherent network-on-chip systems. This paper focuses on high performance circuit designs of multi-stage non-blocking networks as alternatives to crossbars. Existing work shows that Benes networks have much lower transistor count and smaller circuit area but longer delay than crossbars. To reduce the timing delay, we propose to design the Clos network built with larger size switches. Using less than half number of stages than the Benes network, the Clos network with 4 × 4 switches can significantly reduce the timing delay. The circuit designs of both Benes and Clos networks in different sizes are conducted considering two types of implementation of the configurable switch: with N-type metal-oxide-semiconductor logic (NMOS) transistors only and full transmission gates (TGs). The layout and simulation results under 45 nm technology show that the TG-based implementation demonstrates much better signal integrity than its counterpart. Clos networks achieve average 60% lower timing delay than Benes networks with even smaller area and power consumption.",
        "title": "8244"
    },
    {
        "abs": "MSDOS machines have always been the favourite host platforms for transputer networks. Unfortunately, it is hard for MSDOS-hosted transputers to be easily integrated within a network environment, due to the lack of native support for networking on the host. The availability, nowadays, of cheap hardware for Ethernet access has made it possible to use MSDOS machines as general purpose interfaces between TCP-IP networks and peripheral devices (modems, printers etc.). In this paper we show how to use the same approach to interface transputer networks to the Internet, with some advantages over solutions using dedicated hardware, or Unix workstations, as host machines. The result of our work is PCserver, public domain software that allows a single MSDOS machine to host several independent transputer networks which can be completely controlled from other machines on the Internet. Performance issues related to the implementation of PCserver and in general to the network interfacing of transputers are also discussed in some detail.",
        "title": "8270"
    },
    {
        "abs": "Refresh power of dynamic random-access memory (DRAM) has become a critical problem due to the large memory capacity of servers and mobile devices. It is imperative to reduce refresh rate in order to reduce refresh power consumption. However, current methods of refresh rate improvement have limitations such as large area/performance overhead, low system availability, and lack of support at the memory controller. We propose a novel scheme which comprises three essential functions: (1) an adaptive refresh method that adjusts refresh period on each DRAM chip, (2) a runtime method of retention-time profiling that operates inside DRAM chips during idle time thereby improving availability, and (3) a dual-row activation method which improves weak cell retention time at a very small area cost. The proposed scheme allows each DRAM chip to refresh with its own refresh period without requiring the external support. Experiments based on real DRAM chip measurements show that the proposed methods can increase refresh period by 4.5 times at 58 °C by adjusting refresh period in a temperature-aware manner while incurring only a small overhead of 1.05% and 0.02% in DRAM chip area and power consumption, respectively. Below 58 °C, our method improves the refresh period by 12.5% compared with two state-of-the-art methods, AVATAR and in-DRAM ECC. In various memory configurations with SPEC benchmarks, our method outperforms the existing ones in terms of energy-delay product by 19.7% compared with the baseline, and by 15.4% and 12.4%, with respect to AVATAR and in-DRAM ECC, respectively.",
        "title": "8291"
    },
    {
        "abs": "The paper describes how some commercial VLSI microprocessors have been designed: first the Motorola MC68000, then the MC68010 and 68020, and finally the IBM Micro/370. Design and verification methods are described and the software tools used are noted. The evolution of chips, methods and tools is described and some future trends are projected. At present, logic design for standard microprocessors is performed manually then verified on a computer. In the author's opinion this level of automation will not increase in the forseeable future, although the design tools will run faster due to advancing computer technology.",
        "title": "8301"
    },
    {
        "abs": "Recent years have seen an increasing number of scientists employing data parallel computing frameworks, such as Hadoop, in order to run data-intensive applications. Research on data-grouping-aware data placement for Hadoop has become increasingly popular. However, we observe that many data-grouping-aware data placement schemes are static, without taking MapReduce job execution frequency into consideration. Such data placements scheme will lead to severe performance degradation that is way below the potential efficiency of optimal data distribution when executing MapReduce jobs that are executed frequency. In this paper, we propose a new data-grouping-aware dynamic (DGAD) data placement method based on the job execution frequency. Firstly, we build a job access correlation relation model among the data blocks according to the relationships provided by the records about historical data block access. Then we use a clustering algorithm to divide data blocks into clusters according to the job access correlation relation model among the data blocks and propose a data placement algorithm based on data block clusters in order to put correlated data blocks within a cluster on the different nodes. Finally, a series of experiments are carried out in order to verify the method proposed in this paper. Experimental results show that the proposed method can effectively deal with the mass data and can obviously improve the execution efficiency of MapReduce.",
        "title": "8334"
    },
    {
        "abs": "Many cryptographic primitives that are used in cryptographic schemes and security protocols such as SET, PKI, IPSec and VPN's utilize hash functions - a special family of cryptographic algorithms. Hardware implementations of cryptographic hash functions provide high performance and increased security. However, potential faults during their normal operation cause significant problems in the authentication procedure. Hence, the on-time detection of errors is of great importance, especially when they are used in security-critical applications, such as military or space. In this paper, two Totally Self-Checking (TSC) designs are introduced for the two most-widely used hash functions: SHA-1 and SHA-256. To the best of authors’ knowledge, there is no previously published work presenting TSC hashing cores. The achieved fault coverage is 100% in the case of odd erroneous bits. The same coverage is achieved for even erroneous bits, if they are appropriately spread. Additionally, experimental results in terms of frequency, area, throughput, and power consumption are provided. Compared to the corresponding Duplicated with Checking (DWC) architectures, the proposed TSC-based designs are more efficient in terms of area, throughput/area, and power consumption. Specifically, the introduced TSC SHA-1 and SHA-256 cores are more efficient by 16.1% and 20.8% in terms of area and by 17.7% and 23.3% in terms of throughput/area, respectively. Also, compared to the corresponding DWC architectures, the proposed TSC-based designs are on average almost 20% more efficient in terms of power consumption.",
        "title": "8358"
    },
    {
        "abs": "Voting algorithms are used to arbitrate between variant results in a wide range of highly dependable real-time control applications. These applications include N-Modular Redundant hardware systems and diversely designed software systems based on N-Version Programming. The most sophisticated and complex voting algorithms can even tolerate malicious (or Byzantine) variant errors. Voting algorithms can be implemented in either hardware or software depending on the characteristics of the application and the type of voter selected. The behaviour of voting algorithms in multiple error scenarios is considered in this article. Complete disagreement is defined as those cases where no two variant results are the same. A novel algorithm for real-time control applications, the smoothing voter, is introduced and its behaviour compared with three previously published voters. Software implemented error-injection tests, reported here, show that the smoothing voter achieves a compromise between the result selection capabilities of the median voter and the safety features of the majority voter. The smoothing voter is an appropriate voter for applications in which maximising the number of correct outputs and minimising the number of benign errors of the system is the main concern, and a slight degradation in safety can be tolerated.",
        "title": "8437"
    },
    {
        "abs": "In this article we introduce the use of field programmable gate array (FPGA) into the central processing unit (CPU) as part of an arithmetic-logic unit (ALU). As the concept of an in-system configurable FPGA inside the CPU is becoming more and more popular, it is now used mainly for the purpose of testing and evaluating. We suggest that the use of FPGA as an extension to the ALU with its functions that are implemented in the logic circuit (which we call logic-ware) can greatly increase the performance of the CPU.",
        "title": "8527"
    },
    {
        "abs": "The recent spectacular progress in the microelectronic, information, communication, material and sensor technologies created a big stimulus towards development of smart communicating cyber-physical systems (CPS) and Internet of Things (IoT). CPS and IoT are undergoing an explosive growth to a large degree related to advanced mobile systems like smart automotive and avionic systems, mobile robots and wearable devices. The huge and rapidly developing markets of sophisticated mobile cyber-physical systems represent great opportunities, but these opportunities come with a price of unusual system complexity, as well as, stringent and difficult to satisfy requirements of many modern applications. Specifically, smart cars and various wearable systems to a growing degree involve big instant data from multiple complex sensors or other systems, and are required to provide continuous autonomous service in a long time. In consequence, they demand a guaranteed (ultra-)high performance and/or (ultra-)low energy consumption, while requiring a high reliability, safety and security. To adequately address these demands, sophisticated embedded computing and embedded design technologies are needed. After an introduction to modern mobile systems, this paper discusses the huge heterogeneous area of these systems, and considers serious issues and challenges in their design. Subsequently, it discusses the embedded computing and design technologies needed to adequately address the issues and overcome the challenges in order to satisfy the stringent requirements of the modern mobile systems.",
        "title": "8534"
    },
    {
        "abs": "The HypErspectraL Imaging Cancer Detection (HELICoiD) European project aims at developing a methodology for tumor tissue classification through hyperspectral imaging (HSI) techniques. This paper describes the development of a parallel implementation of the Support Vector Machines (SVMs) algorithm employed for the classification of hyperspectral (HS) images of in vivo human brain tissue. SVM has demonstrated high accuracy in the supervised classification of biological tissues, and especially in the classification of human brain tumor. In this work, both the training and the classification stages of the SVMs were accelerated using Graphics Processing Units (GPUs). The acceleration of the training stage allows incorporating new samples during the surgical procedures to create new mathematical models of the classifier. Results show that the developed system is capable to perform efficient training and real-time compliant classification.",
        "title": "8555"
    },
    {
        "abs": "Imprecising the arithmetic hardware blocks is well known as one of the brilliant approaches that increase the performance of digital signal processors (DSP) at the cost of imposing some acceptable errors. Making a trade-off between the performance and the enormous results of a given system is a challenge which has attracted the interest of many researchers in recent years. In this paper, we focus on the design of an imprecise 4:2 compressor which lies at the heart of inexact multipliers. The proposed imprecise 4:2 compressor by utilizing only one majority gate brings significant efficiency in implementation of today's technologies like FinFET and future majority based emerging technologies such as QCA. The evaluation results in both of aforesaid technologies demonstrate the remarkable improvement of the proposed design compared to related works. In addition, employing the proposed imprecise 4:2 compressor in an image processing application confirms the qualitative acceptability of the proposed design.",
        "title": "8656"
    },
    {
        "abs": "In order to solve poor fine searching capacity of artificial fish swarm algorithm and artificial bee colony swarm algorithm in late state to result in insufficient local optimization, hybrid swarm intelligent parallel algorithm research based on multi-core clusters is proposed; Then, reverse learning mechanism is introduced in early stage of algorithm, initialized swarms are evenly distributed, and swarms are randomly divided into two groups to make interactive learning strategy accelerates rate of convergence, and basic artificial fish swarm algorithm and artificial bee colony swarm algorithm are used to make global searching. In late stage of algorithm, niches artificial fish swarm algorithm and Random Perturbation Artificial Bee Colony are used to make local fine searching to the solution obtained in early stage; On this basis, MPI+OpenMP+STM parallel programming model based on multi-core clusters is established for parallel design and analysis. Finally, stimulation experiment indicates optimizing efficiency of this algorithm is higher than single artificial fish swarm algorithm and artificial bee colony swarm algorithm.",
        "title": "8661"
    },
    {
        "abs": "In this paper, we discuss a parallel processor design using eight DSP microprocessors (TMS320C25) and dual-port RAMs for image processing applications. The application of image processing algorithms on this architecture, hardware details, performance analysis, simulation of image processing algorithms and comparison with architectures reported in the literature are also discussed.",
        "title": "8733"
    },
    {
        "abs": "To conserve space and power as well as to harness high performance in embedded systems, high utilization of the hardware is required. This can be facilitated through dynamic adaptation of the silicon resources in reconfigurable systems in order to realize various customized kernels as execution proceeds. Fortunately, the encountered reconfiguration overheads can be estimated. Therefore, if the scheduling of time-consuming kernels considers also the reconfiguration overheads, an overall performance gain can be obtained. We present our policy, experiments, and performance results of customizing and reconfiguring Field-Programmable Gate Arrays (FPGAs) for embedded kernels. Experiments involving EEMBC (EDN Embedded Microprocessor Benchmarking Consortium) and MiBench embedded benchmark kernels show high performance using our main policy, when considering reconfiguration overheads. Our policy reduces the required reconfigurations by more than 50% as compared to brute-force solutions, and performs within 25% of the ideal execution time while conserving 60% of the FPGA resources. Alternative strategies to reduce the reconfiguration overhead are also presented and evaluated.",
        "title": "8746"
    },
    {
        "abs": "The task of an Internet router is to scan the IP headers for the destination address and make a routing decision based on the information. If the processing capability of a router is enhanced to support computation on a datagram, some of the host computation may be delegated to the intermediate routers. The instructions about how to do the processing may be provided by the end hosts. We propose a reliable transport layer protocol, Intermediate Processing Protocol (IPP) for processing within the Internet. The protocol design makes provisions for connection set up handshake, router reservation, intermediate processing, data acknowledgement, buffering and retransmission, flow and congestion control, ordered delivery and security issues.",
        "title": "8889"
    },
    {
        "abs": "It has been more than a decade since general porous applications targeted GPUs to benefit from the enormous processing power they offer. However, not all applications gain speedup running on GPUs. If an application does not have enough parallel computation to hide memory latency, running it on a GPU will degrade the performance compared to what it could achieve on a CPU. On the other hand, the efficiency that an application with high level of parallelism can achieve running on a GPU depends on how well the application’s memory and computational demands are balanced with a GPU’s resources. In this work we tackle the problem of finding a GPU configuration that performs well on a set of GPGPU applications. To achieve this, we propose two models as follows. First, we study the design space of 20 GPGPU applications and show that the relationship between the architectural parameters of the GPU and the power and performance of the application it runs can be learned by a Neural Network (NN). We propose application-specific NN-based predictors that train with 5% of the design space and predict the power and performance of the remaining 95% configurations (blind set). Although the models make accurate predictions, there exist few configurations that their power and performance are mispredicted. We propose a filtering heuristic that captures most of the predictions with large errors by marking only 5% of the configurations in the blind set as outliers. Using the models and the filtering heuristic, one will have the power and performance values for all configurations in the design space of an application. Searching the design space for a set of configurations that meet certain restrictions on the power and performance can be a tedious task as some applications have large design spaces. In the Second model, we propose to employ the Pareto Front multiobjective optimization technique to obtain a subset of the design space that run the application optimally in terms of power and performance. We show that the optimum configurations predicted by our model is very close to the actual optimum configurations. While this method gives the optimum configurations for each application, having a set of GPGPU applications, one may look for a configuration that performs well over all the applications. Therefore, we propose a method to find such a configuration with respect to different performance objectives.",
        "title": "8896"
    },
    {
        "abs": "Commercial multimedia computing applications have recently become a driving force in the development of simple, robust methods to provide real-time transfer of video and audio data while minimising the impact to concurrent data processing tasks. The emergence of these technologies offers an opportunity to reduce the cost of fourth-generation avionics by applying commercial techniques to these systems as well. Evolving standards and technology trends and their application to military avionics are described.",
        "title": "8923"
    },
    {
        "abs": "The performance of software executed on a microprocessor is adversely affected by the basic fetch–execute cycle. A further performance penalty results from the load–execute–store paradigm associated with the use of local variables in most high-level languages. Implementing the software algorithm directly in hardware such as on an FPGA can alleviate these performance penalties. Such implementations are normally developed in a hardware description language such as VHDL or Verilog. More recently, several methods for using C as a hardware description language and for compiling C programs to hardware have been researched. Several software-programming languages compile to an intermediate representation (IR) that is stack based such as Java to Java bytecodes. Forth is a programming language that minimizes the use of local variables by exchanging the load–execute–store paradigm for stack manipulation instructions. This paper introduces a new systems architecture for FPGAs, called flowpaths, which can implement Java bytecodes or software programs written in Forth directly in an FPGA without the need for a microprocessor core. In the flowpath implementation of Forth programs all stack manipulation instructions are represented as simple wire connections that take zero time to execute. In the flowpath implementation of Java bytecodes the normal load–execute–store paradigm is represented as a single sequential operation and stack-manipulation operations become combinational thus executing faster. This paper compares the use of flowpaths in an FPGA generated from Java bytecodes and a high-level Forth program for the Sieve of Eratosthenes with C, Java, and Forth executed on microprocessors and microprocessor cores on FPGAs. The results show that flowpaths perform within a factor of two of a minimal hand-crafted direct hardware implementation of the Sieve and orders of magnitude better than compiling the program to a microprocessor.",
        "title": "9111"
    },
    {
        "abs": "Commercial workloads, such as transaction processing, stress the memory hierarchies of current computers. TPC-B is a transaction processing benchmark that generates significant operating system and database activity, and a TPC-B instruction trace is analyzed for the Motorola 88110 microprocessor. Instruction set usage reveals one branch in every four to five instructions, and instruction cache performance indicates that this branching behavior causes a large number of words to be brought in by cache refills but never used. Code reorganization optimizations to reduce unused words and avoid cache misses are simulated and found to reduce misses in the on-chip instruction cache by almost one fifth.",
        "title": "9189"
    },
    {
        "abs": "Aiming at the requirement of random cross business generated by mass data cryptosystems in security field, in this paper, a pipeline data processing architecture which includes four stages (i.e., dispatch, pretreatment, operation and synchronous reorganization) is proposed to accelerate data stream processing. In our work, attribute such as business id and algorithm identification of job package is used to distinguish different business requests. Hierarchical processing based on data identification is used to implement the mapping between job packages and algorithm IP cores. KSM memory access control logic is used to access association job package’s intermediate state data. And the synchronization module is used to track the operation state to implement the synchronization between the input and output data. These ensure the correctness of cross access on parallel or serial mode. This architecture realizes the high concurrent processing of multiple algorithms and multiple IP cores on one single chip, and solves the problem of random cross encryption and decryption of multiple cryptographic algorithms, multiple keys, multiple IP cores and multiple data streams in many-to-many communication. The prototype system, developed on the XC7K325t FPGA, demonstrates the correctness of cryptographic processing during multi-threads cross data access. Experiments show the system throughput in our approach is higher than existing schemes.",
        "title": "9228"
    },
    {
        "abs": "Debugging of distributed software is approached in this paper by defining specific classes of program events to be monitored by the user while execution is in progress. An elementary event is defined in terms of a range condition and an access mode for a memory location. The concept of a compound event is introduced, expressed in terms of either an accumulated event, a sequential event conjunction, a logical event disjunction or an instantaneous event conjunction. On the occurrence of an event, the actions connected with that event will be performed. Possible actions are trace and break traps, event counting and measurement of time intervals. A specific debugger designed for a realtime multimicroprocessor system and based on the event-action model is presented.",
        "title": "9241"
    },
    {
        "abs": "Corner-case analysis is a well-known technique to cope with occasional deviations occurring during the manufacturing process of semiconductors. However, the increasing amount of process variation in nanometer technologies has made it inevitable to move toward statistical analysis methods, instead of deterministic worst-case-based techniques, at all design levels. We show that by statically considering statistical effects of random and systematic process variation on performance and power consumption of a Multiprocessor System-on-Chip (MPSoC), significant power improvement can be achieved by static software-level optimizations such as task and communication scheduling. Moreover, we analyze and show how the changes in the amount of process variability as well as values of other system constraints affect the achieved power improvement in such system-level optimizations. We employ a mixed-level model of MPSoC critical components so as to obtain the statistical distribution of frequency and power consumption of MPSoCs in presence of both within-die and die-to-die process variations. Using this model, we show that our proposed statistical task scheduling algorithm can achieve substantial power reduction under different values of system constraints. Furthermore, the effectiveness of our proposed statistical task scheduling approach will even increase with the increasing amount of process variation expected to occur in future technologies.",
        "title": "9243"
    },
    {
        "abs": "The implementation of the forward kinematics algorithm to position control robot manipulators is discussed. The objective is to control the manipulator in real time by dividing the task between a network of transputers. The underlying strategy is the fine-grain distribution of tasks as opposed to allocating one processor per joint. The network topology used is taken through diagnostic tests for analysis leading to performance maximization.",
        "title": "9300"
    },
    {
        "abs": "A microprocessor-based data acquisition system with a high-speed A/D and D/A interface is described. The system is connected to an HP9000 computer using the IEEE 488 general-purpose interface bus (GPIB), but is treated as a separate working system. Assembly-language programs have been written to communicate with the standard IEEE 488 commands sent by the HP9000. All the commands sent by the HP9000 are written in basic , allowing powerful data handling. The rate of data transmission to the HP9000 is about 33k samples s−1.",
        "title": "9325"
    },
    {
        "abs": "This paper introduces a multi-layer MIMD system for computer vision applications. The system is provided with three hierarchical processing layers, each of which is dedicated to one level of processing to create a pipelining effect. Unlike the traditional approach, where a 2D mesh connected array is used, layers in this system are linear arrays. Simplicity and expandability are the main advantages. A prototype of the system is implemented using off-the-shelf components and the performance of the various vision operations is analyzed.",
        "title": "9344"
    },
    {
        "abs": "A dual robot manufacturing cell which performs the stripping operation in garment manufacture is described. The mechanical structure of the robots, together with short programming cycle times mean that traditional robot programming techniques are not applicable. The paper describes the novel mechanical design and details the associated computer control system. Executing on standard computer and microprocessor hardware components, details of the software system which automatically generates robot motion programs from Computer Aided Design data are presented.",
        "title": "9400"
    },
    {
        "abs": "This paper proposes an efficient and high performance rectification architecture to be used as a preprocessing module in a complete stereo vision system before the matching correspondence calculus. A complete rectification process is implemented in order to remove the radial and tangential distortion effects due to lenses and to align the left and right raw images acquired by a stereo camera for the epipolar constraining. Thus, the epipolar lines are made collinear with each other and with the image scanning lines in order to reduce the complexity of the matching problem to a one-dimension correspondence search. The image transformation operations required by the rectification process are computed as matrix calculus through a pipelined and efficient hardware design. Unlike the memory mapped rectification function implementations, the proposed solution does not require any external memory block for the storage of pre-computed rectification maps. Moreover, conforming to the camera model adopted by the Stereo MATLAB Calibration Toolbox which is renowned as the most widely used software toolset for estimating the calibration parameters of a stereo camera, the proposed rectification architecture is a ready-to-use hardware solution to be used in stereo vision real-time embedded systems after calibrating the employed stereo camera following the MATLAB Calibration Toolbox procedure. When implemented in a Xilinx XC4VLX60-12ff1148 FPGA chip, the proposed circuit rectifies 640 × 480 and 1280 × 720 stereo images at a frame rate of 367 fps and 120 fps, respectively. The proposed fully pipelined solution uses an efficient raw image buffer system which is opportunely sized in order to store the minimum number of image rows able to guarantee the synchronization between the image buffering and the rectification elaboration without any interruption of the pipelined processing flow. When the proposed rectification system was used for processing the stereo images acquired by the Point Grey Research Bumblebee BB2-03S2 stereo camera, just 32 BRAM blocks were necessary to implement the raw image buffer; thus, after a latency of 136 us (15,387 clock cycles), a continuous flow of left and right rectified image pixels is guaranteed in output, for each inputted left and right couple of raw image pixels, at each clock cycle. When compared to the other implementations present in literature, the proposed solution offers the advantage of not using any external memory with respect to the memory-mapped rectification solutions while offering a more efficient and complete solution reaching the highest speed performance with respect to the on-the-fly computed rectification implementations present in literature.",
        "title": "9409"
    },
    {
        "abs": "Elliptic Curve Cryptography (ECC) is a multilayer system with increased hardware implementation complexity. A wide range of parameters and design choices affect the overall implementation of ECC systems. A variety of hardware implementations of ECC system that vary in parameters are proposed in the literature. Implementation target, underlying finite fields, coordinate system and modular arithmetic algorithms are key design elements that impact the overall implementation outcome. In this paper, we survey the various implementation approaches with the aim of providing a useful reference for hardware designers for building efficient ECC processors. Our literature review consists of four components. First, we list the design options and discuss their impact on ECC implementation. Second, we summarize different approaches and algorithms used in the literature for implementing modular arithmetic operations. Third, we review best practices in the literature for data paths and overall architectures. Fourth, we review the existing parallelism and performance enhancement techniques. In addition, this paper provides comparison of the different binary extension, prime and dual 8 hardware implementations of ECC.",
        "title": "9425"
    },
    {
        "abs": "For 8-bit microprocessors, there is no immediate software solution to the problem raised by block-structured procedures, ROM implanted and called from a program located in a memory area. What happens with the debugging and evaluation of application software intended for a single-board system is that the modules have to be tested one by one through the target system before they are gradually implemented into ROM. The use of an incircuit emulator prevents this difficulty from occurring but, as most of the time only one emulator is available for a few teams, its use seems to be restricted to integration into the hardware. This paper describes a software interface which allows the ROM-implemented code and the RAM-implemented code to communicate. It is thus possible to allocate the code obtained using a high-level language at two noncontiguous locations, the first in ROM for the procedures already checked and the second in RAM for the procedures to be debugged. The use of a monitor allows the debugging of the critical parts and the evaluation of all the software before burning it permanently into ROM.",
        "title": "9462"
    },
    {
        "abs": "A synchronous mode as well as a scan mode of operation are added to a large class of asynchronous circuits, in compliance with LSSD design rules. This enables the application of mainstream tools for design-for-testability and test-pattern generation to asynchronous circuits. The approach is based on a systematic transformation of all single-output sequential gates into synchronous and scannable versions. By exploiting dynamic circuit operation in scan mode, the overhead of this transformation in terms of both circuit cost and circuit delay is kept minimal.",
        "title": "9585"
    },
    {
        "abs": "A large multi-ported rename register file (RRF) is indispensable for simultaneous multithreaded (SMT) processors to hold more intermediate results of in-flight instructions from multiple threads running simultaneously. However, enlarging the RRF incurs longer access delays and more power consumption, both of which are critical to the overall performance and are becoming a bottleneck due to the ever-increasing pipeline depth and issue width in future SMT processors. We propose a novel register renaming scheme called Multi-usable Rename Register with 2-Level renaming and allocating ( 2L-MuRR ), which focuses on more efficient utilization of a fewer number of rename registers. Based on the fact that the effective bit-width of most operands is narrower than the full-bit width of a register entry, 2L-MuRR partitions each rename register into several fields of different widths. Either a single field or a field combination can hold an operand, thus making each rename register multi-usable. In addition, 2L-MuRR postpones the register allocation to the write-back stage, which is similar to the formerly proposed virtual–physical-register (VPR) scheme, further reducing the meaningless RRF occupancy. The simulations show that 2L-MuRR improves the efficiency of the RRF significantly, achieving higher performance with much fewer rename registers. For example, when the RRF size is 60, 2L-MuRR outperforms Trad (traditional register renaming approach) and VPR in terms of IPC by 38% and 11%, respectively, while decreases the RRF occupancy by 37% and 15%, respectively.",
        "title": "9616"
    },
    {
        "abs": "Traditionally, FPGA designers make use of CAD tools for evaluating architectures in terms of the area, delay and power. Recently, analytical methods have been proposed to optimize the architectures faster and easier. A complete analytical power, area and delay model have received little attention to date. In addition, previous works use analytical methods to optimize general-purpose FPGA architectures. Using analytical models for optimizing application-specific FPGA architectures are interesting subjects in the reconfigurable computing field. In this way, designers can investigate the optimized architecture for a set of application circuits and the consumers can find their best architecture among a variety of devices which is optimal for their specific work. In this paper, we complete an analytical FPGA performance model by presenting an analytical model to estimate the dynamic and leakage power and by integrating it into the geometric programming framework. This way, we are able to rapidly analyze various FPGA architectures and select the best one in terms of power consumption as well as area and delay. In the next step, we extend the model for optimizing FPGA architectures for a set of applications. A case of the best architecture for two specific circuits has been investigated in this paper.",
        "title": "9702"
    },
    {
        "abs": "It is common practice that, during the development of a system, the hardware and software subsystems are designed independently and subsequently integrated during the testing phase of the design cycle. Lately, there has been an increasing interest in the design and implementation of embedded systems where the two subsystems are developed concurrently in order to meet performance and cost constraints - a process known as ‘hardware/software codesign’. In our approach to codesign, we are concerned with accelerating the performance of time critical regions of programs which are executed on a conventional microcontroller. A critical region is implemented not in software but in a programmable hardware device to improve performance. This paper presents a general overview of our development environment for the cosynthesis and performance evaluation of general-purpose hardware/software systems. In particular, we highlight the design of the hardware architecture which supports software acceleration.",
        "title": "9844"
    },
    {
        "abs": "Pneumatic drives are the most widely used form of industrial actuation system. However, research in relevant areas is very limited, often carried out, if at all, in isolated environments. There is a lack of strategic thinking and tools, which can enable developments to go forward in an efficient manner. In recognising the advancements in PC-technology, fieldbus systems, microprocessors and computing technologies, the authors advocate the concept of smart components-based servo pneumatic actuation systems. The paper expands on the components-based nature of pneumatic drive systems to identify design templates for smart valves, smart sensors, smart actuators, etc. The development of components-based pneumatic drive systems is considered in the context of a recently completed UK EPSRC funded research project on high speed servo-pneumatic actuation, addressing issues such as simulation tools, algorithm development, sizing, tuning, installation and commissioning. A high speed pneumatically-driven gantry pick-and-place handling system with novel design features is presented to demonstrate the capability and attributes of pneumatic drives for more sophisticated applications.",
        "title": "9893"
    },
    {
        "abs": "Parametric loudspeaker system enables sound to be projected and directed to a specific listening area just like a beam of light. The advancement of Field Programmable Gate Array (FPGA) technology opens up a very interesting option for rapid implementation and easy configurable signal processing platform for parametric loudspeaker system. In this paper, the digital signal processing subsystem of the parametric loudspeaker system has been designed and implemented in FPGA platform using the Altera 1S10 device.",
        "title": "10027"
    },
    {
        "abs": "Manufacturers of peripherals are inconsistent in their use of male or female connectors on their equipment. Combined with the increasingly common practice of producing data terminal equipment that in some respects is like data communication equipment (to cut the costs of extra null modem cables), this means that it can be very difficult to make two devices communicate over an RS232C serial link. The paper addresses the problems of incompatible male-female D-type connectors and reversed transmit-receive lines for three-wire communication. These ideas could be expanded for the problems associated with incompatible handshake lines.",
        "title": "10147"
    },
    {
        "abs": "In the Advent of the Internet of Things (IoT), embedded architecture takes an important dimension in terms of energy and accomplishment. The embedded system needs more and more intelligent algorithms for better performance and energy efficiency to fit into an IoT scenario. Moreover, with the existence of high-performance multi-core embedded architectures, achievements of energy efficiency remains in the dark side of the research. Several algorithms such as dynamic frequency scaling, thread mapping, starvation methodologies were proposed in embedded architectures for efficient usages of clock frequencies and these features were used as the energy saving modes in which the consumption of energy in the embedded architectures are being controlled. But these methods have several backlogs which permits the use of consumption in the embedded architectures. Considering the above features, this paper proposes a new methodology PODS(Predictors for Optimized Dynamic Scaling) which integrates a powerful machine learning algorithm for scaling the clock frequencies by the input workloads and allocation of the core depending based on the workload. The proposed framework PODS has different phases of working namely workload extraction, characterization, and optimization using BAT algorithms and prediction extreme Machine - Learning. The algorithm was tested on ARM/Cortex architectures (Raspberry Pi 3 Model B+), an evaluated algorithm using the IoMT benchmarks and various parameters that include energy consumption, accuracy of detection/prediction was determined and analyzed. It is found that the implementation of the proposed framework in the test is seen resulting between 35 and 40% reduction in the consumption of the power.",
        "title": "10179"
    },
    {
        "abs": "This paper describes the development of a prototype speech-controlled cloud-based wheelchair platform. The control of the platform is implemented using a low-cost WebKit Speech API in the cloud. The description of the cloud-based wheelchair control system is provided. In addition to the voice control, a GUI is implemented, which works in a web browser as well as on mobile devices providing live video streaming. Development was done in two phases: first, a small, initial prototype was developed and, second, a full size prototype was build. The accuracy of the speech recognition system was estimated as ranging from approximately 60% to up to 97%, dependent on the speaker. The speech-controlled system latency was measured as well as the latency when the control is provided via touch on a so-called smart device. Measured latencies ranged from 0.4 s to 1.3 s. The platform was also clinically tested, providing promising results of cloud-based speech recognition for further implementation. The developed platform is based on a Quad Core ARM Mini PC GK802 running Ubuntu Linux and an Arduino UNO Microcontroller. Software development was done in JavaScript/ECMA Script, applying node.js.",
        "title": "10191"
    },
    {
        "abs": "The advent of the Internet of Things has motivated the use of Field Programmable Gate Array  (FPGA) devices with Dynamic Partial Reconfiguration  (DPR) capabilities for dynamic non-invasive modifications to circuits implemented on the FPGA. In particular, the ability to perform DPR over the network is essential in the context of a growing number of Internet of Things  (IoT)-based and embedded security applications. However, the use of remote DPR brings with it a number of security threats that could lead to potentially catastrophic consequences in practical scenarios. In this paper, we demonstrate four examples where the remote DPR capability of the FPGA may be exploited by an adversary to launch Hardware Trojan Horse  (HTH) attacks on commonly used security applications. We substantiate the threat by demonstrating remotely-launched attacks on Xilinx FPGA-based hardware implementations of a cryptographic algorithm, a true random number generator, and two processor based security applications - namely, a software implementation of a cryptographic algorithm and a cash dispensing scheme. The attacks are launched by on-the-fly transfer of malicious FPGA configuration bitstreams over an Ethernet connection to perform DPR and leak sensitive information. Finally, we comment on plausible countermeasures to prevent such attacks.",
        "title": "10357"
    },
    {
        "abs": "This paper presents a methodology for the system-level dependability analysis of multiprocessor embedded systems. The methodology is based on fault injection and features an error analysis approach offering to the designer the possibility to specify custom monitoring and classification actions at both application and architecture levels. In particular, a debug-like mechanism offers the possibility to interpret architectural raw data observed during the simulation at application level with a function call/return granularity, thus offering the possibility to analyze the propagation of the errors in the various functionalities of the executed application. A framework for automating the proposed methodology has been implemented within a state-of-the-art SystemC/TLM simulation platform for multiprocessor specifications provided with a fault injection engine. The effectiveness of the methodology has been demonstrated in two different case studies, showing how the proposed approach is able to produce an accurate dependability report highlighting the criticalities in both the architecture and the application of the system under design.",
        "title": "10375"
    },
    {
        "abs": "MPSoC platforms offer solutions to deal with communication limitations for multiple cores on single chip, but many new issues arise within the context. The SegBus platform is one of the solutions for application deployment on multi-core applications. There are many applications where identical data is transferred from the same source towards different destinations. Multicast services may come as a performance improving factor for the interconnection platform, together with interrupt service. In this paper, the task is to analyze, how different services can be designed for the SegBus platform and observe the improvement in system performance. The designer can select the services according to the requirements. The running example is represented by the H.264 encoder. The SegBus platform architecture, the communication mechanism, the allocation of processing elements on the platform, the communication services and their implementation are the main topics elaborated here.",
        "title": "10428"
    },
    {
        "abs": "Nowadays, firmware in low-cost microcontrollers (MCUs) must implement cryptographic primitives in order to support practical applications. Effective protections of such implementations against side-channel attacks, especially the differential power analysis (DPA) attack, are still active topic in embedded device security. Low-cost MCUs lack many features, e.g. true random number generators typically used in modern DPA countermeasures. On the other hand, currently even the low-cost MCUs contain several dozens of kilobytes (kB) of program Flash memory not always completely used by a target firmware. In this paper we propose a new countermeasure against the DPA attack. We use randomly assigned general constant-weight codes ( m -of- n codes) for every intermediate value in a secure embedded device. In an ideal hardware, the equal Hamming weight of the data ensures balanced power consumption for any values in the device and thus it complicates the DPA attack. We demonstrate this method on a table based AES cipher and we propose several implementation enhancements to reduce the size of tables to 24 kB/12 kB that are more suitable for practical MCU implementations. We evaluate the performance of the proposed method in terms of speed, memory usage and we test possible side-channel leakages on a system implemented on ARM Cortex-M3 MCU.",
        "title": "10460"
    },
    {
        "abs": "This paper presents a novel and optimized embedded architecture based FPGA for an efficient and fast computation of grey level co-occurrence matrices (GLCM) and Haralick textures features for use in high throughput image analysis applications where time performance is critical. The originality of this architecture allows for a scalable and a totally embedded on Chip FPGA for the processing of large images. The architecture was implemented on Xilinx Virtex-FPGAs without the use of external memory and/or host machine. The implementations demonstrate that our proposed architecture can deliver a high reduction of the memory and FPGA logic requirements when compared with the state of the art counterparts and it also achieves much improved processing times when compared against optimized software implementation running on a conventional general purpose processor.",
        "title": "10475"
    },
    {
        "abs": "The proliferation of embedded vision in today’s life has necessitated the development of System-on-Chips to perform utmost processing in a single chip rather than discrete components. Embedded vision is bounded by stringent requirements, namely real-time performance, limited energy, and adaptivity to cope with the standards evolution. In this article, an energy-aware self-adaptive System-on-Chip for real-time corner detection is realized on Zynq All Programmable System-on-Chip using Dynamic Partial Reconfiguration. A careful analysis of algorithm and efficient utilization of Zynq resources results in highly parallelized and pipelined architecture outperforms the state-of-the-art. A context-aware configuration scheduler application is developed to adhere to operating context and trades off between video resolution and energy consumption to sustain the uttermost operation time for battery-powered devices while delivering real-time performance. The experiments show that the self-adaptive method achieves 1.77 times longer operation time than a parametrized IP core, with negligible reconfiguration energy overhead. A marginal effect of partial reconfiguration overhead on performance is observed, for instance, only two video frames are dropped for HD1080p60 during the reconfiguration time.",
        "title": "10614"
    },
    {
        "abs": "Partial Runtime Reconfigurable (PRTR) FPGAs allow HW tasks to be placed and removed dynamically at runtime. We make two contributions in this paper. First, we present an efficient algorithm for finding the complete set of Maximal Empty Rectangles on a 2D PRTR FPGA. We also present a HW implementation of the algorithm with negligible runtime overhead. Second, we present an efficient online deadline-constrained task placement algorithm for minimizing area fragmentation on the FPGA by using an area fragmentation metric that takes into account probability distribution of sizes of future task arrivals as well as the time axis. The techniques presented in this paper are useful in an operating system for runtime reconfigurable FPGAs to manage the HW resources on the FPGA when HW tasks that arrive and finish dynamically at runtime.",
        "title": "10641"
    },
    {
        "abs": "A novel median filter has been implemented by using a bit-level systolic array. The filter saves more than 23% of transistors when compared with conventional designs. The median filter may be clocked to a maximum 100 MHz for real-time applications. The bit-level systolic array is designed for unlimited word length and extension to larger window sizes.",
        "title": "10726"
    },
    {
        "abs": "The reducing of the width of quantum reversible circuits makes multiple-valued reversible logic a very promising research area. Ternary logic is one of the most popular types of multiple-valued reversible logic, along with the Subtractor, which is among the major components of the ALU of a classical computer and complex hardware. In this paper the authors will be presenting an improved design of a ternary reversible half subtractor circuit. The authors shall compare the improved design with the existing designs and shall highlight the improvements made after which the authors will propose a new ternary reversible full subtractor circuit. Ternary Shift gates and ternary Muthukrishnan–Stroud gates were used to build such newly designed complex circuits and it is believed that the proposed designs can be used in ternary quantum computers. The minimization of the number of constant inputs and garbage outputs, hardware complexity, quantum cost and delay time is an important issue in reversible logic design. In this study a significant improvement as compared to the existing designs has been achieved in as such that with the reduction in the number of ternary shift and Muthukrishnan-Stroud gates used the authors have produced ternary subtractor circuits.",
        "title": "10752"
    },
    {
        "abs": "The design of a transputer link to an RS232 serial interface tram (transputer module) is presented. This enables a transputer to communicate with serial peripherals via a standard Inmos transputer link. The tram uses the Motorola MC68HC805C4 8-bit microcontroller to interface between the transputer link and the RS232 serial line. This solution allows a high level of programming flexibility and can be used in any transputer motherboard which has a spare size one slot. The results of performance measurements are also shown. These show how a shortcoming in the design was detected and rectified.",
        "title": "10785"
    },
    {
        "abs": "This paper presents a microprocessor adapter for ATM networks. Its transmission functions, which are related to the upstream direction, include cell buffering, header error control, cell assembling, rate coupling, and information insertion. The reception functions, which are related to the downstream direction, include information extraction, rate decoupling, cell buffering, header error detection and correction, cell delineation, connection identity fields extraction and identification, cell disassembling and classification, and idle cell discarding. The transmission direction can be supported optionally by a traffic shaper, which is responsible for adapting emitted traffic to ATM network. The microprocessor adapter, which can be used in terminals or in interworking units and switches, implements basic functions of the lower layers of the ATM protocol reference model. It uses three applications specific integrated circuit (ASIC) chips. The three chip-set and other logic can been used to develop a personal computer (PC) adapter for ATM networks. This PC adapter offers bulk data transfer across the ATM network, internet applications over TCP/IP, interactive applications, LAN–ATM interworking facilities using a PC as a router, multimedia and other services.",
        "title": "10841"
    },
    {
        "abs": "Functionality of electronic components in space is strongly influenced by the impact of radiation induced errors which may interfere with the proper operation of the equipment. In space missions, FPGA implementations are generally protected using computationally expensive radiation-error mitigation techniques such as error co rrecting codes (ECC) and triple modular redundancy (TMR). For high-performance systems, such fault tolerance techniques can prove problematic due to both the added computational requirements and their resulting power overhead. As such it is important to make a proper assessment of the expected error rates to make a proper selection of mitigation techniques. This paper provides an extensive overview of the techniques used for determining the necessity of such mitigation techniques in space missions and other situations where a large radiation dose will be encountered. Given the presented study and radiation analysis, in this paper an experimental example is presented in the form of a case study on the Digital Receiver System (DRS) in the Netherlands–China Low-frequency Explorer (NCLE) mission, which is implemented using a Xilinx Kintex-7 SRAM FPGA. Fault rates are estimated for a five-year mission to the second Earth-Moon Lagrange point (L2) and the chosen fault mitigation strategy as implemented in NCLE–DRS is presented. The effect of potential upsets on the functionality of DRS has been taken into account in order to make error estimations more precise. Thus, two test-benches are developed and presented to experimentally evaluate the effect of upsets in FPGA configuration memory and the data on the DRS final outputs. The approach provided in this paper should generalize well to other space missions, as long as a general estimate of the expected radiation environment is available.",
        "title": "10881"
    },
    {
        "abs": "This paper presents the considerations, design and implementation of a DSP-based acoustic feedback canceller system using the TMS320C25 chip. The system consists of a stand-alone unit containing the DSP hardware and built-in firmware in EPROM. The algorithm employs the use of a 256-point DIF FFT for time-to-frequency conversion; subsequently, a potential frequency is identified and a series of tests performed on it to distinguish it from speech. Once identified, a second-order IIR notch filter is invoked, cancelling the acoustic feedback (pure tone) signal, restoring normal operations to the PA system. The system ‘tracks’ any potential acoustic feedback signal, and a notch filter is invoked where necessary thus eliminating the need for manual equalization. The system's functionality has been proven, and represents an achievement in a DSP application to PA systems.",
        "title": "11067"
    },
    {
        "abs": "Superscalar and VLIW processors can both execute multiple instructions each cycle. Each employs a different instruction scheduling method to achieve multiple instruction execution. Superscalar processors schedule instructions dynamically, and VLIW processors execute statically scheduled instructions. This paper quantitatively compares various superscalar processor architectures with a very long instruction word (VLIW) architecture developed at the University of California, Irvine. An architectural overview and performance analysis of the superscalar processor models and VIPER, a VLIW processor designed to take advantage of the parallelizing capabilities of percolation scheduling, are presented. The motivation for this comparison is to study the capability of a dynamically scheduled processor to obtain the same performance achieved by a statically scheduled processor, and examine the hardware resources required by each.",
        "title": "11160"
    },
    {
        "abs": "In this paper, a 6.7-kbps vector sum excited linear prediction (VSELP) coder with less computational complexity is presented. A very efficient VSELP codebook with nine basis vectors and a heuristic K -selection method (to reduce the search space and complexity) is constructed to obtain the stochastic codebook vector. The nine basis vectors are obtained by optimizing a set of randomly generated basis vectors. During the optimization process, we have trained the basis vectors to give the system apriori knowledge of the characteristics of the input. The coder is implemented on a TMS320C541 digital signal processor. The performance is evaluated by testing the 6.7-kbps VSELP coder with different test speech data taken from different speakers. The quality of the coder is estimated by comparing the performance of the 6.7-kbps VSELP coder with an 8-kbps VSELP speech coder based on the IS-54 standards.",
        "title": "11233"
    },
    {
        "abs": "On-chip multiprocessor can be an alternative to the wide-issue superscalar processor approach which is currently the mainstream to exploit the increasing number of transistors on a silicon chip. Utilization of the cache, especially for the remote data is important in the system using such on-chip multiprocessors since the ratio of the off-chip and the on-chip memory access latencies is higher than traditional board-level implementation of the cache coherent non-uniform memory access (CC-NUMA) multiprocessors. We examine two options to utilize the cache resource of the on-chip multiprocessors whose size is restrained by the die area: (1) the instruction and/or private data are only cached at the L1 cache to leave more space on the L2 cache for the shared data; (2) divide cache area into the L2 and the remote victim caches or use all the area for the L2 cache. Results of execution-driven simulations show that the first option improved the performance up to 15%. For the second option, a remote victim cache with 1/8 of the L2 cache size improved three out of four benchmark programs by 4–8%. However, the combination of L2 and victim caches that divide the cache area into two halves of the same size was outperformed by the L2 cache occupying the entire cache area in three out of four benchmark programs.",
        "title": "11270"
    },
    {
        "abs": "The integration of Multi-Processors System-on-Chip (MPSoCs) into the Internet-of-Things (IoT) context brings new opportunities, but also represent risks. Tight real-time constraints and security requirements should be considered simultaneously when designing MPSoCs. Network-on-Chip (NoCs) are specially critical when meeting these two conflicting characteristics. For instance the NoC design has a huge influence in the security of the system. A vital threat to system security are so-called side-channel attacks based on the NoC communication observations. To this end, we propose a NoC security mechanism suitable for hard real-time systems, in which schedulability is a vital design requirement. We present three contributions. First, we show the impact of the NoC routing in the security of the system. Second, we propose a packet route randomisation mechanism to increase NoC resilience against side-channel attacks. Third, using an evolutionary optimisation approach, we effectively apply route randomisation while controlling its impact on hard real-time performance guarantees. Extensive experimental evidence based on analytical and simulation models supports our findings.",
        "title": "11277"
    },
    {
        "abs": "PUFs (Physical Unclonable Function) are increasingly used in proposals of security architectures for device identification and cryptographic key generation. Many PUF designs for FPGAs proposed up to this day are based on ring oscillators (RO). The classical approach is to compare frequencies of ROs and produce a single output bit from each pair of ROs based on the result of comparison of their frequencies. This ROPUF design requires all ROs to be mutually symmetric and also the number of pairs of ROs is limited in order to preserve the independence of bits in the PUF response. This led us to design a new ROPUF on FPGA which is capable of generating multiple output bits from each pair of ROs and is also allowing to create higher number of pairs of ROs, thereby making the use of ROs more efficient than the classical approach. Our PUF design is based on selecting a particular part of a counter value and using it for the PUF output. By applying Gray code on the counter values, we have considerably improved the PUF’s statistical properties. In principle, this PUF design does not need the ROs to be mutually symmetric, however, it is shown that this ROPUF design has significantly better properties with varying supply voltage when symmetric ROs are used. All of the presented measurements were performed on Digilent Basys 2 FPGA Boards (Xilinx Spartan3E-100 CP132). In this work, we provide a more detailed description of the PUF design on FPGA and the behaviour of ROs with varying supply voltage. Our proposed PUF architecture offers more output bits with required statistical properties from each RO pair than the classical approach, where frequencies of ROs are compared. The presented improvements significantly reduce the dependence on fluctuation of supply voltage.",
        "title": "11298"
    },
    {
        "abs": "Current avionics fiber-optic networks utilize light-emitting diode (LED)-based transmitter optical sources owing to their established high reliability over the commercial and military avionics temperature regimes. One shortcoming of LED-based transmitters is the limited modulation speed and low output power of the LED devices. Recently published high temperature reliability data on commercial-off-the-shelf (COTS) diode lasers appears promising for realizing higher speed (i.e. &gt;1 Gb/s) optical interconnects in avionics systems. Although ruggedized hermetic packaging has been developed for avionics optoelectronic modules, the application of newer generation commercial sector packaging to reduce optoelectronic module production costs is under examination. `Glob-top' sealed chip-on-board, passive alignment silicon micro-optical bench, and non-hennetic laser and detector packaging technologies offer potential cost-reductions in avionics optoelectronic modules. However, the reliability of these new laser sources and packaging options has yet to be thoroughly evaluated for the harsh commercial and military avionics environment.",
        "title": "11321"
    },
    {
        "abs": "Existing algorithms can be automatically translated from software to hardware using High-Level Synthesis (HLS), allowing for quick prototyping or deployment of embedded designs. High-level software is written with a single main memory in mind, whereas hardware designs can take advantage of many parallel memories. The translation and optimization of memory usage, and the generation of resulting architectures, is important for high-performance designs. Tools provide optimizations on memory structures targeting data reuse and partitioning, but generally these are applied separately for a given object in memory. Memory access that cannot be effectively optimized is serialized to the memory, hindering any further parallelization of the surrounding generated hardware. In this work, we present an automated optimization method for creating custom cache memory architectures for HLS generated designs. Our optimization uses runtime profiling data, and is performed at a localized scope. This method combines data reuse savings and memory partitioning to further increase the potential parallelism and alleviate the serialized memory access, increasing performance. Comparisons are made against architectures without this optimization, and against other HLS caching approaches. Results are presented showing this method requires 72% of the number of execution cycles compared to a single-cache design, and 31% compared to designs with no caches.",
        "title": "11335"
    },
    {
        "abs": "This paper presents a new fully reconfigurable 2D convolver designed for FPGA-based image and video processors. The proposed architecture operates on image pixels coded with different bit resolutions and varying kernel weights avoiding power and time-consuming reconfiguration. This is made possible by using new SIMD arithmetic modules purposely designed for the new circuit. When optimized for the XILINX VIRTEX device family, the convolver presented in this work requires just 18.4 ms to perform a 5×5 convolution on a 1024×1024 8-bit pixels image and dissipates only 102.1 mW/MHz. The new circuit can be exploited in all the real-time applications in which adaptive convolutions are required and it can be realized also in many other FPGA device families.",
        "title": "11374"
    },
    {
        "abs": "Implementing a function using a programmable logic array (PLA) can often be very expensive in terms of area. Folding rows and/or columns of a PLA usually leads to a reduction in area. In this paper the problem of fault detection in folded PLAs is considered. A new fault, the ‘cutpoint’ fault, is described and universal test sets for the detection of this fault are presented. Modifications to existing built-in universally testable design techniques for nonfolded PLAs are presented; the new designs are now applicable to folded PLAs.",
        "title": "11434"
    },
    {
        "abs": "An 8088 development system is described which combines the advantages of a personal computer (IBM PC), with high-level language support in the form of a c compiler, and a rack-based system. A suite of programs has been developed to enable the c compiler output to be converted into a form suitable for execution on the rack-based target as either ROM- or RAM-resident applications.",
        "title": "11551"
    },
    {
        "abs": "This paper studies the performance of a DRAM component as a function of its structure and the locality of the memory stream. We present a method and a tool for retrieving scalar values for temporal and spatial locality and discuss caches as locality filters. Combinations of cache systems and DRAM configurations having varying number of banks are simulated, and the locality of the DRAM input memory stream is analyzed. The results show that there is a usable amount of locality in the post-cache memory stream, but it is poorly utilized by the current DRAM structures. The developed scalar metrics are found to be suitable for outlining and understanding the DRAM performance. Analyzing locality and considering possibilities to utilize it by DRAMs will become essential in the future, as the DRAM row access time becomes increasingly dominant.",
        "title": "11552"
    },
    {
        "abs": "We present a self-reconfigurable embedded system for a switched beam smart antenna that can steer the beam pattern into a desired direction, handle arithmetic overflow, and respond to user-specified constraints on accuracy and resources. The main component (adaptive beamforming) is implemented as a fully customized hardware in fixed-point arithmetic. By varying the numerical formats and desired beam patterns, we generate a set of hardware configurations, which can then be selected at run-time via a dynamic manager implemented in software. The self-reconfigurable embedded system is implemented and tested on a Programmable System-on-Chip (PSoC). Results are presented in terms of resources, accuracy, and run-time hardware adaptation. This work illustrates the benefits of run-time reconfiguration technology on PSoCs for the implementation of switched-beam smart antennas.",
        "title": "11557"
    },
    {
        "abs": "Power management is an important part of handheld systems such as PDAs, smartphones, and other battery operated digital devices. A handheld system can transition the nodes of a DRAM to low power state to reduce energy consumption. We propose an efficient method for dynamic power management (DPM) of DRAM based on accessed physical addresses. The proposed method also reduces the number of times resynchronization is done. There is no need to collect scattered pages, as in conventional page clustering mechanisms that focus on virtual memory (VM). Simulation result shows that the proposed method reduces Energy ∗ Delay Product by as much as 75% when compared to DRAMs with no DPM.",
        "title": "11561"
    },
    {
        "abs": "This Paper describes the Low Power Non linear Feedback Shift Register (NFSR) for Radio Frequency Identification (RFID) System. RFID systems are widely used in many places for product tracking, monitoring the objects and more. The RFID tag stores its distinctive Electronic Product Code (EPC) with related product information within the tag's memory and encrypts this information before its send to the reader . The RFID tags encrypt the data using Pseudo random numbers. Mostly Linear Feedback Shift Register (LFSR) are used to generating pseudo-random sequences which is less security. Nonlinear Feedback Shift Registers (NFSR) is getting to be more famous in recent years because of the insecurity of LFSR. The output sequence of the LFSR is a linear function of the previous stage, it is easily predictable by intruders. Because of this, NFSR is used in many security systems for generating Pseudo-random numbers. The output sequence of NFSR is irrelevant to the previous stage. In this paper, we proposed a new architecture for NFSR, in this model NFSR is controlled by an LFSR with irregular clocking to generate maximal length sequences. The proposed model is designed using 16 nm CMOS technology and operated in the sub-threshold region. The examination is done using Tanner EDA-Industry Standard design environment. The simulation results demonstrate that the irregular clocking architecture reduces the total power consumption by 30 percent.",
        "title": "11573"
    },
    {
        "abs": "This paper presents an overall view of the architecture and design of a programmable logic controller (PLC). The main objectives of the work are to design, develop, and implement a versatile PLC processor module (PLCPM) based on an industrial open bus architecture called VMEbus (IEEE 1014 Versa Module Euro-standard). The controller is inserted inside the VME crate and controls the industrial process via input and output modules that reside in the crate. The PLCPM is designed to be an intelligent module through the use of a Motorola MC68000 CPU. A method of distributed arbitration protocol, based on an algorithmic state-machine design approach, is added to the design of this module. This facility makes PLCPM suitable to work inside a VMEbus environment. The PLCPM therefore becomes adequate for use in multiprocessing PLC systems. The controller uses a host personal computer (IBM-PC) as a versatile and indispensable system component for process development, monitoring, control and supervision. Software and firmware programs are developed and written for both host-PC and PLCPM using standard C-language and 68000 assembly language, respectively. This results in a sequential control algorithm for the PLCPM and windowing user interface for the host-PC. Ladder diagram programming language is supported by this user interface.",
        "title": "11679"
    },
    {
        "abs": "The Wireless Sensor Network research field has been growing and becoming more mature during the last decade since novel technologies and research lines have emerged targeting its usability under different real scenarios. One of the key topics to assure the efficiency and effectiveness of these technologies in final applications is the quality of the service and the reliability of the whole system, which strongly depends on the communication/topology capabilities as well as routing strategies within the WSN. In this context, it is essential to evaluate the implementation of routing algorithms and network connectivity in actual deployments, as a support to theoretical simulation models that cannot predict certain constraints and limitations in the system behavior. These are the main reasons why a real implementation of a flexible AODV-based routing protocol using a modular HW-SW node platform is proposed in this work, in addition to its practical assessment under real conditions by using a novel in-situ WSN performance evaluation tool. This tool has been created as a support for users during the in-field deployment analysis and diagnosis in real environments, in order to correlate theoretical results with the operation of the network beyond the typical study of routing performance with WSN simulators.",
        "title": "11725"
    },
    {
        "abs": "Task assignment in a heterogeneous multiprocessor is a NP-hard problem, so approximate methods are used to solve the problem. In this paper the Modified Binary Particle Swarm Optimization (Modified BPSO) algorithm and Novel Binary Particle Swarm (Novel BPSO) Optimization are applied to solve the real-time task assignment in heterogeneous multiprocessor. The problem consists of a set of independent periodic task, which has to be assigned to a heterogeneous multiprocessor without exceeding the utilization bound. The objective is to schedule maximum number of tasks with minimum energy consumption. The execution times and deadlines of the tasks are assumed to be known. Here Modified BPSO performance is compared with Novel BPSO and Ant Colony Optimization algorithm (ACO). Experimental results show that Modified BPSO performs better than Novel BPSO and ACO for consistent utilization matrix and ACO performs better than Modified BPSO and Novel BPSO for inconsistent utilization matrix.",
        "title": "11791"
    },
    {
        "abs": "The 80386 is a high-performance third-generation microprocessor that is now standard in most top-of-the-range PCs. Like all similar processors operating at clock rates above 30 MHz, the 80386 must use cache memory if it is to operate efficiently. Without cache memory, the user must either pay a very high price for very fast RAM or employ slower memory by introducing wait states. This application note describes the 80386 bus interface and demonstrates how it can be interfaced to IDT cache tag RAMs to create a cache system. Although the report describes a relatively basic cache system, it covers all design considerations ranging from system timing to the programming of the PALs needed to implement the interface. A.C.",
        "title": "11857"
    },
    {
        "abs": "NCR 52832 (Figure 1) and 52864 5-V-only E2PROMS can be used in applications where occasional insystem programmability and long-term data retention without power (non-volatility) are needed. Applications include the storage of programmable character fonts in intelligent terminals, storage of data or firmware in remote systems that need periodic updating, user-programmable video games, and RAM backup.",
        "title": "11888"
    },
    {
        "abs": "Tomorrow’s Micro-Air-Vehicles (MAVs) could be used as scouts in many civil and military missions without any risk to human life. MAVs have to be equipped with sensors of several kinds for stabilization and guidance purposes. Many recent findings have shown, for example, that complex tasks such as 3-D navigation can be performed by insects using optic flow (OF) sensors although insects’ eyes have a rather poor spatial resolution. At our Laboratory, we have been performing electrophysiological, micro-optical, neuroanatomical and behavioral studies for several decades on the housefly’s visual system, with a view to understanding the neural principles underlying OF detection and establishing how OF sensors might contribute to performing basic navigational tasks. Based on these studies, we developed a functional model for an Elementary Motion Detector (EMD), which we first transcribed into electronic terms in 1986 and subsequently used onboard several terrestrial and aerial robots. Here we present a Field Programmable Gate Array (FPGA) implementation of an EMD array , which was designed for estimating the OF in various parts of the visual field of a MAV. FPGA technology is particularly suitable for applications of this kind, where a single Integrated Circuit (IC) can receive inputs from several photoreceptors of similar (or different) shapes and sizes located in various parts of the visual field. In addition, the remarkable characteristics of present-day FPGA applications (their high clock frequency, large number of system gates, embedded RAM blocks and Intellectual Property (IP) functions, small size, light weight, low cost, etc.) make for the flexible design of a multi-EMD visual system and its installation onboard MAVs with extremely low permissible avionic payloads.",
        "title": "11914"
    },
    {
        "abs": "This paper proposes a cost-effective solution to the virtual cache synonym problem. In the proposed solution, a minimal hardware addition guarantees correct handling of the synonym problem whereas a simple modification to the virtual-to-physical address mapping in the operating system optimizes the performance. The key to the proposed solution is a small, physically-indexed cache called a U-cache . The U-cache maintains the reverse translation information of cache blocks that belong to unaligned virtual pages, where unaligned means that the lower bits of the virtual page number that are used to index the virtual cache do not match those of the corresponding physical page number. The biggest advantage of the U-cache approach is that it leaves room for software optimization in the form of mapping alignment. Performance evaluation based on memory reference traces from a real system shows that the U-cache, with only a few entries, performs almost as well as (in some cases outperforms) a fully-configured hardware-based solution when more than 95% of mappings are aligned.",
        "title": "11965"
    },
    {
        "abs": "This paper deals with the idea of implementing a complete network service on a chip. Herein, we propose an original design together with an efficient implementation of an authoritative domain name system (DNS) server on a Virtex 5 FPGA circuit. The proposed approach exploits the use of a hardware accelerator, MicroBlaze soft-processor cores and an adequate mapping between the DNS specifications and the hardware architecture leading to a Multi-Processor System on Chip (MPSoC). We propose new architectures by translating the DNS specifications to hardware form through the “Specification and Description Language” (SDL) tool. The proposed implementation allows significant reduction in power consumption together with significant performance and security improvement. The proposed architectures have been successfully implemented and tested on an actual network. The obtained results show a query performance improvement of around 200% with respect to the “Berkeley Internet Name Domain” (BIND) 9 server.",
        "title": "12042"
    },
    {
        "abs": "Design and reuse has become a very common practice in the electronics design industry. IP cores are easily sold by designers to system integrators. However, several cases of counterfeiting and illegal copying have been reported and design protection techniques have been developed in response. Among these techniques, we focus on modifications at logic level aimed at active design protection. This is the first paper to provide a formal description and definition of the following techniques used to protect integrated circuits and IP cores against theft, counterfeiting, cloning and illegal copy: logic encryption, logic obfuscation, logic masking, and logic locking. In the second part of the paper, we present a new technique to insert gates in the data path of a logic circuit in order to lock it. Based on graph analysis, this method involves low overhead implementation and is more than ten thousand times faster than former fault analysis-based logic masking techniques when it comes to selecting the nodes to modify. Finally, we discuss the design requirements of a strong design protection scheme.",
        "title": "12080"
    },
    {
        "abs": "An efficient VLSI architecture for minimized sorting network (Vasanth sorting) in rank ordering application to remove salt and pepper noise is proposed. The basic operation in salt and pepper noise removal is rank ordering. In this work, a novel 2D sorting technique referred to as Vasanth sorting is proposed for a fixed 3 × 3 window. Vasanth sorting requires only 25 comparators to sort 9 elements of the window. A parallel architecture is developed for Vasanth sorting with 25 comparators. The processing element of the parallel architecture is an 8-bit data comparator (Two cell comparator). The performance of the proposed sorting technique is compared with the different sorting technique which is targeted for XCV1000-5bg560 on XILINX 7.1i and xc7v2000t-2flg1925 Xilinx 14.7 project manager respectively with Modelsim 10.4a for simulation and XST compiler tool for synthesis using VHDL. It was found that the parallel architecture developed for Vasanth sorting requires the only ¼ of the area in FPGA when compared to existing sorting techniques. The combinational delay of the proposed architecture was also twice as less like their counterparts. The power consumption of the logic was 7mw. Hence the above performances make Vasanth sorting a better choice compared to existing techniques for rank ordering.",
        "title": "12098"
    },
    {
        "abs": "In this paper, we investigate the feasibility of a 2-Dimensional Optical Code Division Multiple Access (2D-OCDMA) system, with electronic coding and decoding functions. We develop a modified construction method of Multi-Wavelength Optical Orthogonal Codes (MWOOC) that permits high flexibility in the code parameters choice. The 2D code performance is calculated for a Conventional Correlation Receiver (CCR) and a more complex one, named Parallel Interference Cancellation (PIC) receiver. For example, for a Bit Error Rate (BER) ⩽10−9, and 30 simultaneous users, we show that contrary to the CCR, the use of a PIC receiver leads to workable 2D electric coding solutions.",
        "title": "12120"
    },
    {
        "abs": "Personal computers are becoming almost as common as pocket calculators. In particular, their use in laboratories for measurement and control is growing. The next step after computer-controlled instruments is the development of a personal computer with an instrument as a component part. An automatic pH titrator has been designed. The system presented is based on an Apple Ile. Two plug-in cards are used. One card integrates analogue measurement and analogue-to-digital conversion. The other card controls a syringe driven by a stepper motor. The software control system for the personal pH meter is also described.",
        "title": "12151"
    },
    {
        "abs": "The promising advent of Fully Electric Vehicles (FEVs) also means a shift towards fully electrical control of the existing and new vehicle functions. In particular, critical X-by-wire functions require sophisticated redundancy solutions. As a result, the overall Electric/Electronic (E/E) architecture of a vehicle is becoming even more complex and costly. The SafeAdapt project provides an integrated approach for engineering such adaptive, complex and safe systems, ranging from tool chain support, reference architectures, system modeling and networking, up to early validation and verification. In this paper, we give an overview of the SafeAdapt project methodology. We also describe a particular aspect of the project which is the validation of the system adaptive behavior. To validate the adaptive behavior of a vehicle system, an architecture description language for automotive embedded systems (i.e. EAST-ADL) is used for designing the system. The system design model is then used for generating the embedded software. To ensure that the system behaves correctly at runtime, its adaptive behavior is analyzed using fault injection and monitoring techniques on a virtual platform.",
        "title": "12218"
    },
    {
        "abs": "A personal computer based static fault detection system for digital integrated circuits (IC) is described. The system detects functional as well as logical faults and declares a ‘go/no go’ condition for ICs. The process of developing a database containing test vectors and control vectors for testing various ICs is illustrated by examples. The system interface hardware is simple to implement and its Microsoft C based software is flexible to accomodate expansion of the database for new ICs. The application of the testing scheme can be extended to test digital circuits constructed on printed circuit boards.",
        "title": "12281"
    },
    {
        "abs": "The VMS bus was conceived as a high-speed serial bus for passing short, urgent messages within a VME backplane. It has been extended to allow for communication between two or more VME backplanes or ‘crates’, but it can also be used to link computers with dissimilar buses. The paper compares VMS with other serial bus standards, and shows how transmission errors on the bus can be detected. A method for linking the STEbus to VME systems using the VMS bus is described; this method can be extended to link other buses such as the IBM PC bus.",
        "title": "12406"
    },
    {
        "abs": "The BOAR emulation system is targeted to hardware/software (HW/SW) codevelopment of advanced embedded DSP and telecom systems. The challenge of the BOAR system is efficient customization of programmable hardware, and dedicated partitioning routine to target applications and structures, which allows quite high overall system performance. The system allows multiple configurations for communication between processors and field programmable gate arrays (FPGAs) making the BOAR system an efficient tool for real-time HW/SW coverification. The reprogrammable hardware of the emulation tool is based on four Xilinx 4000-series devices, two Texas TMS320C50 signal processors and one Motorola MC68302 microcontroller. With current devices the BOAR hardware provides approximately 40–70 kgates of logic capacity in DSP applications. The emulation capacity can be expanded by connecting several similar boards in chain. The system has also a versatile internal reprogrammable test environment for test bench development, performance evaluations and design debugging. The logic development environment is based on the Synopsys synthesis tools and an automatic design management software, which performs resource mapping and performance-driven design partitioning between FPGAs. The emulation hardware is currently connected to logic and software development environments via an RS-232C bus. The BOAR emulation system has been found a very efficient platform for real-life prototyping of different types of DSP algorithms and systems, and validating correct functionality of a VHDL macro library.",
        "title": "12442"
    },
    {
        "abs": "The fractional Fourier transform is a time–frequency distribution and an extension of the classical Fourier transform. There are several known applications of the fractional Fourier transform in the areas of signal processing, especially in signal restoration and noise removal. This paper provides an introduction to the fractional Fourier transform and its applications. These applications demand the implementation of the discrete fractional Fourier transform on a digital signal processor (DSP). The details of the implementation of the discrete fractional Fourier transform on ADSP-2192 are provided. The effect of finite register length on implementation of discrete fractional Fourier transform matrix is discussed in some detail. This is followed by the details of the implementation and a theoretical model for the fixed-point errors involved in the implementation of this algorithm. It is hoped that this implementation and fixed-point error analysis will lead to a better understanding of the issues involved in finite register length implementation of the discrete fractional Fourier transform and will help the signal processing community make better use of the transform.",
        "title": "12538"
    },
    {
        "abs": "Aggressive scaling of the CMOS process technology allows the fabrication of highly integrated chips, and enables the design of the network-on-chip (NoC). However, it also leads to widespread reliability problems. A reliable NoC system must operate normally even in the face of a lot of transistor failures. Aiming towards permanent faults on communication links, we introduce a fault-tolerant MPI-like communication protocol. It detects the link failure if there exist unresponsive requests and automatically starts the new path exploration. The region flooding algorithm is proposed to search for a fault-free path and reroute packets to avoid system stalls. The experimental result shows our approach significantly reduces the latency compared with the basic flooding algorithm. The maximum latency reduction is 25% under the bit complement traffic pattern. Also, it brings only 2% fault tolerance loss.",
        "title": "12552"
    },
    {
        "abs": "The demands of many applications are such that performance is a key goal, but this requirement is typically difficult to achieve as the fastest devices demand the fastest memory, and hence an unrealistic system cost. This paper discusses the basic philosophy of RISC and the requirements that it imposes on the system architecture and the application software. In particular the AMD Am29000 processor is analysed in detail, with emphasis on achieving around 20 Vax MIPS with relatively slow 45 ns SRAM.",
        "title": "12626"
    },
    {
        "abs": "A parallel processing simulator, NEWTS, has been developed for network systems. This simulator consists of a number of microprocessors, which are interconnected with one another by hierarchical common buses and which excute simulation programs concurrently. This simulator is far cheaper than a general-purpose large computer and enables carrying out efficient and high speed network simulations. This simulator has been applied to a telephone network model, and it's usefulness confirmed. This paper describes an outline of the NEWTS hardware and software.",
        "title": "12650"
    },
    {
        "abs": "We propose a new methodology for Built-In Self-Test (BIST) where contrary to the traditional scan-path based Logic BIST, the proposed solution for test generation does not need any additional hardware, and will not have any impact on the working performance of the system. A class of digital systems organized as pipe-lined signal processing architectures is targeted. The on-line generated signal data used for processing in the system serve as test pattern sources. Testing under normal working conditions and with typically processed data, allows exercising of the system on-line and at-speed, facilitating the detection of dynamic faults like delays and cross-talks to achieve high test quality. The proposed new self-test method is free from the negative aspect of over-testing, compared to the traditional Logic BIST approaches, and uses minimal amount of added hardware. Experimental research was based on the case study of specialized bio-signal processor architecture. The experiments showed promising results in reducing the cost of testing and achieving high fault coverage.",
        "title": "12709"
    },
    {
        "abs": "In order to use a multiprocessor effectively it is necessary to develop efficient techniques for the assignment of the computations to the processors available. We must previously be capable of identifying all the computations that can be executed in parallel. In this paper we present a procedure for the efficient programming of a distributed memory, message-passing hypercube multicomputer. This procedure is applied to a sequential program and results in a parallel program which can be expressed in a parallel language. We also present a language with these characteristics called ACLAN (Array C LANguage), as well as the software package ACLE (Array C Language Emulator) for the simulation of ACLAN programs in sequential computers. We have satisfactorily applied the parallelization procedure to a large number of algorithms in the fields of matrix algebra and image processing. We present the results obtained.",
        "title": "12744"
    },
    {
        "abs": "The support of fast, deterministic, compact and reliable task schedulers is seen as an essential aspect of modern embedded hard real-time systems. This paper describes the design, development and use of a dedicated microcontroller-based task scheduler. The co-processor is based on an Intel 8032 microcontroller, being designed for use with most modern target processors. A variety of scheduling algorithms are supported, including round robin, fixed priority and deadline scheduling schemes.",
        "title": "12797"
    },
    {
        "abs": "This paper describes parallel processing architectures for advanced signal processing. Advanced architectures are those that make use of nonlinear functions such as reciprocal, square root, arctangent etc. To accomplish this, a nonlinear cell has been devised which performs any one of four such nonlinear functions in two clock cycles. The paper focuses on typical advanced algorithms encountered in signal processing, such as Givens rotation, L-U decomposition, Cholesky update and direct Cholesky factorization. The use of our high-speed multi-function nonlinear cell can lead to a several-fold higher throughput over the customary software based approaches.",
        "title": "12812"
    },
    {
        "abs": "A complete design of a small ‘private automatic branch exchange’ (PABX) system serving 32 subscribers is introduced, using the simplest and most direct design procedures. The system uses space division analogue switching concepts and allows four internal and two external calls at the same time. A logical design is first carried out to give the evolution of the state of a subscriber during call processing, followed by a complete design of the microprocessor circuits. All external circuits responsible for counting and storing dialled digits, generating telephone tones and interfacing are described. All programs and subroutines are discussed and several flowcharts are given to describe the controlling program. Finally the advantages, disadvantages and the system limitations are discussed. The design uses an Intel 8080 microprocessor with 2 kbyte of EPROM and 256 byte of RAM.",
        "title": "12824"
    },
    {
        "abs": "Recent works have proven the functionality of electrostatically controlled graphene p–n junctions that can serve as basic primitive for the implementation of a new class of compact graphene-based reconfigurable multiplexer logic gates. Those gates, referred as RG-MUXes, while having higher expressive power and better performance w.r.t. standard CMOS gates, they also have the drawback of being intrinsically less power/energy efficient. In this work we address this problem from a circuit perspective, namely, we revisit RG-MUXes as devices that can operate adiabatically and hence with ultra-low (ideally, almost zero) power consumption. More specifically, we show how to build basic logic gates and, eventually, more complex logic functions, by appropriately interconnecting graphene-based p–n junctions as to implement the adiabatic charging principle. We provide a comparison in terms of power and performance against both adiabatic CMOS and their non-adiabatic graphene-based counterparts; characterization results collected from SPICE simulations on a set of representative functions show that the proposed ultra-low power graphene circuits can operate with 1.5–4 orders of magnitude less average power w.r.t. adiabatic CMOS and non-adiabatic graphene counterparts respectively. When it comes to performance, adiabatic graphene shows 1.3 (w.r.t. adiabatic CMOS) to 4.5 orders of magnitude (w.r.t. non-adiabatic technologies) better power-delay product.",
        "title": "12859"
    },
    {
        "abs": "We show a tool supporting efficient model checking of LOTOS programs. LOTOS is a well-known specification language for concurrent and distributed systems. The main functionality of the tool is the syntactic reduction of a program with respect to a logic formula expressing a property to be checked. The method is useful to reduce the state-explosion problem in model checking. The tool is integrated with the Concurrency Workbench of North Carolina. The tool also supports a windows user interface.",
        "title": "12947"
    },
    {
        "abs": "Part 1 of the paper1 outlined the concepts behind the ISO/OSI reference model and how this framework has been used by the MAP initiative to select and define a communication protocol suitable for manufacturing environments. Part 2 considers the background organization supporting MAP, describes the relationship between MAP and other standardization initiatives, outlines some of the features of emerging MAP products and anticipates ways in which MAP products will be used to develop integrated manufacturing facilities.",
        "title": "12951"
    },
    {
        "abs": "The architectural model for the support of orthogonal persistence proposed in this paper is based on persistent modules (both for code and files) uniformly structured according to the information-hiding principle, persistent processes based on the procedure calling model, and persistent module capabilities. It is shown how this combination of features can provide a strong basis for very secure operating systems in terms of user authentication, file security, directory security and communication between users. An implementation based on the MONADS-PC computer is briefly described.",
        "title": "12967"
    },
    {
        "abs": "Network-on-Chip (NoC) architectures have been adopted by chip multi-processors (CMPs) as a flexible solution to the increasing delay in the deep sub-micron regime. However, the shrinking feature size limits the performance of NoCs due to power and area constraints. In this paper, we propose three 3D floorplanning methods for a Triplet-based Hierarchical Interconnection Network (THIN) which is a new high performance NoC. The proposed floorplanning methods use both Manhattan and Y-architecture routing architectures so as to improve the performance, reduce the power consumption and area requirement of THIN. A cycle accurate simulator was developed based on Noxim NoC simulator and ORION 2.0 energy model. The proposed floorplanning methods show up to 24.69% energy and 8.84% area reduction at best compared with 3D Mesh. Our analysis concludes that THIN is not only a feasible but also a low-power and area-efficient NoC at physical level.",
        "title": "13033"
    },
    {
        "abs": "AES (Advanced Encryption Standard) is one of the most popular symmetric key encryption algorithms. S-box (Substitution block) is main block in AES. In contrast to many previous works which have employed only one of memory or non-memory based approaches to implement S-box, we propose efficient methods by combining these approaches. We perform area-delay efficient multipliers and multiplicative inverters in [formula omitted]. We employ loop-unrolling, fully pipelining, and sub-pipelining techniques in all proposed methods. Moreover, we insert registers of pipelining in optimal placements. These reasons demonstrate that proposed methods not only try to keep the advantages of previous works but also try to decrease their disadvantages. By using above techniques, we propose three high-throughput AES implementations in ECB mode and one ultra-high throughput AES implementation in CTR mode. Our AES implementations in ECB mode using Xilinx Virtex-5 (XC5VLX85-FF676-3) and Virtex-6 (XC6VLX240T-FF784-3) FPGAs achieve high throughputs of 82.4 Gbps and 102.9 Gbps and maximum operational frequencies of 644.33 MHz and 803.98 MHz respectively. Compared to the best previous works, these implementations improve data throughput by [formula omitted] and [formula omitted]. Our AES implementation in CTR mode on Xilinx Virtex-6 (XC6VLX240T-FF784-3) FPGA achieves a high throughput of 260.15 Gbps and maximum operational frequency of 508.104 MHz.",
        "title": "13061"
    },
    {
        "abs": "In this work, a reversible single precision floating-point square root is proposed using modified non-restoring algorithm. To our knowledge, this is the first work proposed for floating-point square root using reversible logic. The main block involved in the implementation of reversible square root using modified non-restoring technique is Reversible Controlled-Subtract-Multiplex. Further, optimized Reversible Controlled-Subtract-Multiplex blocks are introduced in order to minimize the number of reversible gates used, number of constant inputs used, number of garbage outputs produced as well as the quantum cost. The proposed reversible single precision floating-point square root is realized using an 8-bit reversible adder, an 8-bit and a 25-bit reversible shift register, 12-bit reversible unsigned square root, 6-bit reversible unsigned square root, 4-bit reversible unsigned square root, 3-bit reversible unsigned square root and ten 1-bit reversible unsigned square root units.",
        "title": "13064"
    },
    {
        "abs": "Realtime, multiprocessor embedded systems are one application area where response times, throughput, reliability and fault-tolerance constitute the major design criteria. Hence the distribution and management of the application software is a critical function. A prototype loosely-coupled multiprocessor system has been designed and implemented for use in fault-tolerant realtime applications. This paper discusses the organization and structure of the total system, concentrating in particular on the software requirements of the communication and executive (kernel) functions. The communication system is based on a token passing bus protocol for use with single-board computers connected via a fast parallel bus. The kernel is designed to support functional partitioning of application programs, and can be implemented using standard compilers. No special multiprocessing features are required. Most of the software for this system has been written in modula-2 , though assembly language programming has been used in a few specialized areas.",
        "title": "13088"
    },
    {
        "abs": "The Newcastle Connection is a transparent layer of software designed to make several Unix systems appear as one. Based on Ethernet, it opens up applications not normally associated with Unix-system networks. This paper describes the structure of the Newcastle Connection. System calls, name-neighbour space, address space, facilities for ‘super-users’, remote system access and large-scale administration are covered.",
        "title": "13253"
    },
    {
        "abs": "The design philosophy behind the 16-bit operating system, MS-D05 2.0, supplied by Microsoft, is outlined. MS-DOS has grown in strength since it was launched on the market in 1981, to the point where it is now possibly the most widely used 8086-based operating system in the world. The reasons behind its success, and the facilities it can offer the applications developer are also covered in this article.",
        "title": "13317"
    },
    {
        "abs": "In this paper we propose a new general purpose VLSI architecture called ring-connected trees (RCT) for parallel processing. RCT requires less hardware in terms of processing elements and connecting links compared to a mesh-of-tree of comparable size and its diameter is less than that of mesh. It requires less chip area, less maximum edge length and crossing number compared to those required by mesh-of-tree [1] [F.T. Leighton, Layout for the shuffle-exchange graph and lower bound techniques for VLSI, Ph.D. dissertation, Department of Mathematics, MIT 1981] under the Grid model of Thompson [2] [C.D. Thompson, Area-time complexity for VLSI. Technical report, Division of Computer Science, University of California, Berkeley, CA, January 1984]. By using spare PEs and links, RCT is made to tolerate multiple faults. Suitability of this architecture for multipurpose applications is demonstrated by designing parallel version of algorithms for a number of common computational problems. This structure requires linear and sublinear time for these algorithms and this is quite reasonable considering the simpler nature of the architecture.",
        "title": "13412"
    },
    {
        "abs": "In the laboratory, a designer may conveniently analyse microprocessor circuits and their peripherals by means of suitable logic analysers. This is possible because he will have a detailed knowledge of his system and the time to learn the optimum methods of using his analyser. Once this system is a part of a product, it would be preferable if a less complicated means of verifying the processor's correct activity were available should a failure occur. An extremely convenient and inexpensive verification and diagnostic system may be designed into a product using two complementary techniques, selftest and signature analysis. The material cost of implementing such a system can be very small indeed whereas the savings in troubleshooting time and test equipment costs can be substantial.",
        "title": "13421"
    },
    {
        "abs": "The trend toward higher integration and expanded functions of microprocessors has become increasingly important in the design of workstations and embedded controllers. The paper outlines the Gmicro F32 family of microprocessors — F32/300, F32/200 and F32/100 — together with the peripheral devices such as the floatingpoint unit, DMA controller, interrupt request controller and cache memory parts. Software support and development tools are considered.",
        "title": "13548"
    },
    {
        "abs": "As technology advances, the number of cores in Chip MultiProcessor systems and MultiProcessor Systems-on-Chips keeps increasing. The network must provide sustained throughput and ultra-low latencies. In this paper we propose new pipelined switch designs focused in reducing the switch latency. We identify the switch components that limit the switch frequency: the arbiter. Then, we simplify the arbiter logic by using multiple smaller arbiters, but increasing greatly the switch area. To solve this problem, a second design is presented where the routing traversal and arbitrations tasks are mixed. Results demonstrate a switch latency reduction ranging from 10% to 21%. Network latency is reduced in a range from 11% to 15%.",
        "title": "13556"
    },
    {
        "abs": "In June of this year, the ARINC 629 digital communication system began supporting revenue service as the primary means of digital communication on the 777, Boeing's first fly-by-wire aircraft. This achievement comes as a result of almost two decades of development effort at Boeing and its suppliers. Increasing demands for information exchange between aircraft systems necessitated development of high-speed, high-reliability communication data buses. Development costs for such systems are extensive, and reuse in future airplane designs is highly desirable. ARINC 629, a working example of such a data bus development program, has the capability to support applications beyond the 777 with extensive benefits to be gained in development, integration and certification costs. This paper begins with a brief working description of the ARINC 629 digital communication system, touching on both current mode and fiber-optic implementations. It further describes the architecture utilized on the 777 and the validation effort required to support certification of a new data bus and new airplane systems architecture. Performance experience during the 777 flight test program and a brief in-service history are discussed and, finally, efforts in several fiber-optic in-service evaluation programs are presented.",
        "title": "13573"
    },
    {
        "abs": "Logic Built-In Self Test (LBIST) is a popular technique for applications requiring in-field testing of digital circuits. LBIST incorporates test generation and response-capture on-chip. It requires no interaction with a large, expensive tester. LBIST offers test time reduction due to at-speed test pattern application, makes possible test data re-usability at many levels, and enables test-ready IP. However, the traditional pseudo-random pattern-based LBIST often has a low test coverage. This paper presents a new method for on-chip generation of deterministic test patterns based on registers with non-linear update. Our experimental results on 7 real designs show that the presented approach can achieve a higher stuck-at coverage than the test point insertion with less area overhead. We also show that registers with non-linear update are asymptotically smaller than memories required to store the same test patterns in a compressed form.",
        "title": "13589"
    },
    {
        "abs": "The relationship between technology and healthcare due to the rise of intelligent Internet of Things (IoT), Artificial Intelligence (AI), and the rapid public embracement of medical-grade wearables has been dramatically transformed in the past few years. AI-powered IoT enabled disruptive changes and unique opportunities to the healthcare industry through personalized services, tailored content, improved availability and accessibility, and cost-effective delivery. Despite these exciting advancements in the transition from clinic-centric to patient-centric healthcare, many challenges still need to be tackled. The key to successfully unlock and enable this horizon shift is adopting hierarchical and collaborative architectures to provide a high level of quality in key attributes such as latency, availability, and real-time analytics. In this paper, we propose a holistic AI-driven IoT eHealth architecture based on the concept of Collaborative Machine Learning approach in which the intelligence is distributed across Device layer, Edge/Fog layer, and Cloud layer. This solution enables healthcare professionals to continuously monitor health-related data of subjects anywhere at any time and provide real-time actionable insights which ultimately improves the decision-making power. The feasibility of such architecture is investigated using a comprehensive ECG-based arrhythmia detection case study. This illustrative example discusses and addresses all important aspects of the proposed architecture from design implications such as corresponding overheads, energy consumption, latency, and performance, to mapping and deploying advanced machine learning techniques (e.g., Convolutional Neural Network) to such architecture.",
        "title": "13665"
    },
    {
        "abs": "Using Yourdon's realtime structured analysis and structured design method (RTSA/SD) for the development of realtime software for embedded microcontroller targets is discussed. The main emphasis of the discussion focusses on the runtime environment necessary to support RTSA/SD state machines and transformations. The possibility of using an existing SDL oriented operating system as a runtime environment is analysed. As an example of using RTSA/SD, an asynchronous communication link protocol is partially modelled and its implementation discussed.",
        "title": "13829"
    },
    {
        "abs": "The increasing number of applications implemented on modern vehicles leads to the use of multi-core platforms in the automotive field. As the number of I/O interfaces offered by these platforms is typically lower than the number of integrated applications, a solution is needed to provide access to the peripherals, such as the Controller Area Network (CAN), to all applications. Emulation and virtualization can be used to implement and share a CAN bus among multiple applications. Furthermore, cyber-physical automotive applications often require time synchronization. A time synchronization protocol on CAN has been recently introduced by AUTOSAR. In this article we present how multiple applications can share a CAN port, which can be on the local processor tile or on a remote tile. Each application can access a local time base, synchronized over CAN, using the AUTOSAR Application Programming Interface (API). We evaluate our approach with four emulation and virtualization examples, trading the number of applications per core with the speed of the software emulated CAN bus.",
        "title": "13858"
    },
    {
        "abs": "The deployment problem (DP) of a wireless sensor network (WSN) is no doubt a critical issue because the strategies it takes will not only strongly impact the overall performance but also the power consumed by the sensors in such a system. This means that a good solution to the deployment problem (DP) cannot just “enhance its performance,” it can also “save the energy” to prolong the lifetime of a wireless sensor network (WSN). However, finding an optimum solution for most DPs with limited computation resources is still a challenging research problem, especially for the NP-hard optimization problems. Compared to exhaustive search and deterministic algorithms, metaheuristics provides an alternative way to solve these optimization problems, by looking for a near optimal solution using limited computation resources within a reasonable time. Aimed at providing not only a systematic survey of metaheuristics for solving the deployment problem but also a roadmap for researchers working on WSN and metaheuristics to further develop and improve a wireless sensor network system, this paper begins with an overview of WSN and the deployment problem, followed by discussions on metaheuristics and how to use them to solve the DP. Then, a comprehensive comparison between metaheuristics for the DP is given. Finally, open issues and future trends of this field are addressed.",
        "title": "13893"
    },
    {
        "abs": "This paper presents the implementation of the coarse-grained reconfigurable architecture (CGRA) DART with on-line error detection intended for increasing fault-tolerance. Most parts of the data paths and of the local memory of DART are protected using residue code modulo 3, whereas only the logic unit is protected using duplication with comparison. These low-cost hardware techniques would allow to tolerate temporary faults (including so called soft errors caused by radiation), provided that some technique based on re-execution of the last operation is used. Synthesis results obtained for a 90 nm CMOS technology have confirmed significant hardware and power consumption savings of the proposed approach over commonly used duplication with comparison. Introducing one extra pipeline stage in the self-checking version of the basic arithmetic blocks has allowed to significantly reduce the delay overhead compared to our previous design.",
        "title": "13932"
    },
    {
        "abs": "Gallium arsenide (GaAs) is now being considered by the semiconductor industry as a serious challenger to the dominance of silicon in high-speed digital circuits. In addition, with the accelerated progress in developing GaAs ICs for microwave and optoelectronics applications, major future markets are projected. This paper surveys the product areas and associated system applications for GaAs and related III-V compound semiconductor devices and ICs. Particular attention is given to the three main IC product areas of digital, analogue and optoelectronics and their applications to both military and civil systems. The strongest market currently is for military analogue devices. The projected digital market in the 1990s, however, dominates the other IC product areas.",
        "title": "13991"
    },
    {
        "abs": "We present a specification composition technique for improving the reliability of message passing applications composed by the Ensemble methodology. In Ensemble, applications are built by composing reusable executable program components designed with scalable communication interfaces. The composition is controlled by scripts. We define reusable specification components associated to program components, as well as their composition directed by the same Ensemble scripts, thus obtaining specifications of applications. We propose an extension of coloured Petri nets, which is used to define specification components. Composed specifications and applications may be validated or verified by available tools.",
        "title": "14003"
    },
    {
        "abs": "Power utilization assumes a massive part in any of the integrated circuits, and it’s rundown as a standout amongst essential difficulties in the universal innovation guide into semiconductors. Generally in integrated circuit, flip-flop and clock distribution system consume a lot of energy since they make and utilize the most extreme number of internal transitions. In the clock distribution system, the clock signal circulates from a typical point to every one of the components that required for the circuit. However this capacity is more important to the synchronous framework, much consideration needs to provide for the attributes of these clock signals. In the sequential circuits, a clock distribution system spends a lot of power given the high operating frequency of high capacitance. An existing approach to reducing the limits of a clock signal is based on the quantity of clocked transistors. In this, an advanced procedure is proposed and evaluated by utilizing Dual-Edge Triggered Flip-Flop (DETFF) depends on the Dynamic Signal Driving (DSD) strategy. This DETFF is executed in sequential circuits that have been ordered using Tanner Electronic Design Automation (EDA) tool which is used to simulate and examine control by using Dynamic Signal Driving (DSD) strategy. The outcomes demonstrate that the total power utilization is decreased in sequential benchmark circuit design. A number of Flip flops have been designed by various technologies such as reducing area, delay, and power, but this proposed dynamic signal driving scheme can be used for any integrated circuit- that can be reduced to all these three parameters to give the best trade-off for a particular ASIC platform.",
        "title": "14063"
    },
    {
        "abs": "The adoption of custom microelectronics by an original equipment manufacturer (OEM) for a new or improved company product is not without risk that the overall design procedure may have problems. This is particularly possible during the first-time adotion by a manufacturer, where no prior company experience of potential pitfalls is available. Among the difficulties with custom microelectronics is that several parties are involved in the chip design and fabrication procedure, with potential misunderstandings and delays across these interfaces. Final proof that the chip design exactly matches the product requirements also has to wait for delivery of prototype ICs from the vendor. The paper therefore considers the various risk factors which the OEM should appreciate when deciding to use, or not use, a custom IC in a new product, the final company decision requiring an educated managerial assessment of the different design styles which can be used.",
        "title": "14078"
    },
    {
        "abs": "We present a hardware architecture for real-time digital video stabilization for high-performance embedded systems. The stabilization algorithm analyzes the current and past video frames and obtains a motion estimation vector, which is then filtered to isolate unwanted camera movements from intentional panning. The vector is then used to correct the output video frame. The paper describes our hardware architecture for motion estimation, filtering and correction and its implementation on a Xilinx Spartan-6 LX45 FPGA. We evaluate our results on several benchmark video sequences, both visually and using the Inter-frame Transformation Fidelity index (ITF). Running on the [formula omitted]-pixel video output of an infrared camera, our FPGA implementation successfully compensates involuntary camera motion at a maximum throughput of 104.15 frames per second with a 100 MHz clock. The power consumption added to the FPGA by the image stabilization core is only 24.16 mW.",
        "title": "14087"
    },
    {
        "abs": "Future mobile terminals are expected to support an ever increasing number of Radio Access Technologies (RAT) concurrently. This imposes a challenge to terminal designers already today. Software Defined Radio (SDR) solutions are a compelling alternative to address this issue in the digital baseband, given its high flexibility and low Non-Recurring Engineering (NRE) cost. However, the challenge still remains in the Digital Front-End (DFE), where many operations are too complex or energy hungry to be implemented as software instructions. Thus, new architectures are needed to feed the SDR digital baseband while keeping complexity and energy consumption at bay. In this article the architecture of a Digital Front-End Receiver (DFE-Rx) for the next-generation mobile terminals is presented. The flexibility needed for multi-standard support is demonstrated by detecting, synchronizing and reporting carrier-frequency offset, of multiple concurrent radio standards. Moreover, the proposed architecture has been fabricated in a 65 nm CMOS low power high-VT cell technology in a die size of 5 mm2. The core module of the DFE-Rx, the synchronization engine, has been measured at 1.2 V and reports an average power consumption of 1.9 mW during Wireless Local Area Network (WLAN) reception and 1.6 mW during configuration, while running at 10 MHz.",
        "title": "14119"
    },
    {
        "abs": "In this paper, an effective fault-tolerant framework offering very high error coverage with zero detection latency is proposed to protect the data paths of VLIW processor cores. The feature of zero detection latency is essential to real-time error-recovery. The proposed framework provides the error-handling schemes of varying hardware complexity, performance and error coverage to be selected. A case study with an experimental VLIW architecture implemented in VHDL was used to demonstrate the impacts of our technique on hardware overhead and performance degradation. The fault injection experiments were performed to characterize the effects of fault-occurring frequency as well as workload variations on the error coverage, and the permanent faults on the length of time spent for error-recovery. The results observed from the experiments show that our approach can well protect the VLIW data paths even in a very severe fault scenario. As a result, the proposed fault-tolerant VLIW core is quite suitable for the highly dependable embedded applications.",
        "title": "14155"
    },
    {
        "abs": "Aiming at the DES design scheme against power analysis attacks introduced by Standart et al., an improved scheme is presented in this paper. In the improved scheme, eight dummy S-Boxes are proposed to make the power consumption of the DES S-Box logic gates constant instead of random, and it can make the same difficulties for power analysis attackers consuming 98% less memories as compared with the previous scheme. By analyzing the improved scheme in theory and using an accurate circuit simulator, the secure efficacy of the improved one is verified. The verification results indicate that the improved scheme can satisfy the practical applications against power analysis attacks, and it can be also introduced into the FPGA implementations of other cryptographic algorithms’ S-Box against power analysis attacks.",
        "title": "14192"
    },
    {
        "abs": "Minimizing the finish time of a set of independent tasks executed on a multiprocessor system, where no faults occur, is a well-known NP-complete problem. However, an approximated schedule can be found efficiently by means of the so called largest processing time (LPT) rule. This scheduling problem when extended to faulty environments may seem daunting. However, this paper shows how to extend the LPT rule to a fault-tolerant non-uniform memory access (NUMA) multiprocessor employing a primary copy and a backup copy for each task in order to handle processor failures. The concept of expected schedule length is also introduced to evaluate the performance of the resulting schedule in case of processor failures. Simulation studies are used to reveal interesting trade-offs associated with the scheduling algorithm.",
        "title": "14200"
    },
    {
        "abs": "Floating point digital signal processing technology has become the primary method for real time signal processing in most digital systems presently. However, the challenges in the implementation of floating point arithmetic on FPGA are that, the hardware modules are larger, have longer latency and high power consumption. In this work, a novel efficient reversible floating point fused arithmetic unit architecture is proposed confirming to IEEE 754 standard. By utilizing reversible logic circuits and implementation with adiabatic logic, power efficiency is achieved. The hardware complexity is reduced by employing fused elements and latency is improved by decomposing the operands in the realization of floating point multiplier and square root. To validate the design, the proposed unit was used for realization of FFT and FIR filter which are important applications of a DSP processor. As detection is one of the core baseband processing operations in digital communication receivers and the detection speed determines the data rates that can be achieved, the proposed unit has been used to implement the detection function. Simulation results and comparative studies with existing works demonstrate that the proposed unit efficiently utilizes the number of gates, has reduced quantum cost and produced less garbage outputs with low latency, thereby making the design a computational and power efficient one.",
        "title": "14335"
    },
    {
        "abs": "This paper describes and evaluates a fully digital circuit for one-way master-to-slave, highly precise time synchronization in a low-power wearable system equipped with a set of sensor nodes. These sensors are connected to each other in a mesh topology, with conductive yarns used as one-wire bidirectional communication links. The circuit is designed to perform synchronization in the MAC layer, so that the deterministic part of the clock skew between nodes is kept constant and compensated with a single message exchange. In each sensor node, the synchronization circuit provides a programmable clock signal and a real-time counter for time stamping. Experimental results from a fabricated ASIC (in a CMOS 0.35 [formula omitted]m technology) show that the circuit keeps the one-hop average clock skew below 4.6 ns and that the skew grows linearly as the hop distance to the reference node increases. The sub-microsecond average clock skew achieved by the proposed solution satisfies the requirements of many wearable sensor network applications.",
        "title": "14417"
    },
    {
        "abs": "Doppler blood flow spectral estimation is a technique for non-invasive cardiovascular disease detection. Blood flow velocity and disturbance may be determined by measuring the spectral mean frequency and bandwidth, respectively. The work presented here, evaluates a high performance parallel-Doppler Signal Processing architecture (SHARC) for the computation of a parametric model-based spectral estimation method known as the modified covariance algorithm. The model-based method incorporates improvement in frequency resolution when compared with Fast Fourier Transform (FFT)-based methods. However, the computational complexity and the need for real-time response of the algorithm, makes necessary the use of high performance processing in order to fulfil such demands. Sequential and parallel implementations of the algorithm are introduced. A performance analysis of the implementations is also presented, demonstrating the effectiveness of the algorithm and the feasibility for real-time response of the system. The results open a greater scope for utilising this architecture in implementing new and more complex methods. The results are applied to the development of a real-time spectrum analyser for pulsed Doppler blood flow instrumentation.",
        "title": "14550"
    },
    {
        "abs": "Lifetime is one of the major Quality of Service factors for Wireless Sensor Networks (WSN). As sensor nodes are generally battery-powered devices, the network lifetime can be extended over a reasonable time span by lessening the energy consumption of nodes. Reducing the amount of data transmission can effectively minimize the energy consumption, the bandwidth requirement and network congestions. In a WSN, denser deployment of nodes results in a high spatial correlation between data generated by neighboring nodes. Slow varying nature of many physical phenomenon results in similar sensor observations over the period. In this proposed work, a two level data reduction technique is employed. Here the Data and Energy Aware Passive (DEAP) clustering approach is introduced to divide the sensor network into data similar clusters. A Dual Prediction (DP) based reporting is deployed between cluster members and their Cluster Head (CH). This first level data reduction is attributed to the temporal correlation of data over the time. In CHs, the data from multiple data similar nodes are aggregated to reduce the spatial data redundancy. The proposed method DEAP-DP is verified with real world datum and has achieved up to 68% data reduction at 0.5 °C error tolerance.",
        "title": "14621"
    },
    {
        "abs": "Networking devices such as switches and routers have traditionally had fixed functionality. They have the logic for the union of network protocols matching the application and market segment for which they have been designed. Possibility of adding new functionality is limited. One of the aims of Software Defined Networking is to make packet processing devices programmable. This provides for innovation and rapid deployment of novel networking protocols. The first step in processing of packets is packet parsing. In this paper, we present a custom processor for packet parsing. The parser is protocol-independent and can be programmed to parse any sequence of headers. It does so without the use of a Ternary Content Addressable Memory. As a result, the area and power consumption are noticeably smaller than in the state of the art. Moreover, its output is the same as that of the parser used in the Reconfigurable Match Tables (RMT). With an area no more than that of parsers in the RMT architecture, it sustains aggregate throughput of 3.4 Tbps in the worst case which is an improvement by a factor of 5.",
        "title": "14691"
    },
    {
        "abs": "In recent years, the rapid increase in the land mobile frequency spectrum usage in New Zealand has imposed increasingly stringent performance requirements on the general land mobile radio operation and its management. One of the more pressing problems confronting the Land Mobile Service Organizations and the Spectrum Regulatory Authorities is that of establishing an usable propagation model for evaluating various propagation related problems. This article describes the design of a prototype propagation field strength measuring apparatus which analyses propagation parameters automatically in real time. It thus presents an economic option to other more expensive methods used.",
        "title": "14717"
    },
    {
        "abs": "The Intel Multibus I is a standard backplane bus (IEEE 796) intended for use in high-performance microprocessor systems, particularly in 8086, 80286 and 80386 systems. This application note describes how the 80386 microprocessor is interfaced to Multibus I. Like many of today's microprocessor buses, Multibus I supports multiprocessing and permits a potential master to request control of the bus and receive it. A large part of the note deals with the 82289 bus arbiter and the 82288 bus controller, which greatly facilitate the design of an interface between the 80386 and the Multibus I.",
        "title": "14719"
    },
    {
        "abs": "The paper considers what has held up development of parallel machines and outlines what the future may hold. The transputer is compared with other high performance processors and the paper considers the likely commercial impact of this device. In particular, the paper looks at those applications in which parallel processing appears to have the greatest potential.",
        "title": "14733"
    },
    {
        "abs": "The paper describes the most relevant aspects of research activity, still under development, concerning the analysis of the performance of advanced co-operating pneumatic units and components driven by different distributed control systems. Field bus, Actuator–Sensor interface (ASi) and servo-positioning strategies are applied to flexible and innovative pneumatic devices, in order to test the behaviour of different interconnected subsystems under simulated working conditions. An original approach oriented to the on-line monitoring and diagnostics of electrical and pneumatic components involved in the experimental workbench is described.",
        "title": "14746"
    },
    {
        "abs": "This paper describes the relatively simple algorithm and its implementation in the FPGA-based onboard hardware for quick calculating estimators of point source coordinates within an image formed by onboard IR-sensor. The algorithm based on the iterative least square method uses the model which describes the cases when the spot is put on both three and two squares, small quantity of samples for channel impulses, voltage shifts and correlated white noise for each photoelement. The paper contains the results of modeling and the description of the FPGA-based hardware that allows replacing routine processing of whole image.",
        "title": "14776"
    },
    {
        "abs": "Microarchitecture is difficult to be evaluated on the silicon level, due to the time and cost. Accordingly, most researches rely on cycle-accurate simulators to evaluate the performance. At the time of development, the cycle-accurate simulator must have been validated. However, off-the-shelf processors have been continuously improved, since the cycle-accurate simulator was developed. Thus, the improved features should be implemented into the simulator for accurate performance evaluation. In order to explore the accuracy of the cycle-accurate simulator, we modified the cycle-accurate model (Sim-Outorder) of Simplescalar suite to adopt off-the-shelf processor (ARM1136) features. The difference between the IPC (instruction per cycle) of the modified model (Sim-ARM1136) and the IPC of the original model (Sim-Outorder) is 19%, on average. Furthermore, it is difficult to find a relation between the simulation results from Sim-ARM1136 and those from Sim-Outorder, which might mislead to different conclusions.",
        "title": "14861"
    },
    {
        "abs": "One of the most flexible and modular approaches to reconfigurable systems is a bus-based approach. In order to get realistic performance estimates of these systems, detailed modeling of the processor as well as the bus and memory hierarchy is required. In addition, when coupling one or more reconfigurable units with a superscalar, out-of-order issue, load/store RISC CPU using the on-chip system bus, there are issues relating to cache coherency that need to be addressed. We have developed a cycle accurate co-simulator that uses a ‘C’ model of the processor and HDL models of the bus and reconfigurable units. We have also made modifications to the CPU pipeline to allow for non-cacheable accesses to the reconfigurable unit. This is reported in the paper. We have used this simulator to look at (a) The speedup obtained for two examples, namely, matrix multiplication and Lempel–Ziv compression, (b) the speedup obtained when there is a context switch from one application to the other and full reconfiguration is employed and (c) speedup obtained with partial reconfiguration. These results are reported in the paper.",
        "title": "14939"
    },
    {
        "abs": "Modern digital signal processors (DSPs) execute diverse applications ranging from digital filters to video decoding. These applications have drastically different arithmetic precision and scratch pad memory (SPM) size requirements. To minimize power consumption, DSPs often support aggressive dynamic voltage/frequency scaling (DVFS) techniques, requiring on-chip memory, such as SPM, to operate at low voltages. However, increasing process variations with aggressive technology scaling have significantly increased the failure rate of on-chip memory designed with small transistors operating at low voltages. Consequently, designs must use either larger and/or more transistors to have memory cells satisfy a target minimum operating voltage ([formula omitted]) under a failure rate constraint. Yet using larger and/or more transistors for the SPM, which consumes a large fraction of the chip area, is costly. In this paper, we first propose SPM designs that exploit (i) the characteristics of applications and (ii) the tradeoffs between memory cell size and [formula omitted]. Our approach can reduce the SPMs chip area by up to 17% and [formula omitted] by up to 52.5 mV. Second, we exploit the error-tolerant characteristics of some applications. Our proposed SPM can support lower [formula omitted] with less mean square error than a conventional SPM with shortened word width. For error-sensitive applications that require high precision, we can lower [formula omitted] at the cost of reduced memory capacity. This approach may negatively impact the performance of applications with large memory footprints. However, we demonstrate that such applications are typically constrained by their execution latency requirements and are likely to operate at higher voltages/frequencies than applications with smaller memory footprints to satisfy their real-time execution constraints.",
        "title": "14965"
    },
    {
        "abs": "The VME backplane bus is suitable for use in instrumentation and control crates in industrial environments. The paper describes the implementation of an interface that allows the G64 bus to be used as an I/O section in VME-based crates. The design is implemented in a double Eurocard format using programmable logic devices (PLDs). The interface works as an I/O bus master or DMA source, and is used with an MVME101 master CPU board in a laboratory data acquisition system.",
        "title": "15068"
    },
    {
        "abs": "The development of IC technology makes Network-on-Chip (NoC) an attractive architecture for future massive parallel systems. Task migration optimize the overall communication performance of NoCs since the changing phases of execution make static task mapping insufficient. It is well-known that the communication behavior of many applications are predictable, which makes it feasible to use prediction to guide task migration. The triggering of activating a task migration is also important. In this paper, we first defined and analyzed predictabilities of applications, and then compared different ways of triggering for migration. We then modified the Genetic Algorithm (GA) based task remapping and proposed two other task migration algorithms: Simple Exchange (SE) and Benefit Assess (BA). A mechanism called node lock is also used to reduce unnecessary and costly migrations. Simulation results on real applications from PARSEC benchmark suites show that the SE, BA and GA algorithms can reduce 21.4%, 34.0% and 34.9% of number of hops, and 17.3%, 27.2% and 26.3% in terms of average latency respectively, compared with the system without task migration; BA and SE reduce 72.0% and 78.7% of migrations without significant performance degradation compared with GA, and the node lock mechanism can further remove 37.3% and 46.0% of migrations while achieving almost the same performance.",
        "title": "15179"
    },
    {
        "abs": "Phoenix MapReduce is a multi-core programming framework that is used to automatically parallelize and schedule programs. This paper presents a novel scratchpad memory architecture that is used accelerate MapReduce applications by indexing and processing the key/value pairs. The proposed scratchpad memory scheme can be mapped onto programmable logic or multi-core processors chips as a coprocessor to accelerate MapReduce applications. The proposed architecture has been implemented in a Zynq FPGA with two embedded ARM cores. The performance evaluation shows that the proposed scheme can reduce up to 2.3[formula omitted] the execution time and up to 1.7[formula omitted] the energy consumption.",
        "title": "15180"
    },
    {
        "abs": "An implementation of a document retrieval system developed on Hathi-2 is described. Hathi-2 is a multiprocessor system built from IMS T800 transputers. An efficient distributed search strategy in large free text databases is derived following the ‘processor farm’ paradigm for programming parallel and distributed systems. The processor farm approach is considered for several different transputer network configurations. The main purpose of the paper is to study the applicability of this approach to transputer networks and to report on the performance figures observed for the document retrieval system.",
        "title": "15209"
    },
    {
        "abs": "The growing complexity of Field Programmable Gate Arrays (FPGA’s) is leading to architectures with high input cardinality look-up tables (LUT’s). This paper describes a methodology for area-optimal combinational technology mapping, specifically designed for such FPGA architectures. This methodology, called LURU, leverages the parallel search capabilities of Content-Addressable Memories (CAM’s) to outperform traditional mapping algorithms in both execution time and quality of results. The LURU algorithm is fundamentally different from other techniques for technology mapping in that LURU uses textual representations of circuit topology in order to efficiently store and search for circuit patterns in a CAM. A circuit is mapped to the target LUT technology using exact, inexact, or hybrid matching techniques. Common subcircuit expressions (CSE’s) are also identified and used for architectural optimization—a small set of CSE’s is shown to effectively cover an average of 96% of the test circuits. LURU was tested with the ISCAS ’85 suite of combinational benchmark circuits and compared with the mapping algorithms FlowMap and CutMap. The area requirement of LURU’s mapping is, on average, 20% less than FlowMap or CutMap. The asymptotic runtime complexity of LURU is shown to be better than that of both FlowMap and CutMap.",
        "title": "15271"
    },
    {
        "abs": "This paper presents a wearable wireless sensor system designed for real-time gait pattern analysis in glaucoma patients. Many clinical studies have reported that glaucoma patients experienced mobility issues such as walking slowly and bumping into obstacles frequently. The gait attributes of glaucoma patients, however, have not been studied in the literature. We design and develop a shoe-integrated sensing system for objective bio-information collection, utilize signal processing algorithms for feature estimation and leverage machine learning as well as statistical analysis approaches for gait pattern examination. The developed sensor platform is utilized in a randomized clinical trial conducted at UCLA Stein Eye Institute with 19 participants. Our trial involved both glaucoma patients and age-matched healthy participants performing a series of gait tests. With the captured sensor data, we develop signal processing and machine learning algorithms to provide a quantitative comparison between gait characteristics in older adults with and without glaucoma. Our results demonstrate that machine learning algorithms achieve an accuracy of over 80% in distinguishing extracted gait features of those with glaucoma from healthy individuals. Our results also demonstrate significant difference between two groups based on extracted gait features. In particular, several features are highly discriminative with a p-value of less than[formula omitted].",
        "title": "15407"
    },
    {
        "abs": "Fast digital signal processing (DSP) chips often lack facilities for general system control and therefore are used as subsystems in larger workstations. The signal processing capability of the system can be significantly enhanced if the signal processor and host processor are allowed to operate concurrently. This paper presents an implementation of a dual-bank data memory to enable parallel operations of a TMS32010 DSP chip and a 68000 host processor. The multipass fast Fourier transform algorithm is divided into concurrent tasks for these processors. The results presented show an enhancement in processing speed.",
        "title": "15425"
    },
    {
        "abs": "The importance of high-level languages used in programming embedded microprocessors cannot be overestimated. It has become increasingly undesirable to use assembler language for complex applications, particularly in those areas of process automation which require sophisticated control algorithms. As a result, high-level languages are gradually replacing assembler language. This paper presents the implementation of a number of modifications to an existing high-level language cross-compiler, MPL, to allow efficient cross-software development for the Motorola MC6800/MC6809 family of embedded microprocessors. Also included is an example of an interrupt-driven microprocessor application using floating-point calculations, which was developed on a Vax/VMS system with the help of this cross-compiler.",
        "title": "15576"
    },
    {
        "abs": "A cost-effective computer-aided methodology is established for the performance analysis of distributed processing systems. Software tools (based on probabilistic and simulation models) to support this methodology are briefly described. The methodology and software tools are illustrated by applying them to the analysis of a local area network. An instrumentation system is used to monitor a prototype network and to assess the accuracy of the models. The models are shown to be good approximations. Although the case study considers the analysis of a file-server system, the software tools are quite general and may be used for other types of distributed processing system.",
        "title": "15608"
    },
    {
        "abs": "A two-bit microcomputer as the simplest von Neumann computer system was used to provide experience in system integration for undergraduate electrical engineering students. Three areas were covered: construction on a board with PLDs, designing a set of physical masks with a CMOS logic compiler and fabricating an actual CMOS chip by a simplified CMOS wafer process. They accomplished the whole procedure, from designing to testing the CMOS digital LSI chip.",
        "title": "15652"
    },
    {
        "abs": "Integrating reconfigurable fabrics in SOCs requires an accurate estimation of the layout area of the reconfigurable fabrics in order to properly optimize the architectural-level design of the fabrics and accommodate early floor-planning. This work examines the accuracy of using minimum width transistor area, a widely-used area model in many previous FPGA architectural studies, in accurately predicting layout area. In particular, the layout areas of LUT multiplexers are used as a case study. We found that compared to the minimum width transistor area, the traditional metal area based stick diagrams can provide much more accurate layout area estimations. In particular, minimum width transistor area can underestimate the layout area of LUT multiplexers by as much as a factor of 2–3 while stick diagrams can achieve over 90% accuracy in layout area estimation while remaining IC-process independent.",
        "title": "15664"
    },
    {
        "abs": "This article discusses the possibilities of using FPGAs in order to construct fast PLCs that execute serial-cyclic program control loop. The PLCs bistable function blocks of the IEC 61131-3 standard with particular emphasis on the possible FPGA implementations are presented. The FPGA hardware support is implemented in such a way that it does not interfere with the normal, serial-cyclic program execution. It enables a significant reduction in the timing of the result execution after loading new input data in the memory cells. Moreover, such an implementation of bistable function blocks allows for seamless use in any programming language. Both the IL text language and the graphical languages (LD and FBD) can freely use the advantages of such a solution.",
        "title": "15745"
    },
    {
        "abs": "A common bus, carrying information and electrical power between the various components of a digital system, is as important a part of the system as the microprocessor chip itself. The electrical properties of buses often cause design problems as insufficient attention is paid to them. This article deals with the properties of three types of bus line, considering the interfacing circuitry required by each type in detail.",
        "title": "15856"
    },
    {
        "abs": "Soft errors caused by strikes arising from energetic particles pose a significant reliability concern for computing systems. In this study, we first introduce a model for soft error occurrence and propagation in cache memories. Based on this model, we define a metric called Architectural Vulnerability Factor for Caches (AVFC), which represents the probability with which a fault in the cache can be visible in the final output of the program. We then propose three architectural schemes for improving reliability. Our first scheme prevents an error from propagating to the lower levels in the memory hierarchy by not forwarding the unmodified data words of dirty cache blocks to the L2 cache at write-backs. The second scheme selectively invalidates cache blocks to reduce their vulnerable periods. To reduce the performance overhead caused by block invalidation, our third scheme tries to bring a fresh copy of the invalidated block into the cache via prefetching. The experimental results for the SPEC2000 suite show that, based on the proposed model, our first and third schemes together can improve the data reliability roughly 96% at the cost of less than 1% overhead in execution time, quite more than data improvements achieved by either two well-known techniques, namely write-through and early write-back cache mechanisms.",
        "title": "15966"
    },
    {
        "abs": "This paper reports the design and development of reconfigurable (up to 8192-point), data parallel, constant geometry fast Fourier transform (CG-FFT) architectures based on Network-on-Chip (NoC) paradigm. Twiddle factor multiplications have been realized using pipelined CORDIC rotators in the proposed architecture in order to ensure its high throughput. Mapping of FFT functions to cores has been done by considering the proposed signal flow graph (SFG) for CG-FFT architecture, which helps in optimizing the design of network components (routers and network interfaces) and reducing the latency of FFT computation. The proposed input-size aware architecture can withstand faults in other processing elements (PEs) as it can accomplish the entire FFT computation using only one PE as well. When mapped onto mesh based NoC, the proposed architectures could achieve reduction in latency by 5×, compared to several existing FFT architectures on NoC. Hardware realization of the PE and the network components of the proposed architectures have been done using Xilinx Kintex-7 family field-programmable gate array (FPGA) device. The maximum operating frequency of a PE in the proposed architecture has been found to be 184.010 MHz, which meets the timing specifications of several application standards, such as DVB-T/H, DAB, 802.11a/n and UWB. In addition to the FPGA-prototype, the proposed architectures have also been synthesized in ASIC design flow to obtain area and power results.",
        "title": "15971"
    },
    {
        "abs": "Employing an updated C2MOS (clocked CMOS) technique, two types of speech synthesizer LSI circuits, based on the Parcor (partial correlation) and the ADM (adoptive delta modulation) methods, and recording watch system, are introduced and described. These LSI circuits and system have several functions &amp;#x02022; • The new Parcor LSI circuit has the circuits needed by the Parcor synthesis algorithm. It has a 64 kbit speech data ROM, output low pass filter and preamplifier. Using only this LSI circuit, 30 s to 60 s of speech can be synthesized. &amp;#x02022; • The new ADM LSI circuit has encoding and decoding circuits, a 64 kbit speech data ROM and RAM control circuit. The record and synthesis system can be easily constructed with this LSI circuit and RAM. &amp;#x02022; • The recording watch system consists of the watch LSI circuit with the ADM system and the analogue LSI circuit. In the Parcor system, various high quality and low data-rate speech outputs are obtainable. The ADM system is applied for recording and synthesizing. By applying these systems to meet market needs, it is possible to achieve good cost performance in a simple system.",
        "title": "16058"
    },
    {
        "abs": "Pipelining is used in almost all recent processor architectures to increase the performance. It is, however, difficult to achieve the theoretical speedup of pipelining, because code dependencies cause delays in execution. Superscalar processor designers must use complex techniques like forwarding, register renaming and branch prediction to reduce the loss of performance due to this problem. In this paper we outline and evaluate abstract Minimal Pipeline Architecture (MPA) featuring cross-bar interconnect of functional units and special two level pipelining. According to our evaluation MPA is not more complex than a basic superscalar architecture using out of order execution, but offers remarkably better performance.",
        "title": "16085"
    },
    {
        "abs": "The work presents a configurable network interface (NI) macrocell to be integrated in Spidergon network-on-chip (NoC) infrastructures, and addresses the problem of its functional verification. The NI architecture supports multiple native bus for the IP cells connected to the NoC and the conversion of data size, protocol and frequency between the NoC and each IP. Differently from many state-of-art NI designs the proposed macrocell features also the hardware implementation of advanced networking features such as security, order handling, error management, store and forward transmission, memory remapping, power management. Such a configurable and complex design poses several challenges in terms of functional verification. Direct HDL testbenches fails covering corner cases and typically are based on handwritten testbenches that are error-prone. In formal methods the verification engineer tries to extract deterministic laws/relationships internal to the HDL description, and then to prove theorems to check the netlist functional behavior. However in complex designs the state explosion problem limits model checking, and the cost of theorem proving becomes prohibitive because of the amount of skilled manual guidance it requires. To overcome such issues a constrained-random coverage-driven approach is presented and customized to be applied to the novel NI as design under test (DUT). Starting from DUT specifications, a software verification platform is created performing these tasks: generating traffic patterns which are constrained-random, i.e. random within variations ranges specified by the user; monitoring the DUT outputs and checking them according to pre-programmed rules; parsing collected outputs into a functional coverage scheme to check if all possible cases have been stressed and covered by the tests. This enables a coverage-driven verification: the user continues developing and running tests until there are no holes left in the coverage plan. As result of this verification strategy full code and functional coverage is achieved. Implementation results of the verified NI core in 45 nm and 65 nm CMOS technologies are also provided and compared to state-of-art NI designs.",
        "title": "16164"
    },
    {
        "abs": "A variety of techniques for handling interrupts using the Modula-2 language are compared. Two different implentations of the language on the PC are used and the response times of the interrupt routines are measured and compared with a simple assembly language procedure. The techniques used range from the implementation of the assembly routine in Modula-2, to use of the standard IOTRANSFER procedure and the in-built process structure of Modula-2. The response times measured varied from 160 μs to 375 μs as compared with 60 μs for the assembly language routine. The reasons for these variations are discussed. All measurements were taken on a standard IBM PC running with a clock rate of 4.77 MHz.",
        "title": "16168"
    },
    {
        "abs": "Reliability is becoming a major design concern in contemporary microprocessors since soft error rate is increasing due to technology scaling. Therefore, design time system vulnerability estimation is of paramount importance. Architectural Vulnerability Factor (AVF) is an early vulnerability estimation methodology. However, AVF considers that the value of a bit in a clock cycle is either required for Architecturally Correct Execution (i.e. ACE-bit) or not (i.e. unACE-bit); therefore, AVF cannot distinguish the vulnerability impact level of an ACE-bit. In this study, we present a new dimension which takes into account the vulnerability impact level of a bit. We introduce Bit Impact Factor metric which, we believe, will be helpful for extending AVF evaluation to provide a more accurate vulnerability analysis.",
        "title": "16199"
    },
    {
        "abs": "Correlation power analysis (CPA) is one of the most common side-channel attacks today, posing a threat to many modern ciphers, including AES. In the final step of this attack, the cipher key is usually extracted by the attacker by visually examining the correlation traces for each key guess. The naïve way to extract the correct key algorithmically is selecting the key guess with the maximum Pearson correlation coefficient. We propose another key distinguisher based on a significant change in the correlation trace rather than on the absolute value of the coefficient. Our approach performs better than the standard maximization, especially in the noisy environment, and it allows to significantly reduce the number of acquired power traces necessary to successfully mount an attack in noisy environment, and in some cases make the attack even feasible.",
        "title": "16316"
    },
    {
        "abs": "Considering that Elliptic Curve Digital Signature Algorithm (ECDSA) implementations need to be efficient, flexible and Side Channel Attack (SCA) resistant, in this paper, a design approach and architecture for ECDSA and the underlined scalar multiplication operation is proposed for GF (2 k ), satisfying the above three directives. To achieve that, in the paper, Binary Edwards Curves (BECs) are adopted as an alternative to traditional Weierstrass Elliptic Curves (ECs) for GF (2 k ) since they offer intrinsic SCA resistance against simple attacks due to their uniformity, operation regularity and completeness. To achieve high performance and flexibility, we propose a hardware/software ECDSA codesign approach where scalar multiplication is implemented in hardware and integrated in the ECDSA functionality through appropriate drivers of an ECDSA software stack. To increase BEC scalar multiplier performance and introduce SCA resistance we adopt and expand a parallelism design strategy/methodology where GF (2 k ) operations of a scalar multiplier round for both point operations performed in this round are reordered and assigned into parallelism layer in order to be executed concurrently. Within this strategy we include hardware and software based SCA countermeasures that rely on masking/randomization and hiding. While scalar randomization is realized by the ECDSA software stack in an easy way, in order to achieve resistance using hardware means, we propose and introduce in every scalar multiplier round, within the parallelism layers, projective coordinates randomization of all the round’s output points. So, in our approach, considering that with the proposed parallelism plan in every scalar multiplier round BEC point operations are performed in parallel and that the round’s output points are randomized with a different number in each round, we manage to achieve maximum SCA resistance. To validate this resistance, we introduce and realize a leakage assessment process on BEC scalar multipliers for the first time in research literature. This process is based on real measurements collected from a controlled SAKURA X environment with a GF (2233) based scalar multiplier implementation. Using Welch’s t -test we investigate possible information leakage of the multiplier’s input point and scalar and after an extended analysis we find trivial leakage. Finally, we validate the ECDSA architecture and its scalar multiplier efficiency by implementing it on a Zynq 7000 series FPGA Avnet Zedboard and collecting very promising, well balanced, results on speed and hardware resources in comparison with other works.",
        "title": "16571"
    },
    {
        "abs": "This paper presents an object-oriented interface for parallel programming, and an algorithm for automatic translation into parallel programs. The programming interface consists of a restricted subset of the object-oriented language C++. Parallelism is defined explicitly at the abstract level of object definitions and method invocations within a single C++ program. The translator algorithm first generates a machine-independent communication graph and proceeds with the creation of the parallel programs, which are demonstrated for transputer systems with the HELIOS operating system. The necessary communication statements are generated automatically.",
        "title": "16746"
    },
    {
        "abs": "The main objective of this work is to design and construct a small multiple microprocessor system based on the cross-bar switch architecture. The motives behind this are twofold; firstly there seems to be great potential in the architecture, and secondly, few previous constructional projects employing this architecture have been reported. A system is presented employing three Z80 microprocessors in a master-slave configuration with a floppy disc based support system. The various design decisions are outlined, and an estimate is made of the reliability of the hardware. This work, in particular, has highlighted the flexibility and simplicity of the architecture, in addition to the obvious potential of high speed of operation. It is also found that parallel programs can easily map onto the system, though this is not dealt with here.",
        "title": "16785"
    },
    {
        "abs": "This paper presents a next generation integrated IN architecture among Flexible Intelligent Network Elements (FINE) based on the multimedia multimode (client, server and agent) IN Service Node concept (Proc. ICCE '97, Cannes (1997) 77), to perform a crafty Unified Media Communications Service (UMCS) across heterogeneous networks. The FINE architecture represents a network of configurable interworking elements allowing user access to a Unified Message Store (UMS) and Universal Communications Channel (UCC) via traditional PSTN/ISDN/PLMN equipment such as telephones, pagers and fax machines on the one side, and networked computers and mobile terminals equipped with mail readers and Web browsers on the other side, to enable both on-line and off-line interactions. A virtual cluster of FINEs within an integrated personal communications network is dynamically configured in a distributed or centralised manner according to user profile requirements, network size and performance to provide artful messaging, telephony and I3E services (Intelligent Inter-, Intra- and Extranet) in a changing environment. The FINE consists of a Channel Matrix Switch (CMS), several Resource Platforms (RP) containing Media Conversion Processors (MCP) and Channel Managers (CM) to perform the media translation and routing in the required interchange formats, an Internet Gateway (IG) to hold the subscribers' mailboxes and provide the internet connectivity, and a FINE Controller (FINEC) to realise the FINE Service Logic (FSL) and manage the FINEs. Each FINEC can be deployed in some of the three modes — independent (server), dependent (client) and autonomous (agent) — with respect to the user/network configuration, thus allowing a dynamically configurable (per user/ per node/ per service) centralised or distributed service architecture. The FINE itself is organised along with other nodes by a Network Operation Support Environment (NOSE), a service oriented and TMN compliant Operation, Administration &amp; Maintenance Centre. Service logic can be transferred, installed and mounted on demand among the FINE Controllers and the terminal equipment to provide optimal QoS.",
        "title": "16790"
    },
    {
        "abs": "IP over ATM, Lan Emulation and other technologies of data communications over ATM use ATM switched VCC (SVCC) as the underlying transferring channel. This article analyzes a delayed releasing SVCC based method under multiple sources condition. A single-queue method and a HOL multi-priority queue method are explored. Both theoretical analysis and simulation results are given in this article. It is showed that multi-priority queue method requires less releasing timer T and hence the SVCC usage is made more efficient. A two-queue scheme is proposed as an application of HOL scheduling delayed releasing SVCC scheme. An optimum design target with multiple sources is defined at the end of this article and optimum design method under some simple cases are also included.",
        "title": "17005"
    },
    {
        "abs": "Deep layer processing and increasing line rates present a memory challenge to processor–memory communications located on network line cards. In this paper, we introduce a packet-based, off-chip interconnect to increase the throughput of memory system currently used on line cards. The 3D-bus architecture allows multiple packet processing elements on a line card to access multiple memory modules. Our network-on-board includes a routing protocol as well as a node switching mechanism to minimize packet congestion and packet loss. The main advantage of the proposed architecture is to increase the network processor off-chip memory bandwidth while diminishing the latency otherwise caused by the single bus competition. Performance results show that our interconnect significantly outperforms its competitors, such as shared-bus, PCI Express, infiniband and HyperTransport, reaching peak throughput beyond 400 Gbps. Moreover, it provides other high performance qualities including low latency, off-chip scalability, low transmission failure-rate and high memory bandwidth.",
        "title": "17022"
    },
    {
        "abs": "Correlation coefficient is frequently used to obtain cardiac rhythm by peak estimation and appreciate differences in the signal compared to a pattern. This work focuses on the description of a real-time correlation assessment procedure. Applied to electrocardiogram (ECG) signals, a new correlation value is obtained every new sample and pulse detection information is provided. The ECG pattern is internally stored and can be changed when desired. This procedure is useful in Systems on Chip implementation and can be applied to design compact ECG monitoring systems consisting on a system on chip where programmable logic offloads the main processor. A Xilinx FPGA device has been used for prototyping.",
        "title": "17042"
    },
    {
        "abs": "A unique method has been developed to scavenge energy from monocrystaline solar cells to power wireless router nodes used in indoor applications. The system’s energy harvesting module consists of solar cells connected in series-parallel combination to scavenge energy from 34W fluorescent lights. A set of ultracapacitors were used as the energy storage device. Two router nodes were used as a router pair at each route point to minimize power consumption. Test results show that the harvesting circuit which acted as a plug-in to the router nodes manages energy harvesting and storage, and enables near-perpetual, harvesting aware operation of the router node.",
        "title": "17080"
    },
    {
        "abs": "The design, operation, supporting software and application of a microprocessor-based digital correlator is described. The device has 128 channels and can measure auto- or cross-correlation functions in the range of 100 ns to 10 s. The device operates in either the single clipping or scaling mode and is especially appropriate for applications where cost is a major consideration. Experimental results measuring correlation functions are shown.",
        "title": "17125"
    },
    {
        "abs": "This paper presents a central memory subsystem for a tightly coupled multimicroprocessor system. The design supports high-speed single-cycle and burst-mode data transfers. It supports a high level of data integrity through autonomous retries upon error detection, and provides a high level of test support. The multiprocessor system employs a distributed cache using the VMEbus. The memory subsystem has special facilities to guarantee data consistency in the different caches.",
        "title": "17166"
    },
    {
        "abs": "In this paper, a unified Field Programmable Gate Array (FPGA) based Advanced Encryption Standard (AES) encryptor/decryptor design is presented by proposing a symmetric ST-Box structure. This structure fully utilizes high capacity (32 Kb) Block RAM (BRAM) by accommodating all encryption and decryption lookup operations within a single BRAM in the form of single integrated Look-Up-Table. This design also caters the inherent asymmetric nature of encryption and decryption coefficients for a unified hardware. Further the symmetry at BRAM output is maintained to use a single XOR network during both encryption and decryption. The performance of design is enhanced by proposing a duty-cycle based accessing technique. It explores the switching capabilities of BRAM and effectively minimizes the ON time of BRAM by changing duty-cycle of input clock. This enables us to access single BRAM 4 times per clock. Effectiveness of design is further measured by implementing it, in both iterative and pipelined architectures. Our proposed iterative design on Virtex-7 proved to be the smallest 128-bit unified AES core with 48.70% reduced resources and the best Throughput Per Slice (TPS) of 11.56. Similarly our pipelined design saved 59.01% area and has the highest throughput of 45.69 Gbps.",
        "title": "17277"
    },
    {
        "abs": "Whilst the road transport industry has already started to take advantage of the increased functionality offered by the use of programmable electronic systems, the current Type Approval mechanism, whereby all road transport equipment is certified as being fit and safe for its purpose, cannot adequately assess them. This paper describes the work done as part of an EC DRIVE project to propose a European standard for the development of safe road transport telematic systems. In particular the philosophy behind the software standard and its certification criteria, where most of the new problems lie, is discussed. A solution is proposed that is pragmatic, meaningful and workable.",
        "title": "17282"
    },
    {
        "abs": "A solution to the development of hardware for realtime signal processing applications using an existing Unix-based microprocessor development system is described. The system used is based upon a single special purpose digital signal processor controlled by a general purpose microprocessor. The modularity of the approach allows the possibility of control of several signal processing modules performing concurrent tasks. A particularly attractive feature of this development system is that it may be extended at low cost to accommodate new processors as they become available from manufacturers.",
        "title": "17294"
    },
    {
        "abs": "In recent years, design of low power high-speed nano computing systems have drawn more attention. Reversible Logic is a technique popularly used to design the computing systems to achive them. In a computer, the arithmetic logic unit (ALU) is the fundamental computing module. In this paper, a novel Reversible Arithmetic and Logic Unit is proposed, where a single module performs both arithmetic and logical operations. The possible arithmetic operation includes transfer, addition with carry, subtract, complement and increment. The possible logical operations are AND, OR, XOR, COPY and CONSTANT. The control signal is a key element which defines and alters the data path in order to produce either arithmetic or logical output. The fault-tolerant KMD gates are utilized to construct the ALU. Here, two approaches are discussed for the construction of ALU. In the first approach, ALU is constructed using KMD gates only, whereas in the second approach, combination of KMD, Toffoli and Fredkin gates are used. The functional realization is performed in Quantum Cellular Automata. Quantum circuit is also derived for the same. The obtained results are evident for the improved Quantum cost up to 69%, Constant input up to 40% and Number of gates up to 21% compared to the existing designs.",
        "title": "17297"
    },
    {
        "abs": "Advancements of Machine Learning (ML) in the field of computer vision have paved the way for its potential application in many other fields. Researchers and Hardware domain experts are exploring possible applications of Machine Learning in optimizing many aspects of the Hardware development process. In this paper, we propose a novel approach for predicting area and multiple Firmware metrics of Hardware components from specifications. The flow uses an existing RTL generation framework for generating valid data samples that enable ML algorithms to train the learning models. The approach has been successfully employed to predict the area and Firmware measurements of real-life Hardware components such as Control and Status Register (CSR) interfaces, that are ubiquitous in embedded systems. With our method we are able to perform an estimation on the area of an Hardware component with more than 98% accuracy and 600 x faster than the existing methods. In addition, we are able to rank the features according to their importance in final area estimations. Finally, we are as well able to predict with an accuracy of approx. 85% the size and the CPU running cycles of a Firmware program embedded on the same Hardware component. This method, as a whole, is an important approach towards an accurate and fast estimation in the context of Hardware/Software trade-off analysis.",
        "title": "17310"
    },
    {
        "abs": "A digital control system for distributed temperature control in a catalytic chemical reactor producing hydrogen is required. The system uses a 6809-based S/09 microcomputer from Southwest Technical Products Corp. The paper first overviews the reaction process, the reactor and its modelling, and then discusses the microcomputer and data acquisition system. The implementations of power control and energy requirement calculation are presented before a summary of the complete control algorithm; a listing of the control algorithm is supplied as an appendix. The final modified PID control algorithm adopted shows significant improvements in temperature adjustment time when compared to manual and open loop control methods.",
        "title": "17317"
    },
    {
        "abs": "The paper discusses the implementation by the 68000 microprocessor of hardware and software initiated exceptions. The three categories of hardware exception — resets, bus errors and interrupts — are covered first, with detailed discussion of how vectored and autovectored interrupts are processed. The software initiated exceptions discussed are illegal operation codes, traces, emulator-mode exceptions and traps. An example of the use of a trap handler is given.",
        "title": "17500"
    },
    {
        "abs": "In this paper, a programmable digital neuron architecture using pulsewidth-coded information is presented where the different transfer functions, ramp, sigmoid and Gaussian functions, can be generated. The neuron has been extensively simulated. The proposed programmable digital neuron is then applied to the design and analysis of a Gaussian perceptron neural model and its learning algorithm. The modular approach was adopted in the design of the programmable digital neuron to facilitate ease of expansion.",
        "title": "17535"
    },
    {
        "abs": "In high-performance processors, the accuracy of branch prediction plays a significant role in enhancing computer execution. A new hardware approach is presented in this paper to dynamically predict branch directions using path information. As an execution path contains large information, we compress the large information using a technique based on the linear feedback shift register (LFSR) that is widely used in testing and error correction coding. A modified version of LFSR, called windowed LFSR, is developed to calculate the signature of a path in one cycle. Since the windowed LFSR has a very regular structure, it can be easily implemented. For most of the benchmarks, the proposed prediction scheme shows better prediction accuracy than the pattern-based predictions.",
        "title": "17538"
    },
    {
        "abs": "The paper presents an automated environment for fast design space exploration and automatic generation of FFT/IFFT macrocells with minimum circuit and memory complexity within the numerical accuracy budget of the target application. The effectiveness of the tool is demonstrated through FPGA and CMOS implementations (90 nm, 65 nm and 45 nm technologies) of the baseband processing in embedded OFDM transceivers. Compared with state-of-art FFT/IFFT IP cores, the proposed work provides macrocells with lower circuit complexity while keeping the same system performance (throughput, transform size and accuracy) and is the first addressing the requirements of all OFDM standards including MIMO systems: 802.11 WLAN, 802.16 WMAN, Digital Audio and Video Broadcasting in terrestrial, handheld and hybrid satellite-scenarios, Ultra Wide Band, Broadband on Power Lines, xDSL.",
        "title": "17554"
    },
    {
        "abs": "Software design techniques for tolerating both hardware and software faults have been developed over the past few decades. Paradoxically, it is essential that fault-tolerant software is designed with the highest possible rigour to prevent faults in itself. Such rigour is provided by formal methods and aided by model checking. We illustrate an approach to fault-tolerant software design based on communicating sequential processes through a running example.",
        "title": "17561"
    },
    {
        "abs": "Commercial off-the-shelf (COTS) middleware, such as real-time CORBA, is gaining acceptance in the distributed real-time and embedded (DRE) community. Existing COTS specifications, however, do not effectively separate quality of service (QoS) policy configurations and adaptations from application functionality. DRE application developers therefore often intersperse code that provisions resources for QoS guarantees and program adaptation mechanisms throughout DRE applications, making it hard to configure, validate, modify, and evolve complex DRE applications. This paper illustrates how (1) standard component-based middleware can be enhanced to flexibly compose static QoS provisioning policies with application logic, (2) adaptive middleware capabilities enable developers to abstract and encapsulate reusable dynamic QoS provisioning and adaptive behaviors, and (3) component-based middleware and adaptive middleware capabilities can be integrated to provide a total QoS provisioning solution for DRE applications.",
        "title": "17603"
    },
    {
        "abs": "A microprocessor-based circuit is described which is capable of measuring the torque-velocity characteristic of the mechanical analogue of a superconducting transmission line. This model consists of 30 pendulums connected to each other with flexible couplings. The model is driven by compressed air. The circuit is composed of two parts. The first part measures the air velocity; this is equivalent to measurement of the applied torque. The second circuit measures the average velocity of a kink on the analogue line. Both measurements are converted to analogue signals and recorded on an x-y plotter as a point on the torque-velocity characteristic. This characteristic corresponds directly to the current-voltage characteristic of the actual transmission line.",
        "title": "17666"
    },
    {
        "abs": "In this paper, the Genetic Algorithm (GA) is used to evolve the weight and the interconnection of the neural network to solve the Unit Commitment problem. We will emphasize on the determination of the appropriate GA parameters to evolve the neural network, i.e. the population size and probabilities of crossover and mutation, and the method used for selection amongst generations such as Tournament selection, Roulette Wheel selection and Ranking selection. Performance comparisons are conducted to analyze the learning curve of different parameters, to find out which has a dominant influence on the effectiveness of the algorithm.",
        "title": "17716"
    },
    {
        "abs": "The power consumption is a key metric to design computing platforms. In particular, the variety and complexity of current applications fueled an increasing number of run-time power-aware optimization solutions to dynamically trade the computational power for the power consumption. In this scenario, the online power monitoring methodologies are the core of any power-aware optimization, since the incorrect assessment of the run-time power consumption prevents any effective actuation. This work proposes PowerTap , an all-digital power modeling methodology for designing online power monitoring solutions. In contrast with state-of-the-art solutions, PowerTap adds domain-specific constraints to the data-driven power modeling problem. PowerTap identifies the power model iteratively to balance the accuracy error of the power estimates and the complexity of the final monitoring infrastructure. As a representative use-case, we employed a complex hardware multi-threaded SIMD processor, also considering different operating clock frequencies. The RTL implementation of the identified power model targeting an Xilinx Artix 7 XC7A200T FPGA highlights an accuracy error within 1.79% with an area overhead of 9.95% (LUT) and 3.87% (flip flops) and an average power overhead of 12.17 mW regardless of the operating conditions, i.e., number of software threads and operating frequency.",
        "title": "17725"
    },
    {
        "abs": "For in-order processors, the stack distance theory is a well-known means to fast model LRU-cache behaviors . However, it cannot be applied directly on out-of-order processors due to the changing of stack distance histograms by mechanisms such as reordering executions, speculative loads, load-in-store operations and non-blocking issues. This paper proposes an Artificial Neural Network (ANN) model to fast forecast private LRU-cache behaviors on out-of-order processors. To verify our model in real commercial applications, the evaluation scenarios chosen in this paper, not only include traditional embedded benchmark suits, such as Mibench 1.0 and Mediabench II, but also embrace Android applications from Mobybench 2.0 benchmark suit as well. Compared with results from Gem5 simulations, the average root mean square error of our ANN model is less than 6% with the prediction speed increasing about 2.5× –3×.",
        "title": "17741"
    },
    {
        "abs": "Many choices of VME hardware module now exist but software still continues to be the major cost in systems development. As development teams face more complex projects, productivity can be improved by four approaches: the use of standard system architecture, the use of software building blocks, the use of better tools and an increase in the level of software engineering management. Some characteristics of realtime systems programming are presented. Next an overview of the VMEbus is given. Then a target operating environment for 68000-based applications in PASCAL is described. The software components are called pSOS, pHILE and pROP. These provide the realtime executive or kernel, file management system and runtime support for PASCAL respectively. Languages other than PASCAL can be used with pSOS and pHILE, but a preference is expressed for PASCAL over the language C.",
        "title": "17743"
    },
    {
        "abs": "NAND flash memory has become the mainstream storage medium for both enterprise high performance computers and embedded systems. However, over the past several decades, the storage primitives that access secondary storage have remained unchanged, forcing NAND flash memory to serve merely as a block device like hard disk drive. Recently, several emerging storage primitives have been presented to explore the potential value of non-volatile memory devices. Although these primitives can significantly boost the access performance by providing virtual to logical address mappings, they still suffer from large RAM footprint to maintain the address mapping table and require further support for update operations. This paper presents ESP to optimize E merging S torage P rimitives with virtualization for flash memory storage systems. We propose two optimization strategies, virtual duplication and mapping prefetching to solve the critical issues in existing emerging storage primitives. The objective is to reduce unnecessary flash memory accesses and keep RAM footprint of address mapping table well under control. We have evaluated ESP on an embedded development platform. Experimental results show that ESP can significantly improve the write/read performance and reduce over 30% of garbage collection operations.",
        "title": "17852"
    },
    {
        "abs": "The design of smart grid systems have been proposed in several literature. However, the actual commercial feasibility of smart grids or micro grids still faces several problems including technical infrastructure related issues and market economy issues. One critical issue in the design of commercially feasible smart grids is how the electricity trading can be performed fairly among micro grids. In existing works, this issue has been mainly addressed by using some sort of priority scheme among buyers and sellers in electricity trading, which obviously does not guarantee a fair trading scheme. As a result, the commercial feasibility of existing works is at stake and will not work as proposed. This work tries to address this issue by proposing a Fair Energy Resource Allocation (FERA) method for smart grids. The proposed method has been implemented in a FIPA-compliant Multi-Agent System (MAS) based smart grid control system and evaluated against state-of-the-art round robin and priority based allocation methods. For trading among 30 micro grids, it is demonstrated that the proposed method results in a high fairness index of 96.22% even in the worst case, while the round robin scheme and the priority scheme result in a worst-case fairness index of only 57.8% and 11.29%, respectively. Thus, in the long term under different ratios of buyers and sellers, the proposed method is the only method that can achieve a very high fairness index in the worst case. Averaging over different ratios of buyers and sellers, the proposed method results in a fairness index of 99.57%, which is much higher that achieved by the round robin method (84.04%) and the priority scheme (63.56%). As far as cost saving is concerned, based on the cost saving opportunity (CSO) metric, in the long term (10,000 rounds of trading), the proposed method results in a CSO of 51.48%, which is much higher than that by the other two methods; round robin method results in 14.07% and priority-based method results in 34.44%.",
        "title": "17871"
    },
    {
        "abs": "Designing many-core systems for intensive signal processing applications necessitates refreshing common practices in the design of Electronic Design Automation (EDA) tools. This should change the focus of the EDA community from old-fashioned foundations including heavy implementations and time-consuming simulations, to recent engineering practices promoting rapid design and acceptable accuracy. Hence, the challenge of the EDA community comes to define a design process for many-core systems which enables four goals: modeling conciseness, estimation accuracy, design rapidity, and exploration flexibility. In this scenario, the modeling task can be defined as a delicate design phase that requires choosing the adequate Model of Computation (MoC) at each step of the design process. Design models at each level of abstraction provide the basis for applying analysis, synthesis or verification techniques. Since modeling concepts and techniques can have a large influence on the quality, accuracy and rapidity of results, models and corresponding abstraction levels should be well-defined with clear and unambiguous semantics. In this paper, we aim to provide an analysis and comparative overview of common MoCs used in the specification and performance analysis of intensive signal processing applications and many-core architectures. After identifying conventional classifications, we propose a new taxonomy of MoCs based on their purpose in the design flow. The heterogeneity of MoCs in EDA tools is also discussed and various tools are reviewed and compared.",
        "title": "18012"
    },
    {
        "abs": "Utilizing hardware resources efficiently is vital to building the future generation of high-performance computing systems. The sparse matrix – dense vector multiplication (SpMV) kernel, which is notorious for its poor efficiency on conventional processors, is a key component in many scientific computing applications and increasing SpMV efficiency can contribute significantly to improving overall system efficiency. The major challenge in implementing SpMV efficiently is handling the input-dependent memory access patterns, and reconfigurable logic is a strong candidate for tackling this problem via memory system customization. In this work, we consider three schemes (all off-chip, all on-chip, caching) for servicing the irregular-access component of SpMV and investigate their effects on accelerator efficiency. To combine the strengths of on-chip and off-chip random accesses, we propose a hardware-software caching scheme named NCVCS that combines software preprocessing with a nonblocking cache to enable highly efficient SpMV accelerators with modest on-chip memory requirements. Our results from the comparison of the three schemes implemented as part of an FPGA SpMV accelerator show that our scheme effectively combines the high efficiency from on-chip accesses with the capability of working with large matrices from off-chip accesses.",
        "title": "18057"
    },
    {
        "abs": "A microphone array processing system has been developed for enhanced speech acquisition. The key aim is to allow the microphone array to direct its own pattern to selectively enhance a desired signal while attenuating interference. Basic design considerations are to keep the part count low and system cost down. While the prototype system is designed for research use, the design readily extends to production systems. Each channel of the eight element array consists of an omnidirectional microphone, analogue signal conditioning and amplification circuitry, a TLC32044 A/D subsystem and a serial buffer. Programmable logic circuits are used to force the eight channels to sample the inputs simultaneously, and to multiplex and send the data to a TMS320C30 development system using the TMS320C30 serial port protocol. The microphone array data is processed in order to form beams, null out interfering sources and to determine the direction of arrival of desired speech signals. Experimental results with the generalized sidelobe canceller beamformer, and Capon's direction finding technique, demonstrate the array's utility and functionality.",
        "title": "18067"
    },
    {
        "abs": "How many different types of commercial jet aircraft have been built since 1950? In fact, there have been only a handful of commercial jets, although there have been many variants. The same is true of microprocessors. There have been many variations on relatively few themes. Recent developments in semiconductor technology, the influence of RISC architectures, and the migration of mainframe characteristics to silicon have led to the appearance of some radically new designs. The Intel i860 is one such processor. This article describes the important features of the 64-bit i860: its pipelined organization, floating-point unit, its inherent parallelism, memory management unit, on-chip cache and graphics unit. These features, together with its 80 MFLOPS peak single-precision performance, make the i860 a formidable new computing engine.",
        "title": "18075"
    },
    {
        "abs": "Kalman filter is known as one of the best linear estimators. Its use is often limited owing to the fact that computation of the next state requires considerable time or resources limiting its application scope. Recently, a new approach to Kalman filter implementation using field programmable logic has been proposed. Combining a custom-configurable microcomputer core with a Kalman filter functional unit leads to a high-speed custom-configurable low-cost Kalman filter microcomputer. The approach provides flexibility of implementation of an embedded Kalman filter with various degrees of emphasis on hardware or software depending on the system requirements in time-resource design space. This allows new applications in prediction, control and other areas in which it has not been feasible to apply it. The Kalman filter microcomputer is described and its customization discussed.",
        "title": "18146"
    },
    {
        "abs": "Dynamic voltage scaling (DVS) is a technique which is widely used to save energy in a real time system. Recent research shows that it has a negative impact on the system reliability. In this paper, we consider the problem of the system reliability and focus on a periodic task set that the task instance shares resources. Firstly, we present a static low power scheduling algorithm for periodic tasks with shared resources called SLPSR which ignores the system reliability. Secondly, we prove that the problem of the reliability-aware low power scheduling for periodic tasks with shared resources is NP-hard and present two heuristic algorithms called SPF and LPF respectively. Finally, we present a dynamic low power scheduling algorithm for periodic tasks with shared resources called DLPSR to reclaim the dynamic slack time to save energy while preserving the system reliability. Experimental results show that the presented algorithm can reduce the energy consumption while improving the system reliability.",
        "title": "18209"
    },
    {
        "abs": "Recently, a serial implementation of the one-bit auto- and cross-correlation functions (ACF and CCF respectively) in a field programmable gate array (FPGA) has been developed, based on asynchronous delay elements and counters, known as the counterbased correlation. This paper proposes a method of parallelizing this otherwise serial process, offering significant improvements in the applicability of this approach to more types of ACF. Furthermore, the possibility of obtaining lag results from a parallel data sequence without first shifting the entire sequence has been realized, hence decreasing the number of clock cycles necessary for the calculation of the ACF. A synchronous design was preferred here for reasons of stability and portability, the technology of choice again being an FPGA. The advantages offered by the counterbased implementation in terms of device area usage and speed still apply. A practical implementation in the instrumentation of an upcoming space mission is also discussed.",
        "title": "18225"
    },
    {
        "abs": "This paper presents the design and implementation of a protocol offload engine that processes TCP/IP and remote direct memory access (RDMA) protocols by means of hardware/software coprocessing. In the offload engine, time-consuming operations such as TCP/IP header generation are implemented as hardware to improve performance. The software performs control operations and RDMA header generation. In the experiments and analyses, it is proved that the hardware can provide satisfactory performance to process all operations at speeds of over 1 Gbps. Our engine can offload most protocol processing overheads – up to 95% to 100% – from the host CPU. Finally, although the embedded processors operate with a 300 MHz clock that is seven times slower than the clock of the host CPU, our engine shows maximum bandwidths of 673 Mbps for TCP/IP and 551 Mbps for RDMA on a gigabit Ethernet network.",
        "title": "18246"
    },
    {
        "abs": "The major functional modules of the MetaStep Plus system are described and the relevant commands and their usage for each of the MetaStep processors are explained. An application example is given and all the source files and commands required for the Am29325 floating-point processor are presented. The generated microcode has been used in developing the software for an Am29300 microprogrammable computer board, which has been applied to the speeding up of highly repetitive portions of the programs in three-dimensional image generation.",
        "title": "18267"
    },
    {
        "abs": "DRAM technology requires refresh operations to be performed in order to avoid data loss due to capacitance leakage. Refresh operations consume a significant amount of dynamic energy, which increases with the storage capacity. To reduce this amount of energy, prior work has focused on reducing refreshes in off-chip memories. However, this problem also appears in on-chip eDRAM memories implemented in current low-level caches. The refresh energy can dominate the dynamic consumption when a high percentage of the chip area is devoted to eDRAM cache structures. Replacement algorithms for high-associativity low-level caches select the victim block avoiding blocks more likely to be reused soon. This paper combines the state-of-the-art MRUT replacement algorithm with a novel refresh policy. Refresh operations are performed based on information produced by the replacement algorithm. The proposed refresh policy is implemented on top of an energy-aware eDRAM cache architecture, which implements bank-prediction and swap operations to save energy. Experimental results show that, compared to a conventional eDRAM design, the proposed energy-aware cache can achieve by 72% refresh energy savings. Considering the entire on-chip memory hierarchy consumption, the overall energy savings are 30%. These benefits come with minimal impact on performance (by 1.2%) and area overhead (by 0.4%).",
        "title": "18446"
    },
    {
        "abs": "Integrated circuit designs are verified through the use of circuit simulators before being reproduced in real silicon. In order for any circuit simulation tool to accurately predict the performance of a CMOS design, it should generate models to predict the transistor’s electrical characteristics. The circuit simulation tools have access to massive amounts of data that are not only dynamic but generated at high speed in real time, hence making fast simulation a bottleneck in integrated circuit design. Using all the available data is prohibitive due to memory and time constraints. Accurate and fast sampling has been shown to enhance processing of large datasets without knowing all of the data. However, it is difficult to know in advance what size of the sample to choose in order to guarantee good performance. Thus, determining the smallest sufficient dataset size that obtains the same accurate model as the entire available dataset remains an important research question. This paper focuses on adaptively determining how many instances to present to the simulation tool for creating accurate models. We use Support Vector Machines (SVMs) with Chernoff inequality to come up with an efficient adaptive sampling technique, for scaling down the data. We then empirically show that the adaptive approach is faster and produces accurate models for circuit simulators as compared to other techniques such as progressive sampling and Artificial Neural Networks.",
        "title": "18556"
    },
    {
        "abs": "The design of a signal-strength acquisition system used for evaluating the nature of portable radio channels within buildings is discussed. Based on an 80C31 single chip microprocessor, the unit features low power operation, an analogue interface, data retention under power-off conditions and a serial communication link for data transfer. System performance is illustrated by way of results obtained during tests on a 2.34 GHz radio link. The unit described may be readily adapted to record other analogue voltage quantities in a portable mode.",
        "title": "18572"
    },
    {
        "abs": "We present a parallel algorithm which trains artificial neural networks on the TUTNC (Tampere University of Technology NeuroComputer) parallel tree shape neurocomputer. The neurocomputer is designed for parallel computation and it is suitable for several artificial neural network models. Detailed mathematical notations are used to describe the parallel back-propagation algorithm. Performance analyses show that we can effectively utilize both broadcasting and global adding between processing units in the presented parallel implementation. By using the given algorithm it is possible to add more processing units to the system without any change in the number of communication transactions required.",
        "title": "18579"
    },
    {
        "abs": "Approximate computing of non-trivial numeric functions is a well-known design technique and, therefore, used in several different application areas. Its main idea is the relaxation of conventional correctness constraints to achieve high performance results in terms of throughput and/or energy consumption. In this paper we propose an automated approximate design method for the fast hardware generation of two-dimensional numeric functions. By using multiplier-less gradients in combination with an advanced non-uniform segmentation scheme, high hardware performance is achieved. To qualify our approach, exhaustive evaluation is carried out, considering six different two-variable numeric functions. In a first step, a global complexity estimation is performed with varying design constraints on the algorithmic level. Out of this, most suitable candidates are selected for logic and physical CMOS synthesis. The results are compared to actual references highlighting our work as a powerful approach for the hardware-based calculation of two-variable numeric functions, especially in terms of throughput and energy consumption.",
        "title": "18588"
    },
    {
        "abs": "Shrinking size of transistors has enabled us to integrate more and more logic elements into FPGA chips leading to higher computing power. However, it also brings a serious concern to the leakage power dissipation of the FPGA devices. One of the major reasons for leakage power dissipation in FPGA is the utilization of prefetching technique to minimize the reconfiguration overhead (delay) in Partially Reconfigurable (PR) FPGAs. This technique creates delays between the reconfiguration and execution parts of a task, which may lead up to 38% leakage power of FPGA since the SRAM-cells containing reconfiguration information cannot be powered down. In this work, a resource management approach (RMA) containing scheduling, placement and post-placement stages has been proposed to address the aforementioned issue. In scheduling stage, a leakage-aware priority function is derived to cope with the leakage power. The placement stage uses a cost function that allows designers to determine the desired trade-off between performance and leakage-saving. The post-placement stage employs a heuristic approach to close the gaps between reconfiguration and execution of tasks, hence further reduce leakage waste. To further examine the trade-off between performance (schedule length) and leakage waste, we propose a framework to utilize the Genetic Algorithm (GA) for exploring the design space and obtaining Pareto optimal design points. Addressing the time-consuming limitation of GA, we apply Regression technique and Clustering algorithm to build predictive models for the Pareto fronts using a training task graph dataset. Experiments show that our approach can achieve large leakage savings for both synthetic and real-life applications with acceptable extended deadline. Furthermore, different variants of the proposed approach can reduce leakage power by 40–65% when compared to a performance-driven approach and by 15–43% when compared to state-of-the-art works. It’s also proven that our Machine Learning Optimization framework can estimate the Pareto front for new coming task graphs 10x faster than well-established GA approach with only 10% degradation in quality.",
        "title": "18617"
    },
    {
        "abs": "A system developed to overcome the limitations imposed by the original Transputer Development System (TDS2) upon file handling and graphics generation on PC-hosted systems is presented. TFGS was developed with two main objectives: to offer a programming interface consistent with popular PC-oriented languages while providing upwards compatibility with the TDS2. The techniques used to accomplish the above, and the tests that were made to evaluate the system's performance, are described. Finally, an analysis of the exhibited performance, based on a Petri net model, is given.",
        "title": "18831"
    },
    {
        "abs": "The CORDIC (coordinate rotation digital computer) algorithm is an iterative technique that can be used to compute many arithmetic functions using mainly shifts and additions making it ideal for FPGA implementation. In the early 1990s, Yu Hen Hu developed an equation for the overall quantisation error (OQE) experienced by the CORDIC algorithm when computing a vector magnitude. This equation could be used to find the most efficient architecture that would give a desired level of accuracy thus avoiding a trial and error approach. In this paper, we note that in fact the OQE overestimates the error in many cases, thus yielding inefficient architectures. Hence, this paper presents an updated equation for the OQE which is more accurate in predicting the error. To illustrate the improved accuracy of the new OQE expression, comparisons are made between CORDIC systems found using both versions of the OQE algorithm and Direct systems computing a vector magnitude. This comparison is of interest as it shows that CORDIC systems based on the new OQE expression use considerably fewer FPGA resources than CORDIC systems found using the original algorithm or equivalent direct designs. Given the widespread use of CORDIC in FPGA designs, particularly in DSP, this is significant.",
        "title": "18870"
    },
    {
        "abs": "In transmission planning, we need a good network design to minimize the construction cost and to determine the optimal location of substations. This paper presents a feasible design method, using two mathematical techniques (Dijkstra's algorithm and Transportation algorithm) to optimize substation sizes and service boundaries, given alternative locations for the substations under reliability constraints. A reliability index, the Average Annual Customer Interruption Rate (AACIR) is used to determine the reliability of loads. Appropriate measures can then be taken to increase the reliability of the system.",
        "title": "18947"
    },
    {
        "abs": "The object of this paper is to present a solution to the problem involved in dissemination of jobs by different microprocessors in a multimicroprocessor environment. In particular, the problem associated with systems where microprocessors with different machine language sets are to cooperate with each other on carrying out tasks. It presents a method of identifying bytes in a set of machine stored information to be ready for the translation process.",
        "title": "19025"
    },
    {
        "abs": "A memory manager system for a network of transputers is discussed. An application initiates the system and controls it during its lifetime. The term ‘memory manager’ is used as follows: on each processor, the system can manage a memory area provided by the application programmer, and controls concurrent file access (a file can be viewed as a kind of memory). These two tasks have been highly integrated. Some concepts and design decisions are presented to show that the system offers more than just a transputer version of conventional multitasking software.",
        "title": "19222"
    },
    {
        "abs": "This paper presents an efficient and optimized carrier phase independent programmable Symbol Timing Recovery (STR) circuit. The novel structure is highly versatile. In fact, it can be configured at runtime to work in different conditions. All BPSK, QPSK and OQPSK modulations are supported thanks to runtime variable control coefficients. This approach also provides flexibility in performances and support for different sampling rates. The proposed circuit is presented in a Digital PLL loop structure and it is designed according to the Software Defined Radio (SDR) philosophy, which requires ever more flexible communication solutions able to support different protocols and standards. High performances are reached by the proposed hardware implementation, moreover, flexibility is guaranteed by the configurable architecture. When implemented with a Xilinx XC4VLX60 FPGA chip, the new circuit reaches the maximum running frequency of 108.7 MHz, thus sustaining a symbol rate of 10 MSps when 10 samples per symbol are employed.",
        "title": "19260"
    },
    {
        "abs": "Industry uses an enormous variety of robots with different capabilities to perform different tasks. A possible alternative might be to use modular robots. These could be assembled with a combination of joints, links, tools and sensors as required to perform a particular task. This would require a robot control system which could be readily set up to control an arbitrary robot configuration. This paper describes the implementation of a modification of the Newton-Raphson algorithm to produce the inverse kinematics solutions to fulfil this requirement. The system is based on using the parallel processing capabilities of the Inmos transputer. Positioning accuracy experimental results from two robots are presented.",
        "title": "19293"
    },
    {
        "abs": "Asynchronous circuit design is a promising technology for large-scale multi-core systems. As a family of asynchronous circuits, Quasi-delay-insensitive (QDI) circuits have been widely used to build chip-level long interconnects due to their tolerance to delay variations. However, QDI interconnects are vulnerable to faults. Traditional fault-tolerant techniques for synchronous circuits cannot be easily used to protect QDI interconnects. This paper focuses on protecting QDI interconnects from transient faults. The first contribution of this paper is a fault-tolerant delay-insensitive redundant check code named DIRC. Using DIRC in 4-phase 1-of-n QDI pipelines, all 1-bit and some multi-bit transient faults are tolerated. The DIRC and basic pipeline stages are mutually exchangeable. Arbitrary basic stages can be replaced by DIRC ones to strengthen fault-tolerance. This feature permits designers to use DIRC flexibly according to the practical design requirement. The second contribution is a redundant technique protecting the acknowledge wires (RPA). Experimental results indicate that DIRC pipelines have moderate area and speed overheads. Compared with unprotected basic pipelines, the average speed decrease of DIRC pipelines is less than 50%, with the 128-bit 1-of-2 DIRC pipeline only 28% slower. In severe environments with multi-bit transient faults, the fault-tolerance capability of DIRC pipelines increases thousands-fold.",
        "title": "19299"
    },
    {
        "abs": "Arithmetic coding is the data compression techniques, which encodes the data by generating the code string that represents a functional value between 0 and 1. In this paper, we propose a modified-Adaptive Binary-RC (Range Coder) or M-ABRC. Our algorithm minimizes the multiplication bit capacity through introducing the VLSI architecture, proposed algorithm uses the LUP (Look UP Table)-VSW (Virtual Sliding Window) for the probability estimation. In order to achieve the higher compression rate, our method M-ABRC has been implemented, this in terms provides the better adoption probability in encoding phase and also gives the absolute estimation of low-EBS(entropy binary sources). Moreover In order to evaluate the algorithm we have compared with the several existing technique, comparison takes place based on the two parameter i.e. device utilization and the power dissipation (static and dynamic).",
        "title": "19397"
    },
    {
        "abs": "Inmos links are frequently used to connect IBM PC compatible host computers and parallel networks of transputers or other microprocessors. Most of the commercially available ISA bus to Inmos link adaptor interfaces are compatible with those embodied in either the Inmos IMS B004 transputer development board or the IMS B008 TRAM motherboard. Despite the widespread adoption of these ‘standards’ by Inmos and a host of third-party suppliers, details of how these interfaces operate, and how to implement them electrically, are not readily available. This paper provides full schematics, PLD source code and a detailed analysis of the operation of B004 and B008 compliant ISA bus to Inmos link adaptor interfaces, as well as schematics and PLD source code for an ISA bus to dual Inmos link adaptor interface.",
        "title": "19430"
    },
    {
        "abs": "We describe a SystemC library for specifying, modeling, and simulating hardware pipelines. The library includes a set of overloaded operators defining a pipeline expression language that allows the user to quickly specify the architecture of the pipeline. The pipeline expression is used to derive the connectivity of the SystemC modules that define the stages of the pipeline and to automatically insert latches and control modules between the stages to handle the proper routing of transactions through pipeline. Using the SystemC simulator the pipeline can then be simulated and evaluated. The pipeline expression language sits on top of SystemC, exposes all of the features of C++ and SystemC enabling the user to specify, evaluate, and analyze pipeline architectures.",
        "title": "19479"
    },
    {
        "abs": "The Horizon 2020 MANGO project aims at exploring deeply heterogeneous accelerators for use in High-Performance Computing systems running multiple applications with different Quality of Service (QoS) levels. The main goal of the project is to exploit customization to adapt computing resources to reach the desired QoS. For this purpose, it explores different but interrelated mechanisms across the architecture and system software. In particular, in this paper we focus on the runtime resource management, the thermal management, and support provided for parallel programming, as well as introducing three applications on which the project foreground will be validated.",
        "title": "19510"
    },
    {
        "abs": "Cryptography finds its application in various objects used in our everyday life. GSM communication, credit cards, tickets for public transport or RFID tags employ cryptographic features either to protect privacy or to ensure trustworthy authentication. However, many such objects are vulnerable to certain cryptanalytic attacks. In this review we discuss how FPGA-based cryptanalytic hardware may compromise GSM communication, or how standard laboratory equipment may be used for breaking Smart Card security. This review summarizes keynote speech that was given at 5th Mediterranean Conference on Embedded Computing (MECO’2016).",
        "title": "19551"
    },
    {
        "abs": "Resizable caches can trade-off capacity for access speed to dynamically match the needs of the workload. In single-threaded cores, resizable caches have demonstrated their ability to improve processor performance by adapting to the phases of the running application. In Simultaneous Multi-Threaded (SMT) cores, the caching needs can vary greatly across the number of threads and their characteristics, thus, offering even more opportunities to dynamically adjust cache resources to the workload. In this paper, we demonstrate that the preferred control methodology for data cache reconfiguring in a SMT core changes as the number of running threads increases. In workloads with one or two threads, the resizable cache control algorithm should optimize for cache miss behavior because misses typically form the critical path. In contrast, with several independent threads running, we show that optimizing for cache hit behavior has more impact, since large SMT workloads have other threads to run during a cache miss. Moreover, we demonstrate that these seemingly diametrically opposed policies are closely related mathematically; the former minimizes the arithmetic mean cache access time (which we will call AMAT), while the latter minimizes its harmonic mean. We introduce an algorithm (HAMAT) that smoothly and naturally adjusts between the two strategies with the degree of multi-threading. We extend a previously proposed Globally Asynchronous, Locally Synchronous (GALS) processor core with SMT support and dynamically resizable caches. We show that the HAMAT algorithm significantly outperforms the AMAT algorithm on four-thread workloads while matching its performance on one and two thread workloads. Moreover, HAMAT achieves overall performance improvements of 18.7%, 10.1%, and 14.2% on one, two, and four thread workloads, respectively, over the best fixed-configuration cache design.",
        "title": "19600"
    },
    {
        "abs": "The transfer of digital signals between individual units is the main bottleneck in high speed processing devices: interconnections introduce delay, and the noise originated within the system itself may cause errors. These are referred to as “signal integrity” problems; handling them requires familiarity with the analog and high frequency domains. The paper describes the section on Signal Integrity of a CD-ROM developed for university courses and for continuing education programs, motivating the extensive use of interactive multimedia and describing the design guidelines. Some examples on specific topics included in the package are also provided.",
        "title": "19605"
    },
    {
        "abs": "SDRAM memories are a commodity technology which deliver fast, cheap and high capacity external memory in many cost-sensitive embedded applications. When designing with SDRAM memory, the memory bandwidth available is strongly dependent on the sequence of addresses requested. For applications with hard real-time performance requirements, it is prudent to perform at compile time, some form of analysis to guarantee those hard real-time deadlines are met. In general with SDRAM memories, this analysis is difficult, and this leads to conservative implementations. On-chip memory buffers can make possible data reuse and request reordering which together ensure bandwidth on an SDRAM interface is used efficiently. This paper outlines an automated procedure for synthesizing application-specific address generators which exploit data-reuse in on-chip memory and transaction reordering on an external memory interface. We quantify the impact this has on memory bandwidth over a range of representative benchmarks. Across a range of parameterized designs, we observe up to 50× reduction in the quantity of data fetched from external memory. This, combined with reordering of the transactions, allows up to 128× reduction in the memory access time of certain memory-intensive benchmarks implemented in an FPGA. Since the synthesis procedure results in monotonic memory addressing functions, we can extract tight worst-case execution (WCET) bounds that are useful in system analysis. We show that we can extract performance guarantees which are significantly tighter than the absolute worst-case SDRAM performance.",
        "title": "19655"
    },
    {
        "abs": "The 82786 graphics coprocessor has two separate and independent processors on chip, one for drawing and another for display. To use this device to full advantage the device must be programmed to allow efficient use of its resources. The paper describes how the processor sees the 82786. It describes the register model for the programmer and illustrates the data structures in memory which are used for communication between the CPU and the 82786. Examples are presented of how the graphics and display processors are programmed and used by the CPU to draw pictures and display windows on screen.",
        "title": "19656"
    },
    {
        "abs": "Mechanisms that can generate maximum-length sequences (M-sequences) are of particular interest in pseudoexhaustive or pseudorandom built-in Test Pattern Generation (TPG). The characteristic polynomial of these mechanisms is restricted to be chosen among primitive polynomials only, since the latter require only one initialization state (seed). Recently, it was shown that any non-primitive irreducible polynomial can be used as characteristic polynomial to generate an M-sequence in the minimum number of cycles and with hardware overhead bounded by a low constant irrespectively of the number of seeds. In this paper, we describe and analyze the hardware overhead of two alternative schemes for the same purpose. These schemes offer even lower hardware overhead than the original one for a large majority of non-primitive irreducible polynomials of each degree. We also catalog each non-primitive irreducible polynomial of degree 12–22 according to the least costly of these schemes that can be used with it to generate an M-sequence.",
        "title": "19672"
    },
    {
        "abs": "In present computer structures in view of distributed data centers (DC) provides flexible management of network topology and functions in real time. This method allows to get rid of the excess costs of expensive equipment and network service. To provide resiliency of network infrastructure of DC it is necessary to use effective fast rerouting algorithms. In this work we propose adaptive rerouting algorithm of data flows in distributed DC based on the paired shifts data.",
        "title": "19711"
    },
    {
        "abs": "Many asynchronous systems are designed based on the successful operation of a handshake protocol for data transaction which meets the timing constraints imposed by different systems. For the practical case of the asynchronous bus operation of a micro-processor system, complex built-in mechanisms are developed to trigger various events to facilitate the operation of this protocol. With particular reference to the wait cycles insertion for a read cycle of the MC6800 microprocessor, we describe the general technique of time-range compatibility reasoning to capture the imprecise and complicated knowledge involved in diagnosing the correctness of a data transfer. An implementation using the expert system shell CLIPS illustrates how a simplified version of the proposed compatibility reasoning is used, together with the time-range constraint satisfaction and constraint propagation techniques previously developed, to solve the MC68000 read cycle problem which would otherwise be tackled only by human domain experts.",
        "title": "19865"
    },
    {
        "abs": "Real-time programming is one of the more demanding aspects of programming and an area where the introduction of the microprocessor has had a large impact both in industrial applications and in education. In this article, the design of a microcomputer-controlled train set is described together with its use in the teaching of real-time programming. Experience has shown that this equipment is both very popular with students and also is able to demonstrate many of the features of control of a real-time system.",
        "title": "19970"
    },
    {
        "abs": "Reconfigurable architectures are being increasingly widely used thanks to their high flexibility. This flexibility is due to their inherent ability to reconfigure (whether dynamically or not) functional blocks. Many configurations can be used and changed while the application is running in the reconfigurable system, and/or the same configuration can be used by several systems at different times. The flexibility of reconfigurable systems is greatly enhanced if they can be reconfigured remotely. But in the case of remote reconfiguration, security is of paramount importance. In this article, the problem of remote configuration is addressed by the concept of disposable configuration, an efficient, secure and reliable mechanism for maintaining the flexibility and security of the system. For the implementation of the secure reconfiguration protocol, we propose to use the secure-by-design HCrypt cryptoprocessor, providing low cost, high level protection against both hardware and software attacks without configuration throughput reduction. We validated the approach on a Xilinx Virtex-6 FPGA but it can be easily mapped to any other FPGA or SoC.",
        "title": "20008"
    },
    {
        "abs": "With the ever increasing industrial demand for bigger, faster and more efficient systems, a growing number of cores is integrated on a single chip. Additionally, their performance is further maximized by simultaneously executing as many processes as possible without regarding their criticality. Even safety critical domains like railway and avionics apply these paradigms under strict certification regulations. As the number of cores is continuously expanding, the importance of cost-effectiveness grows. One way to increase the cost-efficiency of such System on Chip (SoC) is to enhance the way the SoC handles its power resources. By increasing the power efficiency, the reliability of the SoC is raised because the lifetime of the battery lengthens. Secondly, by having less energy consumed, the emitted heat is reduced in the SoC which translates into fewer cooling devices. Though energy efficiency has been thoroughly researched, there is no application of those power saving methods in safety critical domains yet. The EU project SAFEPOWER 1 1 This project and the research leading to these results has received funding from the European Communitys H2020 program [H2020-ICT-2015] under grant agreement 687902. targets this research gap and aims to introduce certifiable methods to improve the power efficiency of mixed-criticality real-time systems (MCRTES). This article will introduce the requirements that a power efficient SoC has to meet and the challenges such a SoC has to overcome.",
        "title": "20009"
    },
    {
        "abs": "A cyber-physical system for a speech-controlled wheelchair has been proposed. It is based on cloud-harvesting principles, which result in significant improvement in command error rate (CER). The overall methodology and developed cloud-harvesting algorithm have been presented, discussed, and tested. The combination of IBM Watson and Google Cloud Speech APIs gave significantly better results than the use of solitary speech recognition APIs. The proposed approach shows the potential for and usability of speech-controlled wheelchairs, as well as in similar applications.",
        "title": "20152"
    },
    {
        "abs": "This paper proposes an architecture for fixed mesh based deformable motion estimation block of video encoder. While block motion estimation assumes purely translational motion in the video, the mesh based technique models non-translational motion as well. ASIC implementation of the proposed architecture (without decimation filter) using Synopsys tool (0.18 µm) at 100 MHz consumes 4.89 mW and occupies 0.076 mm2. It has achieved maximum frequency of 344.8 MHz with patch size of 16 and can process 613 frames of slow and 249 frames of fast motion CIF format video per second. Thus, the proposed architecture fits well for applications like video surveillance and video conferencing.",
        "title": "20196"
    },
    {
        "abs": "In a drive-by-wire steering system for the disabled driver, the safety of the system depends on the reliability of the electronic components. There is however another constraint, that of cost. A microprocessor-based system appears to be the most appropriate type. It was found however that three independent channels were needed to get the required level of reliability. The prototype system described here illustrates some of the problems and some of the potential solutions. Important among these is the balancing of the computations between the parallel channels.",
        "title": "20352"
    },
    {
        "abs": "For aiding hardware compilers in selecting appropriately timed hardware components, for fast timing simulation, and for high level timing model generation, a timing analysis method is developed. This method uses simple RC models with the property that they can recursively be linked to form models for modules consisting of smaller modules. Only the timing parameters of the outer module are significant for delay calculation of the primary outputs. In this paper the linkable models, recursive linking procedure and timing analysis applications based on these models will be described.",
        "title": "20366"
    },
    {
        "abs": "In this article we present an architectural design and analysis of a programmable image processor, named Snake. The processor was designed with a high degree of parallelism to speed up a range of image processing operations. Data parallelism found in array processors has been included into the architecture of the proposed processor. The implementation of commonly used image processing algorithms and their performance evaluation are also discussed. The performance of Snake is also compared with other types of processor architectures.",
        "title": "20433"
    },
    {
        "abs": "Although technology advancement can pack more and more physical registers in processors, the numbers of architectural registers defined by the instruction set architectures (ISAs) remain relatively small on most modern processors. Exposing more architectural registers to compilers and programmers can improve the effectiveness of compiler optimization and the quality of code. However, increasing the number of architectural registers by simply adding extra bits to the register fields of instructions will expand the code size. Therefore, a better way of exposing more ISA registers without significantly expanding the code size is needed. This paper presents a new ISA called Floating Accumulator Architecture ( FAA ) that can expand the number of ISA registers without increasing the instruction length. Unlike the accumulator architecture whose accumulator is a fixed, special register, FAA dynamically chooses a register from the general-purpose register file as the accumulator. In other words, the accumulator in FAA is an alias to some register in the register file at any instruction, and the alias relation can be dynamically updated by FAA at any program points. Since the accumulator implicitly stores the result, the destination register field can be omitted from FAA instructions, resulting in a saving of 3 to 5 bits for each instruction. This new free instruction bit space can be utilized in two possible ways: doubling the number of ISA registers of modern 32-bit RISC processors or maintaining the number of ISA registers for 16-bit instructions on embedded processors. This paper presents the result of utilizing the free bit space to double the number of ISA registers from 16 to 32 on ARM processors, and experimental results show that performance can be improved by 7.6% on average for MediaBench benchmarks.",
        "title": "20489"
    },
    {
        "abs": "Reverse engineering is a great peril for hardware security especially when functional behavior extraction is required. In this paper a new automated mechanism is proposed to encrypt routing topology of the design which leads to hinder reverse engineering during the foundry/fabrication process. Moreover, new special standard cells (Wire Scrambling cells) are proposed corresponding with an automatic design flow to insert the WS-cells inside the netlist with the aim of maximum effectiveness of obfuscation and minimum overhead. The highlight feature of this mechanism is that it can be performed without detailed information about the functionality and structure of the design and hence, it can be automated easily. This methodology is implemented using an academic physical design framework (EduCAD). Experimental results show that reverse engineering can be hindered considerably in cost of negligible overheads in area, power consumption and total wire length.",
        "title": "20559"
    },
    {
        "abs": "This paper presents new architectural concepts for uniprocessor system designs. They result in a uniprocessor design that conforms to the data-driven (i.e. dataflow) computation paradigm. It is shown that usage of this, namely D2-CPU (Data-Driven) processor, follows the natural flow of programs, minimizes redundant (micro)operations, lowers the hardware cost, and reduces the power consumption. We assume that programs are developed naturally using a graphical or equivalent language that can explicitly show all data dependencies. Instead of giving the CPU the privileged right of deciding what instructions to fetch in each cycle (as is the case for CPUs with a program counter), instructions are entering the CPU when they are ready to execute or when all their operand(s) are to be available within a few clock cycles. This way, the application-knowledgeable algorithm, rather than the application-ignorant CPU, is in control. The CPU is used just as a resource, the way it should normally be. This approach results in outstanding performance and elimination of large numbers of redundant operations that plague current processor designs. The latter, conventional CPUs are characterized by numerous redundant operations, such as the first memory cycle in instruction fetching that is part of any instruction cycle, and instruction and data prefetchings for instructions that are not always needed. A comparative analysis of our design with conventional designs proves that it is capable of better performance and simpler programming. Finally, VHDL implementation is used to prove the viability of this approach.",
        "title": "20752"
    },
    {
        "abs": "The paper describes the implementation of a novel smart data processing hardware system having medical diagnostic capabilities. The system is implemented in a FPGA chip for fast design cycle. The system employs a smart agent that can predict the future pathophysiological state of a patient using the past pathophysiological data and can thus predict the approaching critical condition of the patient before criticality actually occurs. This feature has been realized using fuzzy logic. In order to speed up the computation process, pipelined data processing architectures have been employed leading to a speed up of approximately three times. The whole system is realized on Altera Cyclone EP1K6Q240C8 FPGA chip requiring 5308 logic blocks. The system has been designed to be inexpensive, portable and user friendly for rural applications in developing countries.",
        "title": "20795"
    },
    {
        "abs": "As DRAM-based main memory becomes a dominant factor in the energy consumption and cost of any computer system, new non-volatile memory technologies have been proposed to replace DRAMs. For example, PRAM is emerged as a leading alternative for main memory technology. However, the access latency of PRAM is significantly slower than that of DRAM and an interfacing converter is required to at least partly alleviate this latency difference. The interfacing converter sits between PRAM-based main memory and the last level of cache memory. In this paper, we present a proposed dynamic adaptive converter and its management scheme for PRAM-based main memory. In addition to overcoming long access latency, it provides enhanced endurance. The adaptive converter is composed of an aggressive streaming buffer to make better use of spatial locality by dynamically varying fetch size, a write buffer to improve endurance limit, and an adaptive filtering buffer to better utilize temporal locality. Our experimental results show that we can reduce buffer miss rate by about 59%, compared with using a single buffer structure with same space. Our approach also hides PRAM access latency more effectively. It improves the number of superblocks pre-fetched from main memory by 25%. Therefore, the converter shows its effectiveness comparable to a case with larger buffer space, without expending the extra power.",
        "title": "20832"
    },
    {
        "abs": "Contour detection is an algorithm often utilized in picture processing. Sometimes it is useful to localize edges with sub-pixel accuracy. Many methods have been developed for edge detection with sub-pixel accuracy. The question is, how the accuracies of these methods change if the scanned object is moving during the exposure time. In this paper, the impact of object movement on edge detection accuracy is examined. To simulate the moving edge, the accumulative function is defined and then used in three edge detection algorithms with sub-pixel accuracy in 1-D images: algorithm based on approximation of image function with function erf (AEF), method based on statistical moments (GLM) and technique using spatial moments of the 1-D image (SM). Results of simulations with noisy images are presented, the upper and lower 5% quantiles are chosen as accuracy criterion. Gray level moment edge detector (GLM) is used for FPGA implementation, because of its accuracy and simplicity.",
        "title": "20856"
    },
    {
        "abs": "The features of a DMA device designed for use with 286, 186, 086 and 088 Intel-type buses are given. The SAB 82258 is also an I/O controller. The principles of data transfer in various modes are detailed, with some examples of data transfer which may be controlled or modified to suit various environments. Methods of interfacing with a CPU are outlined. This includes programming and the use of the multiplexer channel.",
        "title": "20879"
    },
    {
        "abs": "The concept of moving time division multiplex (TDM) slots has been suggested as a method of avoiding collisions on an Ethernet system carrying voice and data packets. This paper describes a technique for implementing a type of moving TDM slot system with currently available VLSI Ethernet devices. The technique is hardware based and has the advantage of operating within the standard Ethernet parameters and of having a low cost of implementation.",
        "title": "20975"
    },
    {
        "abs": "The paper presents a system design consideration of one hardware and software solution of PC connection to a primary rate ISDN link. The hardware, as an active PC card with an 80C188 microprocessor with Mitel ISDN communication circuits, and with EPLD interface circuits, is completely described. Software architect, with tasks and their modules, is also presented. With Windows 95, PPP or MP protocol, this solution offers to PC WAN communication all the advantages of ISDN services.",
        "title": "21033"
    },
    {
        "abs": "Lack of appropriate compilers for generating configurations and their scheduling is one of the main challenges in the development of reconfigurable computing systems. In this paper, a new iterative design flow for reconfigurable computing systems is proposed that integrates the synthesis and physical design phases to perform a static compilation process. We propose a new temporal partitioning algorithm for partitioning and scheduling, which attempts to decrease the time of reconfiguration on a partially reconfigurable hardware. In addition, we perform an incremental physical design process based on similar configurations produced in the partitioning stage. To validate the effectiveness of our methodology and algorithms, we developed a framework according to the proposed methodology.",
        "title": "21036"
    },
    {
        "abs": "Despite the numerous advantages of digital signal processing (DSP), and the existence of a wealth of sophisticated algorithms, the widespread application of DSP has been hindered by a shortage of cost-effective digital hardware for volume use. Now, with the availability of low-cost high-speed VLSI circuits, such as the Texas Instruments TMS320, the virtues and power of DSP are ready for realtime signal processing applications. Development of the single-chip TMS320 DSP computer began in the late 1970s. The architecture, key novel features, instruction set and capabilities of this device are examined. Support software and hardware development tools for the TMS320 are also included with some of its applications. Typical areas of application of the TMS320 include speech signal processing, telecommunications, robotics, radar signal and sonar signal processing, seismology, image processing, audio recording and reproduction, biomedical instrumentation, acoustic noise measurements and automatic test equipment.",
        "title": "21075"
    },
    {
        "abs": "The main limitation of shared-bus multiprocessors is that the common bus tends to be the primary source for contention, and thus imposes a limit on the number of processors in the system. Alternative architectural features are necessary to reduce the memory bandwidth demands and to increase the bus bandwidth. In this paper, we investigate the cost-performance effects of two enhancements: higher bus transaction rates, e.g., through the use of multiple buses, and shared two-level caches. The performance figures are obtained via simulation with loads derived from traces of real applications, some of which show a significant skew in the distribution of memory bank access. A new multiple bus scheme, called multiple interleaved buses, is described and analysed. This scheme is a generalization of previous approaches, and attempts to balance performance and cost trade-offs in a snoopy-cache multiprocessor environment. The results from simulation show that multiple interleaved buses perform almost as well as multiple independent buses, but with simpler and less costly implementation. Furthermore, multiple interleaved buses are shown to deliver much better performance than interleaved buses when the skew of accesses across the interleaves is large. Shared second-level caches have been shown to be very effective in the design space under consideration. Such systems might offer considerable implementation economies with relatively small design cost. We show that depending on the design point in question, bus operation buffers might be useful in shared second level caches by reducing the effects of high skew and greater multiprocessing level. With the presence of these buffers, the uses of shared caches resulted in only a small throughput degradation.",
        "title": "21108"
    },
    {
        "abs": "The advent of complicated embedded systems with regard to relentless technology scaling and integration of more components into a single chip, have caused these systems to be less reliable. Moreover, these advancements have accompanied with a drastic increase in energy consumption. However, using energy management techniques such as Dynamic Voltage and Frequency Scaling (DVFS) has made reliability issues to be exacerbated. Hence, energy efficiency and fault-tolerance are two conflicting key objectives in the design of efficient real-time embedded systems. In this paper by considering periodic real-time tasks in standby-sparing system, we propose a novel online scheduling technique, which tackles this problem. The aim of this paper is twofold: (1) to show that scheduling primary and backup tasks in different processors in a moderate speed for static-priority real-time tasks will lead to better energy savings; (2) to explore the effectiveness of this technique for mixed-criticality tasks. Our simulation results reveal a significant energy saving (up to 25%) in comparison with the state-of-the-art scheme based on standby-sparing system, while preserving system's original reliability.",
        "title": "21299"
    },
    {
        "abs": "SPA is a synthesised, self-timed, ARM-compatible processor core designed for use in security-sensitive applications. It was incorporated in an experimental smartcard chip which is being used to evaluate the applicability of self-timed logic in secure devices. The system-on-chip was synthesised using the Balsa synthesis system with only a small amount of hand design employed to boost the throughput of the on-chip interconnect. The use of synthesis was mandated by a need for rapid implementation and Balsa proved to be very effective: SPA required only 25% of the design effort of earlier non-synthesised Amulets. Balsa was modified to generate circuits with enhanced security against non-invasive attacks. Initial analyses indicate that the secure SPA achieved up to 80% improvement in resistance against non-invasive attacks albeit at the cost of reduced performance and increased area and power consumption.",
        "title": "21311"
    },
    {
        "abs": "For embedded systems, the power dissipation on buses has become an essential issue in recent years. Many real-time embedded processors, such as DSP processors, adopt the Harvard architecture in which the data and instruction buses are separated to avoid processing-speed degradation. The power dissipation on an instruction bus can be reduced if the switching activities between consecutive instructions on that bus are reduced. Two efficient algorithms, the greedy method and the dynamic programming based method, are proposed to swap commutative source register fields of adjacent instructions. The switching activities on the instruction bus are therefore reduced, without affecting the execution results. Experimental results show that the proposed schemes result in a reduction of as much as 21.43% in the switching activities of consecutive source register fields between commutative blocks. In addition, the proposed schemes can be conveniently integrated with other encoding schemes to further improve the power dissipation on an instruction bus.",
        "title": "21343"
    },
    {
        "abs": "This paper presents ‘surfing,’ a novel variation of wave pipelining. In previous wave pipelined designs, timing uncertainty grows monotonically as data propagates through gates and other logic elements. Our designs propagate a timing pulse along with the data values, and our logic elements have delays that decrease in the presence of the pulse. This produces a surfing effect wherein events are bound in close proximity to the timing pulse. This produces a robust variant of wave-pipelining where timing dispersion is bounded regardless of the length of the pipeline. We demonstrate our approach with the design of a simple proof-of-concept chip.",
        "title": "21391"
    },
    {
        "abs": "Methods to access complex memory-mapped peripherals from modula-2 programs are discussed and it is demonstrated that assembly language modules are rarely needed. Efficient solutions, however, require detailed knowledge of the compiler code generation process and of the target hardware structure. Procedures and data structures are presented which access an ISDN card in a Macintosh Nubus-slot. The solutions offer a standard interface to the operating system and meet the stringent realtime conditions of the 64 kbit s−1 ISDN channels. Examples are given for elegant and for less elegant solutions.",
        "title": "21410"
    },
    {
        "abs": "Examining the validity or accuracy of proposed available bandwidth estimation tools remains a challenging problem. A common approach consists of evaluating a newly developed tool using a combination of simple ns-type simulations and feasible experiments in situ (i.e., using parts of the actual Internet). In this paper, we argue that this strategy tends to fall short of establishing a reliable “ground truth,” and we advocate an alternative in vitro -like methodology for calibrating available bandwidth estimation tools that has not been widely used in this context. Our approach relies on performing controlled laboratory experiments and using a set of tools to visualize and analyze the relevant tool-specific traffic dynamics. We present a case study of how two canonical available bandwidth estimation tools, S pruce and P athload , respond to increasingly more complex cross traffic and network path conditions. We expose measurement bias and algorithmic omissions that lead to poor tool calibration. As a result of this evaluation, we designed a calibrated available bandwidth estimation tool called Y az that builds on the insights of P athload . We show that in head to head comparisons with S pruce and P athload , Y az is significantly and consistently more accurate with respect to ground truth, and reports results more quickly with a small number of probes.",
        "title": "21440"
    },
    {
        "abs": "The design and implementation of a multichannel 12-bit data acquisition and control system for laboratory use is described. The system is provided with software capable of recording experimental data directly as a function of experimental parameters. Control logic, data acquisition, high- and low-speed D/A conversion and system software are discussed. The properties of the system were tested during spectroscopic measurements on metal-insulator-semiconductor (MIS) transistors in the far infrared region of the spectrum.",
        "title": "21531"
    },
    {
        "abs": "‘Transparent’ interfacing allows nonstandard input devices such as speech recognizers to emulate the de facto standard input device, the keyboard, enabling standard commercial software to accept input from speech recognizers, touch sensitive devices, bar code readers etc. without modification. The paper describes experiences with the transparent interfacing of a number of different speech recognizers to microcomputers in order to study the issues involved. Two such systems are described: an Interstate VRM recognizer interfaced to an Acorn BBC model B microcomputer to allow computer access for the physically disabled; and an Interstate VRT300 device interfaced to a Cromemco Z-2D computer to investigate the potential of speech recognition to assist in the preparation of television subtitles. The extent to which the manufacturer of the speech recognizer had provided for transparent interfacing was found to have a profound impact. A number of human factors were also revealed as salient. The authors conclude that transparent interfacing, vital if the potential of new input technologies is to be realized, demands considerable forethought. Manufacturers of novel input devices and of microcomputers should make their products as flexible as possible in terms of I/O capability.",
        "title": "21536"
    },
    {
        "abs": "The transputer communication link is a new standard for system interconnection. It uses the capabilities of VLSI to offer simple, easy-to-use and cheap interconnection for computer systems. This paper describes the transputer's communications link and the considerations which have influenced its design. The advantages of the link are shown in a conventional application. The link adaptor, which interfaces a communications link to the outside world, is introduced. Inmos makes two types of link adaptor. Each is described with example applications.",
        "title": "21555"
    },
    {
        "abs": "In this paper, VLSI-DSP based a real time solution for Digital Scan Conversion (DSC) and speckle reduced imaging (SRI) of an ultrasonography (USG) are proposed. DSC is needed to convert the polar format data to Cartesian coordinate to make them compatible for the display device. To reduce Moirè artifact a new interpolation algorithm is proposed which is a combination of warped distance based adaptive bilinear interpolation method and modified selective sampling technique. This new algorithm is implemented on TMS320C64x + DSP and is capable of processing 640 × 480 size images at the rate of 83 frames per second. Moreover, to reduce speckle noise a modified aggressive region growing filter is implemented on Xilinx XC2VP30 FPGA. It operates at 128.29 MHz frequency, which is equivalent to processing 417 frames per second. With such high frame rate, motion blind detection of the objects may be possible using USG.",
        "title": "21580"
    },
    {
        "abs": "This work is concerned with the implementation of evolutionary based computer algorithms, genetic algorithms (GAs), on microcontrollers. Microcontrollers are low resource platforms not normally associated with GAs, which are typically resource intensive. This implementation will add to a suite of tools, based on different soft computing techniques, which will solve a range of optimization and decision-making problems that are essential in order for an autonomous guided vehicle (AGV) to carry out complex and highly involved tasks. Two examples are presented: path planning and the T-sequencing problem to demonstrate that the GA on an 8051 microcontroller is capable of carrying out necessary optimization and decision-making for AGVs.",
        "title": "21677"
    },
    {
        "abs": "This article presents the construction of a family of ATM switching fabrics to be used as generic components in ATM switching. Such a component absorbs the output contention problem and traffic burtiness without requiring centrally controlled routing, preprocessing of input traffic, or packet buffering. One application is to serve as the main module in a large self-route switch core. The construction is by the deployment of a partial sorter in place of every routing cell in a banyan-type routing network. In order to partition the large fabric into a generic component of the chip size and also to optimize the layout, the specific choice of the underlining banyan-type network is a divide-and-conquer network. Meanwhile, partial sorters are constructed by the k-sorting algorithm, which unifies and generalizes the classical algorithms of binary merge exchange , bitonic sorting , and odd–even merge exchange .",
        "title": "21743"
    },
    {
        "abs": "We present a methodology devoted to real-time system design using multiprocessors. The methodology is proposed according to five aspects: environment, function, behaviour, performance and hardware. The proposed methodology is based on the decomposition of the system development life cycle into six stages: definition of requirements, analysis of the system environment, refinement of the system, analysis of the system behaviour, evaluation of performance and implementation. This methodology is validated by an application which aims to generate transfer frame telemetry that complies with CCSDS recommendations.",
        "title": "21826"
    },
    {
        "abs": "Many multi-transputer applications need to perform disc I/O and to buffer intermediate results between processes. Performance can be improved by using the local memory of all the transputers as a single global resource. This is made possible for all the transputers through an abstract object called a ribbon. A ribbon is a generalization of the file abstraction provided by a modern file system. This paper describes the concept, implementation and application of ribbons.",
        "title": "21860"
    },
    {
        "abs": "A distributed, analogue/digital data acquisition controller implementing the unique programmable counter array (PCA) available on the 80C51FA microcontroller together with its automatic address recognition as a low level communication protocol has been developed. Emphasis is placed on the design of a more flexible distributed controller which is capable of simultaneously handling up to 16 channels of analogue and 16 channels of digital I/O using versions of the industry standard I/O modules. Each individual channel can be independently configured as either input or output. These controllers can be linked together in a multidrop network configuration with a PC as the primary controller. An increase in product functionality at a lower cost is thereby achieved.",
        "title": "21870"
    },
    {
        "abs": "String matching is a time and resource consuming operation that lies at the core of Network Intrusion Detection Systems. In this paper a method and corresponding hardware architecture for string matching is presented. The proposed method is composed of two main steps. The first step performs a pre-detection of signatures alignment, and in the second step the alignment is corrected and the signatures are detected by a matcher. The compact and efficient architecture is designed to share resources among several modules that perform the detection and correction step needed for the string matching. Implementation results in a FPGA Virtex5 device show that the proposed architecture can perform string matching with a database with more than 400 K characters. And is also capable of achieving speeds of more than 30Gbps, which is much higher that previous works reported in the literature.",
        "title": "21915"
    },
    {
        "abs": "The last decade must rank as the most innovatory in the history of computing, witnessing the establishment of the minicomputer and data communications and ushering in the microprocessor and office automation. The 80s will surely continue to expand upon this base, inducing major changes in the computer industry and indeed in all walks of life, though it is unlikely to have the same impact as we now expect rapid rates of change. The next generation, being introduced to computers at school, and in toys, will have lost any fears due to ignorance. However, although the use of computers will advance and the industry in total will thrive, specific companies will see very hard times and people will have to accept retraining as technology changes job specifications.",
        "title": "21991"
    },
    {
        "abs": "In this paper the design of a RISC, pipelined and superscalar processor (Kydon-RISC) is presented. The Kydon-RISC mainly consists of five independent execution units (integer, floating point, branch/jump, load/store, and I/O buffers). The unique features of the Kydon-RISC are the I/O buffers, which have the ability of sending and receiving multiple data packets to/from different resources (or processors) at the same time. A data packet represents the graph forms of the objects extracted from images received and processed by the Kydon vision system. A comparative evaluation among Kydon-RISC and other RISC processors has shown that the Kydon-RISC performs 15–25% better in a set of primitive operations (summation, many multiplication, matrix multiplication, bubble sort, procedure call, etc.) used in any large scale program.",
        "title": "22124"
    },
    {
        "abs": "An SSD generally has a small memory, called cache buffer, to increase its performance and the frequently accessed data are maintained in this cache buffer. These cached data must periodically write back to the NAND Flash memory to prevent the data loss due to sudden power-off, and it should immediately flush all dirty data items into a non-volatile storage media (i.e., NAND Flash memory), when receiving a flush command, while the flush command is supported in Serial ATA (SATA) and Serial Attached SCSI (SAS). Thus, a flush command is an important factor to give significant impact on SSD performance. In this paper, we have investigated the impact of a flush command on SSD performance and have conducted in-depth experiments with versatile workloads, using the modified FlashSim simulator. Our performance measurements using PC and server workloads provide several interesting conclusions. First, a cache buffer without a flush command could improve SSD performance as a cache buffer size increases, since more requested data could be handled in the cache buffer. Second, our experiments have revealed that a flush command might give a negative impact on SSD performance. The average response time per request with a flush command is getting worse compared to not supporting the flush command, as cache buffer size increases. Finally, we have proposed the backend flushing scheme to nullify the negative performance impact of the flush command. The backend flushing scheme first writes the requested data into a cache buffer and sends the acknowledgment of the request completion to a host system. Then, it writes back the data in the cache buffer to NAND Flash memory. Thus, the proposed scheme could improve SSD performance since it might reduce the number of the dirty data items in a cache buffer to write back to NAND Flash memory. All these results suggest that a flush command could give a negative impact on SSD performance and our proposed backend flushing scheme could improve the SSD performance while supporting a flush command.",
        "title": "22250"
    },
    {
        "abs": "Identifying abnormal occasions in an observed area has been of the significant applications in Wireless Sensor Networks (WSNs) and the Internet of Things. Accidents and property harm can be maintained a strategic distance from if precise alarms are informed on time. In conventional checking techniques, a predefined threshold is given, and a signal is activated when the sensor perusing surpasses this threshold. This Single Threshold based Monitoring (STM) experiences the substandard nature of detected data, bringing about numerous false alarms. This work proposes an Adaptive Probabilistic Threshold Monitoring (APTM) technique for WSNs, where a signal is activated if the likelihood of the checked esteem being more significant than a predefined threshold (α) is more impressive than time delay (τ). The tight upper limits of the likelihood that controlled sum is more significant than the predetermined threshold are given. As indicated by the breaking points, probabilistic threshold-based algorithms for conglomeration checking are proposed. Broad execution assessment shows the adequacy of the proposed algorithms—the proposed design centers on observing the driver's level of diversion by checking optical parameters, health condition, driving example of the driver. It additionally screens street and activity conditions, the sudden entry of animal on the roadways. By a broad experimental assessment utilizing original dataset, the proposed algorithms beat the STM strategy in term of false alarm rate MSE is 2db.",
        "title": "22270"
    },
    {
        "abs": "This report describes how a combination of two testbeds can be used to confirm that low level code (typically output from a compiler) is functionally equivalent to the high level code from which it was derived. This method, developed by the company as part of its customer support programme, establishes that code written in a high level language retains its functionality after compilation. In essence the objective is to demonstrate that the low level code (a machine code or assembler language representation) is functionally equivalent to the high level code from which it was derived by comparison of the static and dynamic characteristics of the two forms of the code. Two testbeds (hence the name DUAL) are used to perform static and dynamic analysis of both the high level and the low level codes.",
        "title": "22376"
    },
    {
        "abs": "An overview of a microprocessor-based online accident location system is given. The system, designed mainly for emergency medical services, is not only capable of displaying all accident occurrences at a given location, intersection or street, but can also be used for accident report management and as a tool for guiding lost vehicles. The highway reference system used for this system is LORAN-C.",
        "title": "22381"
    },
    {
        "abs": "In recent years, the security of integrated circuits (ICs) has received more attention as usage of ICs made by untrustworthy foundries has increased in safety-critical systems [1] . In this paper, we introduce a novel approach for fingerprinting the delay of functional paths in a sequential circuit that have millions of transistors, like processors. We present a method for inserting Ring Oscillators (ROs) into scan chain for measuring the delay of each functional path inside the chip. Using the proposed method, the payload part of Trojans will be detected according to their size and cell types. Our method can be used by power-based Trojan detection approaches for finding the trigger part of Trojans. We also introduce an algorithm for quantifying the vulnerability of the nets. We show that a designer can achieve a good trade-off between area overhead reduction and security enhancement while evaluating the vulnerability degree of each net.",
        "title": "22390"
    },
    {
        "abs": "One of the main tasks of a mobile robot in an unknown environment is to build and update a map of the environment and simultaneously determine its location within this map. This problem is referred to as the simultaneous localization and mapping (SLAM) problem. The article introduces scan-matching genetic SLAM (SMG-SLAM), a novel SLAM algorithm. It is based on a genetic algorithm that uses scan-matching for gene fitness evaluation. The main scope of the article is to present a hardware implementation of SMG-SLAM using an field programmable gate array (FPGA). The architecture of the system is described and it is shown that it is up to 14.83 times faster compared to the software algorithm without significant loss in accuracy. The proposed implementation can be used as part of a larger system, providing efficient SLAM for autonomous robotic applications.",
        "title": "22411"
    },
    {
        "abs": "H.264 intra prediction algorithm has a very high computational complexity. In this paper, we propose pixel equality and pixel similarity based techniques for reducing the amount of computations performed by H.264 intra prediction algorithm and therefore reducing the power consumption of H.264 intra prediction hardware. These techniques exploit pixel equality and similarity in a video frame by performing a small number of comparisons among pixels used in prediction equations before the intra prediction process. If the pixels used in prediction equations are equal or similar, prediction equations simplify significantly. By exploiting the equality and similarity of the pixels used in prediction equations, the proposed pixel equality and pixel similarity based techniques reduce the amount of computations performed by 4 × 4 intra prediction modes up to 78% and 89%, respectively, with a small comparison overhead. We also implemented an efficient 4 × 4 intra prediction hardware including the proposed techniques using Verilog HDL. The proposed pixel equality and pixel similarity based techniques reduced the power consumption of this hardware up to 13.7% and 17.2%, respectively. The proposed pixel equality based technique does not affect the PSNR and bitrate. The proposed pixel similarity based technique increases the PSNR slightly for some videos frames and it decreases the PSNR slightly for some videos frames.",
        "title": "22416"
    },
    {
        "abs": "A VLSI efficient multiplier-less architecture for real-time computation of multi-dimensional convolution is presented in this paper. The new architecture performs computations in the logarithmic domain by utilizing novel multiplier-less log2 and inverse-log2 modules which are capable of converting the fraction numbers currently not available in the literature. An effective data handling strategy is developed in conjunction with the logarithmic modules to eliminate the necessity of multipliers in the architecture. The proposed approach reduces hardware resources significantly compared to other approaches maintaining a high degree of accuracy. The architecture is developed as a combined systolic-pipelined design that produces an output in every clock cycle after an initial latency of 93.19 uSec. The architecture is capable of operating with a clock frequency of 99 MHz based on Xilinx’s Virtex II 2v2000ff896-4 FPGA and the throughput of the system is observed as 99 MOPS (million outputs per second). Error analysis performed with the FPGA-based system in the image processing examples of edge detection and noise filtering shows that the proposed architecture produces outputs similar to that obtained by software simulation using Matlab.",
        "title": "22471"
    },
    {
        "abs": "Real-time access to a large terrain database may become a complex problem when the entire system is resident in a vehicle undergoing rapid changes of motion. Due to space and cost limitations of the application, a system has been developed to work on a microcomputer system which includes transputers to allow parallel processing. This paper describes the use of an experimental system which has been constructed to allow continued efficient operation during periods when the physical disc medium is inaccessible.",
        "title": "22541"
    },
    {
        "abs": "The radio frequency spectrum, a scarce resource in mobile communications, has to be efficiently utilized with the objectives of increasing the network capacity and minimizing the interference. A variety of channel assignment strategies have been developed to achieve these objectives. As the cell sizes get smaller, there is a greater need for efficient channel assignment algorithms which are desired to dynamically balance the load of the system. We propose a dynamic multi-channel assignment (DMCA) algorithm where the assignment decision is assisted by the mobiles. Our algorithm is based on the concept of network flows and handles all the events in the system gracefully. Several existing channel assignment algorithms can be easily modeled in the proposed network flow framework model. Simulation results show that DMCA algorithm performs well under heavy traffic conditions and handles different traffic classes gracefully.",
        "title": "22730"
    },
    {
        "abs": "It is common for the network topology to change during its operation, which demands that the network defense system adapt itself for the current topology. Aiming at such need, this paper provides a novel defense model for the dynamic topology network, which includes three modules: network topology discovery , adaptive agents re-configuration mechanism and active defense . The model is based on mobile agent technology, and contains two kinds of agents: topology discovery agent and defense agent. The model uses topology discovery agents to actively probe the current network topology and encodes it. Then the adaptive re-configuration mechanism of the model implements the distribution and migration of the defense agents according to the current topology. Thus, the re-configured defense agents provide active defense for the network. The whole model emerges with the Markov property, which is also analyzed in the paper.",
        "title": "22773"
    },
    {
        "abs": "Analytical models for wormhole routing in multicomputer networks have been widely reported in the literature. Although several recent studies have revealed that pipelined circuit switching (PCS) can provide superior performance characteristics over wormhole routing, there has been hardly any study that describes performance models for PCS. As a result, all existing studies have resorted to simulation when evaluating the performance merits of PCS. In an effort to fill this gap, this paper proposes the first analytical model for PCS in hypercube networks. The validity of the model is demonstrated by comparing analytical results with those obtained through simulation experiments.",
        "title": "22777"
    },
    {
        "abs": "As in general software engineering, the increasing and unsatisfied demand for automation software has focussed attention towards the portability and reuse of software. In this context severe challenges are offered by software components with high quality requirements, such as the capability for multitasking, hard timing conditions, high (forecastable) reliability, as well as for components with direct access to hardware and system software. With the realtime operating system XMod an attempt has been made to design such a system that is portable and, to a certain degree, independent of its environment. A portable software development methodology has been developed based on modula-2 . Using this approach it was possible to implement XMod for different microprocessor classes (Intel, Zilog) without extensive additional effort. A similar approach was used to develop high-level language device drivers and a high-level language symbolic realtime debugger.",
        "title": "22869"
    },
    {
        "abs": "Many-core systems provide a great performance potential with the massively parallel hardware structure. Yet, these systems are facing increasing challenges such as high operating temperatures, high electrical bills, unpleasant noise levels due to active cooling and high battery drainage in mobile devices; factors caused directly by poor energy efficiency. Furthermore by pushing the power beyond the limits of the power envelope, parts of the chip cannot be used simultaneously – a phenomenon referred to as “dark silicon”. Power management is therefore needed to distribute the resources to the applications on demand. Traditional power management systems have usually been agnostic to the underlying hardware, and voltage and frequency control is mostly driven by the workload. Static schedules, on the other hand, can be a preferable alternative for applications with timing requirements and predictable behavior since the processing resources can be more precisely allocated for the given workload. In order to efficiently implement power management in such systems, an accurate model is important in order to make the appropriate power management decisions at the right time. For making correct decisions, practical issues such as latency for controlling the power saving techniques should be considered when deriving the system model, especially for fine timing granularity. In this paper we present an accurate energy model for many-core systems which includes switching latency of modern power saving techniques. The model is used when calculating an optimal static schedule for many-core task execution on systems with dynamic frequency levels and sleep state mechanisms. We derive the model parameters for an embedded processor with the help of benchmarks, and we validate the model on real hardware with synthetic applications that model streaming applications. We demonstrate that the model accurately forecasts the behavior on an ARM multicore platform, and we also demonstrate that the model is not significantly influenced by variances in common type workloads.",
        "title": "23057"
    },
    {
        "abs": "The paper describes how the Q-bus has evolved in the 12 years since its original specification. Use of ‘spare lines’, the addition of parity memory and methods of increasing direct memory access (DMA) bandwidth are covered. Accessing of the I/O page is discussed. Changes to the Q-bus interrupts are reviewed and the Micro PDP-11/83's private memory interconnect (PMI) scheme for speeding memory access is described.",
        "title": "23077"
    },
    {
        "abs": "The tremendous increase in the computing capacity of the embedded architectures has led to widespread deployment of embedded applications. These applications generally exhibit similar patterns in their specification such as filters in which multiply and accumulate operations are repetitive. If such patterns are identified and used for the system design, trade-off between the area and delay can be achieved. This paper proposes a new methodology which allows to implements a design by retrieving similar patterns known as graph isomorphs and interfaces them as HW accelerators in the system-on-chip design flow. An effective algorithm that converges in polynomial time has been proposed to find such similar subgraphs. In the next phase of the design flow, an algorithm has been proposed which performs the scheduling of clusters and minimizes the time overhead. All algorithms have been written in python for parsing the data flow description and test the correctness of the proposed work. The proposed design flow has been applied to five different programs which are sine, cosine, exponent, matrix multiplication and discrete cosine transform (DCT). These have been described as a data flow graph and have been used for results comparison. An estimation table showing the HW and SW parameter of the data flow operators has been developed for timing and area analysis of the programs. The work is an effort to show the clustering and scheduling of a standalone specification which is mapped on static reconfigurable fabric. Reconfigurable computing systems (RCS) are a popular platform for embedded computing applications as they offer a wide exploration in the design space by allowing HW, SW or HW–SW (hybrid) implementation depending on computational demand and resource requirement. These systems have inspired the designers to find new frameworks for achieving the optimized system characteristics under the given constraints. Any static or dynamic HW hardware optimization in an application can be proposed, implemented and easily verified on the chip. The results presented show the comparison of the proposed approach with SW and HW implementation of DCT design on the Xilinx ML507 board. HW timer has been used to find the execution of each implementation. The experimental verification of the proposed algorithms shows that static IP core design flow gives better results.",
        "title": "23151"
    },
    {
        "abs": "Interpretation of intermediate code using general-purpose microprocessors is slow and cumbersome. Typically, the emulated virtual machine is implemented by a software interpreter that executes actions normally performed in hardware. Interpretation has become important, however, especially for the implementation of modern high-level languages. In this paper the configuration of a general-purpose microprocessor assisted by a language co-processor is presented to emulate the virtual modula -2 M-machine designed by Wirth. The related problems, the efficiency and the small incremental hardware complexity are illustrated.",
        "title": "23188"
    },
    {
        "abs": "This paper describes the development of hardware-based techniques for scheduling in high-performance real-time systems. The focus of attention is hard real-time applications, scheduling being implemented within a co-processor unit. Two experimental co-processor structures are described, one based on programmable logic devices, the other being a microprocessor-based one. The relative merits of the two approaches are discussed and major conclusions are highlighted.",
        "title": "23192"
    },
    {
        "abs": "A cache scheme which uses page associative cache descriptors can offer advantages in terms of its impact on cache coherence in the presence of paged transactions, and on the use of local memory to minimize bus loading. It can also be used to preserve cache coherence when processor accesses are cached, i.e. a logical cache, in the presence of an external memory management unit. This paper proposes a mechanism for page associative cache operation for use with Futurebus systems, and analyses how such a cache would overcome the problems common to cache memory designs. Cache protocols are not included in the current Futurebus specification IEEE 896.1, recently standardized, but will be added after further development. Performance of page associative caches is compared with directly mapped caches and fully associative caches.",
        "title": "23207"
    },
    {
        "abs": "Compact area, low delay and good testability properties are important optimization goals in the synthesis of circuits for Boolean functions. Unfortunately, these goals typically contradict each other. Multi-level circuits are often quite small but can have a long delay, due to their unbounded number of levels. On the other hand, circuits with low depth guarantee low delay but are often larger regarding the area requirements. A different optimization goal is good testability which can usually only be achieved by additional hardware overhead. In this paper we propose a synthesis technique that allows to trade-off between area and delay. Moreover, the resulting circuits are 100% testable under the stuck-at fault model. The proposed approach relies on the combination of 100% testable circuits derived from binary decision diagrams and 2-SPP networks. Full testability under the stuck-at fault model is proven and experimental results show the trade-off between area and depth.",
        "title": "23210"
    },
    {
        "abs": "Modern Field Programmable Gate Arrays (FPGAs) posses small feature sizes, and have gained popularity in mission-critical systems. However, FPGA can suffer from faults due to the small feature sizes and harsh external conditions that are faced by a mission-critical system. Therefore, the architecture of FPGA must be tested to ensure a reliable system performance. At the same time, due to the mission-critical nature of a system, the test process should be non-intrusive , i.e., applications and FPGA regions that are not being tested remain unaffected. An online test methodology is, therefore, required that not only verifies the reliability of FPGA architecture, but also does not degrade the performance of other, running FPGA applications. In this paper, we propose an online test methodology that uses hardwired network on chip as test access mechanism, and conducts test on a region-wise basis. Importantly, the proposed test methodology exhibits a non-intrusive behaviour that means it does not affect the applications and FPGA regions, which are not being tested, in terms of configuration, programming, and execution. Our test methodology posses approx. 32 times lower fault detection latency as compared to existing schemes, respectively.",
        "title": "23296"
    },
    {
        "abs": "Of late, Wigner-Ville distribution (WVD)-based time-frequency representation has been shown to become a viable tool in the study of time-varying signals. In this paper, we discuss the implementation details of a WVD-based moving target detector (MTD) on a TMS320C30 floating-point digital signal processor. In this approach, WVD has been employed as a functional block instead of the conventional fast Fourier transform (FFT) processor [1]. A comparison of the implementation and the computational details of MTD based on FFT and WVD has been included. Finally the probability of detection ( P d) and the probability of false alarm ( P fa) curves, in the presence of log-normal clutter, have been presented to support the superiority of the proposed MTD-WVD over MTD-I.",
        "title": "23308"
    },
    {
        "abs": "A novel real-time task allocation and scheduling scheme is proposed for a multi-core system incorporated in a Bidirectional Network-on-Chip (BiNoC) platform. Given a task graph, this scheme seeks to minimize the total execution time by allocating ready-to-execute tasks to as many available cores as possible subject to the real-time deadlines of each task. A refinement process is introduced to update the priority ranking of a task list so as to meet the timing constraints. In particular, the communication overhead is considered by incorporating the packet routing paths and delays into the overall optimization process. In doing so, the flexibility of bidirectional links of BiNoC is exploited to alleviate traffic congestion, such that more tasks could be executed concurrently at different cores and overall execution time be reduced. To validate the effectiveness of this proposed scheme, extensive simulations have been performed. The results clearly demonstrate the superior performance of this proposed scheme compared to existing approaches that did not exploit the flexibility of BiNoC.",
        "title": "23346"
    },
    {
        "abs": "This paper presents a concept for automated architecture synthesis for adaptive multiprocessors on chip, in particular for Field-Programmable Gate-Array (FPGA) devices. Given a parallel program, the intent is to simultaneously allocate processor resources and the corresponding communication network, and at the same time, to map the parallel application to get an optimum application-specific architecture. This approach builds up on a previously proposed design platform that automates system integration and FPGA synthesis for such architectures. As a result, the overall concept offers an automated design approach from application mapping to system and FPGA configuration. The automated synthesis is based on combinatorial optimization. Automation is possible because a solvable Integer Linear Programming (ILP) model that captures all necessary design trade-off parameters of such systems has been found. Experimental results to study the feasibility of the automated synthesis indicate that problems with sizes that can be encountered in the embedded domain can be readily solved. Results obtained underscore the need for an automated synthesis for design space exploration.",
        "title": "23443"
    },
    {
        "abs": "One of the most attractive fields in quantum-dot cellular automata (QCA) is the implementation of configurable structures. In this paper, a novel QCA structure of a configurable flip-flop (CFF) element is introduced. In a bottom-up design approach, this paper firstly proposes well-organized QCA layouts for a positive/negative clock signal generator, a level/edge pulse converter, and a bit transport/toggle storage block. In the next step, these blocks are connected in a cascade construction to form a robust and efficient flip-flop element with a comprehensive configurable capability. The simulation results demonstrate the accuracy of proposed QCA-based structures. Design capability and flexibility of the proposed CFF element are further evaluated through the synthesis of high-level circuits including registers and counters. Consequently, this new element is the best building block to be used in the next generation of configurable systems such as QCA-based FPGAs.",
        "title": "23529"
    },
    {
        "abs": "The Ethernet is a local data communications network. It supports communication between machines connected to a common coaxial cable using transceivers and controllers. The purpose of this designer's guide is to help with interfacing to the transmission system. To aid understanding, the transmission system is described in terms of its parts and important parameters. Circuits for interfacing a controller to the transceiver cable (information outlet) are described. Also included are waveforms for a packet propagating through the system, some tests to verify operation and a summary of the Ethernet specification.",
        "title": "23531"
    },
    {
        "abs": "The normal method of drawing in colour graphics systems with a number of memory planes is either to write separately on each plane or to write simultaneously on all planes. The first of these has the advantage of being flexible with respect to the number of memory planes, but it is very slow compared to the second. Writing simultaneously on all planes requires changes in basic drawing hardware for every addition to the number of memory planes. This paper attempts to introduce flexibility into the second approach: with the same basic drawing hardware and some external hardware, drawing can be performed on all planes simultaneously. The extra hardware does not demand any changes in the circuit timing parameters. A large portion of the extra hardware can be put on a programmable array logic (PAL) chip. The paper also presents an overview of an implementation of this approach, using the 82720 graphics display controller.",
        "title": "23549"
    },
    {
        "abs": "Persistent storage, security, distribution support and high availability are basic properties in persistent system architectures. UNIXTM and BiiN systems support these properties in different ways. BiiN systems are based on a proprietary architecture, which was specifically designed to provide a secure, distributed, object oriented operating environment. On the other hand, UNIX systems went through many evolutionary steps and are now a de facto standard for open systems. Within this evolution, many important features have been added, which are also relevant from the persistent system architecture point of view. A comparison shows that the enhancements in UNIX provide a functionality similar to proprietary systems in addition to the widely required advantages of open system compatibility.",
        "title": "23580"
    },
    {
        "abs": "Increasingly, UK defence procurement is being carried out as part of internationally collaborative programmes. Purchases may be of off-the-shelf design or of equipment meeting the requirements of more than one country. This paper addresses the issue of differing national standards for safety-critical software and the need for harmonisation. Differing standards give rise to a number of issues peculiar to software. One such issue is that all standards for software make requirements on the process used to develop the software. When an existing design is purchased the software development process has been completed and therefore cannot be modified. The requirements of differing standards for physical properties such as structural strength can be compared either by reference to an appropriate scientific theory or by experiment. Unfortunately, neither of these approaches can he used to compare objectively the requirements of software standards, especially when the software is safety-critical. The paper compares some of the existing standards to safety-critical software in military avionics and, describes developments taking place in different countries.",
        "title": "23642"
    },
    {
        "abs": "Nine D-type Flip-Flop (DFF) architectures were implemented in 28 nm FDSOI at a target, subthreshold, supply voltage of 200 mV. The goal was to identify promising DFFs for ultra low power applications. The single-transistor pass gate DFF, the PowerPC 603 DFF and the C2MOS DFF are considered to be the overall best candidates of the nine. The pass gate DFF had the lowest energy consumption per cycle for frequencies lower than 500 kHz and for supply voltages below 400 mV. It was implemented with the smallest physical footprint and it proved to be functional down to the lowest operating voltage of 65 mV in the typical process corner. During Monte Carlo (MC) process and mismatch simulations it was also found that the pass gate DFF is least prone to variations in both minimal setup- and minimal hold-time. Race conditions, during mismatch variations, occurred for the flip-flop that is constructed from NAND and inverter based multiplexers. The pass gate DFF is outperformed slightly when it comes to D-Q-based power-delay product and more significantly when it comes to the maximum clock frequency. The flip-flops having the shortest D-Q delays were the PowerPC 603 and the transmission gate D flip-flop, these also had the lowest D-Q-based power-delay of 26% and 30% respectively of that of the worst-case S2CFF power-delay product.",
        "title": "23692"
    },
    {
        "abs": "This paper discusses the rapidly developing area of field programmable logic devices (FPLD) from the perspective of a user rather than a manufacturer. It aims to introduce the reader to the families of devices that are currently available, to identify common categories having similar characteristics and to summarize the main distinguishing features of specific devices. On the basis of logic block complexity and programming technique, a distinction is made between complex PLDs and field programmable gate arrays (FPGA). The discussion is restricted to those families of devices that offer logic complexities measured in thousands of gates, have the ability to include more than 100 register elements, and include more than 100 I/O pins. The devices considered include Aglotronix CAL, Altera MAX and Flex, AMD MACH, Actel ACT, AT&amp;T ORCA, Concurrent Logic CLi6000, Crosspoint Solutions CP20K, Intel FLEXlogic, Lattice Semiconductor pLSI, Quicklogic pASIC, Xilinx LCA and EPLD.",
        "title": "23713"
    },
    {
        "abs": "This paper presents a discussion of the interconnection services provided by Net/One, a local area network communication system produced commercially by Ungermann-Bass, Inc. Net/One was designed to provide communication among user devices which are attached to the network. The services are provided by the networking software; they include a virtual circuit service, used for logically connecting two devices so they may exchange bytes with one another, and a datagram service, used by intelligent packet-oriented hosts to communicate simultaneously with any number of other such hosts. The paper provides an overview of these services as seen by the user of Net/One. It also offers some discussion on hybrid variants of these services and the additional capabilities they provide for interconnection of user equipment.",
        "title": "23727"
    },
    {
        "abs": "TSX-Plus is a multiuser extension to the RT-11 (realtime) operating system for the DEC LSI-11 and PDP-11 computers. There are some 6000 TSX-Plus users worldwide (about 10% of the RT-11 population). The latest release incorporates a novel scheduling algorithm and automatic data caching which combine to provide high job throughput. The article discusses TSX-Plus and gives an overview of the scheduler.",
        "title": "23823"
    },
    {
        "abs": "The inadequacy of the classical media access protocols for considering the transmitter power as the critical resource and a tunable parameter for increasing the throughput, conserving the battery power, and providing the quality of service for the communication has given a new perception for the power controlled media access protocols in ad hoc networks. This paper presents an insight into the existing work on the power control at the media access level. Various objectives of the power control, distinct approaches, and the issues related to transmission power control are discussed.",
        "title": "23835"
    },
    {
        "abs": "Now that microprocessors and other complex digital devices are being designed into a wide variety of equipment, it is necessary to reconsider testing philosophy and procedures from the beginning. We must consider the potential problems implicit in these new designs: the likely faults, methods of access and detection, equipment and skills required, time and money. Having quantified the problem we must find the necessary solution or solutions. The necessary steps in the development of this strategy are described.",
        "title": "23967"
    },
    {
        "abs": "The operation of a real-time task under Microsoft's Windows 98 Operating System was investigated. The uniformity of the frame rate achieved in an animation program was measured to assess its suitability a visual stimulus in a physiology experiment. Much better performance is obtained when timing of the animation is controlled by the thread scheduler of the operating system, rather than using the hardware timing which is more traditionally used.",
        "title": "24002"
    },
    {
        "abs": "The increasing dependency of modern day information transferred over the network demands an increasing need of efficient implementations of security protocols on server and client devices to process the encryption and decryption of messages. This paper presents the design of an efficient scalable and unified Elliptic Curve Cryptography (ECC) coprocessor that takes advantage of the DSP48E slices on Xilinx FPGAs. The proposed coprocessor is able to evaluate the elliptic curve point multiplication (ECPM) for all 15 curves recommended by the National Institute of Standards and Technology (NIST). The DSP48E slices available in Xilinx FPGAs improve the performance of the coprocessor and provide the ability to evaluate dual-field arithmetic. The coprocessor has been implemented on a Xilinx Virtex-5 FPGA and it requires 4244 registers, 8316 LUTs, 2291 slices, 5 BRAMs and 25 DSP48E slices. It can evaluate the ECPM between 0.857 ms and 9.662 ms for prime curves and between 0.239 ms and 4.523 ms for binary curves. This paper also proposes a set of scalable finite field arithmetic blocks that can be programmed to implement different ECPM algorithms. In comparison with other ECC coprocessors in the current literature, the proposed design is very competitive in terms of timing performance and hardware resource utilization, while combining the scalability and dual-field features that are not available in other designs.",
        "title": "24064"
    },
    {
        "abs": "The latest technologies of integrated circuit manufacturing allow billions of transistors to be arranged on a single chip, enabling the chip to implement a complex parallel system, which requires a communications architecture that has high scalability and a high degree of parallelism, such as a Network-on-Chip (NoC). These technologies are very close to the physical limitations, which increases the faults in manufacturing and at runtime. Therefore, it is essential to provide a method for fault recovery that would enable the NoC to operate in the presence of faults and still ensure deadlock-free routing. The preprocessing of the most probable fault scenarios enables us to anticipate the calculation of deadlock-free routings, reducing the time that is necessary to interrupt the system during a fault occurrence. This work proposes a technique that employs the preprocessing of fault scenarios based on forecasting fault tendencies, which is performed with a fault threshold circuit operating in accordance with high-level software. We propose methods for dissimilarity analysis of scenarios based on cross-correlation measurements of link fault matrices. Experimental results employing RTL simulation with synthetic traffic prove the quality of the analytic metrics that are used to select the preprocessed scenarios. Furthermore, the experiments show the efficacy and efficiency of the proposed dissimilarity methods, quantifying the latency penalization when using the coverage scenarios approach.",
        "title": "24067"
    },
    {
        "abs": "The scintillating technological advancements have redefined the process of communication around the world. Banking, purchases, investments, emails, bill payments, etc. are being managed through online communications and needless to mention the linkage of these internet serves with embedded gadgets. A microcontroller being the low-cost solutions for real-time embedded applications has to handle rigid security algorithms for information security paradigm. The high level of sensitivity in chaos-based systems is highly suitable for the design of encryption schemes due to the randomness offered by them. The minimal memory resource and speed are the factors restricting the use of microcontrollers for implementing chaotic schemes that encrypt image data. This paper presents the design of a chaos-based image encryption algorithm with lightweight properties and its optimised implementation on a 32-bit microcontroller. This work also includes parameters related to the analysis of the security level and performance of the microcontroller that was missed to concentrate by the authors on their similar schemes reported in the literature. The level of safety of the proposed algorithm has been analysed via key sensitivity analysis, encryption quality analysis, randomness analysis, differential analysis, statistical analysis, visual analysis and attack analysis. Additionally, the results of performance analysis regarding smaller memory footprint and better throughput of proposed algorithm guarantee its suitability for real-time embedded applications.",
        "title": "24115"
    },
    {
        "abs": "Sensitive applications are split into the IP cores of the Multi-Processors System-on-Chip (MPSoCs). In order to isolate the sensitive communication among such IP cores, security zones based on conference keys agreement can be built. However, the flexibility and dynamic nature of MPSoCs force reshaping the security zones at runtime. It is still a challenge to achieve efficient computation and distribution of new conference keys in MPSoC environments. In order to solve this problem, in this work we propose the combination of two techniques: i) high performance NoC, able to efficiently communicate data and control packets in the system; and ii) hierarchical group-key management for efficient security zone modification. We implement three hierarchical protocols and we show that by decentralizing the security management of the rekeying process and using a two-level NoC, it is possible to achieve an improvement of the performance compared to the previous flat approaches.",
        "title": "24135"
    },
    {
        "abs": "Microprocessor and digital LSI technology used for process control in heavy industrial plant is subject to interference from electromagnetic (EM) radiation in the vicinity of these sites. This paper considers the tests that are necessary to demonstrate with some level of confidence that such equipment will not malfunction in the EM environment. Susceptibility failure mechanisms are analysed with reference to the transistor junction, the basic building block of modern electronics. The effects of system coupling are considered. Conducted susceptibility test methods include power injection, voltage injection and current injection. The application of these tests is discussed together with some alternative test methods and the ramifications for large system testing. Some minimum testing requirements are presented.",
        "title": "24163"
    },
    {
        "abs": "Bridges easy the interconnection and communication of devices that operate using different buses. In fact, we can see a computer as a hierarchy of buses to which devices are connected. In this paper we design a PCI/MC68000 bridge in order to improve communications between a Personal Computer and a MC68000 based system. The previous interface between both devices was based on the old 16-bit ISA bus, which represented a bottleneck in their communication. However, the methodology described here is generic and can be applied to the design of PCI bridges to other buses. We finish this work with an analysis of the bridge performance improvement which can also be easily adapted to other situations. As an example our interface is used in an interesting situation, i.e., updating the obsolete control unit of a highly valuable system (an industrial robot).",
        "title": "24189"
    },
    {
        "abs": "This paper describes the design and implementation of a meetings system using a microcomputer. In this control system, the following achievements were made: counting and checking the voting in a very short time; establishing who is/is not present at the meeting; establishing whether/not the meeting is quorate; and controlling the debate by being able to call various members to speak. The meetings system provides most of the functions of a manual system but significantly reduces the time wastage — and hence the waste of money — by allowing the chairman and the members of the meeting to communicate with each other by way of electronic signals in a controlled manner and by relegating routine counting and listchecking tasks to a microcomputer.",
        "title": "24219"
    },
    {
        "abs": "With continuous scaling down of the semiconductor technology, the soft errors induced by energetic particles have become an increasing challenge in designing current and next-generation reliable microprocessors. Due to their large share of the transistor budget and die area, cache memories suffer from an increasing vulnerability against soft errors. Previous work based on the vulnerability factor (VF) analysis proposed analytical models to evaluate the reliability of on-chip data and instruction caches. However, we have no possession of a system-level study on the vulnerability of instruction caches. In this paper, we propose a new analytical model to estimate the system-level vulnerability factor for on-chip instruction caches in embedded processors. In our model, the error masking/detection effects in instructions based on the Instruction Set Architecture (ISA) are studied. Our experimental results show that the self-error-masking/detection in instructions will reduce the VF of the instruction caches compared to the previous study. We also exemplify our design methodology by proposing several optimizing schemes to improve the reliability. Benchmarking is carried out to demonstrate the effectiveness of our vulnerability model and optimization approach, which can provide an insightful guidance for the future reliable instruction cache and ISA design.",
        "title": "24220"
    },
    {
        "abs": "Currently, several embedded applications in military, industry, banking transference, e-commerce, biometric systems and others use insecure communication channels such as Internet to transmit or store confidential information. Therefore, the integrity and security of information are an important issue in continuous research. In last years, chaotic systems have been proposed in cryptography due they have several properties related with cryptography properties such as extreme sensibility on initial conditions with confusion and ergodicity with diffusion. In this paper, we present a 32-bit microcontroller implementation of an improved text encryption algorithm (based on Murillo-Escobar’s algorithm [13]) for real-time embedded systems and we present a complete security analysis such as key size analysis, key sensitivity, plain text sensitivity, floating frequency, histograms, N-grams, autocorrelation, information entropy, robustness against classic attacks, randomness analysis, and security characteristics. In addition, the digital implementation and performance are analyzed such as programming details, memory required, frequency system, implementation costs and encryption time. In contrast with recent approaches presented in literature, we present a complete security analysis in both statistical and implementation level, to justify the proposed scheme in a real application. Based in the results, the proposed embedded text encryption system is secure, effective and at low cost, and it could be implemented in real-time cryptosystem based on microcontroller.",
        "title": "24280"
    },
    {
        "abs": "This paper presents an analysis of energy dissipation of a decimation filter chain of four Half Band Digital (HBD) filters operated in the sub-threshold (sub- V T) region with throughput constraints. To combat speed degradation due to scaling of supply voltage, various HBD filters are implemented as unfolded structures. The designs are synthesized in 65 nm CMOS technology with low-power and three threshold options, both as single- V T and as dual- V T. A sub- V T energy model is applied to characterize the designs in the sub- V T domain. Simulation results show that the unfolded by two and four architectures are the most energy efficient for throughput requirements between 250 k samples/s, and 2 M samples/s. By the selection of optimum architectures and standard cells, at the required throughput the simulated minimum energy dissipation for the required throughput per output sample is 164 fJ and 205 fJ, with single supply voltage of 260 mV.",
        "title": "24304"
    },
    {
        "abs": "A smart telemetry compression system has been developed for the ELISMA complex of instruments on MARS-96. Most of the data generated by the individual instruments of the complex require compression before transmission back to Earth since the complex is capable of producing data at many times the allocated ELISMA telemetry data rate. A compression algorithm was produced that could achieve a selectable compression ratio of 5:1, 6:1, or 7:1. An equivalent fuzzy logic controller sets the compression ratio for each data record of each individual instrument depending on: (i) the best ratio as determined by the variability of the data in that record; (ii) the instantaneous level of the data held in the telemetry buffer; and (iii) on the current usage of that instrument's data allocation for the present orbit around Mars.",
        "title": "24314"
    },
    {
        "abs": "Point Multiplication (PM) is considered the most computationally complex and resource hungry Elliptic Curve Cryptography (ECC) mathematical operation. PM hardware accelerator design can follow several approaches that lead to a fast, small or flexible implementation, meeting related application specifications. However, each PM design decision has certain outcomes in utilized hardware resources and computation speed. Such a key design decision is related to the structure of the [formula omitted] multipliers to be employed in the PM accelerator. In this paper, we highlight the [formula omitted] multiplication role in the overall PM performance and investigate what are the trade-offs on a PM accelerator when using bit serial or bit parallel multiplication approach in terms of speed, chip covered area and flexibility. To achieve this goal, we estimate these tradeoffs for a single point operation and specify realistic design cases for bit serial and bit parallel multiplier based PM design approaches. To evaluate the theoretical modeling, a point operation design methodology based on the parallelism and rescheduling of [formula omitted] operations is proposed. This design approach is adapted to two characteristic PM algorithm realizations, the traditional double &amp; add algorithm and the side channel attack resistant Montgomery power ladder algorithm. Our goal is to assess the resulting PM accelerator overall performance so as to achieve high speed with an acceptable cost on chip covered area (hardware resources). Using this methodology, PM is performed in series of [formula omitted] parallelism stages. To test the proposed methodology, 8 PM accelerator use cases are identified that can offer high speed, flexibility, side channel attack resistance or small chip covered area. To provide fair comparisons and results, a common PM architecture is devised and the use case PM accelerators are implemented in FPGA technology. Depending on the designers goal, the proposed architectures and 8 implementations can offer the benefit of either high speed (the proposed work is currently one of the fastest known [formula omitted] bit parallel multiplier based PM realization) or flexibility with reasonable compromises in chip covered area.",
        "title": "24358"
    },
    {
        "abs": "The reliability of FPGA based hardware designs has become an important field of research particularly for space computing. Traditionally, redundancy is utilized in FPGA based designs to achieve reliable or error-tolerant computing. However, the redundant designs vary according to the granularity level and the voter placement algorithms used for the hardware design. The resulting circuit configurations vary in area, latency and power as well as in the achieved reliability. While the evaluation of area, latency and power is done by the FPGA design tools, quantitative data for reliability are usually not derived. Consequently, there is a need for an automated reliability evaluation tool especially considering the huge design space of redundant circuit structures. In this paper, we combine the Boolean difference error calculator (BDEC), a probabilistic reliability model for hardware designs, with a reliability model for fault-tolerant circuit structures. As a result, we are able to study the reliability of fault-tolerant circuit structures at the logic layer. We focus on fault-tolerant circuits to be implemented in FPGAs and show how to extend our combined model from combinational to sequential circuits. For an automated analysis, we develop a MATLAB-based tool utilizing our extended BDEC model. With this tool, we conduct a case study on dynamic reliability management and show how quantitative reliability data obtained from this tool improves the four-dimensional Pareto optimization for area, latency, power and reliability.",
        "title": "24393"
    },
    {
        "abs": "Aging is one of the most crucial reliability issues in recent chip fabrication technologies. Among different causes for aging, ``Bias Temperature Instability'' (BTI) imposes the highest semiconductor degradation rate to processors. Studies show that the stored values in the processors’ cache memories follow specific bit-patterns. The pattern in the data cache is due to the stored narrow-width values, however, in the instruction cache, it goes after the predefined instructions’ opcodes patterns. Hence, data and instruction cache memory cells experience different aging rates due to different stress conditions, which can lead to a decrease in ``Mean Time To Failure'' (MTTF) of the cache structures. This work proposes a new scheme that balances the stress condition over all cache memory cells by swapping bit values within each word of the cache memory. By utilizing this scheme, the aging rate of data and instruction cache memories is mitigated by up to 69% and 83%, respectively, with negligible area and performance overheads.",
        "title": "24663"
    },
    {
        "abs": "SPACE is introduced as a new type of computer architecture, capable of very fast simulation of highly concurrent systems. The machine is designed to be scalable, constructed from a vast array of boards. The decisions made in the design of the board are discussed, and the actual hardware (based on an array of field programmable gate array chips) is described. It is shown that SPACE can be programmed by translating a subset of the language Occam into asynchronous digital logic circuits. The handshake protocol used in these circuits is described and examples are given of circuit components which implement Occam operators. Finally, the method by which these circuit components are formally verified using the Circal process algebra is presented.",
        "title": "24679"
    },
    {
        "abs": "Trace Processors is a promising next-generation microarchitecture that exploits implicit thread-level parallelism (TLP) in conventional applications by employing aggressive control and data speculation techniques. Although high performance can be achieved by trace processors, but in fact, processing element (PE) resources are still underutilized due to frequent trace cache misses and next-trace mispeculations. When trace cache miss occurs, trace dispatch engine must stall and supply nothing to idle PE until the completion of trace construction. When next-trace mispeculation occurs, in addition to trace dispatch engine stall, all speculative execution results after the mispeculated trace must be discarded. All the operations on those squashed traces are useless. When trace processors scales up with more PEs, this problem will become more severe. Addressing to this problem, we propose augmenting multiple thread contexts into trace processors. A combined microarchitecture—Simultaneous Multithreading trace processors (SMT trace processors) is proposed in this paper. By dispatching trace from other threads, the penalties of trace cache miss and next-trace mispeculation can be tolerated. Introducing multiple thread contexts reduce the percentage of wrong-path speculations for each thread and improve PE execution efficiency significantly. Simulation results show that integrating two thread contexts can improve 8-PE trace processors performance 27.7%. When augmenting four and eight thread contexts, the corresponding improvements are 28.7 and 15.4%. And we believe that even higher performance improvement can be expected when we integrate more PEs into SMT trace processors.",
        "title": "24740"
    },
    {
        "abs": "Wireless monitoring of physiological signals is an evolving direction in personalized medicine and home-based e-Health systems. There are several constraints in designing such systems, with two of the most important being energy consumption and data compression. Compressed Sensing (CS) is an emerging data compression technique that can be used to overcome those constraints. This work presents a low-complexity CS hardware implementation on a Field-Programmable Gate Array (FPGA) for the reconstruction of compressively sensed signals using the matching pursuit (MP) algorithm, targeting health-care applications. The proposed hardware design is based on pipeline optimization of the Programmable Logic (PL) implementation performed on the Zynq FPGA, which provides a significant performance enhancement, namely an increased processing speed and a reduced computational time since it is 115x faster than the Matlab implementation and 75x faster than the Processing System (PS) implementation carried out on the same Zynq FPGA device, while achieving alternative a high-quality signal recovery with a Peak Signal to Noise Ratio (PSNR) of 23.8 dB. Comparisons against other state-of-the-art methods showed that the low complexity of the MP algorithm can be exploited for providing almost similar results to more complex algorithms using 87–583 less Digital Signal Processor (DSP) cores, 28–540 less Block RAMs and 10,300 to 84,700 less Look-Up Table (LUT) slices.",
        "title": "24755"
    },
    {
        "abs": "Energy is a critical resource in wireless sensor networks. In this paper, we propose a new approach to maintain network wide energy equivalence and maximize network lifetime. Compared to existing protocols, our approach emphasizes on route maintenance instead of route finding. This means no critical nodes would become the bottleneck of network lifetime. A reroute request packet is sent out from sinks periodically. When the packet reaches a path node, Common Neighbor Switching (CNS) algorithm checks energy difference between the node and its neighbors outside the routing tree. If the difference goes beyond a threshold, double neighbor switching is performed. Two path-rerouting algorithms, namely, Shortest Rerouting (EERS) and Longest Rerouting (EERL), are also presented to show that neighbor switching is better than path rerouting. Simulation results show that CNS outperforms Directed Diffusion in more than 90% cases, while EERS and EERL show only blurry and conditional advantage over directed diffusion.",
        "title": "24764"
    },
    {
        "abs": "The effectiveness of microprocessor-based systems for realizing digital random logic controllers is illustrated by the design of a controller based on an MC68B00, using E2PROM rather than RAM or ROM to enhance flexibility and avoid memory corruption or power interrupt. Moreover, the time response of the system (about 1 ms) can cover a wide spectrum of control applications. The paper is illustrated by a DC motor speed control system.",
        "title": "24833"
    },
    {
        "abs": "Tools for logic synthesis require knowledge of their interfaces in detail. To reduce the new user's introductory period we provide the ARTUS framework presented in this paper. All existing tools are available through a uniform interface. Furthermore, we enforce the controlled sequencing of tool invocations by providing obligatory data checks for validity. The open framework is adaptable to modifications of the tool configuration.",
        "title": "24856"
    },
    {
        "abs": "The increasingly complicated DSP processors and applications with strict timing and code size constraints require design automation tools to consider multiple optimizations such as software pipelining and loop unfolding, and their effects on multiple design parameters. This paper presents an Integrated Framework for Design Optimization and Space Minimization (IDOM) toward finding the minimum configuration satisfying timing and code size constraints. We show an effective way to reduce the design space to be explored through the study of the fundamental properties and relationships among retiming function, unfolding factor, timing, and code size. We present theorems to show that a small set of feasible unfolding factors can be obtained effectively to produce high-quality designs. The IDOM algorithm is proposed to generate a minimal configuration of the design by integrating software pipelining, unfolding, and code size reduction techniques. The experimental results on a set of DSP benchmarks show the efficiency and effectiveness of our technique.",
        "title": "24893"
    },
    {
        "abs": "In recent years, Fast Fourier Transform (FFT) plays a vital role in signal processing application for converting time domain to frequency domain. This paper presents Real FFT architecture which is implemented in radix-2 Decimation- in-Frequency (DIF). Instead of using more number of Random Access Memory (RAM), single Dual port RAM (DRAM) is used to store the intermediate data results. In Processing Element (PE), normal multiplier and adder has been used in previous works. In this paper, Vedic Multiplier (VM) and Carry Lookahead Adder (CLA) is used for improving the Real FFT operation and it is implemented in Verilog code. Application Specified Integrated Circuits (ASIC) and Field Programmable Gate Array (FPGA) performances are evaluated for all the architectures. In DRAM-VM-CLA architecture, LUT, flip flop, slices, and frequency are improved in FPGA performance as well as area, power, and delay are improved in ASIC performance. In ASIC 180 nm technology, 81.14% of area, 45.98% of delay, and 89.77% of Area Delay Product (ADP) is minimized in DRAM-VM-CLA as well as 72.64% of area, 5.44% of delay, and 74.10% of ADP is reduced in ASIC 45 nm technology.",
        "title": "24899"
    },
    {
        "abs": "Mobile technology evolution and mobile based education are highly needed for academic system. Especially designing a physical antenna is a tough task so that Simulation solution has been used in the laboratory. Even though it is costlier and it could be used only in LAN based computers for an academic purpose but not used in outside. To be clear, the designing of an antenna with institution studies are needed. To overcome and achieve the objective of the paper which proposes to develop a mobile based designing antennas such as Dipole, Loop antennas. The performances of antenna have been carried out in mobile platform. This is an attempt to predict parameters of specified antenna in the android platform. Through this application the students could able to predetermine the characteristic of specified antenna easily. This application will be highly useful and recommended to the students. The proposed work has been designed by Eclipse software and Android studio software. Finally the performance of the antenna parameters has been carried with the help of Android-Antenna Tool (AAT).",
        "title": "24907"
    },
    {
        "abs": "Multistage networks are finding increasing use in building clusters of workstations and PCs. The networks that support such connectivity must provide high bandwidth, low latency, scalability, low cost, high level of usability, and reliability [1]. A key parameter that influences the network performance characteristics such as maximum latency, throughput, scalability and tree saturation, is the arbitration policy implemented in routers.",
        "title": "24932"
    },
    {
        "abs": "The concept of the kernel, i.e. the time critical part of a real-time operating system, and its dedicated co-processor, especially tailored for embedded applications, are presented. The co-processor acts as a system controller and operates in conjunction with one or more conventional processors in hard real-time environments. It is composed of three physically and clearly separated layers which vary with regard to implementation, speed and complexity. Correspondingly, the model of the operating system kernel is hierarchically structured, and functions are mapped to these layers, observing the inherent parallelism of recognition and handling of different kinds of events expected in such environments and higher level kernel functions such as tasking operations. The operating system functions are supported by the high level real-time programming language constructs. Since many embedded systems are safety related, the software employed must be highly dependable. Therefore, the kernel was proven correct with formal methods, which represents a major innovation in software technology.",
        "title": "24937"
    },
    {
        "abs": "Coherence protocols consume an important fraction of power to determine which coherence action to perform. Specifically, on CMPs with shared cache and directory-based coherence protocol implemented as a duplicate of local caches tags, we have observed that a big fraction of directory lookups cause a miss, because the block looked up is not allocated in any local cache. To reduce the number of directory lookups and therefore the power consumption, we propose to add a filter before the directory access. We introduce two filter implementations. In the first one, filtering information is explicitly kept in the shared cache for every block. In the second one, filtering information is decoupled from the shared cache organization, so the filter size does not depend on the shared cache size. We evaluate our filters in a CMP with 8 in-order processors with 4 threads each and a memory hierarchy with write-through local caches and a shared cache. We show that, for SPLASH2 benchmarks, the proposed filters reduce the number of directory lookups performed by 60% while power consumption is reduced by ∼28%. For Specweb2005, the number of directory lookups performed is reduced by 68% (44%), while directory power consumption is reduced by 19% (9%) using the first (second) filter implementation.",
        "title": "25054"
    },
    {
        "abs": "This paper sums up the experience at SINTEF Telecom and Informatics on analysis of safety critical systems. After a short description of the system under consideration, the paper naturally falls into two parts. The first one is a describtion of two modifications, how they were implemented and how they were analysed for safety. The second one contains a discussion of the three methods used—FTA, FMECA and code analysis. We here concentrate on how these methods differ in focus, the knowledge and information needed, and the types of problems they can handle. The paper's conclusion is that all three methods are needed when analysing the modifications of a safety critical system. The knowledge needed and the problem focus will, however, differ.",
        "title": "25055"
    },
    {
        "abs": "DICE is a shared-bus multiprocessor based on a distributed shared-memory architecture, known as cache-only memory architecture (COMA). Unlike previous COMA proposals for large-scale multiprocessing, DICE utilizes COMA to effectively decrease the speed gap between modem high-performance microprocessors and the bus. DICE tries to optimize COMA for a shared-bus medium, in particular to reduce detrimental effects of the cache coherence and the `last memory block' problem on replacement. In this paper, we present a global bus design of DICE based on the IEEE futurebus+ backplane bus and the Texas Instruments chip-set. Our design demonstrates that necessary bus transactions for DICE can be done efficiently with existing standard bus signals. Considering the benefits of COMA and the moderate design complexity it adds to the conventional shared-bus multiprocessor design, a bus-based COMA multiprocessor, such as DICE, can become a viable candidate for future shared-bus multiprocessor designs.",
        "title": "25061"
    },
    {
        "abs": "Just as hardware integrated circuits, or components, can be used to inexpensively manufacture a product line of related hardware systems, reusable software components can be used to create software systems. This is accomplished by developing a common framework for a product line of related software systems that forms the component operating environment. A development architecture is presented based on our work using Object Oriented Analysis and Design techniques to create reusable software components that combine with aircraft specific customizations to form an avionics software system.",
        "title": "25071"
    },
    {
        "abs": "The IEEE Futurebus is intended for use in multiprocessor systems and incorporates a protocol for maintaining memory and cache coherence in shared-memory architectures. An early version of the protocol was specified by a set of formulae expressing state transitions for the various modules connected to the bus. The paper shows how these formulae can be expressed as a logic program and the program used to investigate the internal consistency of the formulae, to stimulate the bus traffic and state transitions involved in shared memory transactions, and to check the mapping between different cache coherence protocols and the Futurebus. The program is written in Prolog and the paper includes some observations on the use of Prolog as a language for CAD applications. Pure Prolog has been used, making the program suitable for execution on the Delphi parallel Prolog architecture.",
        "title": "25110"
    },
    {
        "abs": "With the increase in the complexity of models and lack of flexibility offered by the analog computers, coupled with the advancements in digital hardware, the simulation industry has subsequently moved to digital computers and increased usage of programming languages such as C, C++, and MATLAB. However, the reduced time-step required to simulate complex and fast systems imposes a tighter constraint on the time within which the computations have to be performed. The sequential execution of these computations fails to cope with the real-time constraints which further restrict the usefulness of Real-Time Simulation (RTS) in a Virtual Reality (VR) environment. In this paper, we present a methodology for the design and implementation of RTS algorithms, based on the use of Field-Programmable Gate Array (FPGA) technology. We apply our methodology to an 8th order steering valve subsystem of a vehicle with relatively low response time requirements and use the FPGA technology to improve the response time of this model. Our methodology utilizes traditional hardware/software co-design approaches to generate a heterogeneous architecture for an FPGA-based simulator by porting the computationally complex regions to hardware. The hardware design was optimized such that it efficiently utilizes the parallel nature of FPGAs and pipelines the independent operations. Further enhancement was made by building a hardware component library of custom accelerators for common non-linear functions. The library also stores the information about resource utilization, cycle count, and the relative error with different bit-width combinations for these components, which is further used to evaluate different partitioning approaches. In this paper, we illustrate the partitioning of a hardware-based simulator design across dual FPGAs, initiate RTS using a system input from a Hardware-in-the-Loop (HIL) framework, and use these simulation results from our FPGA-based platform to perform response analysis. The total simulation time, which includes the time required to receive the system input over a socket (without HIL), software initialization, hardware computation, and transfer of simulation results back over a socket, shows a speedup of 2 × as compared to a similar setup with no hardware acceleration. The correctness of the simulation output from the hardware has also been validated with the simulated results from the software-only design.",
        "title": "25147"
    },
    {
        "abs": "This article addresses the multiplication of one data sample with multiple constants using addition/subtraction and shift operations, i.e., the multiple constant multiplications (MCM) operation. In the last two decades, many efficient algorithms have been proposed to implement the MCM operation using the fewest number of addition and subtraction operations. However, due to the NP-hardness of the problem, almost all the existing algorithms have been heuristics. The main contribution of this article is the proposal of an exact depth-first search algorithm that, using lower and upper bound values of the search space for the MCM problem instance, finds the minimum solution consuming less computational resources than the previously proposed exact breadth-first search algorithm. We start by describing the exact breadth-first search algorithm that can be applied on real mid-size instances. We also present our recently proposed approximate algorithm that finds solutions close to the minimum and is able to compute better bounds for the MCM problem. The experimental results clearly indicate that the exact depth-first search algorithm can be efficiently applied to large size hard instances that the exact breadth-first search algorithm cannot handle and the heuristics can only find suboptimal solutions.",
        "title": "25155"
    },
    {
        "abs": "Bus-based shared memory multiprocessors with private caches and snooping write-invalidate cache coherence protocols are dominant form of small- to medium-scale parallel machines today. In these systems, the high memory latency poses the major hurdle in achieving high performance. One way to cope with this problem is to use various techniques for tolerating high memory latency. Software-controlled cache prefetching and data forwarding are two widely used techniques for tolerating high memory latency in scalable cache-coherent shared memory multiprocessors. However, some previous studies have shown that these techniques are not so effective in bus-based shared memory multiprocessors. In this paper, we propose a novel software-controlled technique called cache injection, which combines consumer and producer initiated approach, and broadcasting nature of bus. Performance evaluation based on program-driven simulation and a set of scientific applications and test benchmarks shows that cache injection is highly effective in reducing misses and bus traffic.",
        "title": "25210"
    },
    {
        "abs": "New standards in signal, multimedia, and network processing for embedded electronics are characterized by computationally intensive algorithms, high flexibility due to the swift change in specifications. In order to meet demanding challenges of increasing computational requirements and stringent constraints on area and power consumption in fields of embedded engineering, there is a gradual trend towards coarse-grained parallel embedded processors. Furthermore, such processors are enabled with dynamic reconfiguration features for supporting time- and space-multiplexed execution of the algorithms. However, the formidable problem in efficient mapping of applications (mostly loop algorithms) onto such architectures has been a hindrance in their mass acceptance. In this paper we present (a) a highly parameterizable, tightly coupled, and reconfigurable parallel processor architecture together with the corresponding power breakdown and reconfiguration time analysis of a case study application, (b) a retargetable methodology for mapping of loop algorithms, (c) a co-design framework for modeling, simulation, and programming of such architectures, and (d) loosely coupled communication with host processor.",
        "title": "25221"
    },
    {
        "abs": "Embedded systems have strict timing and code size requirements. Software pipelining is one of the most important optimization techniques to improve the execution time of loops by increasing the parallelism among successive loop iterations. However, there is no effective techniques exist for solving the software pipelining problem on nested loops. The existing software pipelining techniques for single loops can only explore the parallelism of the innermost loop, so the final timing performance is inferior. While multi-dimensional (MD) retiming can explore the outer loop parallelism, it introduces large overheads in loop index generation and code size due to loop transformation. In this paper, we show how the computation time and code size of a pipelined nested loop is affected by execution sequence and retiming, assuming there is no loop unfolding. We present the theory of Software PIpelining for NEsted loops (SPINE) to reveal the relationship among the computation time of an iteration, the execution sequence, and the software pipelining degree of a nested loop using retiming concepts. Two algorithms of Software PIpelining for NEsted loops (SPINE) are proposed based on the fundamental understanding of the properties of software pipelining for nested loops: the SPINE-FULL algorithm generates fully parallelized loops with the minimal overheads. The SPINE-ROW-WISE algorithm achieves the maximal parallelism in an iteration with a fixed row-wise execution sequence. Therefore, the overheads due to loop transformation are minimal. Our technique can be directly applied to imperfect nested loops. The experimental results show that the average improvement on the execution time of the pipelined loop generated by SPINE is 71.7% compared with that generated by the standard software pipelining technique. The average code size is reduced by 69.5% compared with that generated by the MD retiming technique.",
        "title": "25223"
    },
    {
        "abs": "The increasing on-chip temperature has become a critical issue in the multiprocessor system on chip (MPSoC). It does not only disturb the reliability and performance of the system but it also increases the power consumption. Load balancing is a common scheduling technique to address the thermal issues where the load is transferred to relatively less used cores. However, during the transfer of workload, the destination cores may be in sleep mode and thus take some time to switch to the running mode. It results in wastage of some processing time. This paper presents a thermal-aware load balancing technique that avoids thermal emergencies while eliminating the switching delays. The proposed technique estimates the time taken by a task set to reach the temperature threshold values with the help of offline recorded thermal profiles of datasets. Moreover, cores are selected in a round-robin manner to equilibrate thermal gradients. The technique is based on Global Earliest Deadline First (GEDF) scheduling algorithm and considers ambient temperature, thermal cycles and energy consumption. The proposed technique is tested in a simulation environment comprising of scheduling and thermal simulation model. The results show that the technique reduces the average temperature up to 17%, thermal cycles up to 15%, energy consumption up to 20% and lowers the temporal and spatial gradients as compared to the commonly used predictive thermal-aware and thermal balancing techniques.",
        "title": "25251"
    },
    {
        "abs": "Secure electronic and internet transactions require public key cryptosystems to establish and distribute shared secret information for use in the bulk encryption of data. For security reasons, key sizes are in the region of hundred's of bits. This makes cryptographic procedures slow in software. Hardware accelerators can perform the computationally intensive operations far quicker. Field-Programmable Gate Arrays are well-suited for this application due to their reconfigurability and versatility. Elliptic Curve Cryptosystems over GF( p ) have received very little attention to date due to the seemingly more attractive finite field GF(2 m ). However, we present a GF( p ) Arithmetic Logic Unit which can perform 160-bit arithmetic at clock speeds of up to 50 MHz.",
        "title": "25308"
    },
    {
        "abs": "The communication architecture in a distributed memory multi-processor system must provide reliable and flexible communications with minimal latency to fully utilize the performance advantage offered by the inherent parallelism. An inefficiently designed architecture introduces ‘hot spots’ in the inter-processor communications and degrades the potential speed-up possible. This paper compares existing switching strategies on multi-processor systems and describes the development of an adaptive switching architecture for a circuit-switched multi- processor system with an arbitrary interconnection topology. The switching architecture employs dynamic reconfiguration of the communications path to provide highly efficient and fault-tolerant inter-processor communications.",
        "title": "25342"
    },
    {
        "abs": "This paper presents CMP-VR (Chip-Multiprocessor with Victim Retention), an approach to improve cache performance by reducing the number of off-chip memory accesses. The objective of this approach is to retain the chosen victim cache blocks on the chip for the longest possible time. It may be possible that some sets of the CMPs last level cache (LLC) are heavily used, while certain others are not. In CMP-VR, some number of ways from every set are used as reserved storage. It allows a victim block from a heavily used set to be stored into the reserve space of another set. In this way the load of heavily used sets are distributed among the underused sets. This logically increases the associativity of the heavily used sets without increasing the actual associativity and size of the cache. Experimental evaluation using full-system simulation shows that CMP-VR has less off-chip miss-rate as compared to baseline Tiled CMP. Results are presented for different cache sizes and associativity for CMP-VR and baseline configuration. The best improvements obtained are 45.5% and 14% in terms of miss rate and cycles per instruction (CPI) respectively for a 4 MB, 4-way set associative LLC. Reduction in CPI and miss rate together guarantees performance improvement.",
        "title": "25424"
    },
    {
        "abs": "In this paper, an Anti-lock braking system (ABS) is designed using a fuzzy logic controller (FLC). The FLC is designed in 0.35 μm standard CMOS process and this design of the circuit makes it having high speed and low power consumption. Also, all the controlling signals and input signals are in voltage form and there is no need for digital programming. Voltage mode design of the circuits leads to the convenient use of FLC with sensors. The objective function is designed to keep the wheel slip to a desired level so that maximum wheel tractive force and maximum vehicle deceleration are achieved. By training this function to the Adaptive Nero-Fuzzy Inference System (ANFIS) of MATLAB software the main parameters of fuzzy controller singletons and shape of Membership Function Generators (MFGs) are measured, then all the circuits of the proposed fuzzy logic controller is designed using Complementary Metal-Oxide-Semiconductor (CMOS) circuits, at the end simulation results of the fuzzy logic controller using Hspice simulator and BSIM3v3 (Berkeley Short-Channel Insulated gate field effect transistor Model) parameters verifies the performance and functionality of the design.",
        "title": "25649"
    },
    {
        "abs": "In October 1981, Brown University completed the Installation of a 300 MHz broadband communication network which ties together 110 campus buildings. The network backbone consists of redundant 300 MHz midsplit CATV systems which are run in underground conduits. Network outlets or taps are being installed in all lecture rooms, offices, laboratories and student dormitory rooms throughout the university. More than 1000 terminals, computer ports and other devices are currently operating on the network. The design and construction of the network, cost, performance and operating experience are discussed. Initial applications and management of the network are also described.",
        "title": "25696"
    },
    {
        "abs": "Interfacing between remote analogue sensors and a microcontroller requires the simplicity of the serial output analogue-to-digital converter. This application note uses TI's 8 bit serial output ADC, TLC549, and the microcontroller's parallel ports to implement such an interface. It shows the hardware and software requirements and discusses the analogue considerations.",
        "title": "25891"
    },
    {
        "abs": "The problems of replacing existing process controllers with microprocessor-based systems are discussed. A design methodology for implementing a microprocessor-based process control system from an existing design is given, such that strict functional equivalence between the original design and the microprocessor is maintained. The design methodology combines multilevel hierarchical systems and graph theory to deduce a low complexity model of the process control system, which is then analysed and verified interactively on a host computer. The resulting data is formatted and then downloaded into the microprocessor memory of the process controller.",
        "title": "26003"
    },
    {
        "abs": "The paper describes the experiences of running and assessing a semicustom design assignment for final year honours degree students. It describes the CAD equipment required and gives an overview of the prerequisite knowledge required of the students involved. It also describes the method of running such an assignment, the timescales involved and the means of assessment. A design example is included to show the level of complexity of the design. The lessons learned are that such an assignment is feasible and valuable and that the experiences given to the students highlight that the difficulties of semicustom design are not limited to the technical aspects but include such commercial considerations as project management.",
        "title": "26338"
    },
    {
        "abs": "Many processes require controllers with an instant response (e.g. motor control, CNC machines). A high-performance PLC can be constructed with use of programmable logic devices. A lack of custom synthesis tools disables the use of standard languages widely accepted by automation designers. The paper presents the systematic process of a PLC program synthesis to hardware structure. An input PLC program is given according to the IEC61131-3 standard. The synthesis process has been developed for implementation of a program described with the LD and SFC languages. The essential idea of synthesis process is obtaining a massively parallel operating hardware structure that significantly reduces response processing time. The PLC program is translated into originally developed dedicated graph structure that enables a wide range of optimizations. In the next step, it is mapped into a hardware structure. In order to reduce resource requirements, a strategy with resource sharing is shown, which is an original extension of general mapping concepts. Modern FPGAs are equipped with arithmetic cores dedicated for signal processing, inspiring the development of the original DSP48 block mapping strategy. It attempts to utilize all features of the block in the pipelined calculation model. The considerations are summarized with the implementation result compared against standard PLC implementation, a mutual comparison of general hardware mapping, and with the use of DSP48 units.",
        "title": "26342"
    },
    {
        "abs": "The mapping process of high performance embedded applications to today’s multiprocessor system-on-chip devices suffers from a complex toolchain and programming process. The problem is the expression of parallelism with a pure imperative programming language, which is commonly C. This traditional approach limits the mapping, partitioning and the generation of optimized parallel code, and consequently the achievable performance and power consumption of applications from different domains. The Architecture oriented paraLlelization for high performance embedded Multicore systems using scilAb (ALMA) European project aims to bridge these hurdles through the introduction and exploitation of a Scilab-based toolchain which enables the efficient mapping of applications on multiprocessor platforms from a high level of abstraction. The holistic solution of the ALMA toolchain allows the complexity of both the application and the architecture to be hidden, which leads to better acceptance, reduced development cost, and shorter time-to-market. Driven by the technology restrictions in chip design, the end of exponential growth of clock speeds and an unavoidable increasing request of computing performance, ALMA is a fundamental step forward in the necessary introduction of novel computing paradigms and methodologies.",
        "title": "26425"
    },
    {
        "abs": "Multi-terminal switching lattices are typically exploited for modeling switching nano-crossbar arrays that lead to the design and construction of emerging nanocomputers. Typically, the circuit is represented on a single lattice composed by four-terminal switches. In this paper, we propose a two-layer model in order to further minimize the area of regular functions, such as autosymmetric and D-reducible functions, and of decomposed functions. In particular, we propose a switching lattice optimization method for a special class of “regular” Boolean functions, called autosymmetric functions. Autosymmetry is a property that is frequent enough within Boolean functions to be interesting in the synthesis process. Each autosymmetric function can be synthesized through a new function (called restriction ), depending on less variables and with a smaller on-set, which can be computed in polynomial time. In this paper we describe how to exploit the autosymmetry property of a Boolean function in order to obtain a smaller lattice representation in a reduced minimization time. The original Boolean function can be constructed through a composition of the restriction with some EXORs of subsets of the input variables. Similarly, the lattice implementation of the function can be constructed using some external lattices for the EXORs, whose outputs will be inputs to the lattice implementing the restriction. Finally, the output of the restriction lattice corresponds to the output of the original function. Experimental results show that the total area of the obtained lattices is often significantly reduced. Moreover, in many cases, the computational time necessary to minimize the restriction is smaller than the time necessary to perform the lattice synthesis of the entire function. Finally, we propose the application of this particular lattice composition technique, based on connected multiple lattices, to the synthesis on switching lattices of D-reducible Boolean functions, and to the more general framework of lattice synthesis based on logic function decomposition.",
        "title": "26556"
    },
    {
        "abs": "This paper presents an FPGA-oriented implementation methodology for the MPEG-4 video decoder based on a hardware/software co-design approach. The MPEG-4 decoder is based on MoMuSys optimized reference software combined with new hardware VLSI architectures. New architectures for input demultiplexing, variable length decoding and inverse discrete cosine transform are developed. All software and hardware structures are evaluated in terms of visual quality, computational complexity and memory bandwidth metrics. The presented implementation is compared with an optimized reference software-based solution. Simulation results demonstrate a reduction of decoder complexity, especially speed and memory bandwidth, while maintaining an acceptable quality of decoded sequences. The proposed hardware additions provide 30% speed improvement over software solution, thereby reducing the clock rate required to process full-rate video from 300 MHz down to 213 MHz. The MPEG-4 decoder was functionally tested on a Flextronics FPGA prototyping board.",
        "title": "26579"
    },
    {
        "abs": "Effective exploitation of the application-specific parallel patterns and computation operations through their direct implementation in hardware is the base for construction of high-quality application-specific (re-) configurable application specific instruction set processors (ASIPs) and hardware accelerators for modern highly-demanding applications. Although it receives a lot of attention from the researchers and practitioners, a very important problem of hardware reuse in ASIP and accelerator synthesis is clearly underestimated and does not get enough attention in the published research. This paper is an effect of an industry and academic collaborative research. It analyses the problem of hardware sharing, shows its high practical relevance, as well as a big influence of hardware sharing on the major circuit and system parameters, and its importance for the multi-objective optimization and tradeoff exploitation. It also demonstrates that the state-of-the-art synthesis tools do not sufficiently address this problem and gives several guidelines related to enhancement of the hardware reuse.",
        "title": "26598"
    },
    {
        "abs": "For a wide range of applications that make use of a vector coprocessor, its resources are not highly utilized due to the lack of sustained data parallelism, which often occurs due to insufficient vector parallelism or vector-length variations in dynamic environments. The motivation of our work stems from (a) the omnipresence of vector operations in high-performance scientific and emerging embedded applications; (b) the mandate for multicore designs to make efficient use of on-chip resources for low power and high performance; (c) the need to often handle a variety of vector sizes; and (d) vector kernels in application suites may have diverse computation needs. Our objective is to provide a versatile design framework that can facilitate vector coprocessor sharing among multiple cores in a manner that maximizes resource utilization while also yielding very high performance at reduced area and energy costs. We have previously proposed three basic shared vector coprocessor architectures based on coarse-grain temporal, fine-grain temporal, and vector lane sharing that were implemented in SystemVerilog [15] . Our new paper presents substantially improved versions of these architectures that are implemented in synthesized RTL for higher accuracy. We herein evaluate these vector coprocessor sharing policies for a dual-core system using the floating-point performance, resource utilization and power consumption metrics. Benchmarking for FIR filtering, FFT, matrix multiplication, LU decomposition and sparse matrix vector multiplication shows that these coprocessor sharing policies yield high utilization, high performance and low energy per operation. Fine-grain temporal sharing most often provides the best performance among the three policies; it is followed by vector lane and then coarse-grain temporal sharing. It is also shown that, per core exclusive access to the vector resources does not maximize their utilization. This benchmarking involves various scenarios for each application, where the scenarios differ in terms of the vector length and the parallelism-oriented coding technique.",
        "title": "26599"
    },
    {
        "abs": "In this paper, a novel Fuzzy Fractional Order Proportional Integral Derivative (FFOPID) controller is proposed to control the liquid level of Two Tank Spherical Interacting System (TTSIS). Spherical tanks are widely used in process industries due to its high storage capacity. These are non-linear due to its varying surface area with respect to its height and hence the level control of a spherical tank system is a challenging task. Fractional Order Proportional Integral Derivative (FOPID) controller is designed for a liquid level control of a spherical tank which is modelled as a fractional Order System. The performance indices such as rise time, settling time, peak time and peak overshoot are analysed for the proposed control scheme and compared to conventional controllers such as PI, PID, PID-SMC, FOPID. Moreover the time integral performance measures such as ITAE, IAE and ISE are analysed. Experimental results are shown to validate the obtained results with those of simulations. It has been proven that FFOPID outperforms all other existing techniques in terms of various time domain specifications and time integral performance measures.",
        "title": "26620"
    },
    {
        "abs": "Intel’s XScale which has powered many multimedia applications uses scoreboard to control instruction execution. Scoreboard stalls the pipeline whenever a source operand or functional unit is needed but not available. While waiting for the availability of the resources, the processor accesses the scoreboard every cycle. Such accesses consume energy without contributing to performance. We address this inefficiency by investigating stall behaviour and introduce an adaptive technique to avoid regular access to the scoreboard during stall periods. Our study shows that by using our technique and for the representative subset of MiBench benchmark suite studied here, it is possible to reduce scoreboard energy consumption by up to 33% while maintaining performance cost within 0.25%.",
        "title": "26636"
    },
    {
        "abs": "Physical Unclonable Functions (PUFs) enable the generation of device-unique, on-chip, and digital identifiers by exploiting the manufacturing process variation. The past decade has seen an extensive effort in PUF design. Yet, most PUF constructions are regarded as stand-alone hardware building blocks. In contrast, we propose PUF constructions that are tightly integrated into the design of a micro-processor. The proposed PUFs are essentially a collection of time-to-digital converters that are integrated into the custom instruction or memory-mapped interface of a processor. Therefore, the processor can issue the PUF challenges and collect the associated responses using instruction executions. This integration enables practical, run-time physical authentication and it allows flexible post-processing mechanisms using software. In this article, we describe the design, implementation, and the performance analysis details of such hardware/software co-designed authentication mechanisms on FPGAs. We propose two variants of the PUF architecture: a synchronous module that requires minimal place and route constraints utilizing the common clock of the SoC, and an asynchronous alternative that is independent of the clock but realized with a controlled placement. We implemented the synchronous architecture on the Altera Cyclone-IV FPGAs and performed a large-scale characterization on 55 boards. The asynchronous design is realized on the Xilinx Virtex-5 FPGAs and tested on 100 boards. Measurements reveal that the proposed solutions can authenticate trillions of devices and provide better performance than the ring oscillator based alternative.",
        "title": "26640"
    },
    {
        "abs": "During the last decade, an increasing number of primary and secondary fly-by-wire systems have been developed for civil aircraft. These systems have reduced the gap in form and implementation technology which had developed between civil and military electronic flight control systems. However, there remain significant differences. These differences include systems architecture, redundancy management, external interfaces, software methods and, to a lesser extent, hardware. This paper describes some of those differences and why they exist. In particular, the impact of performance, reliability, availability and safety requirements is considered together with the effect of industry traditions and standards.",
        "title": "26717"
    },
    {
        "abs": "The possibility of using PCI Mezzanine Cards (PMCs) in VME modules opens new options for interconnecting crates by means of high-speed interconnection networks (Scalable Coherent Interface (SCI), ATM, Fiber Channel, etc.). These new technologies offer major benefits in the construction of large data acquisition networks where the number of VME crates is important. In this paper we explain how to use an SCI to easily connect VME crates and take advantage of the new scalability offered by SCI.",
        "title": "26734"
    }
]