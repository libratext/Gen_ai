[
    {
        "input": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012)\nimplementation and its naive data parallelism on multiple GPUs. Our performance\non 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014)\nrun on 1 GPU. To the best of our knowledge, this is the first open-source\nPython-based AlexNet implementation to-date.",
        "Revise this with your best effort": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-the-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.",
        "Help me polish this": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.",
        "Rewrite this for me": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.",
        "Make this fluent while doing minimal change": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.",
        "Refine this for me please": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.",
        "Concise this for me and keep all the information": "This report describes a Theano-based AlexNet implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library run on 1 GPU. This is the first open-source Python-based AlexNet implementation to-date.",
        "Improve this in GPT way": "In this report, we describe a GPT-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date."
    },
    {
        "input": "We show that deep narrow Boltzmann machines are universal approximators of\nprobability distributions on the activities of their visible units, provided\nthey have sufficiently many hidden layers, each containing the same number of\nunits as the visible layer. We show that, within certain parameter domains,\ndeep Boltzmann machines can be studied as feedforward networks. We provide\nupper and lower bounds on the sufficient depth and width of universal\napproximators. These results settle various intuitions regarding undirected\nnetworks and, in particular, they show that deep narrow Boltzmann machines are\nat least as compact universal approximators as narrow sigmoid belief networks\nand restricted Boltzmann machines, with respect to the currently available\nbounds for those models.",
        "Revise this with your best effort": "We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufficiently many hidden layers, each containing the same number of units as the visible layer. We also show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. Furthermore, we provide upper and lower bounds on the sufficient depth and width of universal approximators. These results settle various intuitions regarding undirected networks and, in particular, they show that deep narrow Boltzmann machines are at least as compact universal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models.",
        "Help me polish this": "We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufficiently many hidden layers, each containing the same number of units as the visible layer. We also show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. Our results settle various intuitions regarding undirected networks and, in particular, they show that deep narrow Boltzmann machines are at least as compact universal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models.",
        "Rewrite this for me": "We demonstrate that deep, narrow Boltzmann machines can serve as universal approximators for probability distributions over the activity of their visible units, given that they have enough hidden layers - each containing the same number of units as the visible layer. We also show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. Finally, we provide upper and lower bounds on the sufficient depth and width of universal approximators. These results settle various intuitions regarding undirected networks, and show that deep, narrow Boltzmann machines are at least as compact universal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models.",
        "Make this fluent while doing minimal change": "We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufficiently many hidden layers, each containing the same number of units as the visible layer. We also show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. Furthermore, we provide upper and lower bounds on the sufficient depth and width of universal approximators. These results settle various intuitions regarding undirected networks and, in particular, they show that deep narrow Boltzmann machines are at least as compact universal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models.",
        "Refine this for me please": "We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufficiently many hidden layers, each containing the same number of units as the visible layer. We also show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. Finally, we provide upper and lower bounds on the sufficient depth and width of universal approximators.",
        "Concise this for me and keep all the information": "Deep narrow Boltzmann machines are universal approximators of probability distributions, provided they have enough hidden layers. They can be studied as feedforward networks within certain parameter domains. There are upper and lower bounds on the sufficient depth and width of universal approximators.",
        "Improve this in GPT way": "We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufficiently many hidden layers, each containing the same number of units as the visible layer. We also show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. Furthermore, we provide upper and lower bounds on the sufficient depth and width of universal approximators. These results settle various intuitions regarding undirected networks and, in particular, they show that deep narrow Boltzmann machines are at least as compact universal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models."
    },
    {
        "input": "Leveraging advances in variational inference, we propose to enhance recurrent\nneural networks with latent variables, resulting in Stochastic Recurrent\nNetworks (STORNs). The model i) can be trained with stochastic gradient\nmethods, ii) allows structured and multi-modal conditionals at each time step,\niii) features a reliable estimator of the marginal likelihood and iv) is a\ngeneralisation of deterministic recurrent neural networks. We evaluate the\nmethod on four polyphonic musical data sets and motion capture data.",
        "Revise this with your best effort": "Leveraging advances in variational inference, we propose enhancing recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model: \n\n- can be trained with stochastic gradient methods, \n- allows for structured and multi-modal conditionals at each time step, \n- features a reliable estimator of the marginal likelihood, and \n- is a generalisation of deterministic recurrent neural networks. \n\nWe evaluate the method on four polyphonic musical data sets and motion capture data.",
        "Help me polish this": "Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model: \n\n- can be trained with stochastic gradient methods, \n- allows structured and multi-modal conditionals at each time step, \n- features a reliable estimator of the marginal likelihood, and \n- is a generalisation of deterministic recurrent neural networks. \n\nWe evaluate the method on four polyphonic musical data sets and motion capture data.",
        "Rewrite this for me": "We propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model: \n\n- can be trained with stochastic gradient methods, \n- allows structured and multi-modal conditionals at each time step, \n- features a reliable estimator of the marginal likelihood, and \n- is a generalisation of deterministic recurrent neural networks. \n\nWe evaluate the method on four polyphonic musical data sets and motion capture data.",
        "Make this fluent while doing minimal change": "We propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model: \n\n- can be trained with stochastic gradient methods, \n- allows structured and multi-modal conditionals at each time step, \n- features a reliable estimator of the marginal likelihood, and \n- is a generalisation of deterministic recurrent neural networks. \n\nWe evaluate the method on four polyphonic musical data sets and motion capture data.",
        "Refine this for me please": "We propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model: \n- can be trained with stochastic gradient methods, \n- allows structured and multi-modal conditionals at each time step, \n- features a reliable estimator of the marginal likelihood, and \n- is a generalisation of deterministic recurrent neural networks. \nWe evaluate the method on four polyphonic musical data sets and motion capture data.",
        "Concise this for me and keep all the information": "We propose a model that enhances recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model can be trained with stochastic gradient methods, allows structured and multi-modal conditionals at each time step, features a reliable estimator of the marginal likelihood, and is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.",
        "Improve this in GPT way": "We propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows for structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood, and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data."
    },
    {
        "input": "We describe a general framework for online adaptation of optimization\nhyperparameters by `hot swapping' their values during learning. We investigate\nthis approach in the context of adaptive learning rate selection using an\nexplore-exploit strategy from the multi-armed bandit literature. Experiments on\na benchmark neural network show that the hot swapping approach leads to\nconsistently better solutions compared to well-known alternatives such as\nAdaDelta and stochastic gradient with exhaustive hyperparameter search.",
        "Revise this with your best effort": "We describe a general framework for online adaptation of optimization hyperparameters by \"hot swapping\" their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.",
        "Help me polish this": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.",
        "Rewrite this for me": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.",
        "Make this fluent while doing minimal change": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.",
        "Refine this for me please": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.",
        "Concise this for me and keep all the information": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives.",
        "Improve this in GPT way": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search."
    },
    {
        "input": "Many modern multiclass and multilabel problems are characterized by\nincreasingly large output spaces. For these problems, label embeddings have\nbeen shown to be a useful primitive that can improve computational and\nstatistical efficiency. In this work we utilize a correspondence between rank\nconstrained estimation and low dimensional label embeddings that uncovers a\nfast label embedding algorithm which works in both the multiclass and\nmultilabel settings. The result is a randomized algorithm for partial least\nsquares, whose running time is exponentially faster than naive algorithms. We\ndemonstrate our techniques on two large-scale public datasets, from the Large\nScale Hierarchical Text Challenge and the Open Directory Project, where we\nobtain state of the art results.",
        "Revise this with your best effort": "Many modern multiclass and multilabel problems are characterized by an increasing number of output labels. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work, we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state-of-the-art results.",
        "Help me polish this": "Many modern multiclass and multilabel problems are characterized by an increasing number of output labels. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work, we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state-of-the-art results.",
        "Rewrite this for me": "Many modern multiclass and multilabel problems are characterized by an increasing number of output options. In order to improve computational and statistical efficiency for these problems, label embeddings have been shown to be a useful primitive. This work uses a correspondence between rank constrained estimation and low dimensional label embeddings to uncover a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. The techniques are demonstrated on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where state of the art results are obtained.",
        "Make this fluent while doing minimal change": "Many modern multiclass and multilabel problems are characterized by an increasingly large output space. Label embeddings have been shown to be a useful primitive in these cases, able to improve computational and statistical efficiency. In this work, we utilize a correspondence between rank constrained estimation and low dimensional label embeddings to uncover a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.",
        "Refine this for me please": "Many modern multiclass and multilabel problems are characterized by an increasing number of output labels. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work, we utilize a correspondence between rank constrained estimation and low dimensional label embeddings to uncover a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.",
        "Concise this for me and keep all the information": "Many modern multiclass and multilabel problems are characterized by an increasing number of output labels. Label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work, we utilize a correspondence between rank constrained estimation and low dimensional label embeddings to uncover a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.",
        "Improve this in GPT way": "Many modern multiclass and multilabel problems are characterized by an increasingly large output space. In order to improve computational and statistical efficiency, label embeddings have been shown to be a useful primitive. In this work, we utilize a correspondence between rank constrained estimation and low dimensional label embeddings to uncover a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results."
    },
    {
        "input": "Accurate representational learning of both the explicit and implicit\nrelationships within data is critical to the ability of machines to perform\nmore complex and abstract reasoning tasks. We describe the efficient weakly\nsupervised learning of such inferences by our Dynamic Adaptive Network\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\nquestion answering tasks in the bAbI dataset that have proved difficult for\ncontemporary approaches to learning representation (Weston et al., 2015).",
        "Revise this with your best effort": "It is critical for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Help me polish this": "Accurate learning of both explicit and implicit relationships within data is critical for machines to be able to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have been difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Rewrite this for me": "It is critical for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Make this fluent while doing minimal change": "It is critical for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Refine this for me please": "The ability of machines to accurately learn both the explicit and implicit relationships within data is critical to their ability to perform more complex and abstract reasoning tasks. Our Dynamic Adaptive Network Intelligence (DANI) model efficiently learns such inferences in a weakly supervised manner. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Concise this for me and keep all the information": "It is important for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex reasoning tasks. The Dynamic Adaptive Network Intelligence (DANI) model is a way to efficiently learn these inferences through weak supervision. DANI has been shown to outperform other contemporary approaches to learning representation on question answering tasks in the bAbI dataset.",
        "Improve this in GPT way": "We describe the efficient weakly supervised learning of accurate representations of both the explicit and implicit relationships within data by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015)."
    },
    {
        "input": "Automatic speech recognition systems usually rely on spectral-based features,\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\nas, speech perception or/and speech production. Recently, convolutional neural\nnetworks have been shown to be able to estimate phoneme conditional\nprobabilities in a completely data-driven manner, i.e. using directly temporal\nraw speech signal as input. This system was shown to yield similar or better\nperformance than HMM/ANN based system on phoneme recognition task and on large\nscale continuous speech recognition task, using less parameters. Motivated by\nthese studies, we investigate the use of simple linear classifier in the\nCNN-based framework. Thus, the network learns linearly separable features from\nraw speech. We show that such system yields similar or better performance than\nMLP based system using cepstral-based features as input.",
        "Revise this with your best effort": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.",
        "Help me polish this": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.",
        "Rewrite this for me": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. However, recent studies have shown that convolutional neural networks (CNNs) can estimate phoneme conditional probabilities in a completely data-driven manner, using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.",
        "Make this fluent while doing minimal change": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.",
        "Refine this for me please": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.",
        "Concise this for me and keep all the information": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters.",
        "Improve this in GPT way": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. However, these features are extracted based on prior knowledge such as speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input."
    },
    {
        "input": "We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine.",
        "Revise this with your best effort": "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.",
        "Help me polish this": "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic.\n\nOur method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.",
        "Rewrite this for me": "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic.\n\nOur method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.",
        "Make this fluent while doing minimal change": "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.",
        "Refine this for me please": "The Kaldi speech recognition toolkit uses a neural-network training framework that is designed for training DNNs with large amounts of data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, the toolkit uses a method of averaging the neural network parameters periodically (typically every minute or two), and redistributing the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, the toolkit has another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow the periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.",
        "Concise this for me and keep all the information": "The Kaldi speech recognition toolkit uses a neural-network training framework that is designed for training DNNs with large amounts of data using multiple GPU-equipped or multi-core machines. This framework uses a method of averaging the neural network parameters periodically and redistributing the averaged parameters to the machines for further training. This method is combined with an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which allows for periodic-averaging and improves the convergence of SGD on a single machine.",
        "Improve this in GPT way": "We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine."
    },
    {
        "input": "We develop a new method for visualizing and refining the invariances of\nlearned representations. Specifically, we test for a general form of\ninvariance, linearization, in which the action of a transformation is confined\nto a low-dimensional subspace. Given two reference images (typically, differing\nby some transformation), we synthesize a sequence of images lying on a path\nbetween them that is of minimal length in the space of the representation (a\n\"representational geodesic\"). If the transformation relating the two reference\nimages is linearized by the representation, this sequence should follow the\ngradual evolution of this transformation. We use this method to assess the\ninvariance properties of a state-of-the-art image classification network and\nfind that geodesics generated for image pairs differing by translation,\nrotation, and dilation do not evolve according to their associated\ntransformations. Our method also suggests a remedy for these failures, and\nfollowing this prescription, we show that the modified representation is able\nto linearize a variety of geometric image transformations.",
        "Revise this with your best effort": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Help me polish this": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Rewrite this for me": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Make this fluent while doing minimal change": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Refine this for me please": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Concise this for me and keep all the information": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Improve this in GPT way": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations."
    },
    {
        "input": "Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called {\\em pretraining}: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.",
        "Revise this with your best effort": "Why does Deep Learning work?\n\nDeep Learning works because it is able to capture higher-order representations. This is possible because Deep Learning networks are able to learn simple features first, and then build upon these to learn more complex features.",
        "Help me polish this": "Why does Deep Learning work?\n\nDeep Learning works because it is able to capture higher-order representations. By using group theory, Deep Learning is able to find the simplest features first and then build up to more complex features. This allows for a more efficient and accurate learning process.",
        "Rewrite this for me": "Why does Deep Learning work?\n\nDeep Learning works because it captures higher-order representations. It does this by first searching for a good generative model for the input samples, and then repeating the process one layer at a time. This process is called \"pretraining.\"\n\nWe show that the pretraining step has deeper implications than just better initialization of a network. We establish a connection between pretraining and the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of \"shadow\" groups whose elements serve as close approximations.\n\nOver the shadow groups, the pre-training step becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the \"simplest.\" That's why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.",
        "Make this fluent while doing minimal change": "One factor behind the recent resurgence of the subject is a key algorithmic step called {\\em pretraining}: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of {\\em shadow} groups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the {\\em simplest}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.",
        "Refine this for me please": "In short, deep learning works because it captures simple representations first, and then builds on these to capture more complex representations. This process is analogous to the way that a group action can be used to approximate a more complicated object.",
        "Concise this for me and keep all the information": "Deep Learning works because it captures representations. The higher-order representations emerge from the interplay of orbits and stabilizers of group actions. The pre-training step is a search for features with minimal orbits. Intuitively, these features are in a way the simplest. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.",
        "Improve this in GPT way": "We study the question of why Deep Learning works from the perspective of group theory. We show that the key algorithmic step of pretraining is equivalent to a search for features with minimal orbits over shadow groups. This explains why a deep learning network learns simple features first. We then show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper."
    },
    {
        "input": "We present a novel architecture, the \"stacked what-where auto-encoders\"\n(SWWAE), which integrates discriminative and generative pathways and provides a\nunified approach to supervised, semi-supervised and unsupervised learning\nwithout relying on sampling during training. An instantiation of SWWAE uses a\nconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and\nemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the\nreconstruction. The objective function includes reconstruction terms that\ninduce the hidden states in the Deconvnet to be similar to those of the\nConvnet. Each pooling layer produces two sets of variables: the \"what\" which\nare fed to the next layer, and its complementary variable \"where\" that are fed\nto the corresponding layer in the generative decoder.",
        "Revise this with your best effort": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
        "Help me polish this": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
        "Rewrite this for me": "We introduce the \"stacked what-where auto-encoders\" (SWWAE), a novel architecture that integrates discriminative and generative pathways to provide a unified approach to supervised, semi-supervised and unsupervised learning, without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) to encode the input, and employs a deconvolutional net (Deconvnet) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
        "Make this fluent while doing minimal change": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
        "Refine this for me please": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
        "Concise this for me and keep all the information": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
        "Improve this in GPT way": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder."
    },
    {
        "input": "We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks.",
        "Revise this with your best effort": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments, we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks.",
        "Help me polish this": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments, we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks.",
        "Rewrite this for me": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments, we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks.",
        "Make this fluent while doing minimal change": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments, we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks.",
        "Refine this for me please": "We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks.",
        "Concise this for me and keep all the information": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks.",
        "Improve this in GPT way": "We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks."
    },
    {
        "input": "A generative model is developed for deep (multi-layered) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up (pretraining) and top-down\n(refinement) probabilistic learning. Experimental results demonstrate powerful\ncapabilities of the model to learn multi-layer features from images, and\nexcellent classification results are obtained on the MNIST and Caltech 101\ndatasets.",
        "Revise this with your best effort": "A deep convolutional dictionary learning model is developed with a novel probabilistic pooling operation. This allows for efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. The model is powerful in learning multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.",
        "Help me polish this": "A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate the model's ability to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.",
        "Rewrite this for me": "A deep convolutional dictionary learning model is developed with a novel probabilistic pooling operation. This allows for efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. The model is powerful in learning multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.",
        "Make this fluent while doing minimal change": "A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate the model's powerful ability to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.",
        "Refine this for me please": "A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.",
        "Concise this for me and keep all the information": "A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. The model is powerful in learning multi-layer features from images, and obtains excellent classification results on the MNIST and Caltech 101 datasets.",
        "Improve this in GPT way": "A GPT model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets."
    },
    {
        "input": "Motivated by the recent progress in generative models, we introduce a model\nthat generates images from natural language descriptions. The proposed model\niteratively draws patches on a canvas, while attending to the relevant words in\nthe description. After training on Microsoft COCO, we compare our model with\nseveral baseline generative models on image generation and retrieval tasks. We\ndemonstrate that our model produces higher quality samples than other\napproaches and generates images with novel scene compositions corresponding to\npreviously unseen captions in the dataset.",
        "Revise this with your best effort": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Help me polish this": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Rewrite this for me": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. Our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Make this fluent while doing minimal change": "We introduce a model that generates images from natural language descriptions, motivated by recent progress in generative models. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Refine this for me please": "We introduce a model that generates images from natural language descriptions. The proposed model\niteratively draws patches on a canvas, while attending to the relevant words in\nthe description. We compare our model with\nseveral baseline generative models on image generation and retrieval tasks. We\ndemonstrate that our model produces higher quality samples than other\napproaches and generates images with novel scene compositions corresponding to\npreviously unseen captions in the dataset.",
        "Concise this for me and keep all the information": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Improve this in GPT way": "We introduce a new model for generating images from natural language descriptions. This model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks, and demonstrate that our model produces higher quality samples than other approaches."
    },
    {
        "input": "Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10.",
        "Revise this with your best effort": "Convolutional neural networks (CNNs) are effective for large datasets. However, collecting labeled data can be difficult, and in some applications, larger amounts of data are not available. The problem then is how to use CNNs with small data, as CNNs can overfit quickly.\n\nWe present an efficient Bayesian CNN that is more robust to overfitting on small data than traditional approaches. This is done by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters.\n\nOn the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field.\n\nWe show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.",
        "Help me polish this": "Convolutional neural networks (CNNs) are effective for large datasets. However, collecting labeled data can be difficult, and in some applications, larger amounts of data are not available. The problem then is how to use CNNs with small data, as CNNs can overfit quickly.\n\nWe present an efficient Bayesian CNN, which is more robust to overfitting on small data than traditional approaches. This is done by placing a probability distribution over the CNN's kernels. We approximate the model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters.\n\nOn the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field.\n\nWe show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.",
        "Rewrite this for me": "Convolutional neural networks (CNNs) are effective on large datasets. However, it is difficult to label data, and in some applications, larger amounts of data are not available. The problem then is how to use CNNs with small data, as CNNs are prone to overfitting quickly. We present an efficient Bayesian CNN, which is more robust to overfitting on small data than traditional approaches. This is done by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters.\n\nOn the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.",
        "Make this fluent while doing minimal change": "Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10.",
        "Refine this for me please": "Convolutional neural networks (CNNs) are effective for large datasets. However, collecting labeled data can be difficult, and in some applications, larger amounts of data are not available. The problem then is how to use CNNs with small data, as CNNs can overfit quickly.\n\nWe present an efficient Bayesian CNN, which is more robust to overfitting on small data than traditional approaches. This is done by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters.\n\nOn the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field.\n\nWe show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.",
        "Concise this for me and keep all the information": "Convolutional neural networks (CNNs) are effective on large datasets. But when data is limited, CNNs are prone to overfitting. We present a Bayesian CNN that is more robust to overfitting on small data. This is done by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity. We show a considerable improvement in classification accuracy compared to standard techniques.",
        "Improve this in GPT way": "We present an efficient Bayesian CNN that offers better robustness to over-fitting on small data sets than traditional approaches. This is achieved by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters.\n\nOn the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10."
    },
    {
        "input": "We propose a new method for creating computationally efficient convolutional\nneural networks (CNNs) by using low-rank representations of convolutional\nfilters. Rather than approximating filters in previously-trained networks with\nmore efficient versions, we learn a set of small basis filters from scratch;\nduring training, the network learns to combine these basis filters into more\ncomplex filters that are discriminative for image classification. To train such\nnetworks, a novel weight initialization scheme is used. This allows effective\ninitialization of connection weights in convolutional layers composed of groups\nof differently-shaped filters. We validate our approach by applying it to\nseveral existing CNN architectures and training these networks from scratch\nusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or\nhigher accuracy than conventional CNNs with much less compute. Applying our\nmethod to an improved version of VGG-11 network using global max-pooling, we\nachieve comparable validation accuracy using 41% less compute and only 24% of\nthe original VGG-11 model parameters; another variant of our method gives a 1\npercentage point increase in accuracy over our improved VGG-11 model, giving a\ntop-5 center-crop validation accuracy of 89.7% while reducing computation by\n16% relative to the original VGG-11 model. Applying our method to the GoogLeNet\narchitecture for ILSVRC, we achieved comparable accuracy with 26% less compute\nand 41% fewer model parameters. Applying our method to a near state-of-the-art\nnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and\n55% fewer parameters.",
        "Revise this with your best effort": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method",
        "Help me polish this": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our",
        "Rewrite this for me": "We propose a new, more efficient method for creating convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our",
        "Make this fluent while doing minimal change": "We propose a new, more efficient method for creating convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our",
        "Refine this for me please": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method",
        "Concise this for me and keep all the information": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute.",
        "Improve this in GPT way": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method"
    },
    {
        "input": "Distributed representations of words have boosted the performance of many\nNatural Language Processing tasks. However, usually only one representation per\nword is obtained, not acknowledging the fact that some words have multiple\nmeanings. This has a negative effect on the individual word representations and\nthe language model as a whole. In this paper we present a simple model that\nenables recent techniques for building word vectors to represent distinct\nsenses of polysemic words. In our assessment of this model we show that it is\nable to effectively discriminate between words' senses and to do so in a\ncomputationally efficient manner.",
        "Revise this with your best effort": "Distributed representations of words have improved the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not taking into account the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that allows recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.",
        "Help me polish this": "Distributed representations of words have been shown to boost the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not acknowledging the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.",
        "Rewrite this for me": "Distributed representations of words have improved the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not taking into account that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a model that allows recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.",
        "Make this fluent while doing minimal change": "Distributed representations of words have improved the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not taking into account the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.",
        "Refine this for me please": "Distributed representations of words have boosted the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not acknowledging the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.",
        "Concise this for me and keep all the information": "Distributed representations of words have boosted the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not acknowledging the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.",
        "Improve this in GPT way": "Distributed representations of words have boosted the performance of many\nNatural Language Processing tasks. However, usually only one representation per\nword is obtained, not acknowledging the fact that some words have multiple\nmeanings. This has a negative effect on the individual word representations and\nthe language model as a whole. In this paper we present a simple model that\nenables recent techniques for building word vectors to represent distinct\nsenses of polysemic words. In our assessment of this model we show that it is\nable to effectively discriminate between words' senses and to do so in a\ncomputationally efficient manner."
    },
    {
        "input": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for\nlanguage models (LMs). A DENNLM projects the input word history vector onto\nmultiple diverse low-dimensional sub-spaces instead of a single\nhigher-dimensional sub-space as in conventional feed-forward neural network\nLMs. We encourage these sub-spaces to be diverse during network training\nthrough an augmented loss function. Our language modeling experiments on the\nPenn Treebank data set show the performance benefit of using a DENNLM.",
        "Revise this with your best effort": "We propose the Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
        "Help me polish this": "We propose the Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
        "Rewrite this for me": "We propose the Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
        "Make this fluent while doing minimal change": "We propose the Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
        "Refine this for me please": "We propose the Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
        "Concise this for me and keep all the information": "We propose the Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
        "Improve this in GPT way": "We propose the Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM."
    },
    {
        "input": "A standard approach to Collaborative Filtering (CF), i.e. prediction of user\nratings on items, relies on Matrix Factorization techniques. Representations\nfor both users and items are computed from the observed ratings and used for\nprediction. Unfortunatly, these transductive approaches cannot handle the case\nof new users arriving in the system, with no known rating, a problem known as\nuser cold-start. A common approach in this context is to ask these incoming\nusers for a few initialization ratings. This paper presents a model to tackle\nthis twofold problem of (i) finding good questions to ask, (ii) building\nefficient representations from this small amount of information. The model can\nalso be used in a more standard (warm) context. Our approach is evaluated on\nthe classical CF problem and on the cold-start problem on four different\ndatasets showing its ability to improve baseline performance in both cases.",
        "Revise this with your best effort": "A standard approach to Collaborative Filtering (CF), i.e. prediction of user ratings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunately, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. \n\nThis paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases.",
        "Help me polish this": "A standard approach to Collaborative Filtering (CF), i.e. prediction of user ratings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunately, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. \n\nThis paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases.",
        "Rewrite this for me": "A common approach to Collaborative Filtering (CF) is to use Matrix Factorization techniques to compute representations for both users and items from observed ratings, and then use these representations to predict ratings for new users and items. However, this transductive approach cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. This paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets, showing its ability to improve baseline performance in both cases.",
        "Make this fluent while doing minimal change": "A standard approach to Collaborative Filtering (CF), i.e. prediction of user ratings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunately, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. \n\nThis paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases.",
        "Refine this for me please": "A standard approach to Collaborative Filtering (CF) relies on Matrix Factorization techniques. These transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. This paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases.",
        "Concise this for me and keep all the information": "A standard approach to Collaborative Filtering (CF) relies on Matrix Factorization techniques. However, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. This paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases.",
        "Improve this in GPT way": "A standard approach to Collaborative Filtering (CF), i.e. prediction of user ratings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunately, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. \n\nThis paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases."
    },
    {
        "input": "We propose a deep learning framework for modeling complex high-dimensional\ndensities called Non-linear Independent Component Estimation (NICE). It is\nbased on the idea that a good representation is one in which the data has a\ndistribution that is easy to model. For this purpose, a non-linear\ndeterministic transformation of the data is learned that maps it to a latent\nspace so as to make the transformed data conform to a factorized distribution,\ni.e., resulting in independent latent variables. We parametrize this\ntransformation so that computing the Jacobian determinant and inverse transform\nis trivial, yet we maintain the ability to learn complex non-linear\ntransformations, via a composition of simple building blocks, each based on a\ndeep neural network. The training criterion is simply the exact log-likelihood,\nwhich is tractable. Unbiased ancestral sampling is also easy. We show that this\napproach yields good generative models on four image datasets and can be used\nfor inpainting.",
        "Revise this with your best effort": "We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.",
        "Help me polish this": "We propose a deep learning framework called Non-linear Independent Component Estimation (NICE) for modeling complex high-dimensional densities. It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.",
        "Rewrite this for me": "We propose a deep learning framework, called Non-linear Independent Component Estimation (NICE), for modeling complex high-dimensional densities. NICE is based on the idea that a good representation is one in which the data has a distribution that is easy to model. To this end, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution--i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.",
        "Make this fluent while doing minimal change": "We propose a deep learning framework called Non-linear Independent Component Estimation (NICE) for modeling complex high-dimensional densities. It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.",
        "Refine this for me please": "We propose a deep learning framework, called Non-linear Independent Component Estimation (NICE), for modeling complex high-dimensional densities. NICE is based on the idea that a good representation is one in which the data has a distribution that is easy to model. To this end, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.",
        "Concise this for me and keep all the information": "We propose a deep learning framework, called Non-linear Independent Component Estimation (NICE), for modeling complex high-dimensional densities. NICE is based on the idea that a good representation is one in which the data has a distribution that is easy to model. To achieve this, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.",
        "Improve this in GPT way": "We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting."
    },
    {
        "input": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns\nlinearly separable latent representations in an end-to-end fashion. Classic LDA\nextracts features which preserve class separability and is used for\ndimensionality reduction for many classification problems. The central idea of\nthis paper is to put LDA on top of a deep neural network. This can be seen as a\nnon-linear extension of classic LDA. Instead of maximizing the likelihood of\ntarget labels for individual samples, we propose an objective function that\npushes the network to produce feature distributions which: (a) have low\nvariance within the same class and (b) high variance between different classes.\nOur objective is derived from the general LDA eigenvalue problem and still\nallows to train with stochastic gradient descent and back-propagation. For\nevaluation we test our approach on three different benchmark datasets (MNIST,\nCIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and\nCIFAR-10 and outperforms a network trained with categorical cross entropy (same\narchitecture) on a supervised setting of STL-10.",
        "Revise this with your best effort": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and CIFAR-10 and outperforms a network trained with categorical cross entropy (same architecture) on a supervised setting of STL-10.",
        "Help me polish this": "We introduce the Deep Linear Discriminant Analysis (DeepLDA) to learn linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and CIFAR-10 and outperforms a network trained with categorical cross entropy (same architecture) on a supervised setting of STL-10.",
        "Rewrite this for me": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and CIFAR-10 and outperforms a network trained with categorical cross entropy (same architecture) on a supervised setting of STL-10.",
        "Make this fluent while doing minimal change": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns\nlinearly separable latent representations in an end-to-end fashion. Classic LDA\nextracts features which preserve class separability and is used for\ndimensionality reduction for many classification problems. The central idea of\nthis paper is to put LDA on top of a deep neural network. This can be seen as a\nnon-linear extension of classic LDA. Instead of maximizing the likelihood of\ntarget labels for individual samples, we propose an objective function that\npushes the network to produce feature distributions which: (a) have low\nvariance within the same class and (b) high variance between different classes.\nOur objective is derived from the general LDA eigenvalue problem and still\nallows us to train with stochastic gradient descent and back-propagation. For\nevaluation, we test our approach on three different benchmark datasets (MNIST,\nCIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and\nCIFAR-10 and outperforms a network trained with categorical cross entropy (same\narchitecture) on a supervised setting of STL-10.",
        "Refine this for me please": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns\nlinearly separable latent representations in an end-to-end fashion. Classic LDA\nextracts features which preserve class separability and is used for\ndimensionality reduction for many classification problems. The central idea of\nthis paper is to put LDA on top of a deep neural network. This can be seen as a\nnon-linear extension of classic LDA. Instead of maximizing the likelihood of\ntarget labels for individual samples, we propose an objective function that\npushes the network to produce feature distributions which: (a) have low\nvariance within the same class and (b) high variance between different classes.\nOur objective is derived from the general LDA eigenvalue problem and still\nallows to train with stochastic gradient descent and back-propagation. For\nevaluation we test our approach on three different benchmark datasets (MNIST,\nCIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and\nCIFAR-10 and outperforms a network trained with categorical cross entropy (same\narchitecture) on a supervised setting of STL-10.",
        "Concise this for me and keep all the information": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which have low variance within the same class and high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and CIFAR-10 and outperforms a network trained with categorical cross entropy (same architecture) on a supervised setting of STL-10.",
        "Improve this in GPT way": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns\nlinearly separable latent representations in an end-to-end fashion. Classic LDA\nextracts features which preserve class separability and is used for\ndimensionality reduction for many classification problems. The central idea of\nthis paper is to put LDA on top of a deep neural network. This can be seen as a\nnon-linear extension of classic LDA. Instead of maximizing the likelihood of\ntarget labels for individual samples, we propose an objective function that\npushes the network to produce feature distributions which: (a) have low\nvariance within the same class and (b) high variance between different classes.\nOur objective is derived from the general LDA eigenvalue problem and still\nallows to train with stochastic gradient descent and back-propagation. For\nevaluation we test our approach on three different benchmark datasets (MNIST,\nCIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and\nCIFAR-10 and outperforms a network trained with categorical cross entropy (same\narchitecture) on a supervised setting of STL-10."
    },
    {
        "input": "Layer-sequential unit-variance (LSUV) initialization - a simple method for\nweight initialization for deep net learning - is proposed. The method consists\nof the two steps. First, pre-initialize weights of each convolution or\ninner-product layer with orthonormal matrices. Second, proceed from the first\nto the final layer, normalizing the variance of the output of each layer to be\nequal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh)\nshow that the proposed initialization leads to learning of very deep nets that\n(i) produces networks with test accuracy better or equal to standard methods\nand (ii) is at least as fast as the complex schemes proposed specifically for\nvery deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava\net al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets\nand the state-of-the-art, or very close to it, is achieved on the MNIST,\nCIFAR-10/100 and ImageNet datasets.",
        "Revise this with your best effort": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Help me polish this": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Rewrite this for me": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Make this fluent while doing minimal change": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning that consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperimenting with different activation functions (maxout, ReLU-family, tanh) shows that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Refine this for me please": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Concise this for me and keep all the information": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Improve this in GPT way": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets."
    },
    {
        "input": "We introduce a parametric nonlinear transformation that is well-suited for\nGaussianizing data from natural images. The data are linearly transformed, and\neach component is then normalized by a pooled activity measure, computed by\nexponentiating a weighted sum of rectified and exponentiated components and a\nconstant. We optimize the parameters of the full transformation (linear\ntransform, exponents, weights, constant) over a database of natural images,\ndirectly minimizing the negentropy of the responses. The optimized\ntransformation substantially Gaussianizes the data, achieving a significantly\nsmaller mutual information between transformed components than alternative\nmethods including ICA and radial Gaussianization. The transformation is\ndifferentiable and can be efficiently inverted, and thus induces a density\nmodel on images. We show that samples of this model are visually similar to\nsamples of natural image patches. We demonstrate the use of the model as a\nprior probability density that can be used to remove additive noise. Finally,\nwe show that the transformation can be cascaded, with each layer optimized\nusing the same Gaussianization objective, thus offering an unsupervised method\nof optimizing a deep network architecture.",
        "Revise this with your best effort": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Help me polish this": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Rewrite this for me": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Make this fluent while doing minimal change": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Refine this for me please": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Concise this for me and keep all the information": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images.",
        "Improve this in GPT way": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture."
    },
    {
        "input": "We present flattened convolutional neural networks that are designed for fast\nfeedforward execution. The redundancy of the parameters, especially weights of\nthe convolutional filters in convolutional neural networks has been extensively\nstudied and different heuristics have been proposed to construct a low rank\nbasis of the filters after training. In this work, we train flattened networks\nthat consist of consecutive sequence of one-dimensional filters across all\ndirections in 3D space to obtain comparable performance as conventional\nconvolutional networks. We tested flattened model on different datasets and\nfound that the flattened layer can effectively substitute for the 3D filters\nwithout loss of accuracy. The flattened convolution pipelines provide around\ntwo times speed-up during feedforward pass compared to the baseline model due\nto the significant reduction of learning parameters. Furthermore, the proposed\nmethod does not require efforts in manual tuning or post processing once the\nmodel is trained.",
        "Revise this with your best effort": "We propose flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks, has been extensively studied, and different heuristics have been proposed to construct a low-rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequences of one-dimensional filters across all directions in 3D space to obtain comparable performance to conventional convolutional networks. We tested flattened models on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.",
        "Help me polish this": "We propose flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks, has been extensively studied, and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequences of one-dimensional filters across all directions in 3D space to obtain comparable performance to conventional convolutional networks. We tested flattened models on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.",
        "Rewrite this for me": "We propose flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks, has been extensively studied, and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequences of one-dimensional filters across all directions in 3D space to obtain comparable performance to conventional convolutional networks. We tested flattened models on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.",
        "Make this fluent while doing minimal change": "We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks, has been extensively studied, and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened models on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.",
        "Refine this for me please": "We propose flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks, has been extensively studied, and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened models on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.",
        "Concise this for me and keep all the information": "We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters.",
        "Improve this in GPT way": "We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained."
    },
    {
        "input": "In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.",
        "Revise this with your best effort": "In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.",
        "Help me polish this": "In this paper, we introduce a novel deep learning framework, termed Purine.\nPurine is a deep network expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.",
        "Rewrite this for me": "In this paper, we introduce a new deep learning framework called Purine.\n\nPurine is based on a bipartite graph (bi-graph), which consists of interconnected operators and data tensors. This abstraction makes it easy to solve networks using an event-driven task dispatcher.\n\nWe demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be implemented universally by graph composition. This eases researchers from having to code for various parallelization schemes, and the same dispatcher can be used for solving different types of graphs.\n\nScheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduces the communication overhead and helps us achieve approximate linear acceleration.",
        "Make this fluent while doing minimal change": "In this paper, we introduce a novel deep learning framework, termed Purine. Purine is a deep network expressed as a bipartite graph (bi-graph), which is composed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. This eases researchers from coding for various parallelization schemes, and the same dispatcher can be used for solving variant graphs. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduce the communication overhead and help us achieve approximate linear acceleration.",
        "Refine this for me please": "In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.",
        "Concise this for me and keep all the information": "In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.",
        "Improve this in GPT way": "In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration."
    },
    {
        "input": "In this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.",
        "Revise this with your best effort": "In this paper, we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initializing the weights and network state.",
        "Help me polish this": "In this paper, we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.",
        "Rewrite this for me": "In this paper, we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). This model can be used for efficient, large-scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initializing the weights and network state.",
        "Make this fluent while doing minimal change": "In this paper, we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initializing the weights and network state.",
        "Refine this for me please": "The Variational Recurrent Auto-Encoder (VRAE) is a model that combines the strengths of RNNs and SGVB. The model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. The model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.",
        "Concise this for me and keep all the information": "This paper proposes a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.",
        "Improve this in GPT way": "In this paper, we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state."
    },
    {
        "input": "Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation.",
        "Revise this with your best effort": "Current work in lexical distributed representations typically maps each word to a point in low-dimensional space. However, mapping to a density instead provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.",
        "Help me polish this": "Current work in lexical distributed representations typically maps each word to a point vector in low-dimensional space. However, mapping to a density instead provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.",
        "Rewrite this for me": "Density-based distributed representations have many advantages over traditional vector-based representations, including better capturing of uncertainty, more natural expression of asymmetries, and more expressive parameterization of decision boundaries. This paper advocates for density-based distributed representations and presents a method for learning these representations using Gaussian distributions. We compare the performance of density-based representations on various word embedding benchmarks, investigate their ability to model entailment and other asymmetric relationships, and explore novel properties of these representations.",
        "Make this fluent while doing minimal change": "Current work in lexical distributed representations typically maps each word to a point vector in low-dimensional space. Mapping words to densities instead provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.",
        "Refine this for me please": "The current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.",
        "Concise this for me and keep all the information": "Lexical distributed representations map each word to a point in low-dimensional space. Mapping to a density instead provides many advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions.",
        "Improve this in GPT way": "Current work in lexical distributed representations typically maps each word to a point vector in low-dimensional space. However, mapping to a density instead provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation."
    },
    {
        "input": "Multipliers are the most space and power-hungry arithmetic operators of the\ndigital implementation of deep neural networks. We train a set of\nstate-of-the-art neural networks (Maxout networks) on three benchmark datasets:\nMNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:\nfloating point, fixed point and dynamic fixed point. For each of those datasets\nand for each of those formats, we assess the impact of the precision of the\nmultiplications on the final error after training. We find that very low\nprecision is sufficient not just for running trained networks but also for\ntraining them. For example, it is possible to train Maxout networks with 10\nbits multiplications.",
        "Revise this with your best effort": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.",
        "Help me polish this": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.",
        "Rewrite this for me": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.",
        "Make this fluent while doing minimal change": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.",
        "Refine this for me please": "We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. We assess the impact of the precision of the multiplications on the final error after training, and find that very low precision is sufficient not just for running trained networks but also for training them.",
        "Concise this for me and keep all the information": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them.",
        "Improve this in GPT way": "We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications."
    },
    {
        "input": "Multiple instance learning (MIL) can reduce the need for costly annotation in\ntasks such as semantic segmentation by weakening the required degree of\nsupervision. We propose a novel MIL formulation of multi-class semantic\nsegmentation learning by a fully convolutional network. In this setting, we\nseek to learn a semantic segmentation model from just weak image-level labels.\nThe model is trained end-to-end to jointly optimize the representation while\ndisambiguating the pixel-image label assignment. Fully convolutional training\naccepts inputs of any size, does not need object proposal pre-processing, and\noffers a pixelwise loss map for selecting latent instances. Our multi-class MIL\nloss exploits the further supervision given by images with multiple labels. We\nevaluate this approach through preliminary experiments on the PASCAL VOC\nsegmentation challenge.",
        "Revise this with your best effort": "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.",
        "Help me polish this": "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.",
        "Rewrite this for me": "MIL can reduce the need for costly annotation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.",
        "Make this fluent while doing minimal change": "MIL can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.",
        "Refine this for me please": "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.",
        "Concise this for me and keep all the information": "MIL can reduce the need for costly annotation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels.",
        "Improve this in GPT way": "We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge."
    },
    {
        "input": "Recently, nested dropout was proposed as a method for ordering representation\nunits in autoencoders by their information content, without diminishing\nreconstruction cost. However, it has only been applied to training\nfully-connected autoencoders in an unsupervised setting. We explore the impact\nof nested dropout on the convolutional layers in a CNN trained by\nbackpropagation, investigating whether nested dropout can provide a simple and\nsystematic way to determine the optimal representation size with respect to the\ndesired accuracy and desired task and data complexity.",
        "Revise this with your best effort": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.",
        "Help me polish this": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.",
        "Rewrite this for me": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.",
        "Make this fluent while doing minimal change": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.",
        "Refine this for me please": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.",
        "Concise this for me and keep all the information": "Nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.",
        "Improve this in GPT way": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity."
    },
    {
        "input": "Stochastic gradient algorithms have been the main focus of large-scale\nlearning problems and they led to important successes in machine learning. The\nconvergence of SGD depends on the careful choice of learning rate and the\namount of the noise in stochastic estimates of the gradients. In this paper, we\npropose a new adaptive learning rate algorithm, which utilizes curvature\ninformation for automatically tuning the learning rates. The information about\nthe element-wise curvature of the loss function is estimated from the local\nstatistics of the stochastic first order gradients. We further propose a new\nvariance reduction technique to speed up the convergence. In our preliminary\nexperiments with deep neural networks, we obtained better performance compared\nto the popular stochastic gradient algorithms.",
        "Revise this with your best effort": "Stochastic gradient algorithms have been the main focus of large-scale learning problems and they have led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm that utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first-order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.",
        "Help me polish this": "Stochastic gradient algorithms have been the main focus of large-scale learning problems and they have led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm that utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.",
        "Rewrite this for me": "Stochastic gradient algorithms have been the focus of large-scale learning problems for some time now, and have led to important successes in machine learning. The key to the success of these algorithms lies in the careful choice of learning rate and the amount of noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm which utilizes curvature information for automatically tuning the learning rates. This information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.",
        "Make this fluent while doing minimal change": "Stochastic gradient algorithms have been the main focus of large-scale learning problems and they have led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm, which utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.",
        "Refine this for me please": "Stochastic gradient algorithms have been the main focus of large-scale learning problems and they have led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm that utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.",
        "Concise this for me and keep all the information": "Stochastic gradient algorithms are the main focus of large-scale learning problems and have led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm, which utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.",
        "Improve this in GPT way": "Stochastic gradient algorithms have been the main focus of large-scale learning problems for some time now, and have led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm, which utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms."
    },
    {
        "input": "When a three-dimensional object moves relative to an observer, a change\noccurs on the observer's image plane and in the visual representation computed\nby a learned model. Starting with the idea that a good visual representation is\none that transforms linearly under scene motions, we show, using the theory of\ngroup representations, that any such representation is equivalent to a\ncombination of the elementary irreducible representations. We derive a striking\nrelationship between irreducibility and the statistical dependency structure of\nthe representation, by showing that under restricted conditions, irreducible\nrepresentations are decorrelated. Under partial observability, as induced by\nthe perspective projection of a scene onto the image plane, the motion group\ndoes not have a linear action on the space of images, so that it becomes\nnecessary to perform inference over a latent representation that does transform\nlinearly. This idea is demonstrated in a model of rotating NORB objects that\nemploys a latent representation of the non-commutative 3D rotation group SO(3).",
        "Revise this with your best effort": "When an object moves in three dimensions, the image that an observer sees changes. This change can be represented mathematically, and we show that under certain conditions, this representation is equivalent to a combination of simpler representations, which we call elementary irreducible representations. We also show that there is a relationship between the irreducibility of a representation and the statistical dependencies within it. In particular, we show that under restricted conditions, irreducible representations are decorrelated. Finally, we demonstrate how this idea can be used in a model of rotating objects.",
        "Help me polish this": "When an object moves in three dimensions, the observer's image of it changes. If we want to create a good visual representation of the object, it should be one that changes linearly under such motions. However, we show that any such representation is equivalent to a combination of simpler, irreducible representations. This has the surprising consequence that, under certain conditions, these irreducible representations will be decorrelated. When an object is only partially visible, as when it is seen in perspective, the motion group does not have a linear action on the space of images. This means that we need to perform inference over a latent representation that does change linearly under such motions. We demonstrate this in a model of rotating objects that uses a latent representation of the non-commutative 3D rotation group SO(3).",
        "Rewrite this for me": "When an object moves in three dimensions, the image that an observer sees changes. This is because the way that the object is represented in the observer's mind changes. If we want to create a good visual representation of an object, we need to make sure that it changes in a linear way under different motions. However, it is not always possible to do this. Sometimes, we need to use a combination of different representations in order to get a good result. For example, when an object is only partially visible, we need to use a latent representation in order to infer what the object looks like. This is demonstrated in a model of rotating objects.",
        "Make this fluent while doing minimal change": "When an object moves in three dimensions, the observer's image plane changes, as does the visual representation that the observer has learned. It has been shown that any such representation can be reduced to a combination of the most basic, irreducible representations. In this paper, we explore the relationship between irreducibility and the statistical dependency structure of the representation, and show that under certain conditions, these irreducible representations are decorrelated. Our model demonstrates how, under conditions of partial observability, it is necessary to perform inference over a latent representation that does change linearly.",
        "Refine this for me please": "The object is moving in three dimensions, so the image on the observer's retina also moves. If the object is moving linearly, then the image will move in a corresponding linear fashion. However, if the object is moving in a non-linear fashion, then the image will also move in a non-linear fashion.",
        "Concise this for me and keep all the information": "When an object moves in three dimensions, the image seen by an observer changes, as does the visual representation created by a learned model. It has been proposed that a good visual representation is one that changes linearly under such motions. However, it has also been shown that any such representation is equivalent to a combination of elementary irreducible representations. This relationship between irreducibility and the statistical dependency structure of the representation can be used to decorrelate representations under certain conditions. When an object is only partially visible, as when it is seen in perspective, the motion group does not have a linear action on the space of images. In such cases, it is necessary to perform inference over a latent representation that does transform linearly. This idea is demonstrated in a model of rotating objects that employs a latent representation of the non-commutative 3D rotation group SO(3).",
        "Improve this in GPT way": "When a three-dimensional object moves relative to an observer, a change occurs in the observer's visual representation of the object. Starting with the idea that a good visual representation is one that transforms linearly under scene motions, we show that any such representation is equivalent to a combination of the elementary irreducible representations. We derive a relationship between irreducibility and the statistical dependency structure of the representation, by showing that under restricted conditions, irreducible representations are decorrelated. Under partial observability, as induced by the perspective projection of a scene onto the image plane, the motion group does not have a linear action on the space of images, so that it becomes necessary to perform inference over a latent representation that does transform linearly. This idea is demonstrated in a model of rotating NORB objects that employs a latent representation of the non-commutative 3D rotation group SO(3)."
    },
    {
        "input": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise.",
        "Revise this with your best effort": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.",
        "Help me polish this": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.",
        "Rewrite this for me": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. In the recent literature, solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated to perform approximate MIPS in sublinear time. \n\nIn this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). \n\nExperiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.",
        "Make this fluent while doing minimal change": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.",
        "Refine this for me please": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.",
        "Concise this for me and keep all the information": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. In this paper, we compare different solutions for approximate MIPS, based on locality-sensitive hashing (LSH) as well as tree-based solutions, to another extremely simple approach, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.",
        "Improve this in GPT way": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise."
    },
    {
        "input": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\nproposed generative model pairing a top-down generative network with a\nbottom-up recognition network which approximates posterior inference. It\ntypically makes strong assumptions about posterior inference, for instance that\nthe posterior distribution is approximately factorial, and that its parameters\ncan be approximated with nonlinear regression from the observations. As we show\nempirically, the VAE objective can lead to overly simplified representations\nwhich fail to use the network's entire modeling capacity. We present the\nimportance weighted autoencoder (IWAE), a generative model with the same\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\nbound derived from importance weighting. In the IWAE, the recognition network\nuses multiple samples to approximate the posterior, giving it increased\nflexibility to model complex posteriors which do not fit the VAE modeling\nassumptions. We show empirically that IWAEs learn richer latent space\nrepresentations than VAEs, leading to improved test log-likelihood on density\nestimation benchmarks.",
        "Revise this with your best effort": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model that pairs a top-down generative network with a bottom-up recognition network. This model makes strong assumptions about posterior inference, such as the posterior distribution being approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. However, as we show empirically, the VAE objective can lead to overly simplified representations that fail to use the network's entire modeling capacity. \n\nWe present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
        "Help me polish this": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model that pairs a top-down generative network with a bottom-up recognition network to approximate posterior inference. It typically makes strong assumptions about posterior inference, such as that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. However, as we show empirically, the VAE objective can lead to overly simplified representations that fail to use the network's entire modeling capacity.\n\nWe present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
        "Rewrite this for me": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a generative model that pairs a top-down generative network with a bottom-up recognition network. The VAE makes strong assumptions about posterior inference, such as the posterior distribution being approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. However, as we show empirically, the VAE objective can lead to overly simplified representations that fail to use the network's entire modeling capacity. \n\nWe present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
        "Make this fluent while doing minimal change": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model that pairs a top-down generative network with a bottom-up recognition network to approximate posterior inference. It typically makes strong assumptions about posterior inference, such as that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations that fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
        "Refine this for me please": "The variational autoencoder (VAE) is a generative model that pairs a top-down generative network with a bottom-up recognition network. The recognition network approximates posterior inference by making strong assumptions about the posterior distribution, such as that it is approximately factorial and that its parameters can be approximated with nonlinear regression from the observations. However, these strong assumptions can lead to overly simplified representations that fail to use the network's entire modeling capacity.\n\nThe importance weighted autoencoder (IWAE) is a generative model that uses a strictly tighter log-likelihood lower bound derived from importance weighting. This allows the recognition network to use multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors that do not fit the VAE modeling assumptions. As a result, IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
        "Concise this for me and keep all the information": "The variational autoencoder (VAE) is a generative model that pairs a top-down generative network with a bottom-up recognition network. The VAE makes strong assumptions about posterior inference, which can lead to simplified representations that don't use the full capacity of the network. The importance weighted autoencoder (IWAE) is a generative model with the same architecture as the VAE, but with a stricter log-likelihood lower bound. The IWAE uses multiple samples to approximate the posterior, giving it more flexibility to model complex posteriors. The IWAE learns richer latent space representations than the VAE, leading to improved test log-likelihood.",
        "Improve this in GPT way": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model that pairs a top-down generative network with a bottom-up recognition network. This model typically makes strong assumptions about posterior inference, such as the posterior distribution being approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. However, as we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. \n\nWe present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks."
    },
    {
        "input": "This work investigates how using reduced precision data in Convolutional\nNeural Networks (CNNs) affects network accuracy during classification. More\nspecifically, this study considers networks where each layer may use different\nprecision data. Our key result is the observation that the tolerance of CNNs to\nreduced precision data not only varies across networks, a well established\nobservation, but also within networks. Tuning precision per layer is appealing\nas it could enable energy and performance improvements. In this paper we study\nhow error tolerance across layers varies and propose a method for finding a low\nprecision configuration for a network while maintaining high accuracy. A\ndiverse set of CNNs is analyzed showing that compared to a conventional\nimplementation using a 32-bit floating-point representation for all layers, and\nwith less than 1% loss in relative accuracy, the data footprint required by\nthese networks can be reduced by an average of 74% and up to 92%.",
        "Revise this with your best effort": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.",
        "Help me polish this": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.",
        "Rewrite this for me": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.",
        "Make this fluent while doing minimal change": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.",
        "Refine this for me please": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.",
        "Concise this for me and keep all the information": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy.",
        "Improve this in GPT way": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%."
    },
    {
        "input": "The efficiency of graph-based semi-supervised algorithms depends on the graph\nof instances on which they are applied. The instances are often in a vectorial\nform before a graph linking them is built. The construction of the graph relies\non a metric over the vectorial space that help define the weight of the\nconnection between entities. The classic choice for this metric is usually a\ndistance measure or a similarity measure based on the euclidean norm. We claim\nthat in some cases the euclidean norm on the initial vectorial space might not\nbe the more appropriate to solve the task efficiently. We propose an algorithm\nthat aims at learning the most appropriate vectorial representation for\nbuilding a graph on which the task at hand is solved efficiently.",
        "Revise this with your best effort": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the most appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Help me polish this": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the most appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Rewrite this for me": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Make this fluent while doing minimal change": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Refine this for me please": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Concise this for me and keep all the information": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The graph is built based on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Improve this in GPT way": "The efficiency of graph-based semi-supervised algorithms depends on the graph\nof instances on which they are applied. The instances are often in a vectorial\nform before a graph linking them is built. The construction of the graph relies\non a metric over the vectorial space that help define the weight of the\nconnection between entities. The classic choice for this metric is usually a\ndistance measure or a similarity measure based on the euclidean norm. However,\nin some cases the euclidean norm on the initial vectorial space might not\nbe the most appropriate to solve the task efficiently. We propose an algorithm\nthat aims at learning the most appropriate vectorial representation for\nbuilding a graph on which the task at hand is solved efficiently."
    },
    {
        "input": "Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval.",
        "Revise this with your best effort": "Hypernymy, textual entailment, and image captioning can all be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper, we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
        "Help me polish this": "In this paper, we advocate for explicitly modeling the partial order structure of the visual-semantic hierarchy over words, sentences, and images. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
        "Rewrite this for me": "In this paper, we advocate for explicitly modeling the partial order structure of a visual-semantic hierarchy over words, sentences, and images. We introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
        "Make this fluent while doing minimal change": "In this paper, we advocate for explicitly modeling the partial order structure of a visual-semantic hierarchy over words, sentences, and images. We introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
        "Refine this for me please": "The paper argues that hypernymy, textual entailment, and image captioning can all be seen as special cases of a single visual-semantic hierarchy. The paper advocates for explicitly modeling the partial order structure of this hierarchy, and introduces a general method for learning ordered representations. The paper shows that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
        "Concise this for me and keep all the information": "In this paper, we advocate for explicitly modeling the partial order structure of a visual-semantic hierarchy over words, sentences, and images. We introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
        "Improve this in GPT way": "In this paper, we advocate for explicitly modeling the partial order structure of the visual-semantic hierarchy over words, sentences, and images. We introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval."
    },
    {
        "input": "We propose local distributional smoothness (LDS), a new notion of smoothness\nfor statistical model that can be used as a regularization term to promote the\nsmoothness of the model distribution. We named the LDS based regularization as\nvirtual adversarial training (VAT). The LDS of a model at an input datapoint is\ndefined as the KL-divergence based robustness of the model distribution against\nlocal perturbation around the datapoint. VAT resembles adversarial training,\nbut distinguishes itself in that it determines the adversarial direction from\nthe model distribution alone without using the label information, making it\napplicable to semi-supervised learning. The computational cost for VAT is\nrelatively low. For neural network, the approximated gradient of the LDS can be\ncomputed with no more than three pairs of forward and back propagations. When\nwe applied our technique to supervised and semi-supervised learning for the\nMNIST dataset, it outperformed all the training methods other than the current\nstate of the art method, which is based on a highly advanced generative model.\nWe also applied our method to SVHN and NORB, and confirmed our method's\nsuperior performance over the current state of the art semi-supervised method\napplied to these datasets.",
        "Revise this with your best effort": "We propose a new notion of smoothness for statistical models, called local distributional smoothness (LDS), which can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS-based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural networks, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets.",
        "Help me polish this": "We propose a new notion of smoothness for statistical models, called local distributional smoothness (LDS), which can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS-based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural networks, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets.",
        "Rewrite this for me": "We propose a new notion of smoothness for statistical models, called local distributional smoothness (LDS), which can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS-based regularization technique virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbations around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural networks, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets.",
        "Make this fluent while doing minimal change": "We propose a new notion of smoothness for statistical models, called local distributional smoothness (LDS), which can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS-based regularization technique virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural networks, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets.",
        "Refine this for me please": "We propose a new notion of smoothness for statistical models, called local distributional smoothness (LDS), which can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS-based regularization technique virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural networks, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets.",
        "Concise this for me and keep all the information": "We propose a new notion of smoothness for statistical models, called local distributional smoothness (LDS), which can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS-based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural networks, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets.",
        "Improve this in GPT way": "We propose a new notion of smoothness for statistical models, called local distributional smoothness (LDS). This can be used as a regularization term to promote the smoothness of the model distribution. We have named the LDS-based regularization as virtual adversarial training (VAT). \n\nThe LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. \n\nThe computational cost for VAT is relatively low. For neural networks, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. \n\nWe have applied our technique to supervised and semi-supervised learning for the MNIST dataset, and it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We have also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets."
    },
    {
        "input": "The availability of large labeled datasets has allowed Convolutional Network\nmodels to achieve impressive recognition results. However, in many settings\nmanual annotation of the data is impractical; instead our data has noisy\nlabels, i.e. there is some freely available label for each image which may or\nmay not be accurate. In this paper, we explore the performance of\ndiscriminatively-trained Convnets when trained on such noisy data. We introduce\nan extra noise layer into the network which adapts the network outputs to match\nthe noisy label distribution. The parameters of this noise layer can be\nestimated as part of the training process and involve simple modifications to\ncurrent training infrastructures for deep networks. We demonstrate the\napproaches on several datasets, including large scale experiments on the\nImageNet classification benchmark.",
        "Revise this with your best effort": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings, manual annotation of the data is impractical; instead, our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.",
        "Help me polish this": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings, manual annotation of the data is impractical; instead, our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.",
        "Rewrite this for me": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings, manual annotation of the data is impractical; instead, our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.",
        "Make this fluent while doing minimal change": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings, manual annotation of the data is impractical; instead, our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.",
        "Refine this for me please": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings, manual annotation of the data is impractical; instead, our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.",
        "Concise this for me and keep all the information": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.",
        "Improve this in GPT way": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings, manual annotation of the data is impractical; instead, our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark."
    },
    {
        "input": "We provide novel guaranteed approaches for training feedforward neural\nnetworks with sparse connectivity. We leverage on the techniques developed\npreviously for learning linear networks and show that they can also be\neffectively adopted to learn non-linear networks. We operate on the moments\ninvolving label and the score function of the input, and show that their\nfactorization provably yields the weight matrix of the first layer of a deep\nnetwork under mild conditions. In practice, the output of our method can be\nemployed as effective initializers for gradient descent.",
        "Revise this with your best effort": "We provide new, guaranteed methods for training feedforward neural networks with sparse connectivity. We build on the techniques developed previously for learning linear networks and show that they can also be effectively used to learn non-linear networks. We work with the moments involving the label and the score function of the input, and show that their factorization provably gives us the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be used as an effective initializer for gradient descent.",
        "Help me polish this": "We provide guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be employed as effective initializers for gradient descent.",
        "Rewrite this for me": "We provide new, guaranteed methods for training feedforward neural networks with sparse connectivity. We build on the techniques developed previously for learning linear networks and show that they can also be effectively used to learn non-linear networks. We work with the moments involving the label and the score function of the input, and show that their factorization leads to the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be used as an effective initializer for gradient descent.",
        "Make this fluent while doing minimal change": "We provide guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be employed as effective initializers for gradient descent.",
        "Refine this for me please": "We provide new, guaranteed methods for training feedforward neural networks with sparse connectivity. We build on the techniques developed previously for learning linear networks and show that they can also be effectively used to learn non-linear networks. We work with the moments involving the label and the score function of the input, and show that their factorization gives us the weight matrix of the first layer of a deep network under certain conditions. In practice, the output of our method can be used as an effective starting point for gradient descent.",
        "Concise this for me and keep all the information": "We provide new, guaranteed methods for training feedforward neural networks with sparse connectivity. We use the techniques developed previously for learning linear networks and show that they can also be effectively used to learn non-linear networks. We operate on the moments involving the label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be used as an effective initializer for gradient descent.",
        "Improve this in GPT way": "We provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage on the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be employed as effective initializers for gradient descent."
    },
    {
        "input": "Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked sentences. A more subtle\nchallenge is that it is not enough to represent the meaning of each sentence of\na discourse relation, because the relation may depend on links between\nlower-level elements, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted not only\nfrom the distributional representations of the sentences, but also of their\ncoreferent entity mentions. The resulting system obtains substantial\nimprovements over the previous state-of-the-art in predicting implicit\ndiscourse relations in the Penn Discourse Treebank.",
        "Revise this with your best effort": "Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "Help me polish this": "Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also from the representations of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "Rewrite this for me": "Discourse relations are a way of binding smaller linguistic elements together to create a cohesive text. However, it can be difficult to automatically identify discourse relations, because it requires an understanding of the semantics of the linked sentences. A more subtle challenge is that simply representing the meaning of each sentence in a discourse relation is not enough, because the relation may depend on links between lower-level elements, such as entity mentions.\n\nOur solution computes distributional meaning representations by working up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also from the representations of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "Make this fluent while doing minimal change": "Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also from the representations of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "Refine this for me please": "Discourse relations are connections between smaller linguistic elements that create a cohesive text. Automatically identifying these relations is difficult because it requires an understanding of the linked sentences' semantics. A more complicated challenge is that simply representing the meaning of each sentence in a discourse relation is not enough; the relation may depend on links between lower-level elements, such as entity mentions.\n\nOur solution computes meaning representations by working up the syntactic parse tree. This is different from previous work on compositional distributional semantics in that we also compute representations for entity mentions, using a downward compositional pass. Discourse relations are predicted from the distributional representations of the sentences and their coreferent entity mentions. The resulting system outperforms the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "Concise this for me and keep all the information": "Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "Improve this in GPT way": "Discourse relations are an important part of linguistic communication, binding smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions.\n\nOur solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank."
    },
    {
        "input": "In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage.",
        "Revise this with your best effort": "In this work, we propose a new method to integrate two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.",
        "Help me polish this": "In this work, we propose a new method to integrate two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.",
        "Rewrite this for me": "We propose a new method for integrating two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.",
        "Make this fluent while doing minimal change": "In this work, we propose a new method to integrate two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.",
        "Refine this for me please": "Our new method integrates two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.",
        "Concise this for me and keep all the information": "Our model consists of two components: an encoding component, which predicts roles given a set of syntactic and lexical features, and a reconstruction component, which uses roles to predict argument fillers. By jointly minimizing errors in argument reconstruction, we are able to induce roles that largely correspond to those defined in annotated resources. Our method performs on par with the most accurate role induction methods on English, even though we do not incorporate any prior linguistic knowledge about the language.",
        "Improve this in GPT way": "In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage."
    },
    {
        "input": "The notion of metric plays a key role in machine learning problems such as\nclassification, clustering or ranking. However, it is worth noting that there\nis a severe lack of theoretical guarantees that can be expected on the\ngeneralization capacity of the classifier associated to a given metric. The\ntheoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions\n(Balcan et al., 2008) has been one of the first attempts to draw a link between\nthe properties of a similarity function and those of a linear classifier making\nuse of it. In this paper, we extend and complete this theory by providing a new\ngeneralization bound for the associated classifier based on the algorithmic\nrobustness framework.",
        "Revise this with your best effort": "The notion of metric plays a key role in machine learning problems such as classification, clustering or ranking. However, it is worth noting that there is a severe lack of theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a given metric. The theoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions (Balcan et al., 2008) has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classifier based on the algorithmic robustness framework.",
        "Help me polish this": "The notion of metric plays a key role in machine learning problems such as classification, clustering or ranking. However, it is worth noting that there is a severe lack of theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a given metric. The theoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions (Balcan et al., 2008) has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classifier based on the algorithmic robustness framework.",
        "Rewrite this for me": "The notion of metric is key to machine learning problems such as classification, clustering, and ranking. However, it's worth noting that there are very few theoretical guarantees that can be expected on the generalization capacity of the classifier associated with a given metric. The theoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions (Balcan et al., 2008) was one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classifier based on the algorithmic robustness framework.",
        "Make this fluent while doing minimal change": "The notion of metric plays a key role in machine learning problems such as\nclassification, clustering or ranking. However, it is worth noting that there\nis a severe lack of theoretical guarantees that can be expected on the\ngeneralization capacity of the classifier associated to a given metric. The\ntheoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions\n(Balcan et al., 2008) has been one of the first attempts to draw a link between\nthe properties of a similarity function and those of a linear classifier making\nuse of it. In this paper, we extend and complete this theory by providing a new\ngeneralization bound for the associated classifier based on the algorithmic\nrobustness framework.",
        "Refine this for me please": "The notion of metric plays a key role in machine learning problems such as\nclassification, clustering or ranking. However, it is worth noting that there\nis a severe lack of theoretical guarantees that can be expected on the\ngeneralization capacity of the classifier associated to a given metric. The\ntheoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions\n(Balcan et al., 2008) has been one of the first attempts to draw a link between\nthe properties of a similarity function and those of a linear classifier making\nuse of it. In this paper, we extend and complete this theory by providing a new\ngeneralization bound for the associated classifier based on the algorithmic\nrobustness framework.",
        "Concise this for me and keep all the information": "The notion of metric is key to machine learning problems such as classification, clustering, and ranking. However, there are no theoretical guarantees that a given metric will produce good results. The $(\\epsilon, \\gamma, \\tau)$-good similarity functions theory (Balcan et al., 2008) is one attempt to link the properties of a similarity function to the performance of a linear classifier that uses it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classifier based on the algorithmic robustness framework.",
        "Improve this in GPT way": "The notion of metric plays a key role in machine learning problems such as\nclassification, clustering or ranking. However, it is worth noting that there\nis a severe lack of theoretical guarantees that can be expected on the\ngeneralization capacity of the classifier associated to a given metric. The\ntheoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions\n(Balcan et al., 2008) has been one of the first attempts to draw a link between\nthe properties of a similarity function and those of a linear classifier making\nuse of it. In this paper, we extend and complete this theory by providing a new\ngeneralization bound for the associated classifier based on the algorithmic\nrobustness framework. This new generalization bound is tighter than the one in\n(Balcan et al., 2008), and it also applies to a wider range of similarity\nfunctions."
    },
    {
        "input": "We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.",
        "Revise this with your best effort": "We present the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis. We establish a connection to the previously investigated matrix-space models for compositionality, and show they are special cases of the multiplicative recurrent net. Our experiments show that these models perform comparably or better than Elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees.",
        "Help me polish this": "We present the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis. We establish a connection to the previously investigated matrix-space models for compositionality, and show they are special cases of the multiplicative recurrent net. Our experiments show that these models perform comparably or better than Elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees.",
        "Rewrite this for me": "We introduce the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis. We establish a connection to previously investigated matrix-space models for compositionality, and show that they are special cases of the multiplicative recurrent net. Our experiments demonstrate that these models perform comparably or better than Elman-type additive recurrent neural networks, and outperform matrix-space models on a standard fine-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees.",
        "Make this fluent while doing minimal change": "We present the multiplicative recurrent neural network as a general and more effective model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.",
        "Refine this for me please": "We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.",
        "Concise this for me and keep all the information": "We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.",
        "Improve this in GPT way": "We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees."
    },
    {
        "input": "Finding minima of a real valued non-convex function over a high dimensional\nspace is a major challenge in science. We provide evidence that some such\nfunctions that are defined on high dimensional domains have a narrow band of\nvalues whose pre-image contains the bulk of its critical points. This is in\ncontrast with the low dimensional picture in which this band is wide. Our\nsimulations agree with the previous theoretical work on spin glasses that\nproves the existence of such a band when the dimension of the domain tends to\ninfinity. Furthermore our experiments on teacher-student networks with the\nMNIST dataset establish a similar phenomenon in deep networks. We finally\nobserve that both the gradient descent and the stochastic gradient descent\nmethods can reach this level within the same number of steps.",
        "Revise this with your best effort": "Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.",
        "Help me polish this": "Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. However, we provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore, our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. Finally, we observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.",
        "Rewrite this for me": "Finding the minima of a real valued non-convex function over a high dimensional space is a difficult task. However, some functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the majority of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore, our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.",
        "Make this fluent while doing minimal change": "Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. However, we provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore, our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.",
        "Refine this for me please": "Some real valued non-convex functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide.",
        "Concise this for me and keep all the information": "Some real valued non-convex functions defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide.",
        "Improve this in GPT way": "We provide evidence that some real valued non-convex functions defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide."
    },
    {
        "input": "We develop a new statistical model for photographic images, in which the\nlocal responses of a bank of linear filters are described as jointly Gaussian,\nwith zero mean and a covariance that varies slowly over spatial position. We\noptimize sets of filters so as to minimize the nuclear norms of matrices of\ntheir local activations (i.e., the sum of the singular values), thus\nencouraging a flexible form of sparsity that is not tied to any particular\ndictionary or coordinate system. Filters optimized according to this objective\nare oriented and bandpass, and their responses exhibit substantial local\ncorrelation. We show that images can be reconstructed nearly perfectly from\nestimates of the local filter response covariances alone, and with minimal\ndegradation (either visual or MSE) from low-rank approximations of these\ncovariances. As such, this representation holds much promise for use in\napplications such as denoising, compression, and texture representation, and\nmay form a useful substrate for hierarchical decompositions.",
        "Revise this with your best effort": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.",
        "Help me polish this": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.",
        "Rewrite this for me": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.",
        "Make this fluent while doing minimal change": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system.\n\nFilters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances.\n\nAs such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.",
        "Refine this for me please": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.",
        "Concise this for me and keep all the information": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system.\n\nFilters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances.\n\nAs such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.",
        "Improve this in GPT way": "We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions."
    },
    {
        "input": "Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches.",
        "Revise this with your best effort": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. However, we re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
        "Help me polish this": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. However, we re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
        "Rewrite this for me": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. However, we re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
        "Make this fluent while doing minimal change": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
        "Refine this for me please": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. However, recent research has questioned the necessity of different components in the pipeline, finding that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding, we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network, we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
        "Concise this for me and keep all the information": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. However, a new study questions the necessity of different components in the pipeline, finding that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding, a new architecture has been proposed that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet).",
        "Improve this in GPT way": "Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches."
    },
    {
        "input": "Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes.",
        "Revise this with your best effort": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. However, we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units. In fact, we achieve state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.",
        "Help me polish this": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. However, we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.",
        "Rewrite this for me": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. However, we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units. In fact, we achieve state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.",
        "Make this fluent while doing minimal change": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. However, we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.",
        "Refine this for me please": "The activation function of a neuron defines the output of that neuron given an input or set of inputs. A traditional, non-linear activation function is fixed, meaning that it will always produce the same output for a given input. However, we have designed a novel form of piecewise linear activation function that can be learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.",
        "Concise this for me and keep all the information": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. However, we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.",
        "Improve this in GPT way": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. However, we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes."
    },
    {
        "input": "This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper.",
        "Revise this with your best effort": "This paper introduces a new greedy parser that is based on neural networks and uses a new compositional sub-tree representation. The parser and the composition procedure are jointly trained and tightly depend on each other. The composition procedure outputs a vector representation that summarizes the syntactic (parsing tags) and semantic (words) sub-trees. The composition and tagging are achieved over continuous (word or tag) representations and recurrent neural networks. The parser reaches F1 performance that is on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser. The paper provides a fully functional implementation of the method described.",
        "Help me polish this": "This paper introduces a new greedy parser based on neural networks, which uses a new compositional sub-tree representation. The parser and the composition procedure are jointly trained, and tightly depend on each other. The composition procedure outputs a vector representation which summarizes syntactic (parsing tags) and semantic (words) sub-trees. Composition and tagging are achieved over continuous (word or tag) representations, and recurrent neural networks. We reach F1 performance on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser. We provide a fully functional implementation of the method described in this paper.",
        "Rewrite this for me": "This paper introduces a new greedy parser that is based on neural networks and uses a new compositional sub-tree representation. The parser and the composition procedure are jointly trained and depend on each other. The composition procedure outputs a vector representation that summarizes the syntactic (parsing tags) and semantic (words) sub-trees. The composition and tagging are achieved using continuous (word or tag) representations and recurrent neural networks. This parser achieves the same F1 performance as well-known existing parsers, but has the advantage of being faster, thanks to the greedy nature of the parser. The paper includes a fully functional implementation of the described method.",
        "Make this fluent while doing minimal change": "This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The parser and\nthe compositional procedure are jointly trained, and tightly depend on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper.",
        "Refine this for me please": "This paper introduces a new greedy parser that is based on neural networks and uses a new compositional sub-tree representation. The parser and the composition procedure are jointly trained and depend on each other. The composition procedure outputs a vector representation that summarizes the syntactic (parsing tags) and semantic (words) sub-trees. The composition and tagging are achieved over continuous (word or tag) representations and recurrent neural networks. The parser reaches F1 performance that is on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser. The paper provides a fully functional implementation of the method described.",
        "Concise this for me and keep all the information": "This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The parser and\nthe compositional procedure are jointly trained, and tightly depend on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. F1 performance is on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser.",
        "Improve this in GPT way": "This paper introduces a new greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper."
    },
    {
        "input": "Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings.",
        "Revise this with your best effort": "Lateral connections between an encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers, but lateral connections from the encoder to the decoder relieve this pressure. It is shown that abstract, invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verified that adding modulated lateral connections to the model 1) improved the accuracy of the probability model for inputs, as measured by denoising performance; 2) resulted in representations whose degree of invariance grew faster towards the higher layers; and 3) supported the formation of diverse invariant poolings.",
        "Help me polish this": "Adding lateral connections between the encoder and decoder of a denoising autoencoder (dAE) allows higher layers to focus on invariant representations. This relieves the pressure of detailed information needing to be carried through the highest layers. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verified that adding modulated lateral connections to the model 1) improved the accuracy of the probability model for inputs, as measured by denoising performance; 2) resulted in representations whose degree of invariance grew faster towards the higher layers; and 3) supported the formation of diverse invariant poolings.",
        "Rewrite this for me": "The purpose of lateral connections in a denoising autoencoder is to allow the higher layers to focus on invariant representations. This is done by relieving the pressure on the detailed information carried through the highest layers. It is shown that these abstract invariant features can be translated into detailed reconstructions when the invariant features are allowed to modulate the strength of the lateral connection. Three different structures for the denoising autoencoder were compared in experiments using real-world images. The results showed that adding modulated lateral connections to the model improved the accuracy of the probability model for inputs, as measured by denoising performance. It also resulted in representations whose degree of invariance grew faster towards the higher layers. This supported the formation of diverse invariant poolings.",
        "Make this fluent while doing minimal change": "Adding lateral connections between the encoder and decoder allows higher layers of the denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers, but lateral connections from the encoder to the decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verified that adding modulated lateral connections to the model 1) improved the accuracy of the probability model for inputs, as measured by denoising performance; 2) resulted in representations whose degree of invariance grew faster towards the higher layers; and 3) supported the formation of diverse invariant poolings.",
        "Refine this for me please": "The purpose of lateral connections in a denoising autoencoder is to allow higher layers to focus on invariant representations. This means that detailed information does not need to be carried through the highest layers, which relieves pressure on the system. It is shown that these connections can be used to translate abstract invariant features into detailed reconstructions. Three different types of lateral connections were compared in experiments using real-world images, and it was found that modulated lateral connections improve the accuracy of the probability model for inputs, as measured by denoising performance.",
        "Concise this for me and keep all the information": "Adding lateral connections between the encoder and decoder of a denoising autoencoder allows the higher layers to focus on invariant representations. This results in more accurate reconstructions and representations that are more invariant towards changes.",
        "Improve this in GPT way": "Adding lateral connections between the encoder and decoder of a denoising autoencoder (dAE) can improve the model's ability to focus on invariant representations. This is because detailed information does not need to be carried through the highest layers when there are lateral connections. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verified that adding modulated lateral connections to the model 1) improved the accuracy of the probability model for inputs, as measured by denoising performance; 2) resulted in representations whose degree of invariance grew faster towards the higher layers; and 3) supported the formation of diverse invariant poolings."
    },
    {
        "input": "We develop a new method for visualizing and refining the invariances of\nlearned representations. Specifically, we test for a general form of\ninvariance, linearization, in which the action of a transformation is confined\nto a low-dimensional subspace. Given two reference images (typically, differing\nby some transformation), we synthesize a sequence of images lying on a path\nbetween them that is of minimal length in the space of the representation (a\n\"representational geodesic\"). If the transformation relating the two reference\nimages is linearized by the representation, this sequence should follow the\ngradual evolution of this transformation. We use this method to assess the\ninvariance properties of a state-of-the-art image classification network and\nfind that geodesics generated for image pairs differing by translation,\nrotation, and dilation do not evolve according to their associated\ntransformations. Our method also suggests a remedy for these failures, and\nfollowing this prescription, we show that the modified representation is able\nto linearize a variety of geometric image transformations.",
        "Revise this with your best effort": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Help me polish this": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Rewrite this for me": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Make this fluent while doing minimal change": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Refine this for me please": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Concise this for me and keep all the information": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.",
        "Improve this in GPT way": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \"representational geodesic\"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation.\n\nWe use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations."
    },
    {
        "input": "Genomics are rapidly transforming medical practice and basic biomedical\nresearch, providing insights into disease mechanisms and improving therapeutic\nstrategies, particularly in cancer. The ability to predict the future course of\na patient's disease from high-dimensional genomic profiling will be essential\nin realizing the promise of genomic medicine, but presents significant\nchallenges for state-of-the-art survival analysis methods. In this abstract we\npresent an investigation in learning genomic representations with neural\nnetworks to predict patient survival in cancer. We demonstrate the advantages\nof this approach over existing survival analysis methods using brain tumor\ndata.",
        "Revise this with your best effort": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract we present an investigation into learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data.",
        "Help me polish this": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract, we present an investigation into learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data.",
        "Rewrite this for me": "Genomics is rapidly changing medical practices and basic biomedical research by providing insights into disease mechanisms and improving therapeutic strategies, especially in regards to cancer. The ability to predict a patient's future disease course from high-dimensional genomic profiling will be essential to taking advantage of genomic medicine, but presents significant challenges for current survival analysis methods. In this abstract, we present an investigation into using neural networks to learn genomic representations in order to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data.",
        "Make this fluent while doing minimal change": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract, we present an investigation in learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data.",
        "Refine this for me please": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract we present an investigation into using neural networks to learn genomic representations in order to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data.",
        "Concise this for me and keep all the information": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract, we present an investigation in learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data.",
        "Improve this in GPT way": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract we present an investigation in learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data."
    },
    {
        "input": "Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. However, this leads to an\nextensive increase in the computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure.",
        "Revise this with your best effort": "Existing approaches to combining both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiably adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.",
        "Help me polish this": "Existing approaches to combining both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiably adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.",
        "Rewrite this for me": "There are existing approaches that try to combine both additive and multiplicative neural units, but these either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. This can lead to an increase in the computational complexity of the training procedure.\n\nWe present a novel, parameterizable transfer function that is based on the mathematical concept of non-integer functional iteration. This allows the operation each neuron performs to be smoothly and, most importantly, differentiably adjusted between addition and multiplication. This means that the decision between addition and multiplication can be integrated into the standard backpropagation training procedure.",
        "Make this fluent while doing minimal change": "We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiably adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure, without the need for a fixed assignment of operations or discrete optimization.",
        "Refine this for me please": "We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiably adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure, without the need for a fixed assignment of operations or discrete optimization.",
        "Concise this for me and keep all the information": "We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.",
        "Improve this in GPT way": "We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiably adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure, which leads to a significant increase in the computational efficiency of the training procedure."
    },
    {
        "input": "One of the difficulties of training deep neural networks is caused by\nimproper scaling between layers. Scaling issues introduce exploding / gradient\nproblems, and have typically been addressed by careful scale-preserving\ninitialization. We investigate the value of preserving scale, or isometry,\nbeyond the initial weights. We propose two methods of maintaing isometry, one\nexact and one stochastic. Preliminary experiments show that for both\ndeterminant and scale-normalization effectively speeds up learning. Results\nsuggest that isometry is important in the beginning of learning, and\nmaintaining it leads to faster learning.",
        "Revise this with your best effort": "One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues can introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaining isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning.",
        "Help me polish this": "One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaining isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning.",
        "Rewrite this for me": "One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaining isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning.",
        "Make this fluent while doing minimal change": "One of the difficulties of training deep neural networks is caused by\nscaling issues between layers. Scaling issues can introduce exploding / gradient\nproblems, and are typically addressed by careful scale-preserving\ninitialization. We investigate the value of preserving scale, or isometry,\nbeyond the initial weights. We propose two methods of maintaing isometry, one\nexact and one stochastic. Preliminary experiments show that for both\ndeterminant and scale-normalization effectively speeds up learning. Results\nsuggest that isometry is important in the beginning of learning, and\nmaintaining it leads to faster learning.",
        "Refine this for me please": "One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues can introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization.\n\nWe investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaing isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning.\n\nResults suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning.",
        "Concise this for me and keep all the information": "One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaing isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning.",
        "Improve this in GPT way": "One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaing isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning."
    },
    {
        "input": "We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's.",
        "Revise this with your best effort": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Help me polish this": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Rewrite this for me": "We develop a Stochastic Gradient Variational Bayes method to perform posterior inference for the weights of Stick-Breaking processes. This allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), which is a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Make this fluent while doing minimal change": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Refine this for me please": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Concise this for me and keep all the information": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), which has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE learns highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Improve this in GPT way": "We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's."
    },
    {
        "input": "Unsupervised learning on imbalanced data is challenging because, when given\nimbalanced data, current model is often dominated by the major category and\nignores the categories with small amount of data. We develop a latent variable\nmodel that can cope with imbalanced data by dividing the latent space into a\nshared space and a private space. Based on Gaussian Process Latent Variable\nModels, we propose a new kernel formulation that enables the separation of\nlatent space and derives an efficient variational inference method. The\nperformance of our model is demonstrated with an imbalanced medical image\ndataset.",
        "Revise this with your best effort": "Unsupervised learning on imbalanced data can be challenging because, when given imbalanced data, current models are often dominated by the major category and ignore the categories with small amounts of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset.",
        "Help me polish this": "Unsupervised learning on imbalanced data can be challenging because, when given imbalanced data, current models are often dominated by the major category and ignore the categories with small amounts of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset.",
        "Rewrite this for me": "Imbalanced data presents a challenge for unsupervised learning because the current model is often dominated by the major category, ignoring categories with small amounts of data. We develop a latent variable model that can deal with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset.",
        "Make this fluent while doing minimal change": "Unsupervised learning on imbalanced data can be challenging because, when given imbalanced data, current models are often dominated by the major category and ignore the categories with small amount of data. To address this issue, we develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset.",
        "Refine this for me please": "Unsupervised learning on imbalanced data can be difficult because models often focus on the majority category to the detriment of categories with less data. We propose a latent variable model that can address this issue by dividing the latent space into a shared space and a private space. This separation enables the model to more effectively learn from imbalanced data, as demonstrated with a medical image dataset.",
        "Concise this for me and keep all the information": "Unsupervised learning on imbalanced data is difficult because the model is often dominated by the major category and ignores the categories with small amount of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset.",
        "Improve this in GPT way": "Unsupervised learning on imbalanced data is challenging because, when given imbalanced data, current models are often dominated by the major category and ignore the categories with small amount of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset."
    },
    {
        "input": "Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.",
        "Revise this with your best effort": "Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.",
        "Help me polish this": "We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Rewrite this for me": "Generative adversarial networks (GANs) are successful deep generative models that are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Make this fluent while doing minimal change": "Generative adversarial networks (GANs) are successful deep generative models that are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Refine this for me please": "Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.",
        "Concise this for me and keep all the information": "GANs are successful deep generative models that are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Improve this in GPT way": "We propose a novel algorithm that combines the strengths of generative adversarial networks (GANs) and density ratio estimation. Our algorithm is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. This offers a new perspective toward the understanding of GANs and provides a way to improve the training of GANs."
    },
    {
        "input": "This paper shows how one can directly apply natural language processing (NLP)\nmethods to classification problems in cheminformatics. Connection between these\nseemingly separate fields is shown by considering standard textual\nrepresentation of compound, SMILES. The problem of activity prediction against\na target protein is considered, which is a crucial part of computer aided drug\ndesign process. Conducted experiments show that this way one can not only\noutrank state of the art results of hand crafted representations but also gets\ndirect structural insights into the way decisions are made.",
        "Revise this with your best effort": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only outperforms state-of-the-art results of hand-crafted representations, but also provides direct structural insights into the way decisions are made.",
        "Help me polish this": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only outperforms state-of-the-art results of hand-crafted representations, but also provides direct structural insights into the way decisions are made.",
        "Rewrite this for me": "This paper demonstrates how natural language processing (NLP) methods can be used to solve classification problems in cheminformatics. The connection between these two seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this method not only outperforms state-of-the-art results of hand-crafted representations, but also provides direct structural insights into the decision-making process.",
        "Make this fluent while doing minimal change": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only state of the art results of hand-crafted representations can be outranked, but also direct structural insights into the way decisions are made can be gained.",
        "Refine this for me please": "This paper demonstrates how natural language processing (NLP) methods can be used to solve classification problems in cheminformatics. The connection between these two fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. The conducted experiments show that this approach not only outperforms the state-of-the-art results of hand-crafted representations but also provides direct structural insights into the way decisions are made.",
        "Concise this for me and keep all the information": "This paper shows how natural language processing (NLP) methods can be applied to classification problems in cheminformatics. The connection between these fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only state of the art results of hand-crafted representations can be outranked, but also direct structural insights into the way decisions are made can be obtained.",
        "Improve this in GPT way": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only outperforms state-of-the-art results of hand-crafted representations, but also provides direct structural insights into the way decisions are made."
    },
    {
        "input": "We introduce a neural network architecture and a learning algorithm to\nproduce factorized symbolic representations. We propose to learn these concepts\nby observing consecutive frames, letting all the components of the hidden\nrepresentation except a small discrete set (gating units) be predicted from the\nprevious frame, and let the factors of variation in the next frame be\nrepresented entirely by these discrete gated units (corresponding to symbolic\nrepresentations). We demonstrate the efficacy of our approach on datasets of\nfaces undergoing 3D transformations and Atari 2600 games.",
        "Revise this with your best effort": "We introduce a neural network architecture and learning algorithm that produces factorized symbolic representations. Our approach learns these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame. The factors of variation in the next frame are represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.",
        "Help me polish this": "We introduce a neural network architecture and learning algorithm that produces factorized symbolic representations. Our approach learns these concepts by observing consecutive frames, letting all the components of the hidden representation except for a small discrete set (gating units) be predicted from the previous frame. The factors of variation in the next frame are then represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.",
        "Rewrite this for me": "We introduce a neural network architecture and learning algorithm that produces factorized symbolic representations. Our approach learns these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame. The factors of variation in the next frame are represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.",
        "Make this fluent while doing minimal change": "We introduce a neural network architecture and learning algorithm that produces factorized symbolic representations. We propose learning these concepts by observing consecutive frames, letting all but a small discrete set of the hidden representation (gating units) be predicted from the previous frame, and letting the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.",
        "Refine this for me please": "We introduce a neural network architecture and learning algorithm that produces factorized symbolic representations. We propose learning these concepts by observing consecutive frames, letting all but a small discrete set of the hidden representation (gating units) be predicted from the previous frame, and letting the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.",
        "Concise this for me and keep all the information": "We introduce a neural network architecture and learning algorithm that produces factorized symbolic representations. We propose that these concepts are learned by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame, and letting the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.",
        "Improve this in GPT way": "We introduce a GPT-based neural network architecture and learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame, and letting the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games."
    },
    {
        "input": "We look at the eigenvalues of the Hessian of a loss function before and after\ntraining. The eigenvalue distribution is seen to be composed of two parts, the\nbulk which is concentrated around zero, and the edges which are scattered away\nfrom zero. We present empirical evidence for the bulk indicating how\nover-parametrized the system is, and for the edges that depend on the input\ndata.",
        "Revise this with your best effort": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Help me polish this": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Rewrite this for me": "We examine the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution appears to be made up of two parts: the bulk, which is focused around zero, and the edges, which are scattered away from zero. Our empirical evidence for the bulk demonstrates how over-parametrized the system is, and for the edges that show how the input data affects them.",
        "Make this fluent while doing minimal change": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Refine this for me please": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Concise this for me and keep all the information": "The Hessian is a matrix of second-order partial derivatives of a function. We can use it to measure how a loss function changes before and after training. The eigenvalue distribution of the Hessian can be split into two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. The bulk indicates how over-parametrized the system is, and the edges depend on the input data.",
        "Improve this in GPT way": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence that:\n\n- The bulk indicates how over-parametrized the system is.\n- The edges depend on the input data."
    },
    {
        "input": "We introduce a parametric nonlinear transformation that is well-suited for\nGaussianizing data from natural images. The data are linearly transformed, and\neach component is then normalized by a pooled activity measure, computed by\nexponentiating a weighted sum of rectified and exponentiated components and a\nconstant. We optimize the parameters of the full transformation (linear\ntransform, exponents, weights, constant) over a database of natural images,\ndirectly minimizing the negentropy of the responses. The optimized\ntransformation substantially Gaussianizes the data, achieving a significantly\nsmaller mutual information between transformed components than alternative\nmethods including ICA and radial Gaussianization. The transformation is\ndifferentiable and can be efficiently inverted, and thus induces a density\nmodel on images. We show that samples of this model are visually similar to\nsamples of natural image patches. We demonstrate the use of the model as a\nprior probability density that can be used to remove additive noise. Finally,\nwe show that the transformation can be cascaded, with each layer optimized\nusing the same Gaussianization objective, thus offering an unsupervised method\nof optimizing a deep network architecture.",
        "Revise this with your best effort": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Help me polish this": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Rewrite this for me": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Make this fluent while doing minimal change": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Refine this for me please": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
        "Concise this for me and keep all the information": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images.",
        "Improve this in GPT way": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture."
    },
    {
        "input": "Approximate variational inference has shown to be a powerful tool for\nmodeling unknown complex probability distributions. Recent advances in the\nfield allow us to learn probabilistic models of sequences that actively exploit\nspatial and temporal structure. We apply a Stochastic Recurrent Network (STORN)\nto learn robot time series data. Our evaluation demonstrates that we can\nrobustly detect anomalies both off- and on-line.",
        "Revise this with your best effort": "Approximate variational inference is a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line.",
        "Help me polish this": "Approximate variational inference is a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line.",
        "Rewrite this for me": "Approximate variational inference is a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line.",
        "Make this fluent while doing minimal change": "Approximate variational inference is a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line.",
        "Refine this for me please": "Approximate variational inference is a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line.",
        "Concise this for me and keep all the information": "Approximate variational inference is a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line.",
        "Improve this in GPT way": "Approximate variational inference is a powerful tool for learning complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line."
    },
    {
        "input": "We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired.",
        "Revise this with your best effort": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Help me polish this": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Rewrite this for me": "We develop a problem setting to train and test agents' ability to gather information efficiently. We present a collection of tasks where success requires searching through a partially-observed environment for fragments of information. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Make this fluent while doing minimal change": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Refine this for me please": "We develop a problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Concise this for me and keep all the information": "We develop a problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Improve this in GPT way": "We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired."
    },
    {
        "input": "We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.",
        "Revise this with your best effort": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach outperforms recent memory augmented networks.",
        "Help me polish this": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach outperforms recent memory augmented networks.",
        "Rewrite this for me": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
        "Make this fluent while doing minimal change": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
        "Refine this for me please": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
        "Concise this for me and keep all the information": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate that our approach performs significantly better than recent memory augmented networks on several language model datasets.",
        "Improve this in GPT way": "We propose an extension to neural network language models that uses a cache to store and access past hidden activations, which is more efficient and scalable than other memory augmented networks."
    },
    {
        "input": "Motivated by the recent progress in generative models, we introduce a model\nthat generates images from natural language descriptions. The proposed model\niteratively draws patches on a canvas, while attending to the relevant words in\nthe description. After training on Microsoft COCO, we compare our model with\nseveral baseline generative models on image generation and retrieval tasks. We\ndemonstrate that our model produces higher quality samples than other\napproaches and generates images with novel scene compositions corresponding to\npreviously unseen captions in the dataset.",
        "Revise this with your best effort": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Help me polish this": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Rewrite this for me": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. Our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Make this fluent while doing minimal change": "We introduce a model that generates images from natural language descriptions, motivated by recent progress in generative models. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Refine this for me please": "We introduce a model that generates images from natural language descriptions. The proposed model\niteratively draws patches on a canvas, while attending to the relevant words in\nthe description. We compare our model with\nseveral baseline generative models on image generation and retrieval tasks. We\ndemonstrate that our model produces higher quality samples than other\napproaches and generates images with novel scene compositions corresponding to\npreviously unseen captions in the dataset.",
        "Concise this for me and keep all the information": "We introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
        "Improve this in GPT way": "We introduce a new model for generating images from natural language descriptions. This model iteratively draws patches on a canvas, while attending to the relevant words in the description. We compare our model with several baseline generative models on image generation and retrieval tasks, and demonstrate that our model produces higher quality samples than other approaches."
    },
    {
        "input": "We propose a framework for training multiple neural networks simultaneously.\nThe parameters from all models are regularised by the tensor trace norm, so\nthat each neural network is encouraged to reuse others' parameters if possible\n-- this is the main motivation behind multi-task learning. In contrast to many\ndeep multi-task learning models, we do not predefine a parameter sharing\nstrategy by specifying which layers have tied parameters. Instead, our\nframework considers sharing for all shareable layers, and the sharing strategy\nis learned in a data-driven way.",
        "Revise this with your best effort": "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularized by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Help me polish this": "We propose a framework for training multiple neural networks simultaneously, with the aim of encouraging parameter sharing and thus reducing the overall number of parameters required. We regularise the parameters of all models using the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Rewrite this for me": "We propose a framework for training multiple neural networks simultaneously, with the aim of encouraging parameter sharing and reducing overfitting. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Make this fluent while doing minimal change": "We propose a framework for training multiple neural networks simultaneously, with the parameters from all models regularised by the tensor trace norm. This encourages each neural network to reuse others' parameters if possible -- which is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Refine this for me please": "We propose a framework for training multiple neural networks simultaneously, with the parameters from all models being regularised by the tensor trace norm. This encourages each neural network to reuse others' parameters if possible, which is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Concise this for me and keep all the information": "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible. This is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Improve this in GPT way": "We propose a GPT-based framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way."
    },
    {
        "input": "This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method.",
        "Revise this with your best effort": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. The paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Help me polish this": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Rewrite this for me": "This paper presents a deep reinforcement learning agent that uses experience replay to improve stability and sample efficiency. The agent performs well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Make this fluent while doing minimal change": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Refine this for me please": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. The paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Concise this for me and keep all the information": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs well on challenging environments. The paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Improve this in GPT way": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method."
    },
    {
        "input": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Revise this with your best effort": "We introduce a new system for creating pop music. Our model is a hierarchical recurrent neural network, which uses layers and a hierarchical structure to encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conducted several human studies that showed a strong preference for our generated music over that produced by the recent Google method. We also demonstrated two applications of our framework: neural dancing and karaoke, as well as neural story singing.",
        "Help me polish this": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Rewrite this for me": "We introduce a new system for creating pop music. Our model is a type of artificial neural network called a hierarchical recurrent neural network. This network is designed to reflect our existing knowledge about how pop music is composed. For example, the bottom layers of the network generate the melody, while the higher levels produce the drums and chords. We have conducted several studies with human participants, who overwhelmingly preferred the music generated by our system to that produced by a similar system recently developed by Google. In addition, we demonstrate two applications of our framework: neural dancing and karaoke, as well as neural story singing.",
        "Make this fluent while doing minimal change": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show a strong\npreference for our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Refine this for me please": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show our generated music is preferred over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Concise this for me and keep all the information": "We present a novel framework for generating pop music using a hierarchical Recurrent Neural Network. This model encodes prior knowledge about how pop music is composed, with the bottom layers generating the melody and higher levels producing the drums and chords. Our human studies show strong preference for our generated music over that produced by the recent Google method. We also show two applications of our framework: neural dancing and karaoke, as well as neural story singing.",
        "Improve this in GPT way": "We present a novel GPT-based framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing."
    },
    {
        "input": "Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix.",
        "Revise this with your best effort": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Help me polish this": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Rewrite this for me": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We have deployed three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Make this fluent while doing minimal change": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Refine this for me please": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Concise this for me and keep all the information": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We have deployed three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Improve this in GPT way": "Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix."
    },
    {
        "input": "We propose a new method for creating computationally efficient convolutional\nneural networks (CNNs) by using low-rank representations of convolutional\nfilters. Rather than approximating filters in previously-trained networks with\nmore efficient versions, we learn a set of small basis filters from scratch;\nduring training, the network learns to combine these basis filters into more\ncomplex filters that are discriminative for image classification. To train such\nnetworks, a novel weight initialization scheme is used. This allows effective\ninitialization of connection weights in convolutional layers composed of groups\nof differently-shaped filters. We validate our approach by applying it to\nseveral existing CNN architectures and training these networks from scratch\nusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or\nhigher accuracy than conventional CNNs with much less compute. Applying our\nmethod to an improved version of VGG-11 network using global max-pooling, we\nachieve comparable validation accuracy using 41% less compute and only 24% of\nthe original VGG-11 model parameters; another variant of our method gives a 1\npercentage point increase in accuracy over our improved VGG-11 model, giving a\ntop-5 center-crop validation accuracy of 89.7% while reducing computation by\n16% relative to the original VGG-11 model. Applying our method to the GoogLeNet\narchitecture for ILSVRC, we achieved comparable accuracy with 26% less compute\nand 41% fewer model parameters. Applying our method to a near state-of-the-art\nnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and\n55% fewer parameters.",
        "Revise this with your best effort": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method",
        "Help me polish this": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our",
        "Rewrite this for me": "We propose a new, more efficient method for creating convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our",
        "Make this fluent while doing minimal change": "We propose a new, more efficient method for creating convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our",
        "Refine this for me please": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method",
        "Concise this for me and keep all the information": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute.",
        "Improve this in GPT way": "We propose a new method for creating more efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method"
    },
    {
        "input": "Layer-sequential unit-variance (LSUV) initialization - a simple method for\nweight initialization for deep net learning - is proposed. The method consists\nof the two steps. First, pre-initialize weights of each convolution or\ninner-product layer with orthonormal matrices. Second, proceed from the first\nto the final layer, normalizing the variance of the output of each layer to be\nequal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh)\nshow that the proposed initialization leads to learning of very deep nets that\n(i) produces networks with test accuracy better or equal to standard methods\nand (ii) is at least as fast as the complex schemes proposed specifically for\nvery deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava\net al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets\nand the state-of-the-art, or very close to it, is achieved on the MNIST,\nCIFAR-10/100 and ImageNet datasets.",
        "Revise this with your best effort": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Help me polish this": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Rewrite this for me": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Make this fluent while doing minimal change": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning that consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperimenting with different activation functions (maxout, ReLU-family, tanh) shows that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Refine this for me please": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Concise this for me and keep all the information": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
        "Improve this in GPT way": "Layer-sequential unit-variance (LSUV) initialization is a simple method for weight initialization for deep net learning. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n\nExperiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\n\nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets."
    },
    {
        "input": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using\nneural attention in a simple graph-based dependency parser. We use a larger but\nmore thoroughly regularized parser than other recent BiLSTM-based approaches,\nwith biaffine classifiers to predict arcs and labels. Our parser gets state of\nthe art or near state of the art performance on standard treebanks for six\ndifferent languages, achieving 95.7% UAS and 94.1% LAS on the most popular\nEnglish PTB dataset. This makes it the highest-performing graph-based parser on\nthis benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and\n2.2%---and comparable to the highest performing transition-based parser\n(Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show\nwhich hyperparameter choices had a significant effect on parsing accuracy,\nallowing us to achieve large gains over other graph-based approaches.",
        "Revise this with your best effort": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
        "Help me polish this": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
        "Rewrite this for me": "This paper builds off of recent work from Kiperwasser & Goldberg (2016) that uses neural attention in a simple graph-based dependency parser. Our parser is larger but more thoroughly regularized than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
        "Make this fluent while doing minimal change": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using\nneural attention in a simple graph-based dependency parser. We use a larger but\nmore thoroughly regularized parser than other recent BiLSTM-based approaches,\nwith biaffine classifiers to predict arcs and labels. Our parser gets state of\nthe art or near state of the art performance on standard treebanks for six\ndifferent languages, achieving 95.7% UAS and 94.1% LAS on the most popular\nEnglish PTB dataset. This makes it the highest-performing graph-based parser on\nthis benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and\n2.2%---and comparable to the highest performing transition-based parser\n(Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show\nwhich hyperparameter choices had a significant effect on parsing accuracy,\nallowing us to achieve large gains over other graph-based approaches.",
        "Refine this for me please": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
        "Concise this for me and keep all the information": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS.",
        "Improve this in GPT way": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using\nneural attention in a simple graph-based dependency parser. We use a larger but\nmore thoroughly regularized parser than other recent BiLSTM-based approaches,\nwith biaffine classifiers to predict arcs and labels. Our parser gets state of\nthe art or near state of the art performance on standard treebanks for six\ndifferent languages, achieving 95.7% UAS and 94.1% LAS on the most popular\nEnglish PTB dataset. This makes it the highest-performing graph-based parser on\nthis benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and\n2.2%---and comparable to the highest performing transition-based parser\n(Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show\nwhich hyperparameter choices had a significant effect on parsing accuracy,\nallowing us to achieve large gains over other graph-based approaches."
    },
    {
        "input": "Accurate representational learning of both the explicit and implicit\nrelationships within data is critical to the ability of machines to perform\nmore complex and abstract reasoning tasks. We describe the efficient weakly\nsupervised learning of such inferences by our Dynamic Adaptive Network\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\nquestion answering tasks in the bAbI dataset that have proved difficult for\ncontemporary approaches to learning representation (Weston et al., 2015).",
        "Revise this with your best effort": "It is critical for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Help me polish this": "Accurate learning of both explicit and implicit relationships within data is critical for machines to be able to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have been difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Rewrite this for me": "It is critical for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Make this fluent while doing minimal change": "It is critical for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Refine this for me please": "The ability of machines to accurately learn both the explicit and implicit relationships within data is critical to their ability to perform more complex and abstract reasoning tasks. Our Dynamic Adaptive Network Intelligence (DANI) model efficiently learns such inferences in a weakly supervised manner. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).",
        "Concise this for me and keep all the information": "It is important for machines to be able to accurately learn both the explicit and implicit relationships within data in order to perform more complex reasoning tasks. The Dynamic Adaptive Network Intelligence (DANI) model is a way to efficiently learn these inferences through weak supervision. DANI has been shown to outperform other contemporary approaches to learning representation on question answering tasks in the bAbI dataset.",
        "Improve this in GPT way": "We describe the efficient weakly supervised learning of accurate representations of both the explicit and implicit relationships within data by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015)."
    },
    {
        "input": "Spherical data is found in many applications. By modeling the discretized\nsphere as a graph, we can accommodate non-uniformly distributed, partial, and\nchanging samplings. Moreover, graph convolutions are computationally more\nefficient than spherical convolutions. As equivariance is desired to exploit\nrotational symmetries, we discuss how to approach rotation equivariance using\nthe graph neural network introduced in Defferrard et al. (2016). Experiments\nshow good performance on rotation-invariant learning problems. Code and\nexamples are available at https://github.com/SwissDataScienceCenter/DeepSphere",
        "Revise this with your best effort": "Spherical data is found in many applications. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Help me polish this": "Spherical data is found in many applications, and modeling the discretized sphere as a graph can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Rewrite this for me": "There are many applications that use spherical data. By representing the discretized sphere as a graph, we can deal with non-uniformly distributed, partial, and changing samplings. In addition, graph convolutions are more efficient than spherical convolutions. Since equivariance is desired to take advantage of rotational symmetries, we explain how to achieve rotation equivariance using the graph neural network proposed in Defferrard et al. (2016). Our experiments demonstrate good performance on rotation-invariant learning tasks. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Make this fluent while doing minimal change": "Many applications use spherical data. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Graph convolutions are more efficient than spherical convolutions. To exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Refine this for me please": "Spherical data is found in many applications, and by modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Concise this for me and keep all the information": "Spherical data is found in many applications. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems.",
        "Improve this in GPT way": "We propose a new approach to spherical data that models the discretized sphere as a graph. This approach is more efficient than traditional methods, and can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, our approach is equivariant to rotations, which is desired in many applications. We provide code and examples at https://github.com/SwissDataScienceCenter/DeepSphere."
    },
    {
        "input": "High computational complexity hinders the widespread usage of Convolutional\nNeural Networks (CNNs), especially in mobile devices. Hardware accelerators are\narguably the most promising approach for reducing both execution time and power\nconsumption. One of the most important steps in accelerator development is\nhardware-oriented model approximation. In this paper we present Ristretto, a\nmodel approximation framework that analyzes a given CNN with respect to\nnumerical resolution used in representing weights and outputs of convolutional\nand fully connected layers. Ristretto can condense models by using fixed point\narithmetic and representation instead of floating point. Moreover, Ristretto\nfine-tunes the resulting fixed point network. Given a maximum error tolerance\nof 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit.\nThe code for Ristretto is available.",
        "Revise this with your best effort": "High computational complexity is a major obstacle to the widespread usage of Convolutional Neural Networks (CNNs), especially on mobile devices. Hardware accelerators are seen as the most promising solution for reducing both execution time and power consumption. One of the key steps in accelerator development is hardware-oriented model approximation. In this paper, we present Ristretto, a model approximation framework that analyzes a given CNN with respect to the numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.",
        "Help me polish this": "The widespread usage of Convolutional Neural Networks (CNNs) is hindered by high computational complexity, especially on mobile devices. Hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. \n\nIn this paper, we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit.\n\nThe code for Ristretto is available.",
        "Rewrite this for me": "The high computational complexity of Convolutional Neural Networks (CNNs) makes them difficult to use on mobile devices. Hardware accelerators are a promising way to reduce both execution time and power consumption. One important step in accelerator development is hardware-oriented model approximation. In this paper, we present Ristretto, a model approximation framework that analyzes a given CNN with respect to the numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.",
        "Make this fluent while doing minimal change": "High computational complexity has hindered the widespread usage of Convolutional Neural Networks (CNNs), especially in mobile devices. Hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. In this paper we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.",
        "Refine this for me please": "The widespread usage of Convolutional Neural Networks (CNNs) is hindered by high computational complexity, especially on mobile devices. Hardware accelerators are promising for reducing both execution time and power consumption. One important step in accelerator development is hardware-oriented model approximation. In this paper, we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.",
        "Concise this for me and keep all the information": "The widespread usage of Convolutional Neural Networks (CNNs) is hindered by high computational complexity, especially on mobile devices. Hardware accelerators are the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. \n\nRistretto is a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit.",
        "Improve this in GPT way": "The widespread usage of Convolutional Neural Networks (CNNs) is hindered by high computational complexity, especially on mobile devices. Hardware accelerators are promising for reducing both execution time and power consumption. An important step in accelerator development is hardware-oriented model approximation.\n\nIn this paper, we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit.\n\nThe code for Ristretto is available."
    },
    {
        "input": "The diversity of painting styles represents a rich visual vocabulary for the\nconstruction of an image. The degree to which one may learn and parsimoniously\ncapture this visual vocabulary measures our understanding of the higher level\nfeatures of paintings, if not images in general. In this work we investigate\nthe construction of a single, scalable deep network that can parsimoniously\ncapture the artistic style of a diversity of paintings. We demonstrate that\nsuch a network generalizes across a diversity of artistic styles by reducing a\npainting to a point in an embedding space. Importantly, this model permits a\nuser to explore new painting styles by arbitrarily combining the styles learned\nfrom individual paintings. We hope that this work provides a useful step\ntowards building rich models of paintings and offers a window on to the\nstructure of the learned representation of artistic style.",
        "Revise this with your best effort": "The diversity of painting styles is a rich source of visual information for creating images. The degree to which one can learn and accurately capture this visual vocabulary indicates our understanding of the higher-level features of paintings, and possibly images in general. In this work, we investigate the construction of a single, scalable deep network that can accurately capture the artistic style of a diversity of paintings. We demonstrate that such a network can be generalized across a diversity of styles by reducing a painting to a point in an embedding space. Importantly, this model allows users to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window into the structure of the learned representation of artistic style.",
        "Help me polish this": "The diversity of painting styles is a rich source of visual information that can be used to construct an image. The degree to which one can learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work, we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.",
        "Rewrite this for me": "The diversity of painting styles is a rich source of visual information that can be used to create new images. By learning and capturing the visual vocabulary of these styles, we can better understand the higher-level features of paintings, and potentially of images in general. In this work, we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.",
        "Make this fluent while doing minimal change": "The diversity of painting styles is a rich source of visual information for constructing an image. The degree to which one can learn and capture this visual vocabulary parsimoniously measures our understanding of the higher level features of paintings, if not images in general. In this work, we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.",
        "Refine this for me please": "The diversity of painting styles can be captured by a deep neural network, which can then be used to explore new painting styles by combining the styles learned from individual paintings.",
        "Concise this for me and keep all the information": "The diversity of painting styles can be captured by a deep network, which can be used to explore new painting styles.",
        "Improve this in GPT way": "We investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings."
    },
    {
        "input": "Sum-Product Networks (SPNs) are a class of expressive yet tractable\nhierarchical graphical models. LearnSPN is a structure learning algorithm for\nSPNs that uses hierarchical co-clustering to simultaneously identifying similar\nentities and similar features. The original LearnSPN algorithm assumes that all\nthe variables are discrete and there is no missing data. We introduce a\npractical, simplified version of LearnSPN, MiniSPN, that runs faster and can\nhandle missing data and heterogeneous features common in real applications. We\ndemonstrate the performance of MiniSPN on standard benchmark datasets and on\ntwo datasets from Google's Knowledge Graph exhibiting high missingness rates\nand a mix of discrete and continuous features.",
        "Revise this with your best effort": "SPNs are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identify similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. However, we introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features.",
        "Help me polish this": "Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identify similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features.",
        "Rewrite this for me": "SPNs are a class of graphical models that are expressive yet tractable. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identify similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. However, we introduce a practical, simplified version of LearnSPN, called MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features.",
        "Make this fluent while doing minimal change": "SPNs are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identify similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features.",
        "Refine this for me please": "SPNs are a class of graphical models that are expressive yet tractable. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identify similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. MiniSPN is a simplified version of LearnSPN that is faster and can handle missing data and heterogeneous features. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features.",
        "Concise this for me and keep all the information": "SPNs are a class of graphical models that are expressive yet tractable. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identify similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. MiniSPN is a simplified version of LearnSPN that runs faster and can handle missing data and heterogeneous features.",
        "Improve this in GPT way": "Sum-Product Networks (SPNs) are a class of expressive yet tractable\nhierarchical graphical models. LearnSPN is a structure learning algorithm for\nSPNs that uses hierarchical co-clustering to simultaneously identifying similar\nentities and similar features. The original LearnSPN algorithm assumes that all\nthe variables are discrete and there is no missing data. We introduce a\npractical, simplified version of LearnSPN, MiniSPN, that runs faster and can\nhandle missing data and heterogeneous features common in real applications. We\ndemonstrate the performance of MiniSPN on standard benchmark datasets and on\ntwo datasets from Google's Knowledge Graph exhibiting high missingness rates\nand a mix of discrete and continuous features."
    },
    {
        "input": "Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet",
        "Revise this with your best effort": "Recent research on deep neural networks has focused primarily on improving accuracy. However, it is typically possible to identify multiple DNN architectures that achieve a given accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: \n\n1. Smaller DNNs require less communication across servers during distributed training. \n2. Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. \n3. Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. \n\nTo provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n\nThe SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet",
        "Help me polish this": "Recent research on deep neural networks has focused primarily on improving accuracy. However, it is typically possible to identify multiple DNN architectures that achieve a given accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: \n\n1. Smaller DNNs require less communication across servers during distributed training. \n2. Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. \n3. Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. \n\nTo provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n\nThe SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet",
        "Rewrite this for me": "Recent research on deep neural networks has focused primarily on improving accuracy. However, it is typically possible to identify multiple DNN architectures that achieve a given accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: \n1. Smaller DNNs require less communication across servers during distributed training. \n2. Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. \n3. Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. \n\nTo provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n\nThe SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet",
        "Make this fluent while doing minimal change": "Recent research on deep neural networks has focused primarily on improving accuracy. However, it is typically possible to identify multiple DNN architectures that achieve a given accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n\nThe SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet",
        "Refine this for me please": "The recent research on deep neural networks has focused primarily on improving accuracy. However, it is typically possible to identify multiple DNN architectures that achieve the same accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: \n1. Smaller DNNs require less communication across servers during distributed training. \n2. Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. \n3. Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. \n\nTo provide all of these advantages, the researchers propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques the researchers are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).",
        "Concise this for me and keep all the information": "Smaller DNNs have many advantages, including less communication across servers during distributed training, less bandwidth to export a new model from the cloud to an autonomous car, and being more feasible to deploy on FPGAs and other hardware with limited memory.",
        "Improve this in GPT way": "The SqueezeNet architecture is a small DNN architecture that achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques, SqueezeNet can be compressed to less than 0.5MB (510x smaller than AlexNet)."
    },
    {
        "input": "In this paper, we study the problem of question answering when reasoning over\nmultiple facts is required. We propose Query-Reduction Network (QRN), a variant\nof Recurrent Neural Network (RNN) that effectively handles both short-term\n(local) and long-term (global) sequential dependencies to reason over multiple\nfacts. QRN considers the context sentences as a sequence of state-changing\ntriggers, and reduces the original query to a more informed query as it\nobserves each trigger (context sentence) through time. Our experiments show\nthat QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and\nin a real goal-oriented dialog dataset. In addition, QRN formulation allows\nparallelization on RNN's time axis, saving an order of magnitude in time\ncomplexity for training and inference.",
        "Revise this with your best effort": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.",
        "Help me polish this": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.",
        "Rewrite this for me": "In this paper, we investigate the problem of question answering that requires reasoning over multiple facts. We propose the Query-Reduction Network (QRN), a variant of the Recurrent Neural Network (RNN) that can effectively handle both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN regards the context sentences as a sequence of state-changing triggers and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments demonstrate that QRN produces state-of-the-art results in bAbI QA and dialog tasks, as well as in a real-world goal-oriented dialog dataset. Furthermore, the QRN formulation allows parallelization on the RNN's time axis, saving an order of magnitude in time complexity for training and inference.",
        "Make this fluent while doing minimal change": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.",
        "Refine this for me please": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.",
        "Concise this for me and keep all the information": "We study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset.",
        "Improve this in GPT way": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference."
    },
    {
        "input": "We propose a language-agnostic way of automatically generating sets of\nsemantically similar clusters of entities along with sets of \"outlier\"\nelements, which may then be used to perform an intrinsic evaluation of word\nembeddings in the outlier detection task. We used our methodology to create a\ngold-standard dataset, which we call WikiSem500, and evaluated multiple\nstate-of-the-art embeddings. The results show a correlation between performance\non this dataset and performance on sentiment analysis.",
        "Revise this with your best effort": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.",
        "Help me polish this": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements. This can then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.",
        "Rewrite this for me": "We propose a way of automatically generating sets of semantically similar clusters of entities, along with sets of \"outlier\" elements. This can then be used to evaluate word embeddings in the outlier detection task. We created a gold-standard dataset, called WikiSem500, and evaluated multiple state-of-the-art embeddings. The results showed a correlation between performance on this dataset and performance on sentiment analysis.",
        "Make this fluent while doing minimal change": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities, along with sets of \"outlier\" elements. This may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results showed a correlation between performance on this dataset and performance on sentiment analysis.",
        "Refine this for me please": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.",
        "Concise this for me and keep all the information": "We propose a way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.",
        "Improve this in GPT way": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis."
    },
    {
        "input": "Recurrent neural nets are widely used for predicting temporal data. Their\ninherent deep feedforward structure allows learning complex sequential\npatterns. It is believed that top-down feedback might be an important missing\ningredient which in theory could help disambiguate similar patterns depending\non broader context. In this paper we introduce surprisal-driven recurrent\nnetworks, which take into account past error information when making new\npredictions. This is achieved by continuously monitoring the discrepancy\nbetween most recent predictions and the actual observations. Furthermore, we\nshow that it outperforms other stochastic and fully deterministic approaches on\nenwik8 character level prediction task achieving 1.37 BPC on the test portion\nof the text.",
        "Revise this with your best effort": "Recurrent neural networks are widely used for predicting temporal data. Their deep feedforward structure allows them to learn complex sequential patterns. However, it is believed that top-down feedback might be an important missing ingredient which could help disambiguate similar patterns depending on broader context.\n\nIn this paper, we introduce surprisal-driven recurrent networks. These networks take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations.\n\nWe show that our approach outperforms other stochastic and fully deterministic approaches on the enwik8 character level prediction task. We achieve 1.37 BPC on the test portion of the text.",
        "Help me polish this": "Recurrent neural networks are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC on the test portion of the text.",
        "Rewrite this for me": "Recurrent neural nets are widely used for predicting temporal data because their deep feedforward structure allows them to learn complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that our approach outperforms other stochastic and fully deterministic approaches on the enwik8 character level prediction task, achieving 1.37 BPC on the test portion of the text.",
        "Make this fluent while doing minimal change": "Recurrent neural nets are widely used for predicting temporal data because their deep feedforward structure allows them to learn complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on the enwik8 character level prediction task, achieving 1.37 BPC on the test portion of the text.",
        "Refine this for me please": "Recurrent neural nets are widely used for predicting temporal data. Their\ninherent deep feedforward structure allows learning complex sequential\npatterns. It is believed that top-down feedback might be an important missing\ningredient which in theory could help disambiguate similar patterns depending\non broader context. In this paper we introduce surprisal-driven recurrent\nnetworks, which take into account past error information when making new\npredictions. This is achieved by continuously monitoring the discrepancy\nbetween most recent predictions and the actual observations. Furthermore, we\nshow that it outperforms other stochastic and fully deterministic approaches on\nenwik8 character level prediction task achieving 1.37 BPC on the test portion\nof the text.",
        "Concise this for me and keep all the information": "Recurrent neural nets are widely used for predicting temporal data. Their deep feedforward structure allows learning complex sequential patterns. Top-down feedback is an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. Surprisal-driven recurrent networks take into account past error information when making new predictions by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, they outperform other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC on the test portion of the text.",
        "Improve this in GPT way": "Recurrent neural nets are widely used for predicting temporal data. Their\ninherent deep feedforward structure allows learning complex sequential\npatterns. It is believed that top-down feedback might be an important missing\ningredient which in theory could help disambiguate similar patterns depending\non broader context. In this paper we introduce surprisal-driven recurrent\nnetworks, which take into account past error information when making new\npredictions. This is achieved by continuously monitoring the discrepancy\nbetween most recent predictions and the actual observations. Furthermore, we\nshow that it outperforms other stochastic and fully deterministic approaches on\nenwik8 character level prediction task achieving 1.37 BPC on the test portion\nof the text."
    },
    {
        "input": "Although Generative Adversarial Networks achieve state-of-the-art results on\na variety of generative tasks, they are regarded as highly unstable and prone\nto miss modes. We argue that these bad behaviors of GANs are due to the very\nparticular functional shape of the trained discriminators in high dimensional\nspaces, which can easily make training stuck or push probability mass in the\nwrong direction, towards that of higher concentration than that of the data\ngenerating distribution. We introduce several ways of regularizing the\nobjective, which can dramatically stabilize the training of GAN models. We also\nshow that our regularizers can help the fair distribution of probability mass\nacross the modes of the data generating distribution, during the early phases\nof training and thus providing a unified solution to the missing modes problem.",
        "Revise this with your best effort": "Although Generative Adversarial Networks (GANs) have achieved state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
        "Help me polish this": "Although Generative Adversarial Networks (GANs) have achieved state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
        "Rewrite this for me": "Although GANs have shown to be effective for a variety of tasks, they are often unstable and can miss modes. We argue that this is due to the functional shape of the discriminator in high dimensional spaces. This can easily make training stuck or push probability mass in the wrong direction. We introduce several ways of regularizing the objective, which can stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a solution to the missing modes problem.",
        "Make this fluent while doing minimal change": "Although Generative Adversarial Networks (GANs) have been shown to achieve state-of-the-art results on a variety of generative tasks, they are often regarded as being unstable and prone to missing modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
        "Refine this for me please": "Although GANs have shown to be effective for generating data, they are often unstable and can miss important modes. We argue that this is due to the high dimensional space which can easily make training stuck or push probability mass in the wrong direction. We introduce several ways of regularizing the objective which can help stabilize training and prevent missing modes.",
        "Concise this for me and keep all the information": "GANs are state-of-the-art but unstable and prone to missing modes. This is because the functional shape of the discriminator in high dimensional spaces is particular and can easily make training stuck or push probability mass in the wrong direction. We introduce several ways of regularizing the objective, which can stabilize the training of GAN models and help the fair distribution of probability mass across the modes of the data generating distribution.",
        "Improve this in GPT way": "Although Generative Adversarial Networks (GANs) have achieved state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution.\n\nWe introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem."
    },
    {
        "input": "Sample complexity and safety are major challenges when learning policies with\nreinforcement learning for real-world tasks, especially when the policies are\nrepresented using rich function approximators like deep neural networks.\nModel-based methods where the real-world target domain is approximated using a\nsimulated source domain provide an avenue to tackle the above challenges by\naugmenting real data with simulated data. However, discrepancies between the\nsimulated source domain and the target domain pose a challenge for simulated\ntraining. We introduce the EPOpt algorithm, which uses an ensemble of simulated\nsource domains and a form of adversarial training to learn policies that are\nrobust and generalize to a broad range of possible target domains, including\nunmodeled effects. Further, the probability distribution over source domains in\nthe ensemble can be adapted using data from target domain and approximate\nBayesian methods, to progressively make it a better approximation. Thus,\nlearning on a model ensemble, along with source domain adaptation, provides the\nbenefit of both robustness and learning/adaptation.",
        "Revise this with your best effort": "Sample complexity and safety are two of the major challenges when learning policies with reinforcement learning for real-world tasks. This is especially true when the policies are represented using rich function approximators like deep neural networks.\n\nModel-based methods, where the real-world target domain is approximated using a simulated source domain, provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training.\n\nWe introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects.\n\nFurther, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods. This allows the EPOpt algorithm to progressively make the ensemble a better approximation of the target domain.\n\nThus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
        "Help me polish this": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training.\n\nWe introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
        "Rewrite this for me": "Sample complexity and safety are two major challenges that arise when learning policies with reinforcement learning for real-world tasks. This is especially true when the policies are represented using rich function approximators, such as deep neural networks.\n\nOne way to tackle these challenges is to use model-based methods, where the real-world target domain is approximated using a simulated source domain. This allows for the augmentation of real data with simulated data. However, discrepancies between the simulated source domain and the target domain can pose a challenge for simulated training.\n\nWe introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, in order to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
        "Make this fluent while doing minimal change": "Sample complexity and safety are two major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
        "Refine this for me please": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
        "Concise this for me and keep all the information": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
        "Improve this in GPT way": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks.\n\nModel-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training.\n\nWe introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects.\n\nFurther, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation."
    },
    {
        "input": "We introduce Divnet, a flexible technique for learning networks with diverse\nneurons. Divnet models neuronal diversity by placing a Determinantal Point\nProcess (DPP) over neurons in a given layer. It uses this DPP to select a\nsubset of diverse neurons and subsequently fuses the redundant neurons into the\nselected ones. Compared with previous approaches, Divnet offers a more\nprincipled, flexible technique for capturing neuronal diversity and thus\nimplicitly enforcing regularization. This enables effective auto-tuning of\nnetwork architecture and leads to smaller network sizes without hurting\nperformance. Moreover, through its focus on diversity and neuron fusing, Divnet\nremains compatible with other procedures that seek to reduce memory footprints\nof networks. We present experimental results to corroborate our claims: for\npruning neural networks, Divnet is seen to be notably superior to competing\napproaches.",
        "Revise this with your best effort": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.",
        "Help me polish this": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.",
        "Rewrite this for me": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.",
        "Make this fluent while doing minimal change": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.",
        "Refine this for me please": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.",
        "Concise this for me and keep all the information": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks.",
        "Improve this in GPT way": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches."
    },
    {
        "input": "The efficiency of graph-based semi-supervised algorithms depends on the graph\nof instances on which they are applied. The instances are often in a vectorial\nform before a graph linking them is built. The construction of the graph relies\non a metric over the vectorial space that help define the weight of the\nconnection between entities. The classic choice for this metric is usually a\ndistance measure or a similarity measure based on the euclidean norm. We claim\nthat in some cases the euclidean norm on the initial vectorial space might not\nbe the more appropriate to solve the task efficiently. We propose an algorithm\nthat aims at learning the most appropriate vectorial representation for\nbuilding a graph on which the task at hand is solved efficiently.",
        "Revise this with your best effort": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the most appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Help me polish this": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the most appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Rewrite this for me": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Make this fluent while doing minimal change": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Refine this for me please": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, we claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Concise this for me and keep all the information": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The graph is built based on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. However, in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.",
        "Improve this in GPT way": "The efficiency of graph-based semi-supervised algorithms depends on the graph\nof instances on which they are applied. The instances are often in a vectorial\nform before a graph linking them is built. The construction of the graph relies\non a metric over the vectorial space that help define the weight of the\nconnection between entities. The classic choice for this metric is usually a\ndistance measure or a similarity measure based on the euclidean norm. However,\nin some cases the euclidean norm on the initial vectorial space might not\nbe the most appropriate to solve the task efficiently. We propose an algorithm\nthat aims at learning the most appropriate vectorial representation for\nbuilding a graph on which the task at hand is solved efficiently."
    },
    {
        "input": "One major challenge in training Deep Neural Networks is preventing\noverfitting. Many techniques such as data augmentation and novel regularizers\nsuch as Dropout have been proposed to prevent overfitting without requiring a\nmassive amount of training data. In this work, we propose a new regularizer\ncalled DeCov which leads to significantly reduced overfitting (as indicated by\nthe difference between train and val performance), and better generalization.\nOur regularizer encourages diverse or non-redundant representations in Deep\nNeural Networks by minimizing the cross-covariance of hidden activations. This\nsimple intuition has been explored in a number of past works but surprisingly\nhas never been applied as a regularizer in supervised learning. Experiments\nacross a range of datasets and network architectures show that this loss always\nreduces overfitting while almost always maintaining or increasing\ngeneralization performance and often improving performance over Dropout.",
        "Revise this with your best effort": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques, such as data augmentation and novel regularizers such as Dropout, have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance) and better generalization.\n\nOur regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",
        "Help me polish this": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization.\n\nOur regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",
        "Rewrite this for me": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques, such as data augmentation and novel regularizers such as Dropout, have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance) and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",
        "Make this fluent while doing minimal change": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques, such as data augmentation and novel regularizers such as Dropout, have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance) and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",
        "Refine this for me please": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization.\n\nOur regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",
        "Concise this for me and keep all the information": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques have been proposed to prevent overfitting, such as data augmentation and regularizers like Dropout. In this work, we propose a new regularizer called DeCov, which leads to significantly reduced overfitting and better generalization. DeCov encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance.",
        "Improve this in GPT way": "One major challenge in training Deep Neural Networks is preventing\noverfitting. Many techniques such as data augmentation and novel regularizers\nsuch as Dropout have been proposed to prevent overfitting without requiring a\nmassive amount of training data. In this work, we propose a new regularizer\ncalled DeCov which leads to significantly reduced overfitting (as indicated by\nthe difference between train and val performance), and better generalization.\nOur regularizer encourages diverse or non-redundant representations in Deep\nNeural Networks by minimizing the cross-covariance of hidden activations. This\nsimple intuition has been explored in a number of past works but surprisingly\nhas never been applied as a regularizer in supervised learning. Experiments\nacross a range of datasets and network architectures show that this loss always\nreduces overfitting while almost always maintaining or increasing\ngeneralization performance and often improving performance over Dropout."
    },
    {
        "input": "Deep neural networks are commonly trained using stochastic non-convex\noptimization procedures, which are driven by gradient information estimated on\nfractions (batches) of the dataset. While it is commonly accepted that batch\nsize is an important parameter for offline tuning, the benefits of online\nselection of batches remain poorly understood. We investigate online batch\nselection strategies for two state-of-the-art methods of stochastic\ngradient-based optimization, AdaDelta and Adam. As the loss function to be\nminimized for the whole dataset is an aggregation of loss functions of\nindividual datapoints, intuitively, datapoints with the greatest loss should be\nconsidered (selected in a batch) more frequently. However, the limitations of\nthis intuition and the proper control of the selection pressure over time are\nopen questions. We propose a simple strategy where all datapoints are ranked\nw.r.t. their latest known loss value and the probability to be selected decays\nexponentially as a function of rank. Our experimental results on the MNIST\ndataset suggest that selecting batches speeds up both AdaDelta and Adam by a\nfactor of about 5.",
        "Revise this with your best effort": "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. \n\nAs the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank. \n\nOur experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5.",
        "Help me polish this": "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. \n\nAs the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank. \n\nOur experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5.",
        "Rewrite this for me": "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. \n\nAs the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank. \n\nOur experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5.",
        "Make this fluent while doing minimal change": "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. \n\nAs the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank. \n\nOur experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5.",
        "Refine this for me please": "We investigate whether online selection of batches can speed up training for deep neural networks. Our experimental results on the MNIST dataset suggest that selecting batches can speed up training by a factor of about 5.",
        "Concise this for me and keep all the information": "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam.\n\nAs the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank.\n\nOur experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5.",
        "Improve this in GPT way": "We investigate the benefits of online batch selection for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. Our experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5."
    },
    {
        "input": "We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin.",
        "Revise this with your best effort": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset, we demonstrate that our approach outperforms related methods by a significant margin.",
        "Help me polish this": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset, we demonstrate that our approach outperforms related methods by a significant margin.",
        "Rewrite this for me": "We introduce a scalable approach to semi-supervised learning on graph-structured data, which is based on an efficient variant of convolutional neural networks that operate directly on graphs. We explain the choice of our convolutional architecture by appealing to a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that take into account both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset, we demonstrate that our approach outperforms related methods by a significant margin.",
        "Make this fluent while doing minimal change": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset, we demonstrate that our approach outperforms related methods by a significant margin.",
        "Refine this for me please": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset, we demonstrate that our approach outperforms related methods by a significant margin.",
        "Concise this for me and keep all the information": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We demonstrate that our approach outperforms related methods by a significant margin.",
        "Improve this in GPT way": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset, we demonstrate that our approach outperforms related methods by a significant margin."
    },
    {
        "input": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN)\nwhich views the discriminator as an energy function that attributes low\nenergies to the regions near the data manifold and higher energies to other\nregions. Similar to the probabilistic GANs, a generator is seen as being\ntrained to produce contrastive samples with minimal energies, while the\ndiscriminator is trained to assign high energies to these generated samples.\nViewing the discriminator as an energy function allows to use a wide variety of\narchitectures and loss functionals in addition to the usual binary classifier\nwith logistic output. Among them, we show one instantiation of EBGAN framework\nas using an auto-encoder architecture, with the energy being the reconstruction\nerror, in place of the discriminator. We show that this form of EBGAN exhibits\nmore stable behavior than regular GANs during training. We also show that a\nsingle-scale architecture can be trained to generate high-resolution images.",
        "Revise this with your best effort": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN), which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows us to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of the EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
        "Help me polish this": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN), which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows us to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of the EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
        "Rewrite this for me": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN), which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. The generator is trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. This allows for a wide variety of architectures and loss functionals to be used in addition to the usual binary classifier with logistic output. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
        "Make this fluent while doing minimal change": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN), which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows us to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of the EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
        "Refine this for me please": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN), which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. The generator is trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. This allows for a wide variety of architectures and loss functionals to be used in addition to the usual binary classifier with logistic output. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
        "Concise this for me and keep all the information": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN), which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. The generator is trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. This allows for a wide variety of architectures and loss functionals to be used in addition to the usual binary classifier with logistic output. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
        "Improve this in GPT way": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN), which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows us to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of the EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images."
    },
    {
        "input": "Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work.",
        "Revise this with your best effort": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).\n\nHere we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.",
        "Help me polish this": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).\n\nHere we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including the Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks. Our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns.\n\nWe hope others are inspired to build on our preliminary work.",
        "Rewrite this for me": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).\n\nHere we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.",
        "Make this fluent while doing minimal change": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).\n\nHere we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.",
        "Refine this for me please": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).\n\nHere we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks.\n\nWe hope others are inspired to build on our preliminary work. Our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns.",
        "Concise this for me and keep all the information": "Recent research in the deep learning field has produced a plethora of new architectures. However, many groups are composed of inexperienced deep learning practitioners who are baffled by the array of choices and opt to use an older architecture. This study attempts to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. The study also describes several architectural innovations, including the Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks.",
        "Improve this in GPT way": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).\n\nHere we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work."
    },
    {
        "input": "Machine comprehension (MC), answering a query about a given context\nparagraph, requires modeling complex interactions between the context and the\nquery. Recently, attention mechanisms have been successfully extended to MC.\nTypically these methods use attention to focus on a small portion of the\ncontext and summarize it with a fixed-size vector, couple attentions\ntemporally, and/or often form a uni-directional attention. In this paper we\nintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage\nhierarchical process that represents the context at different levels of\ngranularity and uses bi-directional attention flow mechanism to obtain a\nquery-aware context representation without early summarization. Our\nexperimental evaluations show that our model achieves the state-of-the-art\nresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze\ntest.",
        "Revise this with your best effort": "Machine comprehension (MC) requires modeling complex interactions between the context and the query in order to answer a query about a given context paragraph. Recently, attention mechanisms have been successfully extended to MC in order to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper, we introduce the Bi-Directional Attention Flow (BIDAF) network - a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
        "Help me polish this": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically, these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper, we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
        "Rewrite this for me": "Machine comprehension (MC) requires modeling complex interactions between the context and the query in order to answer a query about a given context paragraph. Recently, attention mechanisms have been successfully extended to MC. Typically, these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. The Bi-Directional Attention Flow (BIDAF) network introduced in this paper is a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
        "Make this fluent while doing minimal change": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically, these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper, we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
        "Refine this for me please": "Machine comprehension (MC) requires modeling complex interactions between the context and the query in order to answer a query about a given context paragraph. Recently, attention mechanisms have been successfully extended to MC in order to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. The Bi-Directional Attention Flow (BIDAF) network is a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
        "Concise this for me and keep all the information": "Machine comprehension (MC) requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically, these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper, we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
        "Improve this in GPT way": "The Bi-Directional Attention Flow (BIDAF) network is a state-of-the-art model for machine comprehension (MC), answering a query about a given context paragraph. The BIDAF network uses a bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization."
    },
    {
        "input": "Though with progress, model learning and performing posterior inference still\nremains a common challenge for using deep generative models, especially for\nhandling discrete hidden variables. This paper is mainly concerned with\nalgorithms for learning Helmholz machines, which is characterized by pairing\nthe generative model with an auxiliary inference model. A common drawback of\nprevious learning algorithms is that they indirectly optimize some bounds of\nthe targeted marginal log-likelihood. In contrast, we successfully develop a\nnew class of algorithms, based on stochastic approximation (SA) theory of the\nRobbins-Monro type, to directly optimize the marginal log-likelihood and\nsimultaneously minimize the inclusive KL-divergence. The resulting learning\nalgorithm is thus called joint SA (JSA). Moreover, we construct an effective\nMCMC operator for JSA. Our results on the MNIST datasets demonstrate that the\nJSA's performance is consistently superior to that of competing algorithms like\nRWS, for learning a range of difficult models.",
        "Revise this with your best effort": "Though deep generative models have made great progress, learning and performing posterior inference remains a common challenge, especially for handling discrete hidden variables. This paper is mainly concerned with algorithms for learning Helmholz machines, which are characterized by pairing the generative model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we successfully develop a new class of algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the JSA's performance is consistently superior to that of competing algorithms like RWS, for learning a range of difficult models.",
        "Help me polish this": "Though deep generative models are making progress, learning and performing posterior inference remains a common challenge, especially for handling discrete hidden variables. This paper is mainly concerned with algorithms for learning Helmholtz machines, which are characterized by pairing the generative model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we successfully develop a new class of algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the JSA's performance is consistently superior to that of competing algorithms like RWS, for learning a range of difficult models.",
        "Rewrite this for me": "Though deep generative models are making progress, learning and performing posterior inference remains a common challenge, especially for handling discrete hidden variables. This paper focuses on algorithms for learning Helmholz machines, which are characterized by a generative model paired with an auxiliary inference model. A drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we develop a new class of algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is called joint SA (JSA). We also construct an effective MCMC operator for JSA. Our results on the MNIST datasets show that JSA outperforms competing algorithms like RWS, for learning a range of difficult models.",
        "Make this fluent while doing minimal change": "Though progress has been made, learning and performing posterior inference using deep generative models remains a common challenge, especially for handling discrete hidden variables. This paper is mainly concerned with algorithms for learning Helmholz machines, which are characterized by pairing the generative model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we successfully develop a new class of algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the JSA's performance is consistently superior to that of competing algorithms like RWS, for learning a range of difficult models.",
        "Refine this for me please": "The paper is concerned with developing new algorithms, based on stochastic approximation (SA) theory, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is called joint SA (JSA). The paper demonstrates that the JSA's performance is consistently superior to that of competing algorithms like RWS, for learning a range of difficult models.",
        "Concise this for me and keep all the information": "This paper is concerned with algorithms for learning Helmholz machines, which are characterized by pairing the generative model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, the authors successfully develop a new class of algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, the authors construct an effective MCMC operator for JSA. The authors' results on the MNIST datasets demonstrate that the JSA's performance is consistently superior to that of competing algorithms like RWS, for learning a range of difficult models.",
        "Improve this in GPT way": "Though deep generative models have made great progress, learning and performing posterior inference remains a challenge, especially for models with discrete hidden variables. This paper focuses on algorithms for learning Helmholtz machines, which are characterized by a generative model paired with an auxiliary inference model. A drawback of previous learning algorithms is that they optimize some bounds of the targeted marginal log-likelihood indirectly. In contrast, we develop new algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is called joint SA (JSA). We also construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that JSA outperforms competing algorithms like RWS for learning a range of difficult models."
    },
    {
        "input": "Object detection with deep neural networks is often performed by passing a\nfew thousand candidate bounding boxes through a deep neural network for each\nimage. These bounding boxes are highly correlated since they originate from the\nsame image. In this paper we investigate how to exploit feature occurrence at\nthe image scale to prune the neural network which is subsequently applied to\nall bounding boxes. We show that removing units which have near-zero activation\nin the image allows us to significantly reduce the number of parameters in the\nnetwork. Results on the PASCAL 2007 Object Detection Challenge demonstrate that\nup to 40% of units in some fully-connected layers can be entirely eliminated\nwith little change in the detection result.",
        "Revise this with your best effort": "Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. However, these bounding boxes are highly correlated since they originate from the same image. In this paper, we investigate how to exploit feature occurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result.",
        "Help me polish this": "Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. However, these bounding boxes are highly correlated since they originate from the same image.\n\nIn this paper, we investigate how to exploit feature occurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network.\n\nResults on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result.",
        "Rewrite this for me": "In this paper, we investigate how to use feature occurrence at the image scale to prune the deep neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result.",
        "Make this fluent while doing minimal change": "Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. These bounding boxes are highly correlated since they originate from the same image. In this paper, we investigate how to exploit feature occurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result.",
        "Refine this for me please": "Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. However, these bounding boxes are highly correlated since they originate from the same image. In this paper, the authors investigate how to exploit feature occurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. They show that removing units which have near-zero activation in the image allows them to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result.",
        "Concise this for me and keep all the information": "In this paper, we investigate how to use feature occurrence at the image scale to prune the deep neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result.",
        "Improve this in GPT way": "In this paper, we investigate how to exploit feature occurrence at the image scale to prune the deep neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result."
    },
    {
        "input": "Modeling interactions between features improves the performance of machine\nlearning solutions in many domains (e.g. recommender systems or sentiment\nanalysis). In this paper, we introduce Exponential Machines (ExM), a predictor\nthat models all interactions of every order. The key idea is to represent an\nexponentially large tensor of parameters in a factorized format called Tensor\nTrain (TT). The Tensor Train format regularizes the model and lets you control\nthe number of underlying parameters. To train the model, we develop a\nstochastic Riemannian optimization procedure, which allows us to fit tensors\nwith 2^160 entries. We show that the model achieves state-of-the-art\nperformance on synthetic data with high-order interactions and that it works on\npar with high-order factorization machines on a recommender system dataset\nMovieLens 100K.",
        "Revise this with your best effort": "Modeling interactions between features can improve the performance of machine learning solutions in many domains, such as recommender systems or sentiment analysis. In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with a huge number of entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works just as well as high-order factorization machines on a recommender system dataset like MovieLens 100K.",
        "Help me polish this": "Modeling interactions between features can improve the performance of machine learning solutions in many domains, such as recommender systems or sentiment analysis. In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with a huge number of entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset, MovieLens 100K.",
        "Rewrite this for me": "Modeling interactions between features can help improve the performance of machine learning solutions in many domains. In this paper, we introduce Exponential Machines (ExM), which is a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with a huge number of entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works just as well as high-order factorization machines on a recommender system dataset called MovieLens 100K.",
        "Make this fluent while doing minimal change": "Modeling interactions between features can improve the performance of machine learning solutions in many domains, such as recommender systems or sentiment analysis. In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with a huge number of entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset, MovieLens 100K.",
        "Refine this for me please": "In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.",
        "Concise this for me and keep all the information": "Modeling interactions between features improves the performance of machine learning solutions. In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with a large number of entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset.",
        "Improve this in GPT way": "We introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K."
    },
    {
        "input": "We introduce Deep Variational Bayes Filters (DVBF), a new method for\nunsupervised learning and identification of latent Markovian state space\nmodels. Leveraging recent advances in Stochastic Gradient Variational Bayes,\nDVBF can overcome intractable inference distributions via variational\ninference. Thus, it can handle highly nonlinear input data with temporal and\nspatial dependencies such as image sequences without domain knowledge. Our\nexperiments show that enabling backpropagation through transitions enforces\nstate space assumptions and significantly improves information content of the\nlatent embedding. This also enables realistic long-term prediction.",
        "Revise this with your best effort": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
        "Help me polish this": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
        "Rewrite this for me": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
        "Make this fluent while doing minimal change": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
        "Refine this for me please": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
        "Concise this for me and keep all the information": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. DVBF can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
        "Improve this in GPT way": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction."
    },
    {
        "input": "Traditional dialog systems used in goal-oriented applications require a lot\nof domain-specific handcrafting, which hinders scaling up to new domains.\nEnd-to-end dialog systems, in which all components are trained from the dialogs\nthemselves, escape this limitation. But the encouraging success recently\nobtained in chit-chat dialog may not carry over to goal-oriented settings. This\npaper proposes a testbed to break down the strengths and shortcomings of\nend-to-end dialog systems in goal-oriented applications. Set in the context of\nrestaurant reservation, our tasks require manipulating sentences and symbols,\nso as to properly conduct conversations, issue API calls and use the outputs of\nsuch calls. We show that an end-to-end dialog system based on Memory Networks\ncan reach promising, yet imperfect, performance and learn to perform\nnon-trivial operations. We confirm those results by comparing our system to a\nhand-crafted slot-filling baseline on data from the second Dialog State\nTracking Challenge (Henderson et al., 2014a). We show similar result patterns\non data extracted from an online concierge service.",
        "Revise this with your best effort": "Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings.\n\nThis paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations.\n\nWe confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.",
        "Help me polish this": "End-to-end dialog systems have the potential to be more scalable than traditional dialog systems, which require a lot of handcrafting for each new domain. However, end-to-end dialog systems have not been as successful in goal-oriented settings. This paper proposes a testbed to compare the strengths and weaknesses of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm these results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.",
        "Rewrite this for me": "Dialog systems that require a lot of domain-specific handcrafting in order to be effective limit how well they can be scaled up to new domains. End-to-end dialog systems, which train all components from the dialogs themselves, do not have this limitation. However, the encouraging success recently obtained in chit-chat dialogs may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.",
        "Make this fluent while doing minimal change": "Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, which are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. \n\nThis paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. \n\nWe confirm these results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.",
        "Refine this for me please": "End-to-end dialog systems have the potential to be more scalable than traditional dialog systems, which require a lot of domain-specific handcrafting. However, the success of end-to-end dialog systems in goal-oriented settings has been mixed. This paper proposes a testbed to compare the strengths and weaknesses of end-to-end dialog systems in goal-oriented applications. The testbed is set in the context of restaurant reservation, and requires the system to manipulate sentences and symbols in order to properly conduct conversations, issue API calls, and use the outputs of such calls. The results show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations.",
        "Concise this for me and keep all the information": "End-to-end dialog systems have the potential to be more successful than traditional dialog systems in goal-oriented applications, but they have not been proven to be more successful in chit-chat dialog. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. The testbed is set in the context of restaurant reservation and requires manipulating sentences and symbols to properly conduct conversations, issue API calls, and use the outputs of such calls. The results of the testbed show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations.",
        "Improve this in GPT way": "End-to-end dialog systems have the potential to greatly simplify the process of creating goal-oriented applications. However, these systems have not been extensively tested in this domain. This paper proposes a testbed to evaluate the strengths and weaknesses of end-to-end dialog systems in goal-oriented applications. The testbed is set in the context of restaurant reservation, and requires the system to manipulate sentences and symbols in order to properly conduct conversations, issue API calls, and use the outputs of such calls. The results show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations."
    },
    {
        "input": "Adversarial training provides a means of regularizing supervised learning\nalgorithms while virtual adversarial training is able to extend supervised\nlearning algorithms to the semi-supervised setting. However, both methods\nrequire making small perturbations to numerous entries of the input vector,\nwhich is inappropriate for sparse high-dimensional inputs such as one-hot word\nrepresentations. We extend adversarial and virtual adversarial training to the\ntext domain by applying perturbations to the word embeddings in a recurrent\nneural network rather than to the original input itself. The proposed method\nachieves state of the art results on multiple benchmark semi-supervised and\npurely supervised tasks. We provide visualizations and analysis showing that\nthe learned word embeddings have improved in quality and that while training,\nthe model is less prone to overfitting. Code is available at\nhttps://github.com/tensorflow/models/tree/master/research/adversarial_text.",
        "Revise this with your best effort": "Adversarial training is a regularization technique for supervised learning algorithms that is typically used to prevent overfitting. Virtual adversarial training is an extension of adversarial training that can be used in the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\n\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n\nCode is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.",
        "Help me polish this": "Adversarial training is a regularization technique for supervised learning algorithms that is effective in both the semi-supervised and purely supervised settings. Virtual adversarial training is a similar technique that can be used to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\n\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n\nCode is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.",
        "Rewrite this for me": "Adversarial training is a way to regularize supervised learning algorithms, while virtual adversarial training can extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\n\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n\nCode is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.",
        "Make this fluent while doing minimal change": "Adversarial training provides a means of regularizing supervised learning algorithms, while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting. Code is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.",
        "Refine this for me please": "Adversarial training is a method of regularizing supervised learning algorithms by making small perturbations to the input vector. Virtual adversarial training is a similar method that can be used in the semi-supervised setting. However, both of these methods require making perturbations to numerous entries in the input vector, which is not ideal for high-dimensional inputs such as one-hot word representations.\n\nThe proposed method extends adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. This method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. The code for this method is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.",
        "Concise this for me and keep all the information": "Adversarial training is a method of regularizing supervised learning algorithms by making small perturbations to the input vector. Virtual adversarial training is a similar method that can be used in the semi-supervised setting. However, both of these methods require making perturbations to numerous entries in the input vector, which is not ideal for high-dimensional inputs such as one-hot word representations.\n\nThe proposed method extends adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. This method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. The code for this method is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.",
        "Improve this in GPT way": "Adversarial training is a powerful tool for regularizing supervised learning algorithms. Virtual adversarial training can extend these benefits to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\n\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n\nCode is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text."
    },
    {
        "input": "Unsupervised learning of probabilistic models is a central yet challenging\nproblem in machine learning. Specifically, designing models with tractable\nlearning, sampling, inference and evaluation is crucial in solving this task.\nWe extend the space of such models using real-valued non-volume preserving\n(real NVP) transformations, a set of powerful invertible and learnable\ntransformations, resulting in an unsupervised learning algorithm with exact\nlog-likelihood computation, exact sampling, exact inference of latent\nvariables, and an interpretable latent space. We demonstrate its ability to\nmodel natural images on four datasets through sampling, log-likelihood\nevaluation and latent variable manipulations.",
        "Revise this with your best effort": "Designing models with tractable learning, sampling, inference and evaluation is crucial for unsupervised learning of probabilistic models, a central yet challenging problem in machine learning. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
        "Help me polish this": "Unsupervised learning of probabilistic models is a central, yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference, and evaluation is crucial in solving this task.\n\nWe extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations. This results in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space.\n\nWe demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.",
        "Rewrite this for me": "Designing models that are both tractable to learn and sample from is a central, yet challenging, problem in machine learning. In this work, we extend the space of such models using real-valued non-volume preserving (real NVP) transformations. These are a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, and exact inference of latent variables. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.",
        "Make this fluent while doing minimal change": "Designing models with tractable learning, sampling, inference and evaluation is crucial for unsupervised learning of probabilistic models, a central yet challenging problem in machine learning. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
        "Refine this for me please": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models that are easy to learn, sample, and infer from is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
        "Concise this for me and keep all the information": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets.",
        "Improve this in GPT way": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task.\n\nWe extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space.\n\nWe demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations."
    },
    {
        "input": "This paper is focused on studying the view-manifold structure in the feature\nspaces implied by the different layers of Convolutional Neural Networks (CNN).\nThere are several questions that this paper aims to answer: Does the learned\nCNN representation achieve viewpoint invariance? How does it achieve viewpoint\ninvariance? Is it achieved by collapsing the view manifolds, or separating them\nwhile preserving them? At which layer is view invariance achieved? How can the\nstructure of the view manifold at each layer of a deep convolutional neural\nnetwork be quantified experimentally? How does fine-tuning of a pre-trained CNN\non a multi-view dataset affect the representation at each layer of the network?\nIn order to answer these questions we propose a methodology to quantify the\ndeformation and degeneracy of view manifolds in CNN layers. We apply this\nmethodology and report interesting results in this paper that answer the\naforementioned questions.",
        "Revise this with your best effort": "This paper focuses on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNNs). The goal is to answer several questions, such as: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network?\n\nIn order to answer these questions, the paper proposes a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. This methodology is then applied to answer the aforementioned questions.",
        "Help me polish this": "This paper focuses on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNNs). The goal is to answer several questions, such as: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network?\n\nIn order to answer these questions, the paper proposes a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. This methodology is then applied to answer the aforementioned questions.",
        "Rewrite this for me": "This paper focuses on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNNs). The goal is to answer several questions, such as: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions, the paper proposes a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. The paper reports interesting results that answer the aforementioned questions.",
        "Make this fluent while doing minimal change": "This paper focuses on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNNs). The paper aims to answer several questions, including: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions, the paper proposes a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. The methodology is applied and results are reported that answer the aforementioned questions.",
        "Refine this for me please": "This paper focuses on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNNs). The goal is to answer several questions, such as: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network?\n\nIn order to answer these questions, the paper proposes a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. This methodology is then applied to answer the aforementioned questions.",
        "Concise this for me and keep all the information": "This paper focuses on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNNs). The goal is to answer questions about how CNN representations achieve viewpoint invariance, how this is achieved, and at which layer it is achieved. To do this, the paper proposes a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. The results of applying this methodology answer the aforementioned questions.",
        "Improve this in GPT way": "This paper is focused on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNNs). In order to answer the questions posed in the introduction, we propose a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. We apply this methodology and report interesting results in this paper that answer the aforementioned questions."
    },
    {
        "input": "Bilinear models provide rich representations compared with linear models.\nThey have been applied in various visual tasks, such as object recognition,\nsegmentation, and visual question-answering, to get state-of-the-art\nperformances taking advantage of the expanded representations. However,\nbilinear representations tend to be high-dimensional, limiting the\napplicability to computationally complex tasks. We propose low-rank bilinear\npooling using Hadamard product for an efficient attention mechanism of\nmultimodal learning. We show that our model outperforms compact bilinear\npooling in visual question-answering tasks with the state-of-the-art results on\nthe VQA dataset, having a better parsimonious property.",
        "Revise this with your best effort": "Bilinear models provide richer representations than linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances by taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, which limits their applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, with the state-of-the-art results on the VQA dataset. Our model also has a better parsimonious property.",
        "Help me polish this": "Bilinear models provide richer representations than linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances by taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, which limits their applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, with the state-of-the-art results on the VQA dataset. Our model also has a better parsimonious property.",
        "Rewrite this for me": "Bilinear models provide richer representations than linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances by taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, which limits their applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, with the state-of-the-art results on the VQA dataset. Our model also has a better parsimonious property.",
        "Make this fluent while doing minimal change": "Bilinear models provide richer representations than linear models.\nThey have been applied in various visual tasks, such as object recognition,\nsegmentation, and visual question-answering, and have outperformed linear models by taking advantage of the expanded representations. However,\nbilinear representations tend to be high-dimensional, which can limit\ntheir applicability to computationally complex tasks. We propose low-rank bilinear\npooling using Hadamard product for an efficient attention mechanism of\nmultimodal learning. We show that our model outperforms compact bilinear\npooling in visual question-answering tasks, with the state-of-the-art results on\nthe VQA dataset. Our model also has a better parsimonious property.",
        "Refine this for me please": "Bilinear models provide richer representations than linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances by taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, which limits their applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, with the state-of-the-art results on the VQA dataset. Our model also has a better parsimonious property.",
        "Concise this for me and keep all the information": "Bilinear models provide richer representations than linear models, and have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances. However, bilinear representations tend to be high-dimensional, limiting their applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, with the state-of-the-art results on the VQA dataset.",
        "Improve this in GPT way": "Bilinear models provide richer representations compared with linear models.\nThey have been applied in various visual tasks, such as object recognition,\nsegmentation, and visual question-answering, to get state-of-the-art\nperformances taking advantage of the expanded representations. However,\nbilinear representations tend to be high-dimensional, limiting the\napplicability to computationally complex tasks. We propose low-rank bilinear\npooling using Hadamard product for an efficient attention mechanism of\nmultimodal learning. We show that our model outperforms compact bilinear\npooling in visual question-answering tasks with the state-of-the-art results on\nthe VQA dataset, having a better parsimonious property."
    },
    {
        "input": "The standard interpretation of importance-weighted autoencoders is that they\nmaximize a tighter lower bound on the marginal likelihood than the standard\nevidence lower bound. We give an alternate interpretation of this procedure:\nthat it optimizes the standard variational lower bound, but using a more\ncomplex distribution. We formally derive this result, present a tighter lower\nbound, and visualize the implicit importance-weighted distribution.",
        "Revise this with your best effort": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood than the standard evidence lower bound. However, we give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution. We formally derive this result, present a tighter lower bound, and visualize the implicit importance-weighted distribution.",
        "Help me polish this": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood than the standard evidence lower bound. However, we give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution. We formally derive this result, present a tighter lower bound, and visualize the implicit importance-weighted distribution.",
        "Rewrite this for me": "The standard interpretation of importance-weighted autoencoders is that they\nmaximize a tighter lower bound on the marginal likelihood than the standard\nevidence lower bound. However, we give an alternate interpretation of this procedure:\nthat it optimizes the standard variational lower bound, but using a more\ncomplex distribution. We formally derive this result, present a tighter lower\nbound, and visualize the implicit importance-weighted distribution.",
        "Make this fluent while doing minimal change": "The standard interpretation of importance-weighted autoencoders is that they\nmaximize a tighter lower bound on the marginal likelihood than the standard\nevidence lower bound. However, we give an alternate interpretation of this procedure:\nthat it optimizes the standard variational lower bound, but using a more\ncomplex distribution. We formally derive this result, present a tighter lower\nbound, and visualize the implicit importance-weighted distribution.",
        "Refine this for me please": "The standard interpretation of importance-weighted autoencoders is that they\nmaximize a tighter lower bound on the marginal likelihood than the standard\nevidence lower bound. We give an alternate interpretation of this procedure:\nthat it optimizes the standard variational lower bound, but using a more\ncomplex distribution. We formally derive this result, present a tighter lower\nbound, and visualize the implicit importance-weighted distribution.",
        "Concise this for me and keep all the information": "The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood than the standard evidence lower bound. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution.",
        "Improve this in GPT way": "We give an alternate interpretation of importance-weighted autoencoders: that they optimize the standard variational lower bound, but using a more complex distribution. We formally derive this result, present a tighter lower bound, and visualize the implicit importance-weighted distribution."
    },
    {
        "input": "We present a generalization bound for feedforward neural networks in terms of\nthe product of the spectral norm of the layers and the Frobenius norm of the\nweights. The generalization bound is derived using a PAC-Bayes analysis.",
        "Revise this with your best effort": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. This generalization bound is derived using a PAC-Bayes analysis, which provides a way to measure the quality of the bound.",
        "Help me polish this": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. This generalization bound is derived using a PAC-Bayes analysis.",
        "Rewrite this for me": "We provide a generalization bound for feedforward neural networks that is based on the product of the spectral norm of the layers and the Frobenius norm of the weights. This bound is derived using a PAC-Bayes analysis.",
        "Make this fluent while doing minimal change": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. This generalization bound is derived using a PAC-Bayes analysis.",
        "Refine this for me please": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis, which is a method of bounding the generalization error of a learning algorithm.",
        "Concise this for me and keep all the information": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights.",
        "Improve this in GPT way": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis. This generalization bound can be used to improve the performance of feedforward neural networks."
    },
    {
        "input": "In this paper, we propose to equip Generative Adversarial Networks with the\nability to produce direct energy estimates for samples.Specifically, we propose\na flexible adversarial training framework, and prove this framework not only\nensures the generator converges to the true data distribution, but also enables\nthe discriminator to retain the density information at the global optimal. We\nderive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two\neffective approximation techniques. Empirically, the experiment results closely\nmatch our theoretical analysis, verifying the discriminator is able to recover\nthe energy of data distribution.",
        "Revise this with your best effort": "In this paper, we propose a method for training Generative Adversarial Networks (GANs) so that they can produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove that this framework not only ensures that the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimum. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying that the discriminator is able to recover the energy of data distribution.",
        "Help me polish this": "In this paper, we propose a method for training Generative Adversarial Networks (GANs) to produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove that this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying that the discriminator is able to recover the energy of data distribution.",
        "Rewrite this for me": "In this paper, we propose a method for training Generative Adversarial Networks (GANs) so that they can produce direct energy estimates for samples. Our proposed method is based on a flexible adversarial training framework, which we prove not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.",
        "Make this fluent while doing minimal change": "In this paper, we propose a method for equipping Generative Adversarial Networks with the ability to produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove that this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying that the discriminator is able to recover the energy of data distribution.",
        "Refine this for me please": "In this paper, we propose a method for training Generative Adversarial Networks (GANs) so that they can produce direct energy estimates for samples. This method, which we call a \"flexible adversarial training framework,\" not only ensures that the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.",
        "Concise this for me and keep all the information": "In this paper, we propose a flexible adversarial training framework that not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.",
        "Improve this in GPT way": "In this paper, we propose a new method for training Generative Adversarial Networks (GANs) that enables them to produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove that this framework not only ensures that the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimum. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying that the discriminator is able to recover the energy of data distribution."
    },
    {
        "input": "In this work we perform outlier detection using ensembles of neural networks\nobtained by variational approximation of the posterior in a Bayesian neural\nnetwork setting. The variational parameters are obtained by sampling from the\ntrue posterior by gradient descent. We show our outlier detection results are\ncomparable to those obtained using other efficient ensembling methods.",
        "Revise this with your best effort": "In this work, we perform outlier detection using ensembles of neural networks. We obtain the variational parameters by sampling from the true posterior using gradient descent. We show that our outlier detection results are comparable to those obtained using other efficient ensembling methods.",
        "Help me polish this": "We perform outlier detection using ensembles of neural networks, which are obtained by variational approximation of the posterior in a Bayesian neural network setting. The variational parameters are obtained by sampling from the true posterior using gradient descent. We show that our outlier detection results are comparable to those obtained using other efficient ensembling methods.",
        "Rewrite this for me": "We detect outliers using ensembles of neural networks that are approximate posterior distributions in a Bayesian neural network setting. We get the variational parameters by sampling from the true posterior using gradient descent. Our outlier detection results are comparable to those obtained using other efficient ensembling methods.",
        "Make this fluent while doing minimal change": "In this work, we perform outlier detection using ensembles of neural networks. We obtain the variational parameters by sampling from the true posterior using gradient descent. We show that our outlier detection results are comparable to those obtained using other efficient ensembling methods.",
        "Refine this for me please": "We use a variational approximation of the posterior to perform outlier detection using ensembles of neural networks. The variational parameters are obtained by sampling from the true posterior using gradient descent. We show that our outlier detection results are comparable to those obtained using other efficient ensembling methods.",
        "Concise this for me and keep all the information": "We use ensembles of neural networks to detect outliers, which are obtained by approximating the posterior in a Bayesian neural network setting. The variational parameters are obtained by sampling from the true posterior using gradient descent. Our outlier detection results are comparable to those obtained using other efficient ensembling methods.",
        "Improve this in GPT way": "In this work, we perform outlier detection using ensembles of neural networks. The variational parameters are obtained by sampling from the true posterior by gradient descent. We show that our outlier detection results are comparable to those obtained using other efficient ensembling methods."
    },
    {
        "input": "We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster to the near state-of the art\nperplexity while using significantly less RNN parameters.",
        "Revise this with your best effort": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of the LSTM matrix into the product of two smaller matrices, and the second one is partitioning of the LSTM matrix, its inputs and states into independent groups. Both approaches allow us to train large LSTM networks significantly faster, while using significantly fewer RNN parameters.",
        "Help me polish this": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster while using significantly fewer RNN parameters.",
        "Rewrite this for me": "We present two ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster while using significantly fewer RNN parameters.",
        "Make this fluent while doing minimal change": "We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster while using significantly less RNN parameters.",
        "Refine this for me please": "We present two ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster while using significantly fewer RNN parameters.",
        "Concise this for me and keep all the information": "We present two ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.",
        "Improve this in GPT way": "We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster to the near state-of the art\nperplexity while using significantly less RNN parameters."
    },
    {
        "input": "We present observations and discussion of previously unreported phenomena\ndiscovered while training residual networks. The goal of this work is to better\nunderstand the nature of neural networks through the examination of these new\nempirical results. These behaviors were identified through the application of\nCyclical Learning Rates (CLR) and linear network interpolation. Among these\nbehaviors are counterintuitive increases and decreases in training loss and\ninstances of rapid training. For example, we demonstrate how CLR can produce\ngreater testing accuracy than traditional training despite using large learning\nrates. Files to replicate these results are available at\nhttps://github.com/lnsmith54/exploring-loss",
        "Revise this with your best effort": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. Files to replicate these results are available at https://github.com/lnsmith54/exploring-loss.",
        "Help me polish this": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. \n\nFiles to replicate these results are available at https://github.com/lnsmith54/exploring-loss.",
        "Rewrite this for me": "We observed and discussed previously unreported phenomena while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates.",
        "Make this fluent while doing minimal change": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. \n\nFiles to replicate these results are available at https://github.com/lnsmith54/exploring-loss.",
        "Refine this for me please": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates.",
        "Concise this for me and keep all the information": "We found new phenomena while training residual networks, which we discuss here. Our goal is to understand neural networks better by examining these new empirical results. We identified these behaviors through the use of Cyclical Learning Rates (CLR) and linear network interpolation. They include counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates.",
        "Improve this in GPT way": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. Files to replicate these results are available at https://github.com/lnsmith54/exploring-loss."
    },
    {
        "input": "Machine learning models are often used at test-time subject to constraints\nand trade-offs not present at training-time. For example, a computer vision\nmodel operating on an embedded device may need to perform real-time inference,\nor a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a\nmixture-of-experts model and show how to change its test-time resource-usage on\na per-input basis using reinforcement learning. We test our method on a small\nMNIST-based example.",
        "Revise this with your best effort": "Machine learning models are often used at test-time subject to constraints\nand trade-offs not present at training-time. For example, a computer vision\nmodel operating on an embedded device may need to perform real-time inference,\nor a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work, we describe a\nmixture-of-experts model and show how to change its test-time resource-usage on\na per-input basis using reinforcement learning. We test our method on a small\nMNIST-based example.",
        "Help me polish this": "Machine learning models are often used at test-time subject to constraints\nand trade-offs not present at training-time. For example, a computer vision\nmodel operating on an embedded device may need to perform real-time inference,\nor a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work, we describe a\nmixture-of-experts model and show how to change its test-time resource-usage on\na per-input basis using reinforcement learning. We test our method on a small\nMNIST-based example.",
        "Rewrite this for me": "Machine learning models are often used at test-time subject to constraints\nand trade-offs not present at training-time. For example, a computer vision\nmodel operating on an embedded device may need to perform real-time inference,\nor a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work, we describe a\nmixture-of-experts model and show how to change its test-time resource-usage on\na per-input basis using reinforcement learning. We test our method on a small\nMNIST-based example.",
        "Make this fluent while doing minimal change": "Machine learning models are often used at test-time, subject to constraints and trade-offs not present during training. For example, a computer vision model operating on an embedded device may need to perform real-time inference, or a translation model operating on a cell phone may wish to bound its average compute time in order to be power-efficient. In this work, we describe a mixture-of-experts model and show how to change its test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example.",
        "Refine this for me please": "Machine learning models are often used at test-time subject to constraints\nand trade-offs not present at training-time. For example, a computer vision\nmodel operating on an embedded device may need to perform real-time inference,\nor a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work, we describe a\nmixture-of-experts model and show how to change its test-time resource-usage on\na per-input basis using reinforcement learning. We test our method on a small\nMNIST-based example.",
        "Concise this for me and keep all the information": "Machine learning models are often used at test-time with constraints that don't exist during training. For example, a computer vision model on an embedded device may need to do real-time inference, or a translation model on a cell phone may need to be power-efficient. We describe a mixture-of-experts model and show how to use reinforcement learning to change its resource-usage on a per-input basis. We test our method on a small MNIST-based example.",
        "Improve this in GPT way": "We describe a mixture-of-experts model that can be adapted to different test-time resource-usage requirements using reinforcement learning. We demonstrate our method on a small MNIST-based example."
    },
    {
        "input": "Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples.",
        "Revise this with your best effort": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
        "Help me polish this": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. However, little is known about how robust these policies are to adversarial attacks.\n\nIn this paper, we present a novel study into adversarial attacks on deep reinforcement learning policies. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
        "Rewrite this for me": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. However, little is known about how robust these policies are to adversarial attacks.\n\nIn this paper, we present a novel study into adversarial attacks on deep reinforcement learning policies. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
        "Make this fluent while doing minimal change": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper, we present a novel study into adversarial attacks on deep reinforcement learning policies. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
        "Refine this for me please": "In this paper, we study adversarial attacks on deep reinforcement learning policies. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
        "Concise this for me and keep all the information": "Adversarial examples exist for deep learning architectures, which can be exploited to attack deep reinforcement learning policies. In this paper, we compare the effectiveness of adversarial examples vs. random noise, and present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack. We also explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.",
        "Improve this in GPT way": "In this paper, we study the effectiveness of adversarial attacks on deep reinforcement learning policies. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples."
    },
    {
        "input": "This paper develops variational continual learning (VCL), a simple but\ngeneral framework for continual learning that fuses online variational\ninference (VI) and recent advances in Monte Carlo VI for neural networks. The\nframework can successfully train both deep discriminative models and deep\ngenerative models in complex continual learning settings where existing tasks\nevolve over time and entirely new tasks emerge. Experimental results show that\nVCL outperforms state-of-the-art continual learning methods on a variety of\ntasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Revise this with your best effort": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Help me polish this": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Rewrite this for me": "This paper develops variational continual learning (VCL), a framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Make this fluent while doing minimal change": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Refine this for me please": "This paper develops variational continual learning (VCL), a framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Concise this for me and keep all the information": "This paper develops variational continual learning (VCL), a framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Improve this in GPT way": "This paper develops variational continual learning (VCL), a simple but\ngeneral framework for continual learning that fuses online variational\ninference (VI) and recent advances in Monte Carlo VI for neural networks. The\nframework can successfully train both deep discriminative models and deep\ngenerative models in complex continual learning settings where existing tasks\nevolve over time and entirely new tasks emerge. Experimental results show that\nVCL outperforms state-of-the-art continual learning methods on a variety of\ntasks, avoiding catastrophic forgetting in a fully automatic way."
    },
    {
        "input": "Automatically determining the optimal size of a neural network for a given\ntask without prior information currently requires an expensive global search\nand training many networks from scratch. In this paper, we address the problem\nof automatically finding a good network size during a single training cycle. We\nintroduce *nonparametric neural networks*, a non-probabilistic framework for\nconducting optimization over all possible network sizes and prove its soundness\nwhen network growth is limited via an L_p penalty. We train networks under this\nframework by continuously adding new units while eliminating redundant units\nvia an L_2 penalty. We employ a novel optimization algorithm, which we term\n*adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising\nresults.",
        "Revise this with your best effort": "Automatically determining the optimal size of a neural network for a given task without prior information is a difficult problem that requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce *nonparametric neural networks*, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty. We employ a novel optimization algorithm, which we term *adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising results.",
        "Help me polish this": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce *nonparametric neural networks*, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty. We employ a novel optimization algorithm, which we term *adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising results.",
        "Rewrite this for me": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce nonparametric neural networks, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty. We employ a novel optimization algorithm, which we term adaptive radial-angular gradient descent or AdaRad, and obtain promising results.",
        "Make this fluent while doing minimal change": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce nonparametric neural networks, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty. We employ a novel optimization algorithm, which we term adaptive radial-angular gradient descent or AdaRad, and obtain promising results.",
        "Refine this for me please": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce nonparametric neural networks, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty. We employ a novel optimization algorithm, which we term adaptive radial-angular gradient descent or AdaRad, and obtain promising results.",
        "Concise this for me and keep all the information": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce nonparametric neural networks, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty. We employ a novel optimization algorithm, which we term adaptive radial-angular gradient descent or AdaRad, and obtain promising results.",
        "Improve this in GPT way": "In this paper, we introduce a new framework for automatically finding the optimal size of a neural network for a given task. Our framework, which we call nonparametric neural networks, is a non-probabilistic approach that allows for optimization over all possible network sizes. We prove the soundness of our approach when network growth is limited by an L_p penalty, and we demonstrate the effectiveness of our approach by training networks under our framework that continuously add new units while eliminating redundant units via an L_2 penalty. Our optimization algorithm, which we term adaptive radial-angular gradient descent or AdaRad, obtains promising results."
    },
    {
        "input": "Natural Language Inference (NLI) task requires an agent to determine the\nlogical relationship between a natural language premise and a natural language\nhypothesis. We introduce Interactive Inference Network (IIN), a novel class of\nneural network architectures that is able to achieve high-level understanding\nof the sentence pair by hierarchically extracting semantic features from\ninteraction space. We show that an interaction tensor (attention weight)\ncontains semantic information to solve natural language inference, and a denser\ninteraction tensor contains richer semantic information. One instance of such\narchitecture, Densely Interactive Inference Network (DIIN), demonstrates the\nstate-of-the-art performance on large scale NLI copora and large-scale NLI\nalike corpus. It's noteworthy that DIIN achieve a greater than 20% error\nreduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to\nthe strongest published system.",
        "Revise this with your best effort": "The Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large-scale NLI corpora and large-scale NLI-alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Help me polish this": "The task of natural language inference (NLI) requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large-scale NLI corpora and large-scale NLI-alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Rewrite this for me": "The task of natural language inference (NLI) requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large-scale NLI corpora and large-scale NLI-alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Make this fluent while doing minimal change": "The task of Natural Language Inference (NLI) requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large scale NLI corpora and large-scale NLI alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Refine this for me please": "The IIN is a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. The interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. DIIN, one instance of such architecture, demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus.",
        "Concise this for me and keep all the information": "The IIN is a neural network that is able to understand the relationship between a natural language premise and hypothesis by extracting semantic features from an interaction space. The DIIN is an instance of the IIN that demonstrates state-of-the-art performance on large-scale NLI corpora.",
        "Improve this in GPT way": "We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system."
    },
    {
        "input": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken. We propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2.",
        "Revise this with your best effort": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken.\n\nWe propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2.",
        "Help me polish this": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken.\n\nWe propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2.",
        "Rewrite this for me": "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Make this fluent while doing minimal change": "The ability to deploy neural networks in real-world, safety-critical systems is limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Refine this for me please": "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Concise this for me and keep all the information": "The ability to deploy neural networks in real-world, safety-critical systems is limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples, but most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Improve this in GPT way": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken.\n\nWe propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2."
    },
    {
        "input": "We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's.",
        "Revise this with your best effort": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Help me polish this": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Rewrite this for me": "We develop a Stochastic Gradient Variational Bayes method to perform posterior inference for the weights of Stick-Breaking processes. This allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), which is a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Make this fluent while doing minimal change": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Refine this for me please": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Concise this for me and keep all the information": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), which has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE learns highly discriminative latent representations that often outperform the Gaussian VAE's.",
        "Improve this in GPT way": "We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's."
    },
    {
        "input": "We propose a framework for training multiple neural networks simultaneously.\nThe parameters from all models are regularised by the tensor trace norm, so\nthat each neural network is encouraged to reuse others' parameters if possible\n-- this is the main motivation behind multi-task learning. In contrast to many\ndeep multi-task learning models, we do not predefine a parameter sharing\nstrategy by specifying which layers have tied parameters. Instead, our\nframework considers sharing for all shareable layers, and the sharing strategy\nis learned in a data-driven way.",
        "Revise this with your best effort": "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularized by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Help me polish this": "We propose a framework for training multiple neural networks simultaneously, with the aim of encouraging parameter sharing and thus reducing the overall number of parameters required. We regularise the parameters of all models using the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Rewrite this for me": "We propose a framework for training multiple neural networks simultaneously, with the aim of encouraging parameter sharing and reducing overfitting. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Make this fluent while doing minimal change": "We propose a framework for training multiple neural networks simultaneously, with the parameters from all models regularised by the tensor trace norm. This encourages each neural network to reuse others' parameters if possible -- which is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Refine this for me please": "We propose a framework for training multiple neural networks simultaneously, with the parameters from all models being regularised by the tensor trace norm. This encourages each neural network to reuse others' parameters if possible, which is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Concise this for me and keep all the information": "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible. This is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way.",
        "Improve this in GPT way": "We propose a GPT-based framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way."
    },
    {
        "input": "This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method.",
        "Revise this with your best effort": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. The paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Help me polish this": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Rewrite this for me": "This paper presents a deep reinforcement learning agent that uses experience replay to improve stability and sample efficiency. The agent performs well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Make this fluent while doing minimal change": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Refine this for me please": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. The paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Concise this for me and keep all the information": "This paper presents an actor-critic deep reinforcement learning agent that is stable, sample efficient, and performs well on challenging environments. The paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.",
        "Improve this in GPT way": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method."
    },
    {
        "input": "Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix.",
        "Revise this with your best effort": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Help me polish this": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Rewrite this for me": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We have deployed three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Make this fluent while doing minimal change": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Refine this for me please": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Concise this for me and keep all the information": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation is a modification to an input that changes a classifier's prediction without causing the input to seem substantially different to human perception. We have deployed three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.",
        "Improve this in GPT way": "Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix."
    },
    {
        "input": "We propose a principled method for kernel learning, which relies on a\nFourier-analytic characterization of translation-invariant or\nrotation-invariant kernels. Our method produces a sequence of feature maps,\niteratively refining the SVM margin. We provide rigorous guarantees for\noptimality and generalization, interpreting our algorithm as online\nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations\non synthetic and real-world datasets demonstrate scalability and consistent\nimprovements over related random features-based methods.",
        "Revise this with your best effort": "We propose a method for kernel learning that is based on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Help me polish this": "We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Rewrite this for me": "We propose a method for kernel learning that uses a Fourier-based characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps that iteratively refines the support vector machine margin. We provide theoretical guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related methods that use random features.",
        "Make this fluent while doing minimal change": "We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Refine this for me please": "We propose a method for kernel learning which uses a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Concise this for me and keep all the information": "We propose a method for kernel learning which uses a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Improve this in GPT way": "We propose a principled method for kernel learning, which relies on a\nFourier-analytic characterization of translation-invariant or\nrotation-invariant kernels. Our method produces a sequence of feature maps,\niteratively refining the SVM margin. We provide rigorous guarantees for\noptimality and generalization, interpreting our algorithm as online\nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations\non synthetic and real-world datasets demonstrate scalability and consistent\nimprovements over related random features-based methods."
    },
    {
        "input": "State-of-the-art deep reading comprehension models are dominated by recurrent\nneural nets. Their sequential nature is a natural fit for language, but it also\nprecludes parallelization within an instances and often becomes the bottleneck\nfor deploying such models to latency critical scenarios. This is particularly\nproblematic for longer texts. Here we present a convolutional architecture as\nan alternative to these recurrent architectures. Using simple dilated\nconvolutional units in place of recurrent ones, we achieve results comparable\nto the state of the art on two question answering tasks, while at the same time\nachieving up to two orders of magnitude speedups for question answering.",
        "Revise this with your best effort": "State-of-the-art deep reading comprehension models are typically based on recurrent neural networks (RNNs). RNNs are well suited for processing sequential data such as language, but their sequential nature also limits parallelization and can be a bottleneck for deploying such models to latency-critical scenarios. This is particularly problematic for longer texts. \n\nAs an alternative to RNNs, we present a convolutional architecture. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Help me polish this": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often preclude parallelization within an instance, which can become a bottleneck for deploying such models to latency-critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Rewrite this for me": "State-of-the-art deep reading comprehension models are typically based on recurrent neural nets. However, their sequential nature can make them slow, preventing them from being used in latency-critical scenarios. This is especially problematic for longer texts. An alternative is to use a convolutional architecture. This can be achieved by using dilated convolutional units instead of recurrent ones. This approach can achieve results comparable to the state of the art on two question answering tasks, while at the same time being much faster.",
        "Make this fluent while doing minimal change": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often preclude parallelization within an instance and become the bottleneck for deploying such models to latency critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Refine this for me please": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often have a bottleneck for deploying such models to latency critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Concise this for me and keep all the information": "State-of-the-art deep reading comprehension models are dominated by recurrent neural nets. However, their sequential nature is a natural fit for language, but it also precludes parallelization within an instance and often becomes the bottleneck for deploying such models to latency critical scenarios, which is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Improve this in GPT way": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often preclude parallelization within an instance, which can become a bottleneck for deploying such models to latency-critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering."
    },
    {
        "input": "This report has several purposes. First, our report is written to investigate\nthe reproducibility of the submitted paper On the regularization of Wasserstein\nGANs (2018). Second, among the experiments performed in the submitted paper,\nfive aspects were emphasized and reproduced: learning speed, stability,\nrobustness against hyperparameter, estimating the Wasserstein distance, and\nvarious sampling method. Finally, we identify which parts of the contribution\ncan be reproduced, and at what cost in terms of resources. All source code for\nreproduction is open to the public.",
        "Revise this with your best effort": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling methods. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Help me polish this": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Rewrite this for me": "The purpose of this report is to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). We reproduced five aspects of the experiments performed in the submitted paper: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. We identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Make this fluent while doing minimal change": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Refine this for me please": "The purpose of this report is to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). We reproduced five aspects of the experiments performed in the submitted paper: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. We identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Concise this for me and keep all the information": "The purpose of this report is to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). The paper is reproduced in full and all source code is open to the public.",
        "Improve this in GPT way": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public."
    },
    {
        "input": "Variational Autoencoders (VAEs) were originally motivated (Kingma & Welling,\n2014) as probabilistic generative models in which one performs approximate\nBayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks\nthis interpretation and generalizes VAEs to application domains beyond\ngenerative modeling (e.g., representation learning, clustering, or lossy data\ncompression) by introducing an objective function that allows practitioners to\ntrade off between the information content (\"bit rate\") of the latent\nrepresentation and the distortion of reconstructed data (Alemi et al., 2018).\nIn this paper, we reconsider this rate/distortion trade-off in the context of\nhierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We\nidentify a general class of inference models for which one can split the rate\ninto contributions from each layer, which can then be tuned independently. We\nderive theoretical bounds on the performance of downstream tasks as functions\nof the individual layers' rates and verify our theoretical findings in\nlarge-scale experiments. Our results provide guidance for practitioners on\nwhich region in rate-space to target for a given application.",
        "Revise this with your best effort": "Variational Autoencoders (VAEs) were motivated originally as probabilistic generative models in which one performs approximate Bayesian inference (Kingma & Welling, 2014). The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Help me polish this": "Variational Autoencoders (VAEs) were motivated originally as probabilistic generative models in which one performs approximate Bayesian inference (Kingma & Welling, 2014). The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Rewrite this for me": "Variational Autoencoders (VAEs) were motivated as probabilistic generative models in which one performs approximate Bayesian inference. The proposal of $\\beta$-VAEs breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018). In this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Make this fluent while doing minimal change": "Variational Autoencoders (VAEs) were motivated originally (Kingma & Welling, 2014) as probabilistic generative models, in which one performs approximate Bayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Refine this for me please": "Variational Autoencoders (VAEs) were motivated as probabilistic generative models in which one performs approximate Bayesian inference. The proposal of $\\beta$-VAEs breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Concise this for me and keep all the information": "Variational Autoencoders (VAEs) are a type of probabilistic generative model used for approximate Bayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018). In this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Improve this in GPT way": "Variational Autoencoders (VAEs) were originally motivated (Kingma & Welling,\n2014) as probabilistic generative models in which one performs approximate\nBayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks\nthis interpretation and generalizes VAEs to application domains beyond\ngenerative modeling (e.g., representation learning, clustering, or lossy data\ncompression) by introducing an objective function that allows practitioners to\ntrade off between the information content (\"bit rate\") of the latent\nrepresentation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider the rate/distortion trade-off in the context of\nhierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We\nidentify a general class of inference models for which one can split the rate\ninto contributions from each layer, which can then be tuned independently. We\nderive theoretical bounds on the performance of downstream tasks as functions\nof the individual layers' rates and verify our theoretical findings in\nlarge-scale experiments. Our results provide guidance for practitioners on\nwhich region in rate-space to target for a given application."
    },
    {
        "input": "Methods that learn representations of nodes in a graph play a critical role\nin network analysis since they enable many downstream learning tasks. We\npropose Graph2Gauss - an approach that can efficiently learn versatile node\nembeddings on large scale (attributed) graphs that show strong performance on\ntasks such as link prediction and node classification. Unlike most approaches\nthat represent nodes as point vectors in a low-dimensional continuous space, we\nembed each node as a Gaussian distribution, allowing us to capture uncertainty\nabout the representation. Furthermore, we propose an unsupervised method that\nhandles inductive learning scenarios and is applicable to different types of\ngraphs: plain/attributed, directed/undirected. By leveraging both the network\nstructure and the associated node attributes, we are able to generalize to\nunseen nodes without additional training. To learn the embeddings we adopt a\npersonalized ranking formulation w.r.t. the node distances that exploits the\nnatural ordering of the nodes imposed by the network structure. Experiments on\nreal world networks demonstrate the high performance of our approach,\noutperforming state-of-the-art network embedding methods on several different\ntasks. Additionally, we demonstrate the benefits of modeling uncertainty - by\nanalyzing it we can estimate neighborhood diversity and detect the intrinsic\nlatent dimensionality of a graph.",
        "Revise this with your best effort": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Help me polish this": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification.\n\nUnlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training.\n\nTo learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Rewrite this for me": "Methods that learn representations of nodes in a graph are critical for network analysis, since they enable many downstream learning tasks. Graph2Gauss is an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs, and shows strong performance on tasks such as link prediction and node classification. Most approaches represent nodes as point vectors in a low-dimensional continuous space, but Graph2Gauss embeds each node as a Gaussian distribution, allowing for uncertainty to be captured about the representation. Additionally, Graph2Gauss proposes an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, Graph2Gauss is able to generalize to unseen nodes without additional training. To learn the embeddings, Graph2Gauss adopts a personalized ranking formulation with respect to the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, the benefits of modeling uncertainty are demonstrated - by analyzing it, we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Make this fluent while doing minimal change": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Refine this for me please": "We propose an approach to efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Our approach, Graph2Gauss, embeds each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training.",
        "Concise this for me and keep all the information": "Graph2Gauss is an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification.",
        "Improve this in GPT way": "We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph."
    },
    {
        "input": "This paper explores the use of self-ensembling for visual domain adaptation\nproblems. Our technique is derived from the mean teacher variant (Tarvainen et\nal., 2017) of temporal ensembling (Laine et al;, 2017), a technique that\nachieved state of the art results in the area of semi-supervised learning. We\nintroduce a number of modifications to their approach for challenging domain\nadaptation scenarios and evaluate its effectiveness. Our approach achieves\nstate of the art results in a variety of benchmarks, including our winning\nentry in the VISDA-2017 visual domain adaptation challenge. In small image\nbenchmarks, our algorithm not only outperforms prior art, but can also achieve\naccuracy that is close to that of a classifier trained in a supervised fashion.",
        "Revise this with your best effort": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.",
        "Help me polish this": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.",
        "Rewrite this for me": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), which achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.",
        "Make this fluent while doing minimal change": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.",
        "Refine this for me please": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.",
        "Concise this for me and keep all the information": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant of temporal ensembling, a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.",
        "Improve this in GPT way": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion."
    },
    {
        "input": "Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust.",
        "Revise this with your best effort": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. \n\nBy using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.",
        "Help me polish this": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. \n\nBy using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.",
        "Rewrite this for me": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. \n\nBy using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.",
        "Make this fluent while doing minimal change": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.",
        "Refine this for me please": "The goal of this paper is to understand why adversarial examples can fool a classifier, by using concepts from topology. By investigating the relationship between two metric spaces corresponding to a predictor and an oracle, the paper develops conditions that can determine if a classifier is always robust against adversarial examples.",
        "Concise this for me and keep all the information": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier and adds its oracle in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor and oracle, we develop necessary and sufficient conditions that can determine if the classifier is always robust (strong-robust) against adversarial examples according to the oracle. Interestingly, our theorems indicate that just one unnecessary feature can make the classifier not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.",
        "Improve this in GPT way": "Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust."
    },
    {
        "input": "We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired.",
        "Revise this with your best effort": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Help me polish this": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Rewrite this for me": "We develop a problem setting to train and test agents' ability to gather information efficiently. We present a collection of tasks where success requires searching through a partially-observed environment for fragments of information. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Make this fluent while doing minimal change": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Refine this for me please": "We develop a problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Concise this for me and keep all the information": "We develop a problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.",
        "Improve this in GPT way": "We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired."
    },
    {
        "input": "We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.",
        "Revise this with your best effort": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach outperforms recent memory augmented networks.",
        "Help me polish this": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach outperforms recent memory augmented networks.",
        "Rewrite this for me": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
        "Make this fluent while doing minimal change": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
        "Refine this for me please": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
        "Concise this for me and keep all the information": "We propose an extension to neural network language models that adapts their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate that our approach performs significantly better than recent memory augmented networks on several language model datasets.",
        "Improve this in GPT way": "We propose an extension to neural network language models that uses a cache to store and access past hidden activations, which is more efficient and scalable than other memory augmented networks."
    },
    {
        "input": "Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.",
        "Revise this with your best effort": "Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.",
        "Help me polish this": "We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Rewrite this for me": "Generative adversarial networks (GANs) are successful deep generative models that are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Make this fluent while doing minimal change": "Generative adversarial networks (GANs) are successful deep generative models that are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Refine this for me please": "Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.",
        "Concise this for me and keep all the information": "GANs are successful deep generative models that are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",
        "Improve this in GPT way": "We propose a novel algorithm that combines the strengths of generative adversarial networks (GANs) and density ratio estimation. Our algorithm is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. This offers a new perspective toward the understanding of GANs and provides a way to improve the training of GANs."
    },
    {
        "input": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Revise this with your best effort": "We introduce a new system for creating pop music. Our model is a hierarchical recurrent neural network, which uses layers and a hierarchical structure to encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conducted several human studies that showed a strong preference for our generated music over that produced by the recent Google method. We also demonstrated two applications of our framework: neural dancing and karaoke, as well as neural story singing.",
        "Help me polish this": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Rewrite this for me": "We introduce a new system for creating pop music. Our model is a type of artificial neural network called a hierarchical recurrent neural network. This network is designed to reflect our existing knowledge about how pop music is composed. For example, the bottom layers of the network generate the melody, while the higher levels produce the drums and chords. We have conducted several studies with human participants, who overwhelmingly preferred the music generated by our system to that produced by a similar system recently developed by Google. In addition, we demonstrate two applications of our framework: neural dancing and karaoke, as well as neural story singing.",
        "Make this fluent while doing minimal change": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show a strong\npreference for our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Refine this for me please": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show our generated music is preferred over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.",
        "Concise this for me and keep all the information": "We present a novel framework for generating pop music using a hierarchical Recurrent Neural Network. This model encodes prior knowledge about how pop music is composed, with the bottom layers generating the melody and higher levels producing the drums and chords. Our human studies show strong preference for our generated music over that produced by the recent Google method. We also show two applications of our framework: neural dancing and karaoke, as well as neural story singing.",
        "Improve this in GPT way": "We present a novel GPT-based framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing."
    },
    {
        "input": "We look at the eigenvalues of the Hessian of a loss function before and after\ntraining. The eigenvalue distribution is seen to be composed of two parts, the\nbulk which is concentrated around zero, and the edges which are scattered away\nfrom zero. We present empirical evidence for the bulk indicating how\nover-parametrized the system is, and for the edges that depend on the input\ndata.",
        "Revise this with your best effort": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Help me polish this": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Rewrite this for me": "We examine the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution appears to be made up of two parts: the bulk, which is focused around zero, and the edges, which are scattered away from zero. Our empirical evidence for the bulk demonstrates how over-parametrized the system is, and for the edges that show how the input data affects them.",
        "Make this fluent while doing minimal change": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Refine this for me please": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. We present empirical evidence for the bulk, indicating how over-parametrized the system is, and for the edges, that depend on the input data.",
        "Concise this for me and keep all the information": "The Hessian is a matrix of second-order partial derivatives of a function. We can use it to measure how a loss function changes before and after training. The eigenvalue distribution of the Hessian can be split into two parts: the bulk, which is concentrated around zero, and the edges, which are scattered away from zero. The bulk indicates how over-parametrized the system is, and the edges depend on the input data.",
        "Improve this in GPT way": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence that:\n\n- The bulk indicates how over-parametrized the system is.\n- The edges depend on the input data."
    },
    {
        "input": "In this paper, we propose a new feature extraction technique for program\nexecution logs. First, we automatically extract complex patterns from a\nprogram's behavior graph. Then, we embed these patterns into a continuous space\nby training an autoencoder. We evaluate the proposed features on a real-world\nmalicious software detection task. We also find that the embedding space\ncaptures interpretable structures in the space of pattern parts.",
        "Revise this with your best effort": "In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Help me polish this": "In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task and find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Rewrite this for me": "In this paper, we propose a new technique for extracting features from program execution logs. This technique involves automatically extracting complex patterns from a program's behavior graph and then embedding these patterns into a continuous space. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Make this fluent while doing minimal change": "In this paper, we propose a new feature extraction technique for program execution logs. This technique involves automatically extracting complex patterns from a program's behavior graph and then embedding these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task and find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Refine this for me please": "In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Concise this for me and keep all the information": "We propose a new feature extraction technique for program execution logs. We automatically extract complex patterns from a program's behavior graph and embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task and find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Improve this in GPT way": "In this paper, we propose a new feature extraction technique for program\nexecution logs. First, we automatically extract complex patterns from a\nprogram's behavior graph. Then, we embed these patterns into a continuous space\nby training an autoencoder. We evaluate the proposed features on a real-world\nmalicious software detection task. We also find that the embedding space\ncaptures interpretable structures in the space of pattern parts."
    },
    {
        "input": "We compared the efficiency of the FlyHash model, an insect-inspired sparse\nneural network (Dasgupta et al., 2017), to similar but non-sparse models in an\nembodied navigation task. This requires a model to control steering by\ncomparing current visual inputs to memories stored along a training route. We\nconcluded the FlyHash model is more efficient than others, especially in terms\nof data encoding.",
        "Revise this with your best effort": "We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded that the FlyHash model is more efficient than others, especially in terms of data encoding.",
        "Help me polish this": "We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded that the FlyHash model is more efficient than others, especially in terms of data encoding.",
        "Rewrite this for me": "We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network, to similar but non-sparse models in an embodied navigation task. This required a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded that the FlyHash model is more efficient than others, especially in terms of data encoding.",
        "Make this fluent while doing minimal change": "We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded that the FlyHash model is more efficient than others, especially in terms of data encoding.",
        "Refine this for me please": "We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network, to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded that the FlyHash model is more efficient than others, especially in terms of data encoding.",
        "Concise this for me and keep all the information": "We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network, to similar but non-sparse models in an embodied navigation task. The task requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded that the FlyHash model is more efficient than others, especially in terms of data encoding.",
        "Improve this in GPT way": "We compared the efficiency of the FlyHash model, an insect-inspired sparse\nneural network (Dasgupta et al., 2017), to similar but non-sparse models in an\nembodied navigation task. This requires a model to control steering by\ncomparing current visual inputs to memories stored along a training route. We\nconcluded that the FlyHash model is more efficient than others, especially in terms\nof data encoding."
    },
    {
        "input": "In peer review, reviewers are usually asked to provide scores for the papers.\nThe scores are then used by Area Chairs or Program Chairs in various ways in\nthe decision-making process. The scores are usually elicited in a quantized\nform to accommodate the limited cognitive ability of humans to describe their\nopinions in numerical values. It has been found that the quantized scores\nsuffer from a large number of ties, thereby leading to a significant loss of\ninformation. To mitigate this issue, conferences have started to ask reviewers\nto additionally provide a ranking of the papers they have reviewed. There are\nhowever two key challenges. First, there is no standard procedure for using\nthis ranking information and Area Chairs may use it in different ways\n(including simply ignoring them), thereby leading to arbitrariness in the\npeer-review process. Second, there are no suitable interfaces for judicious use\nof this data nor methods to incorporate it in existing workflows, thereby\nleading to inefficiencies. We take a principled approach to integrate the\nranking information into the scores. The output of our method is an updated\nscore pertaining to each review that also incorporates the rankings. Our\napproach addresses the two aforementioned challenges by: (i) ensuring that\nrankings are incorporated into the updates scores in the same manner for all\npapers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use\nexisting interfaces and workflows designed for scores. We empirically evaluate\nour method on synthetic datasets as well as on peer reviews from the ICLR 2017\nconference, and find that it reduces the error by approximately 30% as compared\nto the best performing baseline on the ICLR 2017 data.",
        "Revise this with your best effort": "In peer review, reviewers are usually asked to provide both scores and rankings for the papers.\nThe scores are then used by Area Chairs or Program Chairs in various ways in\nthe decision-making process. The scores are usually elicited in a quantized\nform to accommodate the limited cognitive ability of humans to describe their\nopinions in numerical values. It has been found that the quantized scores\nsuffer from a large number of ties, thereby leading to a significant loss of\ninformation. To mitigate this issue, conferences have started to ask reviewers\nto additionally provide a ranking of the papers they have reviewed. There are\nhowever two key challenges. First, there is no standard procedure for using\nthis ranking information and Area Chairs may use it in different ways\n(including simply ignoring them), thereby leading to arbitrariness in the\npeer-review process. Second, there are no suitable interfaces for judicious use\nof this data nor methods to incorporate it in existing workflows, thereby\nleading to inefficiencies. We take a principled approach to integrate the\nranking information into the scores. The output of our method is an updated\nscore pertaining to each review that also incorporates the rankings. Our\napproach addresses the two aforementioned challenges by: (i) ensuring that\nrankings are incorporated into the updates scores in the same manner for all\npapers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use\nexisting",
        "Help me polish this": "In peer review, reviewers are usually asked to provide both scores and rankings for the papers.\nThe scores are then used by Area Chairs or Program Chairs in various ways in\nthe decision-making process, while the rankings are used to help mitigate the issue of ties in the scores. However, there are two key challenges with using rankings. First, there is no standard procedure for using\nthis ranking information and Area Chairs may use it in different ways\n(including simply ignoring them), thereby leading to arbitrariness in the\npeer-review process. Second, there are no suitable interfaces for judicious use\nof this data nor methods to incorporate it in existing workflows, thereby\nleading to inefficiencies.\n\nWe take a principled approach to integrate the\nranking information into the scores. The output of our method is an updated\nscore pertaining to each review that also incorporates the rankings. Our\napproach addresses the two aforementioned challenges by: (i) ensuring that\nrankings are incorporated into the updates scores in the same manner for all\npapers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use\nexisting interfaces and workflows designed for scores. We empirically evaluate\nour method on synthetic datasets as well as on peer reviews from the ICLR 2017\nconference, and find that it reduces the error by approximately 30% as compared\nto the best performing baseline on the ICLR 2017 data.",
        "Rewrite this for me": "In peer review, reviewers are usually asked to provide both scores and rankings for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process, while the rankings are used to mitigate the issue of ties in the quantized scores, which leads to a significant loss of information. There is no standard procedure for using the ranking information, and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. There are also no suitable interfaces for judicious use of this data nor methods to incorporate it in existing workflows, thereby leading to inefficiencies. We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updates scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and workflows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and find that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.",
        "Make this fluent while doing minimal change": "In peer review, reviewers are usually asked to provide both scores and rankings for the papers.\nThe scores are then used by Area Chairs or Program Chairs in various ways in\nthe decision-making process, while the rankings are used to mitigate the issue of ties in the scores, which leads to a significant loss of\ninformation. There is no standard procedure for using\nthis ranking information and Area Chairs may use it in different ways\n(including simply ignoring them), thereby leading to arbitrariness in the\npeer-review process. Second, there are no suitable interfaces for judicious use\nof this data nor methods to incorporate it in existing workflows, thereby\nleading to inefficiencies. We take a principled approach to integrate the\nranking information into the scores. The output of our method is an updated\nscore pertaining to each review that also incorporates the rankings. Our\napproach addresses the two aforementioned challenges by: (i) ensuring that\nrankings are incorporated into the updates scores in the same manner for all\npapers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use\nexisting interfaces and workflows designed for scores. We empirically evaluate\nour method on synthetic datasets as well as on peer reviews from the ICLR 2017\nconference, and find that it reduces the error by approximately 30% as compared\nto the best performing baseline on the ICLR 2017 data.",
        "Refine this for me please": "In peer review, reviewers are usually asked to provide both scores and rankings for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process, while the rankings are used to help mitigate the issue of ties in the scores. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing workflows, thereby leading to inefficiencies. We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updates scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and workflows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and find that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.",
        "Concise this for me and keep all the information": "In peer review, reviewers are asked to provide both scores and rankings for papers. The scores are used by Area Chairs or Program Chairs in various ways in the decision-making process, but the rankings are not used in a standard way, which can lead to arbitrariness in the peer-review process. Our method integrates the ranking information into the scores, which reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.",
        "Improve this in GPT way": "In peer review, reviewers are usually asked to provide both scores and rankings for the papers.\nThe scores and rankings are then used by Area Chairs or Program Chairs in various ways in\nthe decision-making process. The scores are usually elicited in a quantized\nform to accommodate the limited cognitive ability of humans to describe their\nopinions in numerical values. It has been found that the quantized scores\nsuffer from a large number of ties, thereby leading to a significant loss of\ninformation. To mitigate this issue, conferences have started to ask reviewers\nto additionally provide a ranking of the papers they have reviewed. There are\nhowever two key challenges. First, there is no standard procedure for using\nthis ranking information and Area Chairs may use it in different ways\n(including simply ignoring them), thereby leading to arbitrariness in the\npeer-review process. Second, there are no suitable interfaces for judicious use\nof this data nor methods to incorporate it in existing workflows, thereby\nleading to inefficiencies. We take a principled approach to integrate the\nranking information into the scores. The output of our method is an updated\nscore pertaining to each review that also incorporates the rankings. Our\napproach addresses the two aforementioned challenges by: (i) ensuring that\nrankings are incorporated into the updates scores in the same manner for all\npapers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use"
    },
    {
        "input": "Many recent studies have probed status bias in the peer-review process of\nacademic journals and conferences. In this article, we investigated the\nassociation between author metadata and area chairs' final decisions\n(Accept/Reject) using our compiled database of 5,313 borderline submissions to\nthe International Conference on Learning Representations (ICLR) from 2017 to\n2022. We carefully defined elements in a cause-and-effect analysis, including\nthe treatment and its timing, pre-treatment variables, potential outcomes and\ncausal null hypothesis of interest, all in the context of study units being\ntextual data and under Neyman and Rubin's potential outcomes (PO) framework. We\nfound some weak evidence that author metadata was associated with articles'\nfinal decisions. We also found that, under an additional stability assumption,\nborderline articles from high-ranking institutions (top-30% or top-20%) were\nless favored by area chairs compared to their matched counterparts. The results\nwere consistent in two different matched designs (odds ratio = 0.82 [95% CI:\n0.67 to 1.00] in a first design and 0.83 [95% CI: 0.64 to 1.07] in a\nstrengthened design). We discussed how to interpret these results in the\ncontext of multiple interactions between a study unit and different agents\n(reviewers and area chairs) in the peer-review system.",
        "Revise this with your best effort": "Many recent studies have found evidence of status bias in the peer-review process of academic journals and conferences. In this article, we investigated the association between author metadata and area chairs' final decisions (Accept/Reject) using our compiled database of 5,313 borderline submissions to the International Conference on Learning Representations (ICLR) from 2017 to 2022.\n\nWe carefully defined elements in a cause-and-effect analysis, including the treatment and its timing, pre-treatment variables, potential outcomes and causal null hypothesis of interest, all in the context of study units being textual data and under Neyman and Rubin's potential outcomes (PO) framework. We found some weak evidence that author metadata was associated with articles' final decisions.\n\nWe also found that, under an additional stability assumption, borderline articles from high-ranking institutions (top-30% or top-20%) were less favored by area chairs compared to their matched counterparts. The results were consistent in two different matched designs (odds ratio = 0.82 [95% CI: 0.67 to 1.00] in a first design and 0.83 [95% CI: 0.64 to 1.07] in a strengthened design).\n\nWe discussed how to interpret these results in the context of multiple interactions between a study unit and different agents (reviewers and area chairs) in the peer-review system.",
        "Help me polish this": "Many recent studies have probed status bias in the peer-review process of academic journals and conferences. In this article, we investigated the association between author metadata and area chairs' final decisions (Accept/Reject) using our compiled database of 5,313 borderline submissions to the International Conference on Learning Representations (ICLR) from 2017 to 2022.\n\nWe carefully defined elements in a cause-and-effect analysis, including the treatment and its timing, pre-treatment variables, potential outcomes and causal null hypothesis of interest, all in the context of study units being textual data and under Neyman and Rubin's potential outcomes (PO) framework.\n\nWe found some weak evidence that author metadata was associated with articles' final decisions. We also found that, under an additional stability assumption, borderline articles from high-ranking institutions (top-30% or top-20%) were less favored by area chairs compared to their matched counterparts.\n\nThe results were consistent in two different matched designs (odds ratio = 0.82 [95% CI: 0.67 to 1.00] in a first design and 0.83 [95% CI: 0.64 to 1.07] in a strengthened design).\n\nWe discussed how to interpret these results in the context of multiple interactions between a study unit and different agents (reviewers and area chairs) in the peer-review system.",
        "Rewrite this for me": "Many recent studies have looked into status bias in the peer-review process of academic journals and conferences. In this article, we investigated the association between author metadata and area chairs' final decisions (Accept/Reject) using our compiled database of 5,313 borderline submissions to the International Conference on Learning Representations (ICLR) from 2017 to 2022.\n\nWe carefully defined elements in a cause-and-effect analysis, including the treatment and its timing, pre-treatment variables, potential outcomes and causal null hypothesis of interest, all in the context of study units being textual data and under Neyman and Rubin's potential outcomes (PO) framework.\n\nWe found some weak evidence that author metadata was associated with articles' final decisions. We also found that, under an additional stability assumption, borderline articles from high-ranking institutions (top-30% or top-20%) were less favored by area chairs compared to their matched counterparts.\n\nThe results were consistent in two different matched designs (odds ratio = 0.82 [95% CI: 0.67 to 1.00] in a first design and 0.83 [95% CI: 0.64 to 1.07] in a strengthened design).\n\nWe discussed how to interpret these results in the context of multiple interactions between a study unit and different agents (reviewers and area chairs) in the peer-review system.",
        "Make this fluent while doing minimal change": "Many recent studies have probed status bias in the peer-review process of academic journals and conferences. In this article, we investigated the association between author metadata and area chairs' final decisions (Accept/Reject) using our compiled database of 5,313 borderline submissions to the International Conference on Learning Representations (ICLR) from 2017 to 2022. We carefully defined elements in a cause-and-effect analysis, including the treatment and its timing, pre-treatment variables, potential outcomes and causal null hypothesis of interest, all in the context of study units being textual data and under Neyman and Rubin's potential outcomes (PO) framework. We found some weak evidence that author metadata was associated with articles' final decisions. We also found that, under an additional stability assumption, borderline articles from high-ranking institutions (top-30% or top-20%) were less favored by area chairs compared to their matched counterparts. The results were consistent in two different matched designs (odds ratio = 0.82 [95% CI: 0.67 to 1.00] in a first design and 0.83 [95% CI: 0.64 to 1.07] in a strengthened design). We discussed how to interpret these results in the context of multiple interactions between a study unit and different agents (reviewers and area chairs) in the peer-review system.",
        "Refine this for me please": "In this article, we investigate the association between author metadata and area chairs' final decisions (Accept/Reject) using our compiled database of 5,313 borderline submissions to the International Conference on Learning Representations (ICLR) from 2017 to 2022. We find some weak evidence that author metadata was associated with articles' final decisions. We also find that, under an additional stability assumption, borderline articles from high-ranking institutions (top-30% or top-20%) were less favored by area chairs compared to their matched counterparts.",
        "Concise this for me and keep all the information": "Many studies have investigated status bias in the peer-review process of academic journals and conferences. This article investigates the association between author metadata and area chairs' final decisions (Accept/Reject) using a compiled database of 5,313 borderline submissions to the International Conference on Learning Representations (ICLR) from 2017 to 2022.\n\nThe study found some weak evidence that author metadata was associated with articles' final decisions. The study also found that, under an additional stability assumption, borderline articles from high-ranking institutions (top-30% or top-20%) were less favored by area chairs compared to their matched counterparts.",
        "Improve this in GPT way": "Many recent studies have probed status bias in the peer-review process of academic journals and conferences. In this article, we investigated the association between author metadata and area chairs' final decisions (Accept/Reject) using our compiled database of 5,313 borderline submissions to the International Conference on Learning Representations (ICLR) from 2017 to 2022. We carefully defined elements in a cause-and-effect analysis, including the treatment and its timing, pre-treatment variables, potential outcomes and causal null hypothesis of interest, all in the context of study units being textual data and under Neyman and Rubin's potential outcomes (PO) framework. We found some weak evidence that author metadata was associated with articles' final decisions. We also found that, under an additional stability assumption, borderline articles from high-ranking institutions (top-30% or top-20%) were less favored by area chairs compared to their matched counterparts. The results were consistent in two different matched designs (odds ratio = 0.82 [95% CI: 0.67 to 1.00] in a first design and 0.83 [95% CI: 0.64 to 1.07] in a strengthened design). We discussed how to interpret these results in the context of multiple interactions between a study unit and different agents (reviewers and area chairs) in the peer-review system."
    },
    {
        "input": "We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack.",
        "Revise this with your best effort": "We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
        "Help me polish this": "We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
        "Rewrite this for me": "We introduce a new method for training neural networks, called Deep Variational Information Bottleneck (Deep VIB). This method is based on a variational approximation to the information bottleneck of Tishby et al. (1999). The key advantage of Deep VIB is that it allows us to use the reparameterization trick for efficient training. We show that models trained with Deep VIB outperform those trained with other regularization methods, in terms of generalization performance and robustness to adversarial attack.",
        "Make this fluent while doing minimal change": "We present a variational approximation to the information bottleneck of\nTishby et al. (1999) using a neural network. This variational approach allows us to parameterize the\ninformation bottleneck model and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack.",
        "Refine this for me please": "We present a new method for training neural networks, called Deep VIB, which outperforms other methods in terms of generalization performance and robustness to adversarial attack. Deep VIB is a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training.",
        "Concise this for me and keep all the information": "We present a variational approximation to the information bottleneck of Tishby et al. (1999) using a neural network. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
        "Improve this in GPT way": "We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack."
    },
    {
        "input": "Attention networks have proven to be an effective approach for embedding\ncategorical inference within a deep neural network. However, for many tasks we\nmay want to model richer structural dependencies without abandoning end-to-end\ntraining. In this work, we experiment with incorporating richer structural\ndistributions, encoded using graphical models, within deep networks. We show\nthat these structured attention networks are simple extensions of the basic\nattention procedure, and that they allow for extending attention beyond the\nstandard soft-selection approach, such as attending to partial segmentations or\nto subtrees. We experiment with two different classes of structured attention\nnetworks: a linear-chain conditional random field and a graph-based parsing\nmodel, and describe how these models can be practically implemented as neural\nnetwork layers. Experiments show that this approach is effective for\nincorporating structural biases, and structured attention networks outperform\nbaseline attention models on a variety of synthetic and real tasks: tree\ntransduction, neural machine translation, question answering, and natural\nlanguage inference. We further find that models trained in this way learn\ninteresting unsupervised hidden representations that generalize simple\nattention.",
        "Revise this with your best effort": "Attention networks have been shown to be an effective way to embed categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",
        "Help me polish this": "We experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",
        "Rewrite this for me": "Attention networks have been shown to be an effective way of embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",
        "Make this fluent while doing minimal change": "Attention networks have been shown to be an effective way of embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",
        "Refine this for me please": "Attention networks have been shown to be an effective way of embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",
        "Concise this for me and keep all the information": "Attention networks are a way of embedding categorical inference within a deep neural network. This work explores incorporating richer structural distributions, encoded using graphical models, within deep networks. It is shown that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference.",
        "Improve this in GPT way": "We propose a method for incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention."
    },
    {
        "input": "We are proposing to use an ensemble of diverse specialists, where speciality\nis defined according to the confusion matrix. Indeed, we observed that for\nadversarial instances originating from a given class, labeling tend to be done\ninto a small subset of (incorrect) classes. Therefore, we argue that an\nensemble of specialists should be better able to identify and reject fooling\ninstances, with a high entropy (i.e., disagreement) over the decisions in the\npresence of adversaries. Experimental results obtained confirm that\ninterpretation, opening a way to make the system more robust to adversarial\nexamples through a rejection mechanism, rather than trying to classify them\nproperly at any cost.",
        "Revise this with your best effort": "We propose using an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. We observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.",
        "Help me polish this": "We propose using an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. We observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.",
        "Rewrite this for me": "We propose using an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. We observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.",
        "Make this fluent while doing minimal change": "We propose using an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. We observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.",
        "Refine this for me please": "We are proposing to use an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.",
        "Concise this for me and keep all the information": "We propose using an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. We argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.",
        "Improve this in GPT way": "We propose using an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost."
    },
    {
        "input": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our\nmethod explicitly models the phrase structures in output sequences using\nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence\nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we\nintroduce a new layer to perform (soft) local reordering of input sequences.\nDifferent from existing neural machine translation (NMT) approaches, NPMT does\nnot use attention-based decoding mechanisms. Instead, it directly outputs\nphrases in a sequential order and can decode in linear time. Our experiments\nshow that NPMT achieves superior performances on IWSLT 2014\nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine\ntranslation tasks compared with strong NMT baselines. We also observe that our\nmethod produces meaningful phrases in output languages.",
        "Revise this with your best effort": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences.\n\nDifferent from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Help me polish this": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences.\n\nDifferent from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Rewrite this for me": "In this paper, we introduce Neural Phrase-based Machine Translation (NPMT). Our method uses Sleep-WAke Networks (SWAN) to explicitly model phrase structures in output sequences. To address the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. NPMT does not use attention-based decoding mechanisms; instead, it directly outputs phrases in a sequential order, allowing for linear-time decoding. Our experiments show that NPMT outperforms strong NMT baselines on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks. We also observe that our method produces meaningful phrases in output languages.",
        "Make this fluent while doing minimal change": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Refine this for me please": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Concise this for me and keep all the information": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines.",
        "Improve this in GPT way": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages."
    },
    {
        "input": "We present LR-GAN: an adversarial image generation model which takes scene\nstructure and context into account. Unlike previous generative adversarial\nnetworks (GANs), the proposed GAN learns to generate image background and\nforegrounds separately and recursively, and stitch the foregrounds on the\nbackground in a contextually relevant manner to produce a complete natural\nimage. For each foreground, the model learns to generate its appearance, shape\nand pose. The whole model is unsupervised, and is trained in an end-to-end\nmanner with gradient descent methods. The experiments demonstrate that LR-GAN\ncan generate more natural images with objects that are more human recognizable\nthan DCGAN.",
        "Revise this with your best effort": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitches the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
        "Help me polish this": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitches the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
        "Rewrite this for me": "We introduce LR-GAN: a generative adversarial model that takes scene structure and context into account. Unlike previous GANs, LR-GAN learns to generate image backgrounds and foregrounds separately and recursively. It then stitches the foregrounds onto the background in a contextually relevant manner to produce a complete, natural image. For each foreground, the model learns to generate its appearance, shape, and pose. The entire model is unsupervised and trained in an end-to-end manner using gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
        "Make this fluent while doing minimal change": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitches the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape, and pose. The whole model is unsupervised and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
        "Refine this for me please": "The proposed GAN, LR-GAN, is able to generate images which take scene structure and context into account. Unlike previous GANs, LR-GAN learns to generate image background and foregrounds separately and recursively. This allows for the foregrounds to be placed in a contextually relevant manner on the background, producing a complete natural image. The model is unsupervised and is trained in an end-to-end manner using gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
        "Concise this for me and keep all the information": "We present LR-GAN, an adversarial image generation model which takes scene structure and context into account. Unlike previous GANs, LR-GAN learns to generate image background and foregrounds separately and recursively, and stitches the foregrounds on the background in a contextually relevant manner to produce a complete natural image. The whole model is unsupervised and trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
        "Improve this in GPT way": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitches the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape, and pose. The whole model is unsupervised and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN."
    },
    {
        "input": "We describe a simple scheme that allows an agent to learn about its\nenvironment in an unsupervised manner. Our scheme pits two versions of the same\nagent, Alice and Bob, against one another. Alice proposes a task for Bob to\ncomplete; and then Bob attempts to complete the task. In this work we will\nfocus on two kinds of environments: (nearly) reversible environments and\nenvironments that can be reset. Alice will \"propose\" the task by doing a\nsequence of actions and then Bob must undo or repeat them, respectively. Via an\nappropriate reward structure, Alice and Bob automatically generate a curriculum\nof exploration, enabling unsupervised training of the agent. When Bob is\ndeployed on an RL task within the environment, this unsupervised training\nreduces the number of supervised episodes needed to learn, and in some cases\nconverges to a higher reward.",
        "Revise this with your best effort": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.",
        "Help me polish this": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.",
        "Rewrite this for me": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.",
        "Make this fluent while doing minimal change": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.",
        "Refine this for me please": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.",
        "Concise this for me and keep all the information": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.",
        "Improve this in GPT way": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward."
    },
    {
        "input": "Maximum entropy modeling is a flexible and popular framework for formulating\nstatistical models given partial knowledge. In this paper, rather than the\ntraditional method of optimizing over the continuous density directly, we learn\na smooth and invertible transformation that maps a simple distribution to the\ndesired maximum entropy distribution. Doing so is nontrivial in that the\nobjective being maximized (entropy) is a function of the density itself. By\nexploiting recent developments in normalizing flow networks, we cast the\nmaximum entropy problem into a finite-dimensional constrained optimization, and\nsolve the problem by combining stochastic optimization with the augmented\nLagrangian method. Simulation results demonstrate the effectiveness of our\nmethod, and applications to finance and computer vision show the flexibility\nand accuracy of using maximum entropy flow networks.",
        "Revise this with your best effort": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution, rather than the traditional method of optimizing over the continuous density directly. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. However, by exploiting recent developments in normalizing flow networks, we are able to cast the maximum entropy problem into a finite-dimensional constrained optimization. We solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.",
        "Help me polish this": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution, rather than the traditional method of optimizing over the continuous density directly. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. However, by exploiting recent developments in normalizing flow networks, we are able to cast the maximum entropy problem into a finite-dimensional constrained optimization. We solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.",
        "Rewrite this for me": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution, rather than the traditional method of optimizing over the continuous density directly. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. However, by exploiting recent developments in normalizing flow networks, we are able to cast the maximum entropy problem into a finite-dimensional constrained optimization. We then solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.",
        "Make this fluent while doing minimal change": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution, rather than the traditional method of optimizing over the continuous density directly. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.",
        "Refine this for me please": "In this paper, we propose a new method for learning a maximum entropy distribution from data. Our method is based on normalizing flow networks, which are a type of neural network that can learn invertible transformations. We show that by using this method, we can cast the maximum entropy problem into a finite-dimensional constrained optimization problem, which can be solved using stochastic optimization and the augmented Lagrangian method. We demonstrate the effectiveness of our method through simulations, and show that it can be applied to finance and computer vision.",
        "Concise this for me and keep all the information": "In this paper, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution, rather than the traditional method of optimizing over the continuous density directly. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. However, by exploiting recent developments in normalizing flow networks, we are able to cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.",
        "Improve this in GPT way": "Maximum entropy modeling is a powerful and popular framework for formulating statistical models given partial knowledge. In this paper, we propose a novel method for learning a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks."
    },
    {
        "input": "With machine learning successfully applied to new daunting problems almost\nevery day, general AI starts looking like an attainable goal. However, most\ncurrent research focuses instead on important but narrow applications, such as\nimage classification or machine translation. We believe this to be largely due\nto the lack of objective ways to measure progress towards broad machine\nintelligence. In order to fill this gap, we propose here a set of concrete\ndesiderata for general AI, together with a platform to test machines on how\nwell they satisfy such desiderata, while keeping all further complexities to a\nminimum.",
        "Revise this with your best effort": "With machine learning being applied to more and more difficult problems on a daily basis, it seems as though general AI may become achievable. However, much of the current research is instead focused on more specific applications, such as image classification or machine translation. This is likely due to the lack of objective methods of measuring progress towards more general machine intelligence. In order to address this issue, we have proposed a set of specific goals for general AI, as well as a platform to test machines on how well they satisfy these goals. By keeping other complexities to a minimum, we hope to provide a more clear path towards general AI.",
        "Help me polish this": "With machine learning being applied to new and daunting problems on a daily basis, general AI starts to look like a realistic and attainable goal. However, the majority of current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe that this is largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose a set of concrete desiderata for general AI, along with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.",
        "Rewrite this for me": "As machine learning is applied to more and more difficult problems with success, the goal of general AI seems achievable. However, most research currently focuses on narrow applications such as image classification or machine translation. We believe this is largely due to the lack of objective ways to measure progress towards broad machine intelligence. To address this issue, we propose a set of concrete desiderata for general AI, along with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.",
        "Make this fluent while doing minimal change": "With machine learning being successfully applied to new daunting problems almost every day, general AI starts to look like an attainable goal. However, most current research instead focuses on important but narrow applications, such as image classification or machine translation. This is believed to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.",
        "Refine this for me please": "We believe that the lack of objective ways to measure progress towards broad machine intelligence is largely due to the focus on narrow applications, such as image classification or machine translation. In order to fill this gap, we propose a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.",
        "Concise this for me and keep all the information": "Most current research in machine learning focuses on narrow applications, such as image classification or machine translation, rather than on general AI. This is largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata.",
        "Improve this in GPT way": "We believe that the lack of objective ways to measure progress towards broad machine intelligence is a major obstacle to the development of general AI. In order to fill this gap, we propose a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata."
    },
    {
        "input": "Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference. They are also difficult to implement in popular\ndeep learning libraries, which are based on static data-flow graphs. We\nintroduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size. We further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature.",
        "Revise this with your best effort": "Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference.\n\nWe introduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size.\n\nWe further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature.",
        "Help me polish this": "Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference.\n\nWe introduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size.\n\nWe further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature.",
        "Rewrite this for me": "Neural networks that compute over graph structures are a natural fit for a variety of problems, including natural language processing (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs.\n\nWe introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.",
        "Make this fluent while doing minimal change": "Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference. They are also difficult to implement in popular\ndeep learning libraries, which are based on static data-flow graphs.\n\nWe introduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size. We further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature.",
        "Refine this for me please": "Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference.\n\nWe introduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size. We further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature.",
        "Concise this for me and keep all the information": "Neural networks that compute over graph structures are a natural fit for a variety of problems, including natural language and cheminformatics. However, these networks do not directly support batched training or inference, and are difficult to implement in popular deep learning libraries.\n\nWe introduce a technique called dynamic batching, which allows us to create static graphs that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models.\n\nUsing the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.",
        "Improve this in GPT way": "We introduce a technique called dynamic batching, which allows us to create static graphs that emulate dynamic computation graphs of arbitrary shape and size. This technique not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. This allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature."
    },
    {
        "input": "Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM.",
        "Revise this with your best effort": "Although deep learning models have been effective in solving natural language processing problems, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper, we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",
        "Help me polish this": "Although deep learning models have been effective in solving natural language processing problems, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns.\n\nIn this paper, we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",
        "Rewrite this for me": "Although deep learning models have been effective in solving natural language processing problems, the mechanism by which they come to their conclusions is often unclear. This lack of understanding of how the models work leads to them being treated as black boxes, which provides no insight into the underlying learned patterns. In this paper, we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",
        "Make this fluent while doing minimal change": "Although deep learning models have been effective in solving natural language processing problems, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper, we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",
        "Refine this for me please": "In this paper, we demonstrate a new approach for tracking the importance of a given input to a Long Short Term Memory network (LSTM) for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",
        "Concise this for me and keep all the information": "In this paper, we demonstrate a new approach for tracking the importance of a given input to a Long Short Term Memory network (LSTM) for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.",
        "Improve this in GPT way": "Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns.\n\nIn this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases.\n\nThis representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM."
    },
    {
        "input": "Deep reinforcement learning has achieved many impressive results in recent\nyears. However, tasks with sparse rewards or long horizons continue to pose\nsignificant challenges. To tackle these important problems, we propose a\ngeneral framework that first learns useful skills in a pre-training\nenvironment, and then leverages the acquired skills for learning faster in\ndownstream tasks. Our approach brings together some of the strengths of\nintrinsic motivation and hierarchical methods: the learning of useful skill is\nguided by a single proxy reward, the design of which requires very minimal\ndomain knowledge about the downstream tasks. Then a high-level policy is\ntrained on top of these skills, providing a significant improvement of the\nexploration and allowing to tackle sparse rewards in the downstream tasks. To\nefficiently pre-train a large span of skills, we use Stochastic Neural Networks\ncombined with an information-theoretic regularizer. Our experiments show that\nthis combination is effective in learning a wide span of interpretable skills\nin a sample-efficient way, and can significantly boost the learning performance\nuniformly across a wide range of downstream tasks.",
        "Revise this with your best effort": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks.\n\nOur approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks.\n\nTo efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
        "Help me polish this": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks.\n\nOur approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks.\n\nTo efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
        "Rewrite this for me": "Deep reinforcement learning has made great strides in recent years, but tasks with either sparse rewards or long horizons remain difficult challenges. To address these issues, we propose a general framework that first learns useful skills in a pre-training environment, then uses those skills to accelerate learning in downstream tasks. Our approach combines the best of intrinsic motivation and hierarchical methods: learning useful skills is guided by a single proxy reward, which can be designed with very little domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement in exploration and the ability to tackle sparse rewards in the downstream tasks. To efficiently pre-train a wide range of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide range of interpretable skills in a sample-efficient way, and can significantly boost learning performance across a wide range of downstream tasks.",
        "Make this fluent while doing minimal change": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
        "Refine this for me please": "Our approach to deep reinforcement learning combines the strengths of intrinsic motivation and hierarchical methods by first learning useful skills in a pre-training environment, and then leveraging the acquired skills to learn faster in downstream tasks. Our approach is guided by a single proxy reward, which requires very minimal domain knowledge about the downstream tasks. Then, a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing us to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
        "Concise this for me and keep all the information": "Deep reinforcement learning has made great strides in recent years, but tasks with sparse rewards or long horizons remain difficult. We propose a framework that first learns useful skills in a pre-training environment, then uses those skills to learn faster in downstream tasks. Our approach combines the best of intrinsic motivation and hierarchical methods: learning useful skills is guided by a single proxy reward, which requires very little domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing us to tackle sparse rewards in the downstream tasks. To efficiently pre-train a wide range of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
        "Improve this in GPT way": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks.\n\nOur approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks.\n\nTo efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks."
    },
    {
        "input": "Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques.",
        "Revise this with your best effort": "Deep generative models have achieved impressive success in recent years. However, there has been little effort to establish formal connections between the two most popular families of generative models: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). This paper aims to fill this gap by showing that GANs and VAEs can be unified through a new formulation. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of the classic wake-sleep algorithm. This unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables techniques to be transferred across research lines in a principled way. For example, we apply the importance weighting method from VAE literature to improve GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show the generality and effectiveness of the transferred techniques.",
        "Help me polish this": "Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques.",
        "Rewrite this for me": "Deep generative models have been achieving impressive success in recent years. Two emerging families of generative model learning, Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. \n\nThe paper interprets sample generation in GANs as performing posterior inference, and shows that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables transfer of techniques across research lines in a principled way. \n\nAs an example, the paper applies the importance weighting method in VAE literatures for improved GAN learning, and enhances VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.",
        "Make this fluent while doing minimal change": "Deep generative models have been achieving impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show the generality and effectiveness of the transferred techniques.",
        "Refine this for me please": "This paper establishes formal connections between GANs and VAEs, showing that they are both based on minimizing KL divergences. This provides a powerful tool for analyzing a diverse set of existing model variants, and enables transfer of techniques between research lines.",
        "Concise this for me and keep all the information": "This paper establishes formal connections between GANs and VAEs, showing that they involve minimizing KL divergences of respective posterior and inference distributions with opposite directions. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables transfer of techniques across research lines in a principled way.",
        "Improve this in GPT way": "Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms. However, this paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques."
    },
    {
        "input": "We consider the problem of detecting out-of-distribution images in neural\nnetworks. We propose ODIN, a simple and effective method that does not require\nany change to a pre-trained neural network. Our method is based on the\nobservation that using temperature scaling and adding small perturbations to\nthe input can separate the softmax score distributions between in- and\nout-of-distribution images, allowing for more effective detection. We show in a\nseries of experiments that ODIN is compatible with diverse network\narchitectures and datasets. It consistently outperforms the baseline approach\nby a large margin, establishing a new state-of-the-art performance on this\ntask. For example, ODIN reduces the false positive rate from the baseline 34.7%\nto 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is\n95%.",
        "Revise this with your best effort": "We propose a new method, ODIN, for detecting out-of-distribution images in neural networks. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task.",
        "Help me polish this": "We propose ODIN, a simple and effective method for detecting out-of-distribution images in neural networks. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%.",
        "Rewrite this for me": "We propose a new method, ODIN, for detecting out-of-distribution images in neural networks. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task.",
        "Make this fluent while doing minimal change": "We propose ODIN, a simple and effective method for detecting out-of-distribution images in neural networks that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%.",
        "Refine this for me please": "We propose a method, ODIN, for detecting out-of-distribution images in neural networks that is simple and effective. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets and outperforms the baseline approach by a large margin.",
        "Concise this for me and keep all the information": "We propose ODIN, a simple and effective method for detecting out-of-distribution images in neural networks. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task.",
        "Improve this in GPT way": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%."
    },
    {
        "input": "A framework is presented for unsupervised learning of representations based\non infomax principle for large-scale neural populations. We use an asymptotic\napproximation to the Shannon's mutual information for a large neural population\nto demonstrate that a good initial approximation to the global\ninformation-theoretic optimum can be obtained by a hierarchical infomax method.\nStarting from the initial solution, an efficient algorithm based on gradient\ndescent of the final objective function is proposed to learn representations\nfrom the input datasets, and the method works for complete, overcomplete, and\nundercomplete bases. As confirmed by numerical experiments, our method is\nrobust and highly efficient for extracting salient features from input\ndatasets. Compared with the main existing methods, our algorithm has a distinct\nadvantage in both the training speed and the robustness of unsupervised\nrepresentation learning. Furthermore, the proposed method is easily extended to\nthe supervised or unsupervised model for training deep structure networks.",
        "Revise this with your best effort": "A framework is presented for unsupervised learning of representations based on the infomax principle for large-scale neural populations. We use an asymptotic approximation to Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method.\n\nStarting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets. The method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets.\n\nCompared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",
        "Help me polish this": "A framework is presented for unsupervised learning of representations based on the infomax principle for large-scale neural populations. We use an asymptotic approximation to Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method.\n\nStarting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets. This method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets.\n\nCompared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",
        "Rewrite this for me": "A framework is presented for unsupervised learning of representations based on the infomax principle for large-scale neural populations. We use an asymptotic approximation to Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets. The proposed method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning.",
        "Make this fluent while doing minimal change": "A framework is presented for unsupervised learning of representations based on the infomax principle for large-scale neural populations. We use an asymptotic approximation to Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method.\n\nStarting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets. The method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets.\n\nCompared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",
        "Refine this for me please": "A framework is presented for unsupervised learning of representations based on the infomax principle for large-scale neural populations. We use an asymptotic approximation to Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method.\n\nStarting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets. The proposed method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning.",
        "Concise this for me and keep all the information": "A framework is presented for unsupervised learning of representations based on the infomax principle for large-scale neural populations. We use an asymptotic approximation to Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets. The proposed method is robust and highly efficient for extracting salient features from input datasets.",
        "Improve this in GPT way": "A framework is presented for unsupervised learning of representations based on the infomax principle for large-scale neural populations. We use an asymptotic approximation to Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method.\n\nStarting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets. This method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks."
    },
    {
        "input": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in\nsequence modeling tasks. However, training RNNs on long sequences often face\nchallenges like slow inference, vanishing gradients and difficulty in capturing\nlong term dependencies. In backpropagation through time settings, these issues\nare tightly coupled with the large, sequential computational graph resulting\nfrom unfolding the RNN in time. We introduce the Skip RNN model which extends\nexisting RNN models by learning to skip state updates and shortens the\neffective size of the computational graph. This model can also be encouraged to\nperform fewer state updates through a budget constraint. We evaluate the\nproposed model on various tasks and show how it can reduce the number of\nrequired RNN updates while preserving, and sometimes even improving, the\nperformance of the baseline RNN models. Source code is publicly available at\nhttps://imatge-upc.github.io/skiprnn-2017-telecombcn/ .",
        "Revise this with your best effort": "Recurrent Neural Networks (RNNs) have continued to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often faces challenges like slow inference, vanishing gradients, and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. The Skip RNN model extends existing RNN models by learning to skip state updates and shorten the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. The proposed model has been evaluated on various tasks and has shown how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.",
        "Help me polish this": "Recurrent Neural Networks (RNNs) have continued to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often faces challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. \n\nThe Skip RNN model extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. The proposed model has been evaluated on various tasks and has shown how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. \n\nSource code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.",
        "Rewrite this for me": "Recurrent Neural Networks (RNNs) have continued to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often faces challenges like slow inference, vanishing gradients, and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. The Skip RNN model extends existing RNN models by learning to skip state updates and shorten the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. The proposed model has been evaluated on various tasks and has shown how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.",
        "Make this fluent while doing minimal change": "Recurrent Neural Networks (RNNs) continue to demonstrate excellent performance in sequence modeling tasks. However, training RNNs on long sequences often faces challenges like slow inference, vanishing gradients, and difficulty in capturing long-term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. The Skip RNN model introduced herein extends existing RNN models by learning to skip state updates and shorten the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. The proposed model is evaluated on various tasks and shown how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.",
        "Refine this for me please": "Recurrent Neural Networks (RNNs) are a type of neural network that is good at sequence modeling tasks. However, training RNNs on long sequences often faces challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. The Skip RNN model is designed to address these issues by learning to skip state updates and shortening the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. The Skip RNN model is evaluated on various tasks and shown to reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.",
        "Concise this for me and keep all the information": "Recurrent Neural Networks (RNNs) are good at sequence modeling tasks. However, training RNNs on long sequences often faces issues like slow inference, vanishing gradients and difficulty in capturing long term dependencies. The Skip RNN model extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint.",
        "Improve this in GPT way": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in\nsequence modeling tasks. However, training RNNs on long sequences often face\nchallenges like slow inference, vanishing gradients and difficulty in capturing\nlong term dependencies. In backpropagation through time settings, these issues\nare tightly coupled with the large, sequential computational graph resulting\nfrom unfolding the RNN in time.\n\nWe introduce the Skip RNN model which extends\nexisting RNN models by learning to skip state updates and shortens the\neffective size of the computational graph. This model can also be encouraged to\nperform fewer state updates through a budget constraint. We evaluate the\nproposed model on various tasks and show how it can reduce the number of\nrequired RNN updates while preserving, and sometimes even improving, the\nperformance of the baseline RNN models."
    },
    {
        "input": "Restart techniques are common in gradient-free optimization to deal with\nmultimodal functions. Partial warm restarts are also gaining popularity in\ngradient-based optimization to improve the rate of convergence in accelerated\ngradient schemes to deal with ill-conditioned functions. In this paper, we\npropose a simple warm restart technique for stochastic gradient descent to\nimprove its anytime performance when training deep neural networks. We\nempirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where\nwe demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively.\nWe also demonstrate its advantages on a dataset of EEG recordings and on a\ndownsampled version of the ImageNet dataset. Our source code is available at\nhttps://github.com/loshchil/SGDR",
        "Revise this with your best effort": "Restarting is a common technique in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR.",
        "Help me polish this": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR.",
        "Rewrite this for me": "Restarting is a common technique in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR.",
        "Make this fluent while doing minimal change": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR.",
        "Refine this for me please": "In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset.",
        "Concise this for me and keep all the information": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset.",
        "Improve this in GPT way": "We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset."
    },
    {
        "input": "Policy gradient methods have achieved remarkable successes in solving\nchallenging reinforcement learning problems. However, it still often suffers\nfrom the large variance issue on policy gradient estimation, which leads to\npoor sample efficiency during training. In this work, we propose a control\nvariate method to effectively reduce variance for policy gradient methods.\nMotivated by the Stein's identity, our method extends the previous control\nvariate methods used in REINFORCE and advantage actor-critic by introducing\nmore general action-dependent baseline functions. Empirical studies show that\nour method significantly improves the sample efficiency of the state-of-the-art\npolicy gradient approaches.",
        "Revise this with your best effort": "Policy gradient methods have achieved remarkable successes in solving\nchallenging reinforcement learning problems. However, it still often suffers\nfrom the large variance issue on policy gradient estimation, which leads to\npoor sample efficiency during training. In this work, we propose a control\nvariate method to effectively reduce variance for policy gradient methods.\nMotivated by the Stein's identity, our method extends the previous control\nvariate methods used in REINFORCE and advantage actor-critic by introducing\nmore general action-dependent baseline functions. Empirical studies show that\nour method significantly improves the sample efficiency of the state-of-the-art\npolicy gradient approaches.",
        "Help me polish this": "Policy gradient methods have achieved great successes in solving difficult reinforcement learning problems. However, it often still suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.",
        "Rewrite this for me": "Policy gradient methods have had a lot of success in solving difficult reinforcement learning problems. However, they often still have a lot of variance in their policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Our method is motivated by Stein's identity and extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.",
        "Make this fluent while doing minimal change": "Policy gradient methods have achieved remarkable successes in solving\nchallenging reinforcement learning problems. However, it still often suffers\nfrom the large variance issue on policy gradient estimation, which leads to\npoor sample efficiency during training. In this work, we propose a control\nvariate method to effectively reduce variance for policy gradient methods.\nMotivated by the Stein's identity, our method extends the previous control\nvariate methods used in REINFORCE and advantage actor-critic by introducing\nmore general action-dependent baseline functions. Empirical studies show that\nour method significantly improves the sample efficiency of the state-of-the-art\npolicy gradient approaches.",
        "Refine this for me please": "Policy gradient methods have achieved many successes in solving difficult reinforcement learning problems. However, a common issue is that these methods often have a large variance in policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Our method is motivated by the Stein's identity and extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.",
        "Concise this for me and keep all the information": "Policy gradient methods are a type of reinforcement learning that have been successful in solving difficult problems. However, they often have a lot of variance, which makes them inefficient when training. This work proposes a control variate method to reduce variance and improve efficiency.",
        "Improve this in GPT way": "Policy gradient methods have achieved remarkable successes in solving\nchallenging reinforcement learning problems. However, it still often suffers\nfrom the large variance issue on policy gradient estimation, which leads to\npoor sample efficiency during training. In this work, we propose a control\nvariate method to effectively reduce variance for policy gradient methods.\nMotivated by the Stein's identity, our method extends the previous control\nvariate methods used in REINFORCE and advantage actor-critic by introducing\nmore general action-dependent baseline functions. Empirical studies show that\nour method significantly improves the sample efficiency of the state-of-the-art\npolicy gradient approaches."
    },
    {
        "input": "Skip connections made the training of very deep networks possible and have\nbecome an indispensable component in a variety of neural architectures. A\ncompletely satisfactory explanation for their success remains elusive. Here, we\npresent a novel explanation for the benefits of skip connections in training\nvery deep networks. The difficulty of training deep networks is partly due to\nthe singularities caused by the non-identifiability of the model. Several such\nsingularities have been identified in previous works: (i) overlap singularities\ncaused by the permutation symmetry of nodes in a given layer, (ii) elimination\nsingularities corresponding to the elimination, i.e. consistent deactivation,\nof nodes, (iii) singularities generated by the linear dependence of the nodes.\nThese singularities cause degenerate manifolds in the loss landscape that slow\ndown learning. We argue that skip connections eliminate these singularities by\nbreaking the permutation symmetry of nodes, by reducing the possibility of node\nelimination and by making the nodes less linearly dependent. Moreover, for\ntypical initializations, skip connections move the network away from the\n\"ghosts\" of these singularities and sculpt the landscape around them to\nalleviate the learning slow-down. These hypotheses are supported by evidence\nfrom simplified models, as well as from experiments with deep networks trained\non real-world datasets.",
        "Revise this with your best effort": "Skip connections have made it possible to train very deep networks, and have become an essential component of many different neural architectures. The reason for their success is not fully understood, but we have a new explanation.\n\nThe difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. These singularities can slow down learning by creating degenerate manifolds in the loss landscape.\n\nWe argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination, and by making the nodes less linearly dependent. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.",
        "Help me polish this": "Skip connections have become an indispensable component in a variety of neural architectures because they make training very deep networks possible. However, a completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. \n\nThe difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. \n\nWe argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.",
        "Rewrite this for me": "Skip connections have made it possible to train very deep networks, and have become an essential component of many different neural architectures. However, the reason for their success is not fully understood. In this paper, we present a new explanation for the benefits of skip connections in training very deep networks.\n\nThe difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes.\n\nThese singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down.\n\nThese hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.",
        "Make this fluent while doing minimal change": "Skip connections have made it possible to train very deep networks, and have become an essential component of many different neural architectures. The full reason for their success is not yet known, but we have a new explanation to offer.\n\nThe main difficulty in training deep networks is due to the presence of singularities. These are points in the loss landscape where the network gets stuck and learning slows down. There are three different kinds of singularities:\n\n1. Overlap singularities, caused by the permutation symmetry of nodes in a given layer.\n2. Elimination singularities, corresponding to the consistent deactivation of nodes.\n3. Singularities generated by the linear dependence of nodes.\n\nSkip connections eliminate these singularities by breaking the permutation symmetry of nodes, reducing the possibility of node elimination, and making nodes less linearly dependent. As a result, the network is able to learn more quickly.",
        "Refine this for me please": "Skip connections are beneficial for training deep networks because they eliminate singularities that slow down learning. These singularities include overlap, elimination, and linear dependence.",
        "Concise this for me and keep all the information": "Skip connections are beneficial for training deep networks because they eliminate singularities that slow down learning.",
        "Improve this in GPT way": "Skip connections are beneficial for training deep networks because they eliminate singularities that slow down learning. These singularities include overlap, elimination, and linear dependence. Skip connections also move the network away from the \"ghosts\" of these singularities, making the landscape around them more conducive to learning."
    },
    {
        "input": "We have tried to reproduce the results of the paper \"Natural Language\nInference over Interaction Space\" submitted to ICLR 2018 conference as part of\nthe ICLR 2018 Reproducibility Challenge. Initially, we were not aware that the\ncode was available, so we started to implement the network from scratch. We\nhave evaluated our version of the model on Stanford NLI dataset and reached\n86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The\nmain difference, as we understand it, comes from the optimizers and the way\nmodel selection is performed.",
        "Revise this with your best effort": "We have tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" submitted to ICLR 2018 conference as part of the ICLR 2018 Reproducibility Challenge. Initially, we were not aware that the code was available, so we started to implement the network from scratch. We have evaluated our version of the model on Stanford NLI dataset and reached 86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The main difference, as we understand it, comes from the optimizers and the way model selection is performed.",
        "Help me polish this": "We have tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" submitted to ICLR 2018 conference as part of the ICLR 2018 Reproducibility Challenge. Initially, we were not aware that the code was available, so we started to implement the network from scratch. We have evaluated our version of the model on Stanford NLI dataset and reached 86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The main difference, as we understand it, comes from the optimizers and the way model selection is performed.",
        "Rewrite this for me": "We tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" submitted to ICLR 2018 conference as part of the ICLR 2018 Reproducibility Challenge. We were not aware that the code was available, so we started to implement the network from scratch. We evaluated our version of the model on Stanford NLI dataset and reached 86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The main difference, as we understand it, comes from the optimizers and the way model selection is performed.",
        "Make this fluent while doing minimal change": "We have tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" submitted to ICLR 2018 conference as part of the ICLR 2018 Reproducibility Challenge. Initially, we were not aware that the code was available, so we started to implement the network from scratch. However, we have since found the code and have been able to evaluate our version of the model on the Stanford NLI dataset. We have reached 86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The main difference, as we understand it, comes from the optimizers and the way model selection is performed.",
        "Refine this for me please": "We tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" submitted to ICLR 2018 conference as part of the ICLR 2018 Reproducibility Challenge. We used the code that was available and we were able to reach 86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The main difference, as we understand it, comes from the optimizers and the way model selection is performed.",
        "Concise this for me and keep all the information": "We tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" submitted to ICLR 2018 conference as part of the ICLR 2018 Reproducibility Challenge. We used the code that was available and we evaluated our version of the model on Stanford NLI dataset. We reached 86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The main difference, as we understand it, comes from the optimizers and the way model selection is performed.",
        "Improve this in GPT way": "We have tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" submitted to ICLR 2018 conference as part of the ICLR 2018 Reproducibility Challenge. Initially, we were not aware that the code was available, so we started to implement the network from scratch. However, we have now obtained the code and are in the process of running it. We have evaluated our version of the model on Stanford NLI dataset and reached 86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The main difference, as we understand it, comes from the optimizers and the way model selection is performed."
    },
    {
        "input": "We have successfully implemented the \"Learn to Pay Attention\" model of\nattention mechanism in convolutional neural networks, and have replicated the\nresults of the original paper in the categories of image classification and\nfine-grained recognition.",
        "Revise this with your best effort": "We have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks, and have replicated the results of the original paper in the categories of image classification and fine-grained recognition.",
        "Help me polish this": "We have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks, and have replicated the results of the original paper in the categories of image classification and fine-grained recognition.",
        "Rewrite this for me": "We have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks, and have replicated the results of the original paper in the categories of image classification and fine-grained recognition.",
        "Make this fluent while doing minimal change": "We have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks, and have replicated the results of the original paper in the categories of image classification and fine-grained recognition.",
        "Refine this for me please": "We have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks, and have replicated the results of the original paper in the categories of image classification and fine-grained recognition.",
        "Concise this for me and keep all the information": "We have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks, and have replicated the results of the original paper in the categories of image classification and fine-grained recognition.",
        "Improve this in GPT way": "We have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks, and have replicated the results of the original paper in the categories of image classification and fine-grained recognition."
    },
    {
        "input": "Computing universal distributed representations of sentences is a fundamental\ntask in natural language processing. We propose a method to learn such\nrepresentations by encoding the suffixes of word sequences in a sentence and\ntraining on the Stanford Natural Language Inference (SNLI) dataset. We\ndemonstrate the effectiveness of our approach by evaluating it on the SentEval\nbenchmark, improving on existing approaches on several transfer tasks.",
        "Revise this with your best effort": "Computing universal distributed representations of sentences is a fundamental task in natural language processing. We propose a method to learn such representations by encoding the suffixes of word sequences in a sentence and training on the Stanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness of our approach by evaluating it on the SentEval benchmark, improving on existing approaches on several transfer tasks.",
        "Help me polish this": "Computing universal distributed representations of sentences is a fundamental task in natural language processing. We propose a method to learn such representations by encoding the suffixes of word sequences in a sentence and training on the Stanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness of our approach by evaluating it on the SentEval benchmark, improving on existing approaches on several transfer tasks.",
        "Rewrite this for me": "We propose a method to learn universal distributed representations of sentences by encoding the suffixes of word sequences in a sentence and training on the Stanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness of our approach by evaluating it on the SentEval benchmark, improving on existing approaches on several transfer tasks.",
        "Make this fluent while doing minimal change": "Computing universal distributed representations of sentences is a fundamental task in natural language processing. We propose a method to learn such representations by encoding the suffixes of word sequences in a sentence and training on the Stanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness of our approach by evaluating it on the SentEval benchmark, and find that it improves on existing approaches on several transfer tasks.",
        "Refine this for me please": "Computing universal distributed representations of sentences is a fundamental task in natural language processing. We propose a method to learn such representations by encoding the suffixes of word sequences in a sentence and training on the Stanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness of our approach by evaluating it on the SentEval benchmark, improving on existing approaches on several transfer tasks.",
        "Concise this for me and keep all the information": "We propose a method to learn universal distributed representations of sentences by encoding the suffixes of word sequences in a sentence and training on the Stanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness of our approach by evaluating it on the SentEval benchmark, improving on existing approaches on several transfer tasks.",
        "Improve this in GPT way": "We propose a method to learn universal distributed representations of sentences by encoding the suffixes of word sequences in a sentence and training on the Stanford Natural Language Inference (SNLI) dataset. We demonstrate the effectiveness of our approach by evaluating it on the SentEval benchmark, improving on existing approaches on several transfer tasks."
    },
    {
        "input": "In many neural models, new features as polynomial functions of existing ones\nare used to augment representations. Using the natural language inference task\nas an example, we investigate the use of scaled polynomials of degree 2 and\nabove as matching features. We find that scaling degree 2 features has the\nhighest impact on performance, reducing classification error by 5% in the best\nmodels.",
        "Revise this with your best effort": "In many neural models, new features are created by using polynomial functions of existing features. For example, in the natural language inference task, we investigated the use of scaled polynomials of degree 2 and above as matching features. We found that scaling degree 2 features had the highest impact on performance, reducing classification error by 5% in the best models.",
        "Help me polish this": "We investigate the use of scaled polynomials of degree 2 and above as matching features in the natural language inference task. We find that scaling degree 2 features has the highest impact on performance, reducing classification error by 5% in the best models.",
        "Rewrite this for me": "We investigate the use of scaled polynomials of degree 2 and above as matching features in the natural language inference task. We find that scaling degree 2 features has the highest impact on performance, reducing classification error by 5% in the best models.",
        "Make this fluent while doing minimal change": "We investigate the use of scaled polynomials of degree 2 and above as matching features in the natural language inference task. We find that scaling degree 2 features has the highest impact on performance, reducing classification error by 5% in the best models.",
        "Refine this for me please": "We investigate the use of scaled polynomials of degree 2 and above as matching features in the natural language inference task. We find that scaling degree 2 features has the highest impact on performance, reducing classification error by 5% in the best models.",
        "Concise this for me and keep all the information": "We investigate the use of scaled polynomials of degree 2 and above as matching features in the natural language inference task. We find that scaling degree 2 features has the highest impact on performance, reducing classification error by 5% in the best models.",
        "Improve this in GPT way": "We investigate the use of scaled polynomials of degree 2 and above as matching features for the natural language inference task. We find that scaling degree 2 features has the highest impact on performance, reducing classification error by 5% in the best models."
    },
    {
        "input": "We present a generalization bound for feedforward neural networks in terms of\nthe product of the spectral norm of the layers and the Frobenius norm of the\nweights. The generalization bound is derived using a PAC-Bayes analysis.",
        "Revise this with your best effort": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. This generalization bound is derived using a PAC-Bayes analysis, which provides a way to measure the quality of the bound.",
        "Help me polish this": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. This generalization bound is derived using a PAC-Bayes analysis.",
        "Rewrite this for me": "We provide a generalization bound for feedforward neural networks that is based on the product of the spectral norm of the layers and the Frobenius norm of the weights. This bound is derived using a PAC-Bayes analysis.",
        "Make this fluent while doing minimal change": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. This generalization bound is derived using a PAC-Bayes analysis.",
        "Refine this for me please": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis, which is a method of bounding the generalization error of a learning algorithm.",
        "Concise this for me and keep all the information": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights.",
        "Improve this in GPT way": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis. This generalization bound can be used to improve the performance of feedforward neural networks."
    },
    {
        "input": "In this work, we investigate Batch Normalization technique and propose its\nprobabilistic interpretation. We propose a probabilistic model and show that\nBatch Normalization maximazes the lower bound of its marginalized\nlog-likelihood. Then, according to the new probabilistic model, we design an\nalgorithm which acts consistently during train and test. However, inference\nbecomes computationally inefficient. To reduce memory and computational cost,\nwe propose Stochastic Batch Normalization -- an efficient approximation of\nproper inference procedure. This method provides us with a scalable uncertainty\nestimation technique. We demonstrate the performance of Stochastic Batch\nNormalization on popular architectures (including deep convolutional\narchitectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
        "Revise this with your best effort": "In this work, we investigate the Batch Normalization technique and propose a probabilistic interpretation of it. We propose a probabilistic model and show that Batch Normalization maximizes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during training and testing. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization - an efficient approximation of the proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
        "Help me polish this": "We investigate the Batch Normalization technique and propose a probabilistic interpretation of it. We show that Batch Normalization maximizes the lower bound of its marginalized log-likelihood. According to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization - an efficient approximation of the proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
        "Rewrite this for me": "We investigate the Batch Normalization technique and propose a probabilistic interpretation of it. We show that Batch Normalization maximizes the lower bound of its marginalized log-likelihood. According to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of the proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
        "Make this fluent while doing minimal change": "In this work, we investigate the Batch Normalization technique and propose a probabilistic interpretation of it. We propose a probabilistic model and show that Batch Normalization maximizes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of the proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
        "Refine this for me please": "We investigate the Batch Normalization technique and propose a probabilistic interpretation of it. We show that Batch Normalization maximizes the lower bound of the marginalized log-likelihood. Then, we design an algorithm which is consistent during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of the proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
        "Concise this for me and keep all the information": "We investigate Batch Normalization and propose a probabilistic interpretation of it. We also design an algorithm which is consistent during train and test, but is computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization.",
        "Improve this in GPT way": "In this work, we investigate the Batch Normalization technique and propose a probabilistic interpretation of it. We propose a probabilistic model and show that Batch Normalization maximizes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of the proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets."
    },
    {
        "input": "It is widely believed that the success of deep convolutional networks is\nbased on progressively discarding uninformative variability about the input\nwith respect to the problem at hand. This is supported empirically by the\ndifficulty of recovering images from their hidden representations, in most\ncommonly used network architectures. In this paper we show via a one-to-one\nmapping that this loss of information is not a necessary condition to learn\nrepresentations that generalize well on complicated problems, such as ImageNet.\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can\nbe fully inverted up to the final projection onto the classes, i.e. no\ninformation is discarded. Building an invertible architecture is difficult, for\none, because the local inversion is ill-conditioned, we overcome this by\nproviding an explicit inverse. An analysis of i-RevNets learned representations\nsuggests an alternative explanation for the success of deep networks by a\nprogressive contraction and linear separation with depth. To shed light on the\nnature of the model learned by the i-RevNet we reconstruct linear\ninterpolations between natural image representations.",
        "Revise this with your best effort": "It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper, we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet.\n\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned. We overcome this by providing an explicit inverse.\n\nAn analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet, we reconstruct linear interpolations between natural image representations.",
        "Help me polish this": "It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper, we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet.\n\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned. We overcome this by providing an explicit inverse.\n\nAn analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet, we reconstruct linear interpolations between natural image representations.",
        "Rewrite this for me": "The success of deep convolutional networks is believed to be based on the progressive discarding of uninformative variability about the input with respect to the problem at hand. This is supported by the difficulty of recovering images from their hidden representations in most commonly used network architectures. In this paper, we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes. This means that no information is discarded. Building an invertible architecture is difficult because the local inversion is ill-conditioned. We overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet, we reconstruct linear interpolations between natural image representations.",
        "Make this fluent while doing minimal change": "It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper, we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet, we reconstruct linear interpolations between natural image representations.",
        "Refine this for me please": "The success of deep convolutional networks is believed to be based on the progressive discarding of uninformative variability about the input with respect to the problem at hand. This is supported by the difficulty of recovering images from their hidden representations in most commonly used network architectures. In this paper, we show that this loss of information is not necessary to learn representations that generalize well on complicated problems, such as ImageNet. We build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet, we reconstruct linear interpolations between natural image representations.",
        "Concise this for me and keep all the information": "The success of deep convolutional networks is believed to be based on gradually discarding information about the input that is not relevant to the task at hand. This is supported by the difficulty of recovering images from hidden representations in most commonly used network architectures. In this paper, we show that this loss of information is not necessary to learn representations that generalize well on complicated tasks such as ImageNet classification. We do this by constructing the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, but we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks: a progressive contraction and linear separation with depth. We conclude by reconstructing linear interpolations between natural image representations.",
        "Improve this in GPT way": "It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper, we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet.\n\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse.\n\nAn analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet, we reconstruct linear interpolations between natural image representations."
    },
    {
        "input": "Deep latent variable models are powerful tools for representation learning.\nIn this paper, we adopt the deep information bottleneck model, identify its\nshortcomings and propose a model that circumvents them. To this end, we apply a\ncopula transformation which, by restoring the invariance properties of the\ninformation bottleneck method, leads to disentanglement of the features in the\nlatent space. Building on that, we show how this transformation translates to\nsparsity of the latent space in the new model. We evaluate our method on\nartificial and real data.",
        "Revise this with your best effort": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model and identify its shortcomings. We then propose a model that circumvents these shortcomings by applying a copula transformation. This transformation leads to disentanglement of the features in the latent space, which in turn leads to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",
        "Help me polish this": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model and identify its shortcomings. We then propose a model that circumvents them by applying a copula transformation. This transformation leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",
        "Rewrite this for me": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model and identify its shortcomings. We then propose a model that circumvents them. To do this, we apply a copula transformation which leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",
        "Make this fluent while doing minimal change": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model and identify its shortcomings. We then propose a model that circumvents them by applying a copula transformation. This transformation restores the invariance properties of the information bottleneck method and leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",
        "Refine this for me please": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model and identify its shortcomings. We then propose a model that circumvents these shortcomings by applying a copula transformation. This transformation leads to disentanglement of the features in the latent space, which in turn leads to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",
        "Concise this for me and keep all the information": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model and identify its shortcomings. We then propose a model that circumvents them by applying a copula transformation. This transformation leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",
        "Improve this in GPT way": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model and identify its shortcomings. We then propose a model that circumvents them by applying a copula transformation. This transformation restores the invariance properties of the information bottleneck method and leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data."
    },
    {
        "input": "We introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with\na simplified set of equations that achieves comparable accuracy, while training\nfaster. We evaluate both models on CLEVR and CoGenT, and show that, transfer\nlearning with fine-tuning results in a 15 point increase in accuracy, matching\nthe state of the art. Finally, in contrast, we demonstrate that improper\nfine-tuning can actually reduce a model's accuracy as well.",
        "Revise this with your best effort": "We introduce a variant of the MAC model that is simpler and achieves comparable accuracy. We evaluate both models on CLEVR and CoGenT, and show that transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Finally, we demonstrate that improper fine-tuning can actually reduce a model's accuracy.",
        "Help me polish this": "We introduce a variant of the MAC model that is simpler and achieves comparable accuracy, while training faster. We evaluate both models on CLEVR and CoGenT, and show that transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Finally, we demonstrate that improper fine-tuning can actually reduce a model's accuracy.",
        "Rewrite this for me": "We introduce a variant of the MAC model that is simpler and achieves comparable accuracy, while training faster. We evaluate both models on CLEVR and CoGenT, and show that transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Finally, we demonstrate that improper fine-tuning can actually reduce a model's accuracy.",
        "Make this fluent while doing minimal change": "We introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with\nsimplified set of equations that achieves comparable accuracy, while training\nfaster. We evaluate both models on CLEVR and CoGenT, and show that transfer\nlearning with fine-tuning results in a 15 point increase in accuracy, matching\nthe state of the art. Finally, in contrast, we demonstrate that improper\nfine-tuning can actually reduce a model's accuracy.",
        "Refine this for me please": "We introduce a variant of the MAC model that is simpler and trains faster, while still achieving comparable accuracy. We evaluate both models on CLEVR and CoGenT, and show that transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Finally, we demonstrate that improper fine-tuning can actually reduce a model's accuracy.",
        "Concise this for me and keep all the information": "We introduce a variant of the MAC model that is simpler and trains faster, while still achieving comparable accuracy. We evaluate both models on CLEVR and CoGenT, and show that transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Finally, we demonstrate that improper fine-tuning can actually reduce a model's accuracy.",
        "Improve this in GPT way": "We introduce a variant of the MAC model that is simpler and trains faster, while still achieving comparable accuracy. We evaluate both models on CLEVR and CoGenT, and show that transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Finally, we demonstrate that improper fine-tuning can actually reduce a model's accuracy."
    },
    {
        "input": "Adaptive Computation Time for Recurrent Neural Networks (ACT) is one of the\nmost promising architectures for variable computation. ACT adapts to the input\nsequence by being able to look at each sample more than once, and learn how\nmany times it should do it. In this paper, we compare ACT to Repeat-RNN, a\nnovel architecture based on repeating each sample a fixed number of times. We\nfound surprising results, where Repeat-RNN performs as good as ACT in the\nselected tasks. Source code in TensorFlow and PyTorch is publicly available at\nhttps://imatge-upc.github.io/danifojo-2018-repeatrnn/",
        "Revise this with your best effort": "The Adaptive Computation Time for Recurrent Neural Networks (ACT) is a promising architecture for variable computation. ACT adapts to the input sequence by being able to look at each sample more than once and learn how many times it should do it. In this paper, we compare ACT to Repeat-RNN, a novel architecture based on repeating each sample a fixed number of times. We found surprising results where Repeat-RNN performs as good as ACT in the selected tasks. Source code in TensorFlow and PyTorch is publicly available at https://imatge-upc.github.io/danifojo-2018-repeatrnn/.",
        "Help me polish this": "Adaptive Computation Time for Recurrent Neural Networks (ACT) is one of the most promising architectures for variable computation. ACT adapts to the input sequence by being able to look at each sample more than once, and learn how many times it should do it. In this paper, we compare ACT to Repeat-RNN, a novel architecture based on repeating each sample a fixed number of times. We found surprising results, where Repeat-RNN performs as good as ACT in the selected tasks. \n\nSource code in TensorFlow and PyTorch is publicly available at https://imatge-upc.github.io/danifojo-2018-repeatrnn/",
        "Rewrite this for me": "One of the most promising architectures for variable computation is Adaptive Computation Time for Recurrent Neural Networks (ACT). ACT adapts to the input sequence by being able to look at each sample more than once and learn how many times it should do it. In this paper, we compare ACT to Repeat-RNN, a novel architecture based on repeating each sample a fixed number of times. We found surprising results where Repeat-RNN performs as good as ACT in the selected tasks. Source code in TensorFlow and PyTorch is publicly available at https://imatge-upc.github.io/danifojo-2018-repeatrnn/.",
        "Make this fluent while doing minimal change": "Adaptive Computation Time for Recurrent Neural Networks (ACT) is a promising architecture for variable computation. ACT adapts to the input sequence by being able to look at each sample more than once, and learn how many times it should do it. In this paper, we compare ACT to Repeat-RNN, a novel architecture based on repeating each sample a fixed number of times. We found surprising results, where Repeat-RNN performs as well as ACT in the selected tasks. Source code in TensorFlow and PyTorch is publicly available at https://imatge-upc.github.io/danifojo-2018-repeatrnn/.",
        "Refine this for me please": "The paper compares the performance of two different types of recurrent neural networks (ACT and Repeat-RNN) on various tasks. The results showed that Repeat-RNN performs just as well as ACT, even though it repeats each input sample a fixed number of times. The source code for both networks is available at the link below.",
        "Concise this for me and keep all the information": "This paper compares the performance of the Adaptive Computation Time (ACT) recurrent neural network architecture to that of a Repeat-RNN architecture, finding that the latter performs just as well as the former in the tasks studied. Code for both architectures is available at the link below.",
        "Improve this in GPT way": "We found that the Repeat-RNN architecture performs as well as the ACT architecture on the selected tasks. Source code in TensorFlow and PyTorch is publicly available at https://imatge-upc.github.io/danifojo-2018-repeatrnn/."
    },
    {
        "input": "Generative adversarial networks (GANs) are able to model the complex\nhighdimensional distributions of real-world data, which suggests they could be\neffective for anomaly detection. However, few works have explored the use of\nGANs for the anomaly detection task. We leverage recently developed GAN models\nfor anomaly detection, and achieve state-of-the-art performance on image and\nnetwork intrusion datasets, while being several hundred-fold faster at test\ntime than the only published GAN-based method.",
        "Revise this with your best effort": "Generative adversarial networks (GANs) are able to model the complex high-dimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Help me polish this": "Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Rewrite this for me": "GANs have the ability to imitate real-world data distributions, which makes them good candidates for anomaly detection. However, few studies have looked into using GANs for anomaly detection. We use recently developed GAN models for anomaly detection and get better results than any other published method, while being much faster.",
        "Make this fluent while doing minimal change": "Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Refine this for me please": "GANs are effective for anomaly detection because they can model complex highdimensional distributions of real-world data.",
        "Concise this for me and keep all the information": "GANs can model complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Improve this in GPT way": "We propose using GANs for anomaly detection, and demonstrate that our approach outperforms existing methods on image and network intrusion datasets. Our approach is also several hundred-fold faster at test time."
    },
    {
        "input": "Natural Language Inference (NLI) task requires an agent to determine the\nlogical relationship between a natural language premise and a natural language\nhypothesis. We introduce Interactive Inference Network (IIN), a novel class of\nneural network architectures that is able to achieve high-level understanding\nof the sentence pair by hierarchically extracting semantic features from\ninteraction space. We show that an interaction tensor (attention weight)\ncontains semantic information to solve natural language inference, and a denser\ninteraction tensor contains richer semantic information. One instance of such\narchitecture, Densely Interactive Inference Network (DIIN), demonstrates the\nstate-of-the-art performance on large scale NLI copora and large-scale NLI\nalike corpus. It's noteworthy that DIIN achieve a greater than 20% error\nreduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to\nthe strongest published system.",
        "Revise this with your best effort": "The Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large-scale NLI corpora and large-scale NLI-alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Help me polish this": "The task of natural language inference (NLI) requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large-scale NLI corpora and large-scale NLI-alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Rewrite this for me": "The task of natural language inference (NLI) requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large-scale NLI corpora and large-scale NLI-alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Make this fluent while doing minimal change": "The task of Natural Language Inference (NLI) requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from an interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such an architecture, the Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on large scale NLI corpora and large-scale NLI alike corpora. It is noteworthy that DIIN achieves a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
        "Refine this for me please": "The IIN is a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. The interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. DIIN, one instance of such architecture, demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus.",
        "Concise this for me and keep all the information": "The IIN is a neural network that is able to understand the relationship between a natural language premise and hypothesis by extracting semantic features from an interaction space. The DIIN is an instance of the IIN that demonstrates state-of-the-art performance on large-scale NLI corpora.",
        "Improve this in GPT way": "We introduce the Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system."
    },
    {
        "input": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken. We propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2.",
        "Revise this with your best effort": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken.\n\nWe propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2.",
        "Help me polish this": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken.\n\nWe propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2.",
        "Rewrite this for me": "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Make this fluent while doing minimal change": "The ability to deploy neural networks in real-world, safety-critical systems is limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Refine this for me please": "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Concise this for me and keep all the information": "The ability to deploy neural networks in real-world, safety-critical systems is limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples, but most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.\n\nWe propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.",
        "Improve this in GPT way": "The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken.\n\nWe propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2."
    },
    {
        "input": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Revise this with your best effort": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Help me polish this": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Rewrite this for me": "Deep neural networks (DNNs) are able to learn complex, non-linear relationships between variables, which has led to them having impressive predictive performance. However, because it is difficult to visualize these relationships, DNNs have been referred to as black boxes, which has limited their applications. To address this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.",
        "Make this fluent while doing minimal change": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Refine this for me please": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Concise this for me and keep all the information": "Deep neural networks (DNNs) are able to learn complex, non-linear relationships between variables, which has led to their impressive predictive performance. However, the lack of ability to effectively visualize these relationships has resulted in DNNs being characterized as black boxes, limiting their applications. To address this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.",
        "Improve this in GPT way": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise."
    },
    {
        "input": "In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples.",
        "Revise this with your best effort": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Help me polish this": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Rewrite this for me": "We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer.",
        "Make this fluent while doing minimal change": "In this work, we address the problem of musical timbre transfer by manipulating the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Refine this for me please": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Concise this for me and keep all the information": "The goal of musical timbre transfer is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. TimbreTron is a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. The Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance.",
        "Improve this in GPT way": "In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples."
    },
    {
        "input": "We consider the task of word-level language modeling and study the\npossibility of combining hidden-states-based short-term representations with\nmedium-term representations encoded in dynamical weights of a language model.\nOur work extends recent experiments on language models with dynamically\nevolving weights by casting the language modeling problem into an online\nlearning-to-learn framework in which a meta-learner is trained by\ngradient-descent to continuously update a language model weights.",
        "Revise this with your best effort": "We investigate the feasibility of using hidden-state representations to improve language modeling, specifically by combining them with medium-term representations encoded in the weights of the language model. Our work builds on recent experiments with language models that have dynamically evolving weights, by formulating the language modeling problem as an online learning-to-learn task. In this framework, a meta-learner is trained by gradient descent to continuously update the weights of the language model.",
        "Help me polish this": "We investigate the task of word-level language modeling, and study the feasibility of combining hidden-states-based short-term representations with medium-term representations encoded in the dynamical weights of a language model. Our work builds upon recent experiments on language models with dynamically evolving weights, by casting the language modeling problem as an online learning-to-learn problem. In this framework, a meta-learner is trained by gradient descent to continuously update the weights of a language model.",
        "Rewrite this for me": "We investigate the feasibility of using hidden-state-based short-term representations in conjunction with medium-term representations encoded in the weights of a language model. Our work builds upon recent experiments on language models with dynamically evolving weights, by framing the language modeling problem as an online learning-to-learn task. In this framework, a meta-learner is trained via gradient descent to continuously update the weights of a language model.",
        "Make this fluent while doing minimal change": "We consider the task of word-level language modeling and study the\npossibility of combining hidden-states-based short-term representations\nwith medium-term representations encoded in dynamical weights of a\nlanguage model. Our work extends recent experiments on language models\nwith dynamically evolving weights by casting the language modeling\nproblem into an online learning-to-learn framework in which a\nmeta-learner is trained by gradient-descent to continuously update a\nlanguage model weights.",
        "Refine this for me please": "We consider the task of word-level language modeling and study the\npossibility of combining hidden-states-based short-term representations with\nmedium-term representations encoded in dynamical weights of a language model.\nOur work extends recent experiments on language models with dynamically\nevolving weights by casting the language modeling problem into an online\nlearning-to-learn framework in which a meta-learner is trained by\ngradient-descent to continuously update a language model weights.",
        "Concise this for me and keep all the information": "We study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.",
        "Improve this in GPT way": "We consider the task of word-level language modeling and study the\npossibility of combining hidden-states-based short-term representations\nwith medium-term representations encoded in dynamical weights of a\nlanguage model. Our work extends recent experiments on language models\nwith dynamically evolving weights by casting the language modeling\nproblem into an online learning-to-learn framework in which a\nmeta-learner is trained by gradient-descent to continuously update a\nlanguage model weights."
    },
    {
        "input": "GANS are powerful generative models that are able to model the manifold of\nnatural images. We leverage this property to perform manifold regularization by\napproximating the Laplacian norm using a Monte Carlo approximation that is\neasily computed with the GAN. When incorporated into the feature-matching GAN\nof Improved GAN, we achieve state-of-the-art results for GAN-based\nsemi-supervised learning on the CIFAR-10 dataset, with a method that is\nsignificantly easier to implement than competing methods.",
        "Revise this with your best effort": "GANS are powerful generative models that are able to model the manifold of natural images. We use this property to approximate the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When we incorporate this into the feature-matching GAN of Improved GAN, we achieve state-of-the-art results for GAN-based semi-supervised learning on the CIFAR-10 dataset. This method is significantly easier to implement than competing methods.",
        "Help me polish this": "GANS are powerful generative models that are able to model the manifold of natural images. We leverage this property to perform manifold regularization by approximating the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When incorporated into the feature-matching GAN of Improved GAN, we achieve state-of-the-art results for GAN-based semi-supervised learning on the CIFAR-10 dataset. This method is significantly easier to implement than competing methods.",
        "Rewrite this for me": "GANS are powerful generative models that can approximate the manifold of natural images. We can use this property to regularize the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When we incorporate this into the feature-matching GAN of Improved GAN, we achieve state-of-the-art results for GAN-based semi-supervised learning on the CIFAR-10 dataset. This method is significantly easier to implement than competing methods.",
        "Make this fluent while doing minimal change": "GANS are powerful generative models that are able to model the manifold of natural images. We use this property to perform manifold regularization by approximating the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When incorporated into the feature-matching GAN of Improved GAN, we achieve state-of-the-art results for GAN-based semi-supervised learning on the CIFAR-10 dataset, with a method that is significantly easier to implement than competing methods.",
        "Refine this for me please": "GANS are powerful generative models that are able to model the manifold of natural images. We use this property to approximate the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When we incorporate this into the feature-matching GAN of Improved GAN, we achieve state-of-the-art results for GAN-based semi-supervised learning on the CIFAR-10 dataset. This method is significantly easier to implement than competing methods.",
        "Concise this for me and keep all the information": "GANS are powerful generative models that can model natural images. We use this to approximate the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When we incorporate this into the feature-matching GAN of Improved GAN, we achieve state-of-the-art results for GAN-based semi-supervised learning on the CIFAR-10 dataset.",
        "Improve this in GPT way": "GANS are powerful generative models that are able to model the manifold of natural images. We leverage this property to perform manifold regularization by approximating the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When incorporated into the feature-matching GAN of Improved GAN, we achieve state-of-the-art results for GAN-based semi-supervised learning on the CIFAR-10 dataset. This method is significantly easier to implement than competing methods."
    },
    {
        "input": "We identify a class of over-parameterized deep neural networks with standard\nactivation functions and cross-entropy loss which provably have no bad local\nvalley, in the sense that from any point in parameter space there exists a\ncontinuous path on which the cross-entropy loss is non-increasing and gets\narbitrarily close to zero. This implies that these networks have no sub-optimal\nstrict local minima.",
        "Revise this with your best effort": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably do not have any bad local valleys. This means that, from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. Therefore, these networks have no sub-optimal strict local minima.",
        "Help me polish this": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.",
        "Rewrite this for me": "We identify a class of over-parameterized deep neural networks which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. Therefore, these networks have no sub-optimal strict local minima.",
        "Make this fluent while doing minimal change": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.",
        "Refine this for me please": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.",
        "Concise this for me and keep all the information": "We identify a class of deep neural networks which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero.",
        "Improve this in GPT way": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This implies that these networks have no sub-optimal strict local minima."
    },
    {
        "input": "Visual Question Answering (VQA) models have struggled with counting objects\nin natural images so far. We identify a fundamental problem due to soft\nattention in these models as a cause. To circumvent this problem, we propose a\nneural network component that allows robust counting from object proposals.\nExperiments on a toy task show the effectiveness of this component and we\nobtain state-of-the-art accuracy on the number category of the VQA v2 dataset\nwithout negatively affecting other categories, even outperforming ensemble\nmodels with our single model. On a difficult balanced pair metric, the\ncomponent gives a substantial improvement in counting over a strong baseline by\n6.6%.",
        "Revise this with your best effort": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.",
        "Help me polish this": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.",
        "Rewrite this for me": "Visual Question Answering (VQA) models have had difficulty accurately counting objects in natural images. We believe this is due to the \"soft attention\" these models use. To fix this problem, we propose a neural network component that allows for more accurate counting from object proposals. Experiments on a toy task show the effectiveness of this component, and we were able to obtain state-of-the-art accuracy on the number category of the VQA v2 dataset, without negatively affecting other categories. In fact, we even outperformed ensemble models with our single model. On a difficult balanced pair metric, the component gave a substantial improvement in counting over a strong baseline by 6.6%.",
        "Make this fluent while doing minimal change": "Visual Question Answering (VQA) models have, so far, struggled with counting objects in natural images. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component. We obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories; even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.",
        "Refine this for me please": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.",
        "Concise this for me and keep all the information": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.",
        "Improve this in GPT way": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%."
    },
    {
        "input": "One of the challenges in the study of generative adversarial networks is the\ninstability of its training. In this paper, we propose a novel weight\nnormalization technique called spectral normalization to stabilize the training\nof the discriminator. Our new normalization technique is computationally light\nand easy to incorporate into existing implementations. We tested the efficacy\nof spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we\nexperimentally confirmed that spectrally normalized GANs (SN-GANs) is capable\nof generating images of better or equal quality relative to the previous\ntraining stabilization techniques.",
        "Revise this with your best effort": "One of the challenges of studying generative adversarial networks is the instability of its training. In this paper, we propose a new weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on the CIFAR10, STL-10, and ILSVRC2012 datasets and found that spectrally normalized GANs are capable of generating images of equal or better quality than previous training stabilization techniques.",
        "Help me polish this": "One of the challenges when studying generative adversarial networks is the instability of its training. In this paper, we propose a new weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on the CIFAR10, STL-10, and ILSVRC2012 datasets, and we confirmed that spectrally normalized GANs (SN-GANs) are capable of generating images of equal or better quality than previous training stabilization techniques.",
        "Rewrite this for me": "One of the challenges with generative adversarial networks is that they can be unstable during training. In this paper, we propose a new weight normalization technique called spectral normalization to help stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on the CIFAR10, STL-10, and ILSVRC2012 datasets, and we found that spectrally normalized GANs (SN-GANs) are capable of generating images of equal or better quality than previous training stabilization techniques.",
        "Make this fluent while doing minimal change": "One of the challenges of studying generative adversarial networks is the instability of its training. In this paper, we propose a new weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on the CIFAR10, STL-10, and ILSVRC2012 datasets and found that spectrally normalized GANs are capable of generating images of equal or better quality than previous training stabilization techniques.",
        "Refine this for me please": "One of the challenges of studying generative adversarial networks is the instability of its training. In this paper, we propose a new weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on the CIFAR10, STL-10, and ILSVRC2012 datasets, and we confirmed that spectrally normalized GANs are capable of generating images of equal or better quality than previous training stabilization techniques.",
        "Concise this for me and keep all the information": "One of the challenges with generative adversarial networks is that they can be unstable during training. This paper proposes a new weight normalization technique called spectral normalization to help stabilize the training of the discriminator. This new normalization technique is computationally light and easy to incorporate into existing implementations. The paper includes experimental results that show that spectrally normalized GANs are capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
        "Improve this in GPT way": "One of the challenges in the study of generative adversarial networks is the\ninstability of its training. In this paper, we propose a novel weight\nnormalization technique called spectral normalization to stabilize the training\nof the discriminator. Our new normalization technique is computationally light\nand easy to incorporate into existing implementations. We tested the efficacy\nof spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we\nexperimentally confirmed that spectrally normalized GANs (SN-GANs) is capable\nof generating images of better or equal quality relative to the previous\ntraining stabilization techniques."
    },
    {
        "input": "Embedding graph nodes into a vector space can allow the use of machine\nlearning to e.g. predict node classes, but the study of node embedding\nalgorithms is immature compared to the natural language processing field\nbecause of a diverse nature of graphs. We examine the performance of node\nembedding algorithms with respect to graph centrality measures that\ncharacterize diverse graphs, through systematic experiments with four node\nembedding algorithms, four or five graph centralities, and six datasets.\nExperimental results give insights into the properties of node embedding\nalgorithms, which can be a basis for further research on this topic.",
        "Revise this with your best effort": "Embedding graph nodes into a vector space can allow the use of machine learning to predict node classes, but the study of node embedding algorithms is immature compared to the natural language processing field because of the diverse nature of graphs. We examine the performance of node embedding algorithms with respect to graph centrality measures that characterize diverse graphs, through systematic experiments with four node embedding algorithms, four or five graph centralities, and six datasets. Experimental results give insights into the properties of node embedding algorithms, which can be a basis for further research on this topic.",
        "Help me polish this": "Embedding graph nodes into a vector space can allow the use of machine learning to predict node classes, but the study of node embedding algorithms is immature compared to the natural language processing field because of the diverse nature of graphs. We examine the performance of node embedding algorithms with respect to graph centrality measures that characterize diverse graphs, through systematic experiments with four node embedding algorithms, four or five graph centralities, and six datasets. Experimental results give insights into the properties of node embedding algorithms, which can be a basis for further research on this topic.",
        "Rewrite this for me": "Node embedding algorithms allow for the use of machine learning to predict node classes, however the study of these algorithms is immature compared to natural language processing. This is due to the diverse nature of graphs. In order to examine the performance of node embedding algorithms with respect to graph centrality measures, four node embedding algorithms, four or five graph centralities, and six datasets were used in systematic experiments. The results of these experiments give insights into the properties of node embedding algorithms, which can be used as a basis for further research.",
        "Make this fluent while doing minimal change": "Embedding graph nodes into a vector space can allow the use of machine learning to predict node classes, for example, but the study of node embedding algorithms is immature compared to the natural language processing field because graphs can be diverse in nature. We examine the performance of node embedding algorithms with respect to graph centrality measures that characterize diverse graphs, through systematic experiments with four node embedding algorithms, four or five graph centralities, and six datasets. Experimental results give insights into the properties of node embedding algorithms, which can be a basis for further research on this topic.",
        "Refine this for me please": "We examine the performance of node embedding algorithms with respect to graph centrality measures that characterize diverse graphs, through systematic experiments with four node embedding algorithms, four or five graph centralities, and six datasets. Experimental results give insights into the properties of node embedding algorithms, which can be a basis for further research on this topic.",
        "Concise this for me and keep all the information": "Node embedding algorithms can be used to predict node classes, but they are not as developed as algorithms for natural language processing. We examined the performance of four node embedding algorithms with respect to four or five graph centralities, and found that they have different strengths and weaknesses.",
        "Improve this in GPT way": "Embedding graph nodes into a vector space can allow the use of machine learning algorithms to predict node classes. However, the study of node embedding algorithms is immature compared to the natural language processing field because of the diverse nature of graphs. We examine the performance of node embedding algorithms with respect to graph centrality measures that characterize diverse graphs, through systematic experiments with four node embedding algorithms, four or five graph centralities, and six datasets. Experimental results give insights into the properties of node embedding algorithms, which can be a basis for further research on this topic."
    },
    {
        "input": "We introduce a new dataset of logical entailments for the purpose of\nmeasuring models' ability to capture and exploit the structure of logical\nexpressions against an entailment prediction task. We use this task to compare\na series of architectures which are ubiquitous in the sequence-processing\nliterature, in addition to a new model class---PossibleWorldNets---which\ncomputes entailment as a \"convolution over possible worlds\". Results show that\nconvolutional networks present the wrong inductive bias for this class of\nproblems relative to LSTM RNNs, tree-structured neural networks outperform LSTM\nRNNs due to their enhanced ability to exploit the syntax of logic, and\nPossibleWorldNets outperform all benchmarks.",
        "Revise this with your best effort": "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.",
        "Help me polish this": "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.",
        "Rewrite this for me": "We introduce a new dataset of logical entailments to compare models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks have the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.",
        "Make this fluent while doing minimal change": "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.",
        "Refine this for me please": "We introduce a new dataset of logical entailments to compare models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. Results show that tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.",
        "Concise this for me and keep all the information": "We introduce a new dataset of logical entailments to compare models' ability to capture and exploit the structure of logical expressions. Results show that tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.",
        "Improve this in GPT way": "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks."
    },
    {
        "input": "Neural network pruning techniques can reduce the parameter counts of trained\nnetworks by over 90%, decreasing storage requirements and improving\ncomputational performance of inference without compromising accuracy. However,\ncontemporary experience is that the sparse architectures produced by pruning\nare difficult to train from the start, which would similarly improve training\nperformance.\n  We find that a standard pruning technique naturally uncovers subnetworks\nwhose initializations made them capable of training effectively. Based on these\nresults, we articulate the \"lottery ticket hypothesis:\" dense,\nrandomly-initialized, feed-forward networks contain subnetworks (\"winning\ntickets\") that - when trained in isolation - reach test accuracy comparable to\nthe original network in a similar number of iterations. The winning tickets we\nfind have won the initialization lottery: their connections have initial\nweights that make training particularly effective.\n  We present an algorithm to identify winning tickets and a series of\nexperiments that support the lottery ticket hypothesis and the importance of\nthese fortuitous initializations. We consistently find winning tickets that are\nless than 10-20% of the size of several fully-connected and convolutional\nfeed-forward architectures for MNIST and CIFAR10. Above this size, the winning\ntickets that we find learn faster than the original network and reach higher\ntest accuracy.",
        "Revise this with your best effort": "Neural network pruning techniques can reduce the number of parameters in a trained network by more than 90%, which decreases storage requirements and improves computational performance without compromising accuracy. However, it is difficult to train a network from scratch with the same sparsity as a pruned network.\n\nWe find that a standard pruning technique naturally uncovers subnetworks with initializations that make them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Help me polish this": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\n\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Rewrite this for me": "Neural network pruning techniques can reduce the number of parameters in a trained network by more than 90%, which decreases storage requirements and improves computational performance without compromising accuracy. However, it is difficult to train a network from scratch with the same sparsity as a pruned network.\n\nWe find that a standard pruning technique naturally uncovers subnetworks with initializations that make them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Make this fluent while doing minimal change": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\n\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Refine this for me please": "We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Concise this for me and keep all the information": "Neural network pruning techniques can reduce the number of parameters in a trained network by over 90%, which decreases storage requirements and improves computational performance without compromising accuracy. However, it is difficult to train a network from the start with the sparse architecture produced by pruning. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Improve this in GPT way": "We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy."
    },
    {
        "input": "We characterize the singular values of the linear transformation associated\nwith a standard 2D multi-channel convolutional layer, enabling their efficient\ncomputation. This characterization also leads to an algorithm for projecting a\nconvolutional layer onto an operator-norm ball. We show that this is an\neffective regularizer; for example, it improves the test error of a deep\nresidual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Revise this with your best effort": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2% to 5.3%.",
        "Help me polish this": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Rewrite this for me": "We show that the singular values of a standard 2D multi-channel convolutional layer can be characterized efficiently, leading to an algorithm for projecting the layer onto an operator-norm ball. This is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Make this fluent while doing minimal change": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, which enables their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Refine this for me please": "We show that the singular values of a standard 2D multi-channel convolutional layer can be efficiently computed, and that this characterization can be used to improve the test error of a deep residual network using batch normalization on CIFAR-10.",
        "Concise this for me and keep all the information": "We show that the operator-norm ball is an effective regularizer for deep residual networks using batch normalization. This improves the test error on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Improve this in GPT way": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%."
    },
    {
        "input": "Understanding theoretical properties of deep and locally connected nonlinear\nnetwork, such as deep convolutional neural network (DCNN), is still a hard\nproblem despite its empirical success. In this paper, we propose a novel\ntheoretical framework for such networks with ReLU nonlinearity. The framework\nexplicitly formulates data distribution, favors disentangled representations\nand is compatible with common regularization techniques such as Batch Norm. The\nframework is built upon teacher-student setting, by expanding the student\nforward/backward propagation onto the teacher's computational graph. The\nresulting model does not impose unrealistic assumptions (e.g., Gaussian inputs,\nindependence of activation, etc). Our framework could help facilitate\ntheoretical analysis of many practical issues, e.g. overfitting,\ngeneralization, disentangled representations in deep networks.",
        "Revise this with your best effort": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, e.g., overfitting, generalization, and disentangled representations in deep networks.",
        "Help me polish this": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Rewrite this for me": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Make this fluent while doing minimal change": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, e.g., overfitting, generalization, and disentangled representations in deep networks.",
        "Refine this for me please": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Concise this for me and keep all the information": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as batch normalization. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Improve this in GPT way": "Our framework could help facilitate theoretical analysis of many practical issues in deep networks, such as overfitting, generalization, and disentangled representations."
    },
    {
        "input": "We present a Neural Program Search, an algorithm to generate programs from\nnatural language description and a small number of input/output examples. The\nalgorithm combines methods from Deep Learning and Program Synthesis fields by\ndesigning rich domain-specific language (DSL) and defining efficient search\nalgorithm guided by a Seq2Tree model on it. To evaluate the quality of the\napproach we also present a semi-synthetic dataset of descriptions with test\nexamples and corresponding programs. We show that our algorithm significantly\noutperforms a sequence-to-sequence model with attention baseline.",
        "Revise this with your best effort": "We present a Neural Program Search algorithm to generate programs from natural language descriptions and a small number of input/output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing a rich domain-specific language (DSL) and defining an efficient search algorithm guided by a Seq2Tree model. To evaluate the quality of the approach, we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline.",
        "Help me polish this": "We present a Neural Program Search algorithm that can generate programs from natural language descriptions and a small number of input/output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing a rich domain-specific language (DSL) and defining an efficient search algorithm guided by a Seq2Tree model. To evaluate the quality of the approach, we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline.",
        "Rewrite this for me": "We introduce a Neural Program Search, an algorithm that can generate programs from natural language descriptions and a few input/output examples. This algorithm combines methods from both the Deep Learning and Program Synthesis fields by designing a rich domain-specific language (DSL) and defining an efficient search algorithm that is guided by a Seq2Tree model. To evaluate the quality of this approach, we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. Our algorithm significantly outperforms a sequence-to-sequence model with attention baseline.",
        "Make this fluent while doing minimal change": "We present a Neural Program Search algorithm to generate programs from natural language descriptions and a small number of input/output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing a rich domain-specific language (DSL) and defining an efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach, we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline.",
        "Refine this for me please": "We present an algorithm that can generate programs from natural language descriptions and a small number of input/output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing a rich domain-specific language (DSL) and defining an efficient search algorithm guided by a Seq2Tree model. To evaluate the quality of the approach, we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline.",
        "Concise this for me and keep all the information": "We present an algorithm that generates programs from natural language descriptions and a small number of input/output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing a rich domain-specific language (DSL) and defining an efficient search algorithm guided by a Seq2Tree model on it.",
        "Improve this in GPT way": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input/output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline by a large margin."
    },
    {
        "input": "Most state-of-the-art neural machine translation systems, despite being\ndifferent in architectural skeletons (e.g. recurrence, convolutional), share an\nindispensable feature: the Attention. However, most existing attention methods\nare token-based and ignore the importance of phrasal alignments, the key\ningredient for the success of phrase-based statistical machine translation. In\nthis paper, we propose novel phrase-based attention methods to model n-grams of\ntokens as attention entities. We incorporate our phrase-based attentions into\nthe recently proposed Transformer network, and demonstrate that our approach\nyields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for\nGerman-to-English translation tasks on WMT newstest2014 using WMT'16 training\ndata.",
        "Revise this with your best effort": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Help me polish this": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Rewrite this for me": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Make this fluent while doing minimal change": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Refine this for me please": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Concise this for me and keep all the information": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Improve this in GPT way": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data."
    },
    {
        "input": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Revise this with your best effort": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Help me polish this": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Rewrite this for me": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Make this fluent while doing minimal change": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Refine this for me please": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Concise this for me and keep all the information": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits.",
        "Improve this in GPT way": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem."
    },
    {
        "input": "We propose a principled method for kernel learning, which relies on a\nFourier-analytic characterization of translation-invariant or\nrotation-invariant kernels. Our method produces a sequence of feature maps,\niteratively refining the SVM margin. We provide rigorous guarantees for\noptimality and generalization, interpreting our algorithm as online\nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations\non synthetic and real-world datasets demonstrate scalability and consistent\nimprovements over related random features-based methods.",
        "Revise this with your best effort": "We propose a method for kernel learning that is based on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Help me polish this": "We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Rewrite this for me": "We propose a method for kernel learning that uses a Fourier-based characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps that iteratively refines the support vector machine margin. We provide theoretical guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related methods that use random features.",
        "Make this fluent while doing minimal change": "We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Refine this for me please": "We propose a method for kernel learning which uses a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Concise this for me and keep all the information": "We propose a method for kernel learning which uses a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.",
        "Improve this in GPT way": "We propose a principled method for kernel learning, which relies on a\nFourier-analytic characterization of translation-invariant or\nrotation-invariant kernels. Our method produces a sequence of feature maps,\niteratively refining the SVM margin. We provide rigorous guarantees for\noptimality and generalization, interpreting our algorithm as online\nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations\non synthetic and real-world datasets demonstrate scalability and consistent\nimprovements over related random features-based methods."
    },
    {
        "input": "This paper develops variational continual learning (VCL), a simple but\ngeneral framework for continual learning that fuses online variational\ninference (VI) and recent advances in Monte Carlo VI for neural networks. The\nframework can successfully train both deep discriminative models and deep\ngenerative models in complex continual learning settings where existing tasks\nevolve over time and entirely new tasks emerge. Experimental results show that\nVCL outperforms state-of-the-art continual learning methods on a variety of\ntasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Revise this with your best effort": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Help me polish this": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Rewrite this for me": "This paper develops variational continual learning (VCL), a framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Make this fluent while doing minimal change": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Refine this for me please": "This paper develops variational continual learning (VCL), a framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Concise this for me and keep all the information": "This paper develops variational continual learning (VCL), a framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
        "Improve this in GPT way": "This paper develops variational continual learning (VCL), a simple but\ngeneral framework for continual learning that fuses online variational\ninference (VI) and recent advances in Monte Carlo VI for neural networks. The\nframework can successfully train both deep discriminative models and deep\ngenerative models in complex continual learning settings where existing tasks\nevolve over time and entirely new tasks emerge. Experimental results show that\nVCL outperforms state-of-the-art continual learning methods on a variety of\ntasks, avoiding catastrophic forgetting in a fully automatic way."
    },
    {
        "input": "This report has several purposes. First, our report is written to investigate\nthe reproducibility of the submitted paper On the regularization of Wasserstein\nGANs (2018). Second, among the experiments performed in the submitted paper,\nfive aspects were emphasized and reproduced: learning speed, stability,\nrobustness against hyperparameter, estimating the Wasserstein distance, and\nvarious sampling method. Finally, we identify which parts of the contribution\ncan be reproduced, and at what cost in terms of resources. All source code for\nreproduction is open to the public.",
        "Revise this with your best effort": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling methods. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Help me polish this": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Rewrite this for me": "The purpose of this report is to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). We reproduced five aspects of the experiments performed in the submitted paper: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. We identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Make this fluent while doing minimal change": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Refine this for me please": "The purpose of this report is to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). We reproduced five aspects of the experiments performed in the submitted paper: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. We identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.",
        "Concise this for me and keep all the information": "The purpose of this report is to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). The paper is reproduced in full and all source code is open to the public.",
        "Improve this in GPT way": "This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public."
    },
    {
        "input": "In this paper, we propose a new feature extraction technique for program\nexecution logs. First, we automatically extract complex patterns from a\nprogram's behavior graph. Then, we embed these patterns into a continuous space\nby training an autoencoder. We evaluate the proposed features on a real-world\nmalicious software detection task. We also find that the embedding space\ncaptures interpretable structures in the space of pattern parts.",
        "Revise this with your best effort": "In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Help me polish this": "In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task and find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Rewrite this for me": "In this paper, we propose a new technique for extracting features from program execution logs. This technique involves automatically extracting complex patterns from a program's behavior graph and then embedding these patterns into a continuous space. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Make this fluent while doing minimal change": "In this paper, we propose a new feature extraction technique for program execution logs. This technique involves automatically extracting complex patterns from a program's behavior graph and then embedding these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task and find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Refine this for me please": "In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Concise this for me and keep all the information": "We propose a new feature extraction technique for program execution logs. We automatically extract complex patterns from a program's behavior graph and embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task and find that the embedding space captures interpretable structures in the space of pattern parts.",
        "Improve this in GPT way": "In this paper, we propose a new feature extraction technique for program\nexecution logs. First, we automatically extract complex patterns from a\nprogram's behavior graph. Then, we embed these patterns into a continuous space\nby training an autoencoder. We evaluate the proposed features on a real-world\nmalicious software detection task. We also find that the embedding space\ncaptures interpretable structures in the space of pattern parts."
    },
    {
        "input": "We propose a single neural probabilistic model based on variational\nautoencoder that can be conditioned on an arbitrary subset of observed features\nand then sample the remaining features in \"one shot\". The features may be both\nreal-valued and categorical. Training of the model is performed by stochastic\nvariational Bayes. The experimental evaluation on synthetic data, as well as\nfeature imputation and image inpainting problems, shows the effectiveness of\nthe proposed approach and diversity of the generated samples.",
        "Revise this with your best effort": "We propose a single neural probabilistic model based on a variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Help me polish this": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Rewrite this for me": "We propose a single neural probabilistic model, based on a variational autoencoder, that can be conditioned on an arbitrary subset of observed features, and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach, and the diversity of the generated samples.",
        "Make this fluent while doing minimal change": "We propose a single neural probabilistic model based on a variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Refine this for me please": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Concise this for me and keep all the information": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The model is trained by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Improve this in GPT way": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples."
    },
    {
        "input": "Variational Autoencoders (VAEs) were originally motivated (Kingma & Welling,\n2014) as probabilistic generative models in which one performs approximate\nBayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks\nthis interpretation and generalizes VAEs to application domains beyond\ngenerative modeling (e.g., representation learning, clustering, or lossy data\ncompression) by introducing an objective function that allows practitioners to\ntrade off between the information content (\"bit rate\") of the latent\nrepresentation and the distortion of reconstructed data (Alemi et al., 2018).\nIn this paper, we reconsider this rate/distortion trade-off in the context of\nhierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We\nidentify a general class of inference models for which one can split the rate\ninto contributions from each layer, which can then be tuned independently. We\nderive theoretical bounds on the performance of downstream tasks as functions\nof the individual layers' rates and verify our theoretical findings in\nlarge-scale experiments. Our results provide guidance for practitioners on\nwhich region in rate-space to target for a given application.",
        "Revise this with your best effort": "Variational Autoencoders (VAEs) were motivated originally as probabilistic generative models in which one performs approximate Bayesian inference (Kingma & Welling, 2014). The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Help me polish this": "Variational Autoencoders (VAEs) were motivated originally as probabilistic generative models in which one performs approximate Bayesian inference (Kingma & Welling, 2014). The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Rewrite this for me": "Variational Autoencoders (VAEs) were motivated as probabilistic generative models in which one performs approximate Bayesian inference. The proposal of $\\beta$-VAEs breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018). In this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Make this fluent while doing minimal change": "Variational Autoencoders (VAEs) were motivated originally (Kingma & Welling, 2014) as probabilistic generative models, in which one performs approximate Bayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Refine this for me please": "Variational Autoencoders (VAEs) were motivated as probabilistic generative models in which one performs approximate Bayesian inference. The proposal of $\\beta$-VAEs breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Concise this for me and keep all the information": "Variational Autoencoders (VAEs) are a type of probabilistic generative model used for approximate Bayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (\"bit rate\") of the latent representation and the distortion of reconstructed data (Alemi et al., 2018). In this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "Improve this in GPT way": "Variational Autoencoders (VAEs) were originally motivated (Kingma & Welling,\n2014) as probabilistic generative models in which one performs approximate\nBayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks\nthis interpretation and generalizes VAEs to application domains beyond\ngenerative modeling (e.g., representation learning, clustering, or lossy data\ncompression) by introducing an objective function that allows practitioners to\ntrade off between the information content (\"bit rate\") of the latent\nrepresentation and the distortion of reconstructed data (Alemi et al., 2018).\n\nIn this paper, we reconsider the rate/distortion trade-off in the context of\nhierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We\nidentify a general class of inference models for which one can split the rate\ninto contributions from each layer, which can then be tuned independently. We\nderive theoretical bounds on the performance of downstream tasks as functions\nof the individual layers' rates and verify our theoretical findings in\nlarge-scale experiments. Our results provide guidance for practitioners on\nwhich region in rate-space to target for a given application."
    },
    {
        "input": "Understanding and characterizing the subspaces of adversarial examples aid in\nstudying the robustness of deep neural networks (DNNs) to adversarial\nperturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local\nintrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to\nstudy adversarial subspaces. It was demonstrated that LID can be used to\ncharacterize the adversarial subspaces associated with different attack\nmethods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign\nattack.\n  In this paper, we use MNIST and CIFAR-10 to conduct two new sets of\nexperiments that are absent in existing LID analysis and report the limitation\nof LID in characterizing the corresponding adversarial subspaces, which are (i)\noblivious attacks and LID analysis using adversarial examples with different\nconfidence levels; and (ii) black-box transfer attacks. For (i), we find that\nthe performance of LID is very sensitive to the confidence parameter deployed\nby an attack, and the LID learned from ensembles of adversarial examples with\nvarying confidence levels surprisingly gives poor performance. For (ii), we\nfind that when adversarial examples are crafted from another DNN model, LID is\nineffective in characterizing their adversarial subspaces. These two findings\ntogether suggest the limited capability of LID in characterizing the subspaces\nof adversarial examples.",
        "Revise this with your best effort": "Understanding and characterizing the subspaces of adversarial examples can help in studying the robustness of deep neural networks (DNNs) to adversarial perturbations. Recently, Ma et al. (ICLR 2018) proposed using local intrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to study adversarial subspaces. It was demonstrated that LID can be used to characterize the adversarial subspaces associated with different attack methods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign attack.\n\nIn this paper, we use MNIST and CIFAR-10 to conduct two new sets of experiments that are absent in existing LID analysis and report the limitation of LID in characterizing the corresponding adversarial subspaces. These are (i) oblivious attacks and LID analysis using adversarial examples with different confidence levels; and (ii) black-box transfer attacks. For (i), we find that the performance of LID is very sensitive to the confidence parameter deployed by an attack, and the LID learned from ensembles of adversarial examples with varying confidence levels surprisingly gives poor performance. For (ii), we find that when adversarial examples are crafted from another DNN model, LID is ineffective in characterizing their adversarial subspaces. These two findings together suggest the limited capability of LID in characterizing the subspaces of",
        "Help me polish this": "Understanding and characterizing the subspaces of adversarial examples can help in studying the robustness of deep neural networks (DNNs) to adversarial perturbations. Recently, Ma et al. (ICLR 2018) proposed using local intrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to study adversarial subspaces. It was demonstrated that LID can be used to characterize the adversarial subspaces associated with different attack methods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign attack.\n\nIn this paper, we use MNIST and CIFAR-10 to conduct two new sets of experiments that are absent in existing LID analysis and report the limitation of LID in characterizing the corresponding adversarial subspaces. These are (i) oblivious attacks and LID analysis using adversarial examples with different confidence levels; and (ii) black-box transfer attacks.\n\nFor (i), we find that the performance of LID is very sensitive to the confidence parameter deployed by an attack, and the LID learned from ensembles of adversarial examples with varying confidence levels surprisingly gives poor performance.\n\nFor (ii), we find that when adversarial examples are crafted from another DNN model, LID is ineffective in characterizing their adversarial subspaces.\n\nThese two findings together suggest the limited capability of LID in character",
        "Rewrite this for me": "Recently, Ma et al. proposed using local intrinsic dimensionality (LID) to study adversarial subspaces in layer-wise hidden representations of deep neural networks (DNNs). It was demonstrated that LID can be used to characterize the adversarial subspaces associated with different attack methods. However, we find that the performance of LID is very sensitive to the confidence parameter deployed by an attack, and that LID is ineffective in characterizing the subspaces of adversarial examples when they are crafted from another DNN model. These findings suggest the limited capability of LID in characterizing the subspaces of adversarial examples.",
        "Make this fluent while doing minimal change": "Recently, Ma et al. proposed using local intrinsic dimensionality (LID) to study adversarial subspaces. LID was demonstrated to be effective in characterizing the adversarial subspaces associated with different attack methods. However, we find that LID is limited in characterizing the subspaces of adversarial examples, especially for oblivious attacks and black-box transfer attacks.",
        "Refine this for me please": "This paper demonstrates the limitations of using local intrinsic dimensionality (LID) to characterize the subspaces of adversarial examples associated with different attack methods. The authors find that LID is very sensitive to the confidence parameter deployed by an attack, and that LID learned from ensembles of adversarial examples with varying confidence levels surprisingly gives poor performance. They also find that when adversarial examples are crafted from another DNN model, LID is ineffective in characterizing their adversarial subspaces.",
        "Concise this for me and keep all the information": "Recently, Ma et al. proposed using local intrinsic dimensionality (LID) to study adversarial subspaces associated with different attack methods. However, we find that the performance of LID is very sensitive to the confidence parameter deployed by an attack, and that when adversarial examples are crafted from another DNN model, LID is ineffective in characterizing their adversarial subspaces.",
        "Improve this in GPT way": "This paper demonstrates the limitations of using local intrinsic dimensionality (LID) to characterize the subspaces of adversarial examples. In particular, we find that LID is ineffective in characterizing the subspaces of adversarial examples generated by oblivious attacks and black-box transfer attacks."
    },
    {
        "input": "Generative adversarial networks (GANs) form a generative modeling approach\nknown for producing appealing samples, but they are notably difficult to train.\nOne common way to tackle this issue has been to propose new formulations of the\nGAN objective. Yet, surprisingly few studies have looked at optimization\nmethods designed for this adversarial training. In this work, we cast GAN\noptimization problems in the general variational inequality framework. Tapping\ninto the mathematical programming literature, we counter some common\nmisconceptions about the difficulties of saddle point optimization and propose\nto extend techniques designed for variational inequalities to the training of\nGANs. We apply averaging, extrapolation and a computationally cheaper variant\nthat we call extrapolation from the past to the stochastic gradient method\n(SGD) and Adam.",
        "Revise this with your best effort": "GANs are notoriously difficult to train, with one common way to tackle this issue being to propose new formulations of the GAN objective. However, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Help me polish this": "Generative adversarial networks (GANs) are a generative modeling approach known for producing appealing samples, but they are notoriously difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Rewrite this for me": "GANs are difficult to train because they are a generative modeling approach that produces appealing samples. One way to tackle this issue is to propose new formulations of the GAN objective. However, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Make this fluent while doing minimal change": "GANs are notoriously difficult to train, with one common way to tackle this issue being to propose new formulations of the GAN objective. However, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Refine this for me please": "GANs are difficult to train because they are a generative modeling approach that produces appealing samples. One way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Concise this for me and keep all the information": "GANs are difficult to train because they are a generative modeling approach. One way to tackle this issue is to propose new formulations of the GAN objective. However, few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Improve this in GPT way": "We propose a new method for training GANs that is based on the general variational inequality framework. This method is designed to address the common difficulties associated with training GANs, and we show that it can be applied to the training of GANs using stochastic gradient descent (SGD) and Adam."
    },
    {
        "input": "Neural message passing algorithms for semi-supervised classification on\ngraphs have recently achieved great success. However, for classifying a node\nthese methods only consider nodes that are a few propagation steps away and the\nsize of this utilized neighborhood is hard to extend. In this paper, we use the\nrelationship between graph convolutional networks (GCN) and PageRank to derive\nan improved propagation scheme based on personalized PageRank. We utilize this\npropagation procedure to construct a simple model, personalized propagation of\nneural predictions (PPNP), and its fast approximation, APPNP. Our model's\ntraining time is on par or faster and its number of parameters on par or lower\nthan previous models. It leverages a large, adjustable neighborhood for\nclassification and can be easily combined with any neural network. We show that\nthis model outperforms several recently proposed methods for semi-supervised\nclassification in the most thorough study done so far for GCN-like models. Our\nimplementation is available online.",
        "Revise this with your best effort": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Help me polish this": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Rewrite this for me": "Recently, neural message passing algorithms for semi-supervised classification on graphs have achieved great success. However, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Make this fluent while doing minimal change": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Refine this for me please": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Concise this for me and keep all the information": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models.",
        "Improve this in GPT way": "Recently, neural message passing algorithms for semi-supervised classification on graphs have achieved great success. However, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online."
    },
    {
        "input": "We identify obfuscated gradients, a kind of gradient masking, as a phenomenon\nthat leads to a false sense of security in defenses against adversarial\nexamples. While defenses that cause obfuscated gradients appear to defeat\niterative optimization-based attacks, we find defenses relying on this effect\ncan be circumvented. We describe characteristic behaviors of defenses\nexhibiting the effect, and for each of the three types of obfuscated gradients\nwe discover, we develop attack techniques to overcome it. In a case study,\nexamining non-certified white-box-secure defenses at ICLR 2018, we find\nobfuscated gradients are a common occurrence, with 7 of 9 defenses relying on\nobfuscated gradients. Our new attacks successfully circumvent 6 completely, and\n1 partially, in the original threat model each paper considers.",
        "Revise this with your best effort": "We find that obfuscated gradients, a kind of gradient masking, can lead to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients may appear to defeat iterative optimization-based attacks, we find that these defenses can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find that obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 of these defenses completely, and 1 partially, in the original threat model each paper considers.",
        "Help me polish this": "We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that can lead to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients may appear to defeat iterative optimization-based attacks, we find that defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find that obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 of these defenses completely, and 1 partially, in the original threat model each paper considers.",
        "Rewrite this for me": "We find that obfuscated gradients, a kind of gradient masking, can lead to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients may appear to defeat iterative optimization-based attacks, we find that defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find that obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 of these defenses completely, and 1 partially, in the original threat model each paper considers.",
        "Make this fluent while doing minimal change": "We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that can lead to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients may appear to defeat iterative optimization-based attacks, we find that defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find that obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 of these defenses completely, and 1 partially, in the original threat model each paper considers.",
        "Refine this for me please": "We find that obfuscated gradients, or gradient masking, is a common occurrence in defenses against adversarial examples, and that our new attack techniques can successfully circumvent these defenses.",
        "Concise this for me and keep all the information": "We find that obfuscated gradients, a kind of gradient masking, can lead to a false sense of security in defenses against adversarial examples. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find that obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",
        "Improve this in GPT way": "We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers."
    },
    {
        "input": "Methods that learn representations of nodes in a graph play a critical role\nin network analysis since they enable many downstream learning tasks. We\npropose Graph2Gauss - an approach that can efficiently learn versatile node\nembeddings on large scale (attributed) graphs that show strong performance on\ntasks such as link prediction and node classification. Unlike most approaches\nthat represent nodes as point vectors in a low-dimensional continuous space, we\nembed each node as a Gaussian distribution, allowing us to capture uncertainty\nabout the representation. Furthermore, we propose an unsupervised method that\nhandles inductive learning scenarios and is applicable to different types of\ngraphs: plain/attributed, directed/undirected. By leveraging both the network\nstructure and the associated node attributes, we are able to generalize to\nunseen nodes without additional training. To learn the embeddings we adopt a\npersonalized ranking formulation w.r.t. the node distances that exploits the\nnatural ordering of the nodes imposed by the network structure. Experiments on\nreal world networks demonstrate the high performance of our approach,\noutperforming state-of-the-art network embedding methods on several different\ntasks. Additionally, we demonstrate the benefits of modeling uncertainty - by\nanalyzing it we can estimate neighborhood diversity and detect the intrinsic\nlatent dimensionality of a graph.",
        "Revise this with your best effort": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Help me polish this": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification.\n\nUnlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training.\n\nTo learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Rewrite this for me": "Methods that learn representations of nodes in a graph are critical for network analysis, since they enable many downstream learning tasks. Graph2Gauss is an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs, and shows strong performance on tasks such as link prediction and node classification. Most approaches represent nodes as point vectors in a low-dimensional continuous space, but Graph2Gauss embeds each node as a Gaussian distribution, allowing for uncertainty to be captured about the representation. Additionally, Graph2Gauss proposes an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, Graph2Gauss is able to generalize to unseen nodes without additional training. To learn the embeddings, Graph2Gauss adopts a personalized ranking formulation with respect to the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, the benefits of modeling uncertainty are demonstrated - by analyzing it, we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Make this fluent while doing minimal change": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
        "Refine this for me please": "We propose an approach to efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Our approach, Graph2Gauss, embeds each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training.",
        "Concise this for me and keep all the information": "Graph2Gauss is an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification.",
        "Improve this in GPT way": "We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph."
    },
    {
        "input": "Convolutional Neural Networks (CNNs) have become the method of choice for\nlearning problems involving 2D planar images. However, a number of problems of\nrecent interest have created a demand for models that can analyze spherical\nimages. Examples include omnidirectional vision for drones, robots, and\nautonomous cars, molecular regression problems, and global weather and climate\nmodelling. A naive application of convolutional networks to a planar projection\nof the spherical signal is destined to fail, because the space-varying\ndistortions introduced by such a projection will make translational weight\nsharing ineffective.\n  In this paper we introduce the building blocks for constructing spherical\nCNNs. We propose a definition for the spherical cross-correlation that is both\nexpressive and rotation-equivariant. The spherical correlation satisfies a\ngeneralized Fourier theorem, which allows us to compute it efficiently using a\ngeneralized (non-commutative) Fast Fourier Transform (FFT) algorithm. We\ndemonstrate the computational efficiency, numerical accuracy, and effectiveness\nof spherical CNNs applied to 3D model recognition and atomization energy\nregression.",
        "Revise this with your best effort": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n\nIn this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.",
        "Help me polish this": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n\nIn this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.",
        "Rewrite this for me": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest, such as omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling, have created a demand for models that can analyze spherical images. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n\nIn this paper, we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.",
        "Make this fluent while doing minimal change": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest, such as omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling, have created a demand for models that can analyze spherical images. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n  In this paper, we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.",
        "Refine this for me please": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n\nIn this paper, we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.",
        "Concise this for me and keep all the information": "Convolutional Neural Networks (CNNs) are the best way to learn from 2D images, but they don't work well with 3D images. In this paper, we introduce the building blocks for constructing spherical CNNs, which are much better at learning from 3D images. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.",
        "Improve this in GPT way": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n  In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression."
    },
    {
        "input": "This paper shows how one can directly apply natural language processing (NLP)\nmethods to classification problems in cheminformatics. Connection between these\nseemingly separate fields is shown by considering standard textual\nrepresentation of compound, SMILES. The problem of activity prediction against\na target protein is considered, which is a crucial part of computer aided drug\ndesign process. Conducted experiments show that this way one can not only\noutrank state of the art results of hand crafted representations but also gets\ndirect structural insights into the way decisions are made.",
        "Revise this with your best effort": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only outperforms state-of-the-art results of hand-crafted representations, but also provides direct structural insights into the way decisions are made.",
        "Help me polish this": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only outperforms state-of-the-art results of hand-crafted representations, but also provides direct structural insights into the way decisions are made.",
        "Rewrite this for me": "This paper demonstrates how natural language processing (NLP) methods can be used to solve classification problems in cheminformatics. The connection between these two seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this method not only outperforms state-of-the-art results of hand-crafted representations, but also provides direct structural insights into the decision-making process.",
        "Make this fluent while doing minimal change": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only state of the art results of hand-crafted representations can be outranked, but also direct structural insights into the way decisions are made can be gained.",
        "Refine this for me please": "This paper demonstrates how natural language processing (NLP) methods can be used to solve classification problems in cheminformatics. The connection between these two fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. The conducted experiments show that this approach not only outperforms the state-of-the-art results of hand-crafted representations but also provides direct structural insights into the way decisions are made.",
        "Concise this for me and keep all the information": "This paper shows how natural language processing (NLP) methods can be applied to classification problems in cheminformatics. The connection between these fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only state of the art results of hand-crafted representations can be outranked, but also direct structural insights into the way decisions are made can be obtained.",
        "Improve this in GPT way": "This paper demonstrates how natural language processing (NLP) methods can be directly applied to classification problems in cheminformatics. The connection between these seemingly separate fields is shown by considering the standard textual representation of a compound, SMILES. The problem of activity prediction against a target protein is considered, which is a crucial part of the computer-aided drug design process. Conducted experiments show that this way not only outperforms state-of-the-art results of hand-crafted representations but also gets direct structural insights into the way decisions are made."
    },
    {
        "input": "The inclusion of Computer Vision and Deep Learning technologies in\nAgriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects.",
        "Revise this with your best effort": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi, 2018) can help in detecting healthy apples\nfrom apples with defects.",
        "Help me polish this": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi, 2018) can help in detecting healthy apples\nfrom apples with defects.",
        "Rewrite this for me": "The use of computer vision and deep learning technologies in agriculture has the potential to increase the quality and productivity of farmers. These technologies can be used to sort fruits and vegetables, and to detect defects in apples.",
        "Make this fluent while doing minimal change": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture can help increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects.",
        "Refine this for me please": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper explores if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects.",
        "Concise this for me and keep all the information": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers. In particular, this paper explores if recent computer vision and deep learning methods such as the YOLOv3 can help in detecting healthy apples from apples with defects, in order to help farmers with post-harvest handling.",
        "Improve this in GPT way": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects."
    },
    {
        "input": "We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster to the near state-of the art\nperplexity while using significantly less RNN parameters.",
        "Revise this with your best effort": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of the LSTM matrix into the product of two smaller matrices, and the second one is partitioning of the LSTM matrix, its inputs and states into independent groups. Both approaches allow us to train large LSTM networks significantly faster, while using significantly fewer RNN parameters.",
        "Help me polish this": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster while using significantly fewer RNN parameters.",
        "Rewrite this for me": "We present two ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster while using significantly fewer RNN parameters.",
        "Make this fluent while doing minimal change": "We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster while using significantly less RNN parameters.",
        "Refine this for me please": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster while using significantly fewer RNN parameters.",
        "Concise this for me and keep all the information": "We present two ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.",
        "Improve this in GPT way": "We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster while using significantly less RNN parameters."
    },
    {
        "input": "State-of-the-art deep reading comprehension models are dominated by recurrent\nneural nets. Their sequential nature is a natural fit for language, but it also\nprecludes parallelization within an instances and often becomes the bottleneck\nfor deploying such models to latency critical scenarios. This is particularly\nproblematic for longer texts. Here we present a convolutional architecture as\nan alternative to these recurrent architectures. Using simple dilated\nconvolutional units in place of recurrent ones, we achieve results comparable\nto the state of the art on two question answering tasks, while at the same time\nachieving up to two orders of magnitude speedups for question answering.",
        "Revise this with your best effort": "State-of-the-art deep reading comprehension models are typically based on recurrent neural networks (RNNs). RNNs are well suited for processing sequential data such as language, but their sequential nature also limits parallelization and can be a bottleneck for deploying such models to latency-critical scenarios. This is particularly problematic for longer texts. \n\nAs an alternative to RNNs, we present a convolutional architecture. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Help me polish this": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often preclude parallelization within an instance, which can become a bottleneck for deploying such models to latency-critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Rewrite this for me": "State-of-the-art deep reading comprehension models are typically based on recurrent neural nets. However, their sequential nature can make them slow, preventing them from being used in latency-critical scenarios. This is especially problematic for longer texts. An alternative is to use a convolutional architecture. This can be achieved by using dilated convolutional units instead of recurrent ones. This approach can achieve results comparable to the state of the art on two question answering tasks, while at the same time being up to two orders of magnitude faster.",
        "Make this fluent while doing minimal change": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often preclude parallelization within an instance and become the bottleneck for deploying such models to latency critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Refine this for me please": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often have a bottleneck for deploying such models to latency critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Concise this for me and keep all the information": "State-of-the-art deep reading comprehension models are dominated by recurrent neural nets. However, their sequential nature is a natural fit for language, but it also precludes parallelization within an instance and often becomes the bottleneck for deploying such models to latency critical scenarios, which is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",
        "Improve this in GPT way": "State-of-the-art deep reading comprehension models that are dominated by recurrent neural nets often preclude parallelization within an instance, which can become a bottleneck for deploying such models to latency-critical scenarios. This is particularly problematic for longer texts. Here, we present a convolutional architecture as an alternative to these recurrent architectures. Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering."
    },
    {
        "input": "In this work, we analyze the reinstatement mechanism introduced by Ritter et\nal. (2018) to reveal two classes of neurons that emerge in the agent's working\nmemory (an epLSTM cell) when trained using episodic meta-RL on an episodic\nvariant of the Harlow visual fixation task. Specifically, Abstract neurons\nencode knowledge shared across tasks, while Episodic neurons carry information\nrelevant for a specific episode's task.",
        "Revise this with your best effort": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Help me polish this": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Rewrite this for me": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Make this fluent while doing minimal change": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Refine this for me please": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Concise this for me and keep all the information": "There are two types of neurons that are revealed when using the reinstatement mechanism- Abstract and Episodic neurons. Abstract neurons encode knowledge that is shared across tasks, while Episodic neurons carry information that is relevant for a specific episode's task.",
        "Improve this in GPT way": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task."
    },
    {
        "input": "The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has\nemerged as a useful tool for thinking about realism and distortion of\nreconstructions in lossy compression. Unlike the rate-distortion function,\nhowever, it is unknown whether encoders and decoders exist that achieve the\nrate suggested by the RDPF. Building on results by Li and El Gamal (2018), we\nshow that the RDPF can indeed be achieved using stochastic, variable-length\ncodes. For this class of codes, we also prove that the RDPF lower-bounds the\nachievable rate",
        "Revise this with your best effort": "The rate-distortion-perception function (RDPF) has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal, we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Help me polish this": "The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Rewrite this for me": "The RDPF is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF, but it is possible to use stochastic, variable-length codes to achieve the RDPF. For this class of codes, the RDPF lower-bounds the achievable rate.",
        "Make this fluent while doing minimal change": "The RDPF has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Refine this for me please": "The RDPF is a tool for thinking about the realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. However, it is possible to achieve the RDPF using stochastic, variable-length codes. For this class of codes, the RDPF lower-bounds the achievable rate.",
        "Concise this for me and keep all the information": "The rate-distortion-perception function (RDPF) is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF, but it can be achieved using stochastic, variable-length codes.",
        "Improve this in GPT way": "The rate-distortion-perception function (RDPF) is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate."
    },
    {
        "input": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our\nmethod explicitly models the phrase structures in output sequences using\nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence\nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we\nintroduce a new layer to perform (soft) local reordering of input sequences.\nDifferent from existing neural machine translation (NMT) approaches, NPMT does\nnot use attention-based decoding mechanisms. Instead, it directly outputs\nphrases in a sequential order and can decode in linear time. Our experiments\nshow that NPMT achieves superior performances on IWSLT 2014\nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine\ntranslation tasks compared with strong NMT baselines. We also observe that our\nmethod produces meaningful phrases in output languages.",
        "Revise this with your best effort": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences.\n\nDifferent from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Help me polish this": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences.\n\nDifferent from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Rewrite this for me": "In this paper, we introduce Neural Phrase-based Machine Translation (NPMT). Our method uses Sleep-WAke Networks (SWAN) to explicitly model phrase structures in output sequences. To address the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. NPMT does not use attention-based decoding mechanisms; instead, it directly outputs phrases in a sequential order, allowing for linear-time decoding. Our experiments show that NPMT outperforms strong NMT baselines on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks. We also observe that our method produces meaningful phrases in output languages.",
        "Make this fluent while doing minimal change": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Refine this for me please": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.",
        "Concise this for me and keep all the information": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines.",
        "Improve this in GPT way": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages."
    },
    {
        "input": "It is by now well-known that small adversarial perturbations can induce\nclassification errors in deep neural networks (DNNs). In this paper, we make\nthe case that sparse representations of the input data are a crucial tool for\ncombating such attacks. For linear classifiers, we show that a sparsifying\nfront end is provably effective against $\\ell_{\\infty}$-bounded attacks,\nreducing output distortion due to the attack by a factor of roughly $K / N$\nwhere $N$ is the data dimension and $K$ is the sparsity level. We then extend\nthis concept to DNNs, showing that a \"locally linear\" model can be used to\ndevelop a theoretical foundation for crafting attacks and defenses.\nExperimental results for the MNIST dataset show the efficacy of the proposed\nsparsifying front end.",
        "Revise this with your best effort": "It is now widely accepted that small adversarial perturbations can lead to classification errors in deep neural networks (DNNs). In this paper, we argue that sparse representations of input data are a key tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against $\\ell_{\\infty}$-bounded attacks, reducing output distortion due to the attack by a factor of roughly $K / N$, where $N$ is the data dimension and $K$ is the sparsity level. We then extend this concept to DNNs, showing that a \"locally linear\" model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.",
        "Help me polish this": "It is now widely accepted that small adversarial perturbations can lead to classification errors in deep neural networks (DNNs). In this paper, we argue that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against $\\ell_{\\infty}$-bounded attacks, reducing output distortion due to the attack by a factor of roughly $K / N$ where $N$ is the data dimension and $K$ is the sparsity level. We then extend this concept to DNNs, showing that a \"locally linear\" model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.",
        "Rewrite this for me": "It is now widely accepted that small changes to input data can lead to classification errors in deep neural networks (DNNs). In this paper, we argue that using sparse representations of data is a key way to combat such attacks. We show that, for linear classifiers, a sparsifying front end can reduce output distortion due to an attack by a factor of roughly $K/N$, where $N$ is the data dimension and $K$ is the sparsity level. We then extend this concept to DNNs, demonstrating that a \"locally linear\" model can be used to develop a theoretical framework for both crafting attacks and defenses. Experimental results on the MNIST dataset show the efficacy of the proposed sparsifying front end.",
        "Make this fluent while doing minimal change": "It is now widely known that small adversarial perturbations can cause classification errors in deep neural networks (DNNs). In this paper, we argue that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against $\\ell_{\\infty}$-bounded attacks, reducing output distortion due to the attack by a factor of roughly $K / N$ where $N$ is the data dimension and $K$ is the sparsity level. We then extend this concept to DNNs, showing that a \"locally linear\" model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.",
        "Refine this for me please": "The paper argues that sparse representations of data are effective in combating classification errors in deep neural networks, and provides a theoretical foundation for developing attacks and defenses against such errors. Experimental results on the MNIST dataset show that the proposed sparsifying front end is effective in reducing output distortion due to classification errors.",
        "Concise this for me and keep all the information": "Small adversarial perturbations can cause classification errors in deep neural networks (DNNs). Sparse representations of input data can help combat such attacks. For linear classifiers, a sparsifying front end can reduce output distortion due to an attack by a factor of roughly $K / N$, where $N$ is the data dimension and $K$ is the sparsity level. This concept can be extended to DNNs. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.",
        "Improve this in GPT way": "It is by now well-known that small adversarial perturbations can induce\nclassification errors in deep neural networks (DNNs). In this paper, we show that sparse representations of the input data are a crucial tool for\ncombating such attacks. We prove that a sparsifying front end is provably effective against $\\ell_{\\infty}$-bounded attacks,\nreducing output distortion due to the attack by a factor of roughly $K / N$\nwhere $N$ is the data dimension and $K$ is the sparsity level. We then extend\nthis concept to DNNs, showing that a \"locally linear\" model can be used to\ndevelop a theoretical foundation for crafting attacks and defenses.\nExperimental results for the MNIST dataset show the efficacy of the proposed\nsparsifying front end."
    },
    {
        "input": "We propose a new sample-efficient methodology, called Supervised Policy\nUpdate (SPU), for deep reinforcement learning. Starting with data generated by\nthe current policy, SPU formulates and solves a constrained optimization\nproblem in the non-parameterized proximal policy space. Using supervised\nregression, it then converts the optimal non-parameterized policy to a\nparameterized policy, from which it draws new samples. The methodology is\ngeneral in that it applies to both discrete and continuous action spaces, and\ncan handle a wide variety of proximity constraints for the non-parameterized\noptimization problem. We show how the Natural Policy Gradient and Trust Region\nPolicy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization\n(PPO) problem can be addressed by this methodology. The SPU implementation is\nmuch simpler than TRPO. In terms of sample efficiency, our extensive\nexperiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and\noutperforms PPO in Atari video game tasks.",
        "Revise this with your best effort": "We propose a new, sample-efficient methodology called Supervised Policy Update (SPU) for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, SPU then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, as well as the Proximal Policy Optimization (PPO) problem, can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show that SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.",
        "Help me polish this": "We propose a new, sample-efficient methodology called Supervised Policy Update (SPU) for deep reinforcement learning. \n\nSPU starts with data generated by the current policy, and formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, SPU then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. \n\nThe methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, as well as the Proximal Policy Optimization (PPO) problem, can be addressed by this methodology. \n\nCompared to TRPO, SPU has a much simpler implementation. In terms of sample efficiency, our extensive experiments show that SPU outperforms TRPO in Mujoco simulated robotic tasks, and outperforms PPO in Atari video game tasks.",
        "Rewrite this for me": "We propose a new, sample-efficient methodology called Supervised Policy Update (SPU) for deep reinforcement learning. \n\nSPU starts with data generated by the current policy. It then formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, SPU converts the optimal non-parameterized policy to a parameterized policy. From there, it draws new samples. \n\nThe methodology is general in that it applies to both discrete and continuous action spaces. It can also handle a wide variety of proximity constraints for the non-parameterized optimization problem. \n\nWe show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. \n\nThe SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show that SPU outperforms TRPO in Mujoco simulated robotic tasks. It also outperforms PPO in Atari video game tasks.",
        "Make this fluent while doing minimal change": "We propose a new, sample-efficient methodology called Supervised Policy Update (SPU) for deep reinforcement learning. SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space, using data generated by the current policy. It then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.",
        "Refine this for me please": "We propose a new, sample-efficient methodology called Supervised Policy Update (SPU) for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.",
        "Concise this for me and keep all the information": "We propose a new, sample-efficient methodology called Supervised Policy Update (SPU) for deep reinforcement learning. SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space, using supervised regression to convert the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.",
        "Improve this in GPT way": "We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks."
    },
    {
        "input": "We present a parameterized synthetic dataset called Moving Symbols to support\nthe objective study of video prediction networks. Using several instantiations\nof the dataset in which variation is explicitly controlled, we highlight issues\nin an existing state-of-the-art approach and propose the use of a performance\nmetric with greater semantic meaning to improve experimental interpretability.\nOur dataset provides canonical test cases that will help the community better\nunderstand, and eventually improve, the representations learned by such\nnetworks in the future. Code is available at\nhttps://github.com/rszeto/moving-symbols .",
        "Revise this with your best effort": "We introduce a parameterized synthetic dataset called Moving Symbols to support\nthe objective study of video prediction networks. By controlling variation in\nseveral instantiations of the dataset, we highlight issues in an existing\nstate-of-the-art approach and propose the use of a performance metric with\ngreater semantic meaning to improve experimental interpretability. Our dataset\nprovides canonical test cases that will help the community better understand,\nand eventually improve, the representations learned by such networks in the\nfuture. Code is available at https://github.com/rszeto/moving-symbols.",
        "Help me polish this": "We present a parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks. By controlling variation in several instantiations of the dataset, we highlight issues in an existing state-of-the-art approach and propose the use of a performance metric with greater semantic meaning to improve experimental interpretability. Our dataset provides canonical test cases that will help the community better understand, and eventually improve, the representations learned by such networks in the future. Code is available at https://github.com/rszeto/moving-symbols.",
        "Rewrite this for me": "We introduce a dataset called Moving Symbols that can be used to study the performance of video prediction networks. This dataset includes several variations that allow for explicit control, which can help to highlight issues in existing state-of-the-art approaches. We also propose the use of a performance metric with greater semantic meaning to improve experimental interpretability. Our dataset provides canonical test cases that will help the community to better understand, and eventually improve, the representations learned by such networks in the future.",
        "Make this fluent while doing minimal change": "We present a parameterized synthetic dataset, Moving Symbols, to support\nthe objective study of video prediction networks. By using several instantiations\nof the dataset in which variation is explicitly controlled, we highlight issues\nin an existing state-of-the-art approach and propose the use of a performance\nmetric with greater semantic meaning to improve experimental interpretability.\nOur dataset provides canonical test cases that will help the community better\nunderstand, and eventually improve, the representations learned by such\nnetworks in the future. Code is available at\nhttps://github.com/rszeto/moving-symbols .",
        "Refine this for me please": "We present a parameterized synthetic dataset called Moving Symbols to support\nthe objective study of video prediction networks. The dataset provides canonical test cases that will help the community better\nunderstand, and eventually improve, the representations learned by such\nnetworks in the future. Code is available at\nhttps://github.com/rszeto/moving-symbols.",
        "Concise this for me and keep all the information": "We present a parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks. Using several instantiations of the dataset in which variation is explicitly controlled, we highlight issues in an existing state-of-the-art approach and propose the use of a performance metric with greater semantic meaning to improve experimental interpretability. Our dataset provides canonical test cases that will help the community better understand, and eventually improve, the representations learned by such networks in the future.",
        "Improve this in GPT way": "We present a parameterized synthetic dataset called Moving Symbols to support\nthe objective study of video prediction networks. Using several instantiations\nof the dataset in which variation is explicitly controlled, we highlight issues\nin an existing state-of-the-art approach and propose the use of a performance\nmetric with greater semantic meaning to improve experimental interpretability.\nOur dataset provides canonical test cases that will help the community better\nunderstand, and eventually improve, the representations learned by such\nnetworks in the future. Code is available at\nhttps://github.com/rszeto/moving-symbols."
    },
    {
        "input": "This work is a part of ICLR Reproducibility Challenge 2019, we try to\nreproduce the results in the conference submission PADAM: Closing The\nGeneralization Gap of Adaptive Gradient Methods In Training Deep Neural\nNetworks. Adaptive gradient methods proposed in past demonstrate a degraded\ngeneralization performance than the stochastic gradient descent (SGD) with\nmomentum. The authors try to address this problem by designing a new\noptimization algorithm that bridges the gap between the space of Adaptive\nGradient algorithms and SGD with momentum. With this method a new tunable\nhyperparameter called partially adaptive parameter p is introduced that varies\nbetween [0, 0.5]. We build the proposed optimizer and use it to mirror the\nexperiments performed by the authors. We review and comment on the empirical\nanalysis performed by the authors. Finally, we also propose a future direction\nfor further study of Padam. Our code is available at:\nhttps://github.com/yashkant/Padam-Tensorflow",
        "Revise this with your best effort": "This work is part of the ICLR Reproducibility Challenge 2019. We are trying to reproduce the results of the conference submission PADAM: Closing The Generalization Gap of Adaptive Gradient Methods In Training Deep Neural Networks. Adaptive gradient methods have been shown to have degraded generalization performance compared to stochastic gradient descent (SGD) with momentum. The authors of PADAM try to address this problem by designing a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. With this method, a new tunable hyperparameter called partially adaptive parameter p is introduced that varies between [0, 0.5]. We have built the proposed optimizer and used it to mirror the experiments performed by the authors. We have reviewed and commented on the empirical analysis performed by the authors. Finally, we also propose a future direction for further study of Padam. Our code is available at: https://github.com/yashkant/Padam-Tensorflow",
        "Help me polish this": "This work is part of the ICLR Reproducibility Challenge 2019. We are trying to reproduce the results of the conference submission PADAM: Closing The Generalization Gap of Adaptive Gradient Methods In Training Deep Neural Networks. Adaptive gradient methods have been shown to have degraded generalization performance compared to stochastic gradient descent (SGD) with momentum. The authors of PADAM try to address this problem by designing a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. With this method, a new tunable hyperparameter called partially adaptive parameter p is introduced that varies between [0, 0.5]. We have built the proposed optimizer and used it to mirror the experiments performed by the authors. We have reviewed and commented on the empirical analysis performed by the authors. Finally, we also propose a future direction for further study of Padam. Our code is available at: https://github.com/yashkant/Padam-Tensorflow",
        "Rewrite this for me": "We are trying to reproduce the results of the PADAM conference submission as part of the ICLR Reproducibility Challenge 2019. In this submission, the authors try to address the problem of a degraded generalization performance of adaptive gradient methods compared to stochastic gradient descent (SGD) with momentum. They do this by designing a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. This new algorithm introduces a tunable hyperparameter called partially adaptive parameter p that varies between [0, 0.5]. We have built the proposed optimizer and are using it to mirror the experiments performed by the authors. We are reviewing and commenting on the empirical analysis performed by the authors. Finally, we are also proposing a future direction for further study of Padam. Our code is available at: https://github.com/yashkant/Padam-Tensorflow",
        "Make this fluent while doing minimal change": "This work is part of the ICLR Reproducibility Challenge 2019. We are trying to reproduce the results of the conference submission PADAM: Closing The Generalization Gap of Adaptive Gradient Methods In Training Deep Neural Networks. Adaptive gradient methods proposed in the past have demonstrated a degraded generalization performance compared to stochastic gradient descent (SGD) with momentum. The authors of PADAM try to address this problem by designing a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. With this method, a new tunable hyperparameter called partially adaptive parameter p is introduced that varies between [0, 0.5]. We have built the proposed optimizer and are using it to mirror the experiments performed by the authors. We are reviewing and commenting on the empirical analysis performed by the authors. Finally, we are also proposing a future direction for further study of Padam. Our code is available at: https://github.com/yashkant/Padam-Tensorflow",
        "Refine this for me please": "This work is a part of the ICLR Reproducibility Challenge 2019. We are trying to reproduce the results of the conference submission PADAM: Closing The Generalization Gap of Adaptive Gradient Methods In Training Deep Neural Networks. Adaptive gradient methods have been shown to have degraded generalization performance compared to stochastic gradient descent (SGD) with momentum. The authors of PADAM try to address this problem by designing a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. With this method, a new tunable hyperparameter called partially adaptive parameter p is introduced that varies between [0, 0.5]. We have built the proposed optimizer and are using it to mirror the experiments performed by the authors. We are reviewing and commenting on the empirical analysis performed by the authors. Finally, we are also proposing a future direction for further study of Padam. Our code is available at: https://github.com/yashkant/Padam-Tensorflow",
        "Concise this for me and keep all the information": "We tried to reproduce the results of the conference submission PADAM, which is a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. Our code is available at: https://github.com/yashkant/Padam-Tensorflow.",
        "Improve this in GPT way": "This work is a part of ICLR Reproducibility Challenge 2019. We try to reproduce the results in the conference submission PADAM: Closing The Generalization Gap of Adaptive Gradient Methods In Training Deep Neural Networks.\n\nAdaptive gradient methods proposed in past demonstrate a degraded generalization performance than the stochastic gradient descent (SGD) with momentum. The authors try to address this problem by designing a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. With this method, a new tunable hyperparameter called partially adaptive parameter p is introduced that varies between [0, 0.5].\n\nWe build the proposed optimizer and use it to mirror the experiments performed by the authors. We review and comment on the empirical analysis performed by the authors. Finally, we also propose a future direction for further study of Padam. Our code is available at: https://github.com/yashkant/Padam-Tensorflow"
    },
    {
        "input": "We present a large-scale empirical study of catastrophic forgetting (CF) in\nmodern Deep Neural Network (DNN) models that perform sequential (or:\nincremental) learning. A new experimental protocol is proposed that enforces\ntypical constraints encountered in application scenarios. As the investigation\nis empirical, we evaluate CF behavior on the hitherto largest number of visual\nclassification datasets, from each of which we construct a representative\nnumber of Sequential Learning Tasks (SLTs) in close alignment to previous works\non CF. Our results clearly indicate that there is no model that avoids CF for\nall investigated datasets and SLTs under application conditions. We conclude\nwith a discussion of potential solutions and workarounds to CF, notably for the\nEWC and IMM models.",
        "Revise this with your best effort": "We present an empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.",
        "Help me polish this": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.",
        "Rewrite this for me": "We present an empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.",
        "Make this fluent while doing minimal change": "We present an empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.",
        "Refine this for me please": "We present an empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.",
        "Concise this for me and keep all the information": "We present an empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.",
        "Improve this in GPT way": "We present a large-scale empirical study of catastrophic forgetting (CF) in\nmodern Deep Neural Network (DNN) models that perform sequential (or:\nincremental) learning. A new experimental protocol is proposed that enforces\ntypical constraints encountered in application scenarios. As the investigation\nis empirical, we evaluate CF behavior on the hitherto largest number of visual\nclassification datasets, from each of which we construct a representative\nnumber of Sequential Learning Tasks (SLTs) in close alignment to previous works\non CF. Our results clearly indicate that there is no model that avoids CF for\nall investigated datasets and SLTs under application conditions. We conclude\nwith a discussion of potential solutions and workarounds to CF, notably for the\nEWC and IMM models."
    },
    {
        "input": "Deep learning models for graphs have advanced the state of the art on many\ntasks. Despite their recent success, little is known about their robustness. We\ninvestigate training time attacks on graph neural networks for node\nclassification that perturb the discrete graph structure. Our core principle is\nto use meta-gradients to solve the bilevel problem underlying training-time\nattacks, essentially treating the graph as a hyperparameter to optimize. Our\nexperiments show that small graph perturbations consistently lead to a strong\ndecrease in performance for graph convolutional networks, and even transfer to\nunsupervised embeddings. Remarkably, the perturbations created by our algorithm\ncan misguide the graph neural networks such that they perform worse than a\nsimple baseline that ignores all relational information. Our attacks do not\nassume any knowledge about or access to the target classifiers.",
        "Revise this with your best effort": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
        "Help me polish this": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
        "Rewrite this for me": "Despite recent success, little is known about the robustness of deep learning models for graphs. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
        "Make this fluent while doing minimal change": "Despite recent success, little is known about the robustness of deep learning models for graphs. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
        "Refine this for me please": "We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
        "Concise this for me and keep all the information": "Deep learning models for graphs have advanced the state of the art on many tasks, but little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
        "Improve this in GPT way": "We investigate the robustness of graph neural networks by perturbing the discrete graph structure during training time. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations can lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information."
    },
    {
        "input": "Multi-domain learning (MDL) aims at obtaining a model with minimal average\nrisk across multiple domains. Our empirical motivation is automated microscopy\ndata, where cultured cells are imaged after being exposed to known and unknown\nchemical perturbations, and each dataset displays significant experimental\nbias. This paper presents a multi-domain adversarial learning approach, MuLANN,\nto leverage multiple datasets with overlapping but distinct class sets, in a\nsemi-supervised setting. Our contributions include: i) a bound on the average-\nand worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss\nto accommodate semi-supervised multi-domain learning and domain adaptation;\niii) the experimental validation of the approach, improving on the state of the\nart on two standard image benchmarks, and a novel bioimage dataset, Cell.",
        "Revise this with your best effort": "Multi-domain learning (MDL) aims to obtain a model with minimal average risk across multiple domains. Our empirical motivation is automated microscopy data, where cultured cells are imaged after being exposed to known and unknown chemical perturbations, and each dataset displays significant experimental bias. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. Our contributions include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell.",
        "Help me polish this": "Multi-domain learning (MDL) aims to obtain a model with minimal average risk across multiple domains. Our empirical motivation is automated microscopy data, where cultured cells are imaged after being exposed to known and unknown chemical perturbations, and each dataset displays significant experimental bias. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. Our contributions include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell.",
        "Rewrite this for me": "Multi-domain learning (MDL) is a method of obtaining a model with minimal average risk across multiple domains. The motivation for this approach is automated microscopy data, where cultured cells are imaged after being exposed to known and unknown chemical perturbations. Each dataset in this scenario displays significant experimental bias. MuLANN is a multi-domain adversarial learning approach that leverages multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. The contributions of this paper include: 1) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; 2) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; and 3) the experimental validation of the approach, which improves on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell.",
        "Make this fluent while doing minimal change": "Multi-domain learning (MDL) aims to obtain a model with minimal average risk across multiple domains. Our empirical motivation is automated microscopy data, where cultured cells are imaged after being exposed to known and unknown chemical perturbations, and each dataset displays significant experimental bias. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. Our contributions include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell.",
        "Refine this for me please": "Multi-domain learning (MDL) is a method of obtaining a model with minimal average risk across multiple domains. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. Our contributions include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell.",
        "Concise this for me and keep all the information": "Multi-domain learning (MDL) is a method of obtaining a model with minimal average risk across multiple domains. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. The contributions of this paper include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell.",
        "Improve this in GPT way": "Multi-domain learning (MDL) aims at obtaining a model with minimal average risk across multiple domains. Our empirical motivation is automated microscopy data, where cultured cells are imaged after being exposed to known and unknown chemical perturbations, and each dataset displays significant experimental bias. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. Our contributions include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell."
    },
    {
        "input": "We propose a neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall.",
        "Revise this with your best effort": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder.\n\nThe encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers).\n\nExtensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Help me polish this": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder.\n\nThe encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers).\n\nExtensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Rewrite this for me": "We propose a neural network for unsupervised anomaly detection which uses a robust subspace recovery layer (RSR layer) in order to extract the underlying subspace from a latent representation of given data and remove outliers which lie away from this subspace. This layer is used within an autoencoder; the encoder maps data into a latent space from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Make this fluent while doing minimal change": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Refine this for me please": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Concise this for me and keep all the information": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder.\n\nThe encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers).\n\nExtensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Improve this in GPT way": "We propose a GPT-based neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall."
    },
    {
        "input": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Revise this with your best effort": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Help me polish this": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Rewrite this for me": "Deep neural networks (DNNs) are able to learn complex, non-linear relationships between variables, which has led to them having impressive predictive performance. However, because it is difficult to visualize these relationships, DNNs have been referred to as black boxes, which has limited their applications. To address this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.",
        "Make this fluent while doing minimal change": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Refine this for me please": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.",
        "Concise this for me and keep all the information": "Deep neural networks (DNNs) are able to learn complex, non-linear relationships between variables, which has led to their impressive predictive performance. However, the lack of ability to effectively visualize these relationships has resulted in DNNs being characterized as black boxes, limiting their applications. To address this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.",
        "Improve this in GPT way": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise."
    },
    {
        "input": "In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples.",
        "Revise this with your best effort": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Help me polish this": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Rewrite this for me": "We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer.",
        "Make this fluent while doing minimal change": "In this work, we address the problem of musical timbre transfer by manipulating the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Refine this for me please": "In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",
        "Concise this for me and keep all the information": "The goal of musical timbre transfer is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. TimbreTron is a method for musical timbre transfer which applies \"image\" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. The Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance.",
        "Improve this in GPT way": "In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples."
    },
    {
        "input": "We propose a novel node embedding of directed graphs to statistical\nmanifolds, which is based on a global minimization of pairwise relative entropy\nand graph geodesics in a non-linear way. Each node is encoded with a\nprobability density function over a measurable space. Furthermore, we analyze\nthe connection between the geometrical properties of such embedding and their\nefficient learning procedure. Extensive experiments show that our proposed\nembedding is better in preserving the global geodesic information of graphs, as\nwell as outperforming existing embedding models on directed graphs in a variety\nof evaluation metrics, in an unsupervised setting.",
        "Revise this with your best effort": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Help me polish this": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Rewrite this for me": "We propose a new node embedding for directed graphs onto statistical manifolds. This embedding is based on a global minimization of pairwise relative entropy, and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. We also analyze the connection between the geometrical properties of such an embedding and its efficient learning procedure. Extensive experiments show that our proposed embedding is better at preserving the global geodesic information of graphs, and outperforms existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Make this fluent while doing minimal change": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Refine this for me please": "We propose a new way of representing nodes in directed graphs as points on a statistical manifold. This is done by minimizing pairwise relative entropy and graph geodesics in a non-linear way. Each node is then represented by a probability density function over a measurable space. We also analyze the connection between the geometrical properties of such an embedding and its efficient learning procedure. Experiments show that our proposed embedding is better at preserving global geodesic information than existing embedding models, and outperforms them in a variety of evaluation metrics.",
        "Concise this for me and keep all the information": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Improve this in GPT way": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting."
    },
    {
        "input": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Revise this with your best effort": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Help me polish this": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Rewrite this for me": "Animal brains are able to learn throughout their lifetime primarily due to changes in synaptic connectivity that are plastic, meaning they can be changed. It's important to note that these changes are not passive, but are actively controlled by neuromodulation. This process is itself controlled by the brain, resulting in the brain's ability to modify itself, which plays an important role in learning and adaptation. This is also a major basis for biological reinforcement learning. Here, we show for the first time that artificial neural networks with this type of neuromodulated plasticity can be trained using gradient descent. We extend previous work on differentiable Hebbian plasticity by proposing a differentiable formulation for the neuromodulation of plasticity. We demonstrate that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic Long Short-Term Memory networks with millions of parameters outperform standard Long Short-Term Memory networks on a benchmark language modeling task. We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.",
        "Make this fluent while doing minimal change": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity that are actively controlled by neuromodulation. This neuromodulation is itself under\nthe control of the brain, resulting in self-modifying abilities that play an important role in learning and adaptation. This is a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Refine this for me please": "The ability of animals to learn throughout their lives is primarily due to the brain's ability to change the connections between neurons (synapses). These changes are not passive, but are actively controlled by neuromodulation, which itself is controlled by the brain. This allows the brain to constantly modify its own structure and function, which is essential for learning and adaptation.\n\nWe show that artificial neural networks can be trained using gradient descent to mimic this process of neuromodulated plasticity. This offers a powerful new framework for training neural networks that can improve performance on both reinforcement learning and supervised learning tasks.",
        "Concise this for me and keep all the information": "Animal brains are able to learn throughout their lifetime due to changes in synaptic connectivity, which are actively controlled by neuromodulation. This self-modifying ability of the brain is key to learning and adaptation, and forms the basis for reinforcement learning. Here, we show that artificial neural networks can be trained with gradient descent to mimic this process. By extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. This improved neural network is then put to the test on both reinforcement learning and supervised learning tasks, outperforming standard models on a benchmark language modeling task.",
        "Improve this in GPT way": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks."
    },
    {
        "input": "Euclidean geometry has historically been the typical \"workhorse\" for machine\nlearning applications due to its power and simplicity. However, it has recently\nbeen shown that geometric spaces with constant non-zero curvature improve\nrepresentations and performance on a variety of data types and downstream\ntasks. Consequently, generative models like Variational Autoencoders (VAEs)\nhave been successfully generalized to elliptical and hyperbolic latent spaces.\nWhile these approaches work well on data with particular kinds of biases e.g.\ntree-like data for a hyperbolic VAE, there exists no generic approach unifying\nand leveraging all three models. We develop a Mixed-curvature Variational\nAutoencoder, an efficient way to train a VAE whose latent space is a product of\nconstant curvature Riemannian manifolds, where the per-component curvature is\nfixed or learnable. This generalizes the Euclidean VAE to curved latent spaces\nand recovers it when curvatures of all latent space components go to 0.",
        "Revise this with your best effort": "Euclidean geometry has been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Help me polish this": "Euclidean geometry has been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Rewrite this for me": "Euclidean geometry is typically the \"workhorse\" for machine learning applications because of its power and simplicity. However, recent studies have shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases (e.g. tree-like data for a hyperbolic VAE), there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Make this fluent while doing minimal change": "Euclidean geometry has historically been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Refine this for me please": "Euclidean geometry has been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Concise this for me and keep all the information": "Euclidean geometry is typically used for machine learning applications because it is powerful and simple. However, recent studies have shown that geometric spaces with constant non-zero curvature can improve representations and performance on various data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases (e.g. tree-like data for a hyperbolic VAE), there is no generic approach unifying and leveraging all three models. The Mixed-curvature Variational Autoencoder is an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Improve this in GPT way": "Euclidean geometry has historically been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0."
    },
    {
        "input": "We explore various methods for computing sentence representations from\npre-trained word embeddings without any training, i.e., using nothing but\nrandom parameterizations. Our aim is to put sentence embeddings on more solid\nfooting by 1) looking at how much modern sentence embeddings gain over random\nmethods---as it turns out, surprisingly little; and by 2) providing the field\nwith more appropriate baselines going forward---which are, as it turns out,\nquite strong. We also make important observations about proper experimental\nprotocol for sentence classification evaluation, together with recommendations\nfor future research.",
        "Revise this with your best effort": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.",
        "Help me polish this": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.",
        "Rewrite this for me": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.",
        "Make this fluent while doing minimal change": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.",
        "Refine this for me please": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.",
        "Concise this for me and keep all the information": "We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong.",
        "Improve this in GPT way": "We explore various methods for computing sentence representations from\npre-trained word embeddings without any training, i.e., using nothing but\nrandom parameterizations. Our aim is to put sentence embeddings on more solid\nfooting by 1) looking at how much modern sentence embeddings gain over random\nmethods---as it turns out, surprisingly little; and by 2) providing the field\nwith more appropriate baselines going forward---which are, as it turns out,\nquite strong. We also make important observations about proper experimental\nprotocol for sentence classification evaluation, together with recommendations\nfor future research."
    },
    {
        "input": "Generative Adversarial Networks (GANs) are one of the most popular tools for\nlearning complex high dimensional distributions. However, generalization\nproperties of GANs have not been well understood. In this paper, we analyze the\ngeneralization of GANs in practical settings. We show that discriminators\ntrained on discrete datasets with the original GAN loss have poor\ngeneralization capability and do not approximate the theoretically optimal\ndiscriminator. We propose a zero-centered gradient penalty for improving the\ngeneralization of the discriminator by pushing it toward the optimal\ndiscriminator. The penalty guarantees the generalization and convergence of\nGANs. Experiments on synthetic and large scale datasets verify our theoretical\nanalysis.",
        "Revise this with your best effort": "Generative Adversarial Networks (GANs) are a popular tool for learning complex high dimensional distributions. However, the generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.",
        "Help me polish this": "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, the generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.",
        "Rewrite this for me": "Generative Adversarial Networks (GANs) are a popular tool for learning complex high dimensional distributions. However, the generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.",
        "Make this fluent while doing minimal change": "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.",
        "Refine this for me please": "GANs are a popular tool for learning complex high dimensional distributions, but their generalization properties have not been well understood. In this paper, we analyze the generalization of GANS in practical settings. We show that discriminators trained with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.",
        "Concise this for me and keep all the information": "GANs are a popular tool for learning complex high dimensional distributions, but their generalization properties are not well understood. This paper analyzes the generalization of GANs in practical settings. It shows that discriminators trained with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. It proposes a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify the theoretical analysis.",
        "Improve this in GPT way": "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis."
    },
    {
        "input": "In this paper we propose to perform model ensembling in a multiclass or a\nmultilabel learning setting using Wasserstein (W.) barycenters. Optimal\ntransport metrics, such as the Wasserstein distance, allow incorporating\nsemantic side information such as word embeddings. Using W. barycenters to find\nthe consensus between models allows us to balance confidence and semantics in\nfinding the agreement between the models. We show applications of Wasserstein\nensembling in attribute-based classification, multilabel learning and image\ncaptioning generation. These results show that the W. ensembling is a viable\nalternative to the basic geometric or arithmetic mean ensembling.",
        "Revise this with your best effort": "In this paper, we propose using Wasserstein barycenters to perform model ensembling in a multiclass or multilabel learning setting. Optimal transport metrics, such as the Wasserstein distance, allow us to incorporate semantic side information such as word embeddings. Using Wasserstein barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that Wasserstein ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.",
        "Help me polish this": "In this paper, we propose to use Wasserstein barycenters to perform model ensembling in a multiclass or multilabel learning setting. Optimal transport metrics, such as the Wasserstein distance, allow us to incorporate semantic side information such as word embeddings. Using Wasserstein barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that Wasserstein ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.",
        "Rewrite this for me": "In this paper, we propose using Wasserstein barycenters to perform model ensembling in a multiclass or multilabel learning setting. Optimal transport metrics, such as the Wasserstein distance, allow us to incorporate semantic side information such as word embeddings. Using Wasserstein barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning, and image captioning generation. These results show that Wasserstein ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.",
        "Make this fluent while doing minimal change": "In this paper, we propose using Wasserstein (W.) barycenters to perform model ensembling in a multiclass or multilabel learning setting. Optimal transport metrics, such as the Wasserstein distance, allow incorporating semantic side information such as word embeddings. Using W. barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that the W. ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.",
        "Refine this for me please": "In this paper, we propose using Wasserstein barycenters to perform model ensembling in a multiclass or multilabel learning setting. Optimal transport metrics, such as the Wasserstein distance, allow us to incorporate semantic side information such as word embeddings. Using Wasserstein barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that Wasserstein ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.",
        "Concise this for me and keep all the information": "We propose using Wasserstein barycenters to find the consensus between models in a multiclass or multilabel learning setting. Optimal transport metrics, such as the Wasserstein distance, allow incorporating semantic side information such as word embeddings. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that the Wasserstein ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.",
        "Improve this in GPT way": "In this paper, we propose a novel approach to model ensembling using Wasserstein barycenters. This approach allows us to incorporate semantic side information such as word embeddings, and to balance confidence and semantics in finding the agreement between models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results demonstrate that Wasserstein ensembling is a viable alternative to traditional methods such as the geometric or arithmetic mean."
    },
    {
        "input": "We present a method that learns to integrate temporal information, from a\nlearned dynamics model, with ambiguous visual information, from a learned\nvision model, in the context of interacting agents. Our method is based on a\ngraph-structured variational recurrent neural network (Graph-VRNN), which is\ntrained end-to-end to infer the current state of the (partially observed)\nworld, as well as to forecast future states. We show that our method\noutperforms various baselines on two sports datasets, one based on real\nbasketball trajectories, and one generated by a soccer game engine.",
        "Revise this with your best effort": "We present a method for learning to integrate temporal information from a learned dynamics model with ambiguous visual information from a learned vision model in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.",
        "Help me polish this": "We present a method for learning to integrate temporal information from a learned dynamics model with ambiguous visual information from a learned vision model in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.",
        "Rewrite this for me": "We introduce a method that can learn to incorporate temporal information, from a learned dynamics model, with unclear visual information, from a learned vision model, in the context of interacting agents. Our method is founded on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained from start to finish to infer the current state of the (partially observed) world, as well as to predict future states. We demonstrate that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.",
        "Make this fluent while doing minimal change": "We present a method for integrating temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method, which is based on a graph-structured variational recurrent neural network (Graph-VRNN), is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.",
        "Refine this for me please": "We present a method for learning to integrate temporal information from a learned dynamics model with ambiguous visual information from a learned vision model in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.",
        "Concise this for me and keep all the information": "We present a method for learning to integrate temporal information from a learned dynamics model with ambiguous visual information from a learned vision model in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.",
        "Improve this in GPT way": "We present a method that learns to integrate temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine."
    },
    {
        "input": "Modern neural networks are over-parametrized. In particular, each rectified\nlinear hidden unit can be modified by a multiplicative factor by adjusting\ninput and output weights, without changing the rest of the network. Inspired by\nthe Sinkhorn-Knopp algorithm, we introduce a fast iterative method for\nminimizing the L2 norm of the weights, equivalently the weight decay\nregularizer. It provably converges to a unique solution. Interleaving our\nalgorithm with SGD during training improves the test accuracy. For small\nbatches, our approach offers an alternative to batch-and group-normalization on\nCIFAR-10 and ImageNet with a ResNet-18.",
        "Revise this with your best effort": "Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and output weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18.",
        "Help me polish this": "Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and output weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18.",
        "Rewrite this for me": "Modern neural networks are over-parametrized. In particular, each rectified\nlinear hidden unit can be modified by a multiplicative factor by adjusting\ninput and output weights, without changing the rest of the network. Inspired by\nthe Sinkhorn-Knopp algorithm, we introduce a fast iterative method for\nminimizing the L2 norm of the weights, equivalently the weight decay\nregularizer. It provably converges to a unique solution. Interleaving our\nalgorithm with SGD during training improves the test accuracy. For small\nbatches, our approach offers an alternative to batch-and group-normalization on\nCIFAR-10 and ImageNet with a ResNet-18.",
        "Make this fluent while doing minimal change": "Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and output weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18.",
        "Refine this for me please": "We introduce a fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer, which provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18.",
        "Concise this for me and keep all the information": "Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and output weights, without changing the rest of the network. We introduce a fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18.",
        "Improve this in GPT way": "We introduce a new fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer. Our method provably converges to a unique solution, and interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18."
    },
    {
        "input": "Spherical data is found in many applications. By modeling the discretized\nsphere as a graph, we can accommodate non-uniformly distributed, partial, and\nchanging samplings. Moreover, graph convolutions are computationally more\nefficient than spherical convolutions. As equivariance is desired to exploit\nrotational symmetries, we discuss how to approach rotation equivariance using\nthe graph neural network introduced in Defferrard et al. (2016). Experiments\nshow good performance on rotation-invariant learning problems. Code and\nexamples are available at https://github.com/SwissDataScienceCenter/DeepSphere",
        "Revise this with your best effort": "Spherical data is found in many applications. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Help me polish this": "Spherical data is found in many applications, and modeling the discretized sphere as a graph can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Rewrite this for me": "There are many applications that use spherical data. By representing the discretized sphere as a graph, we can deal with non-uniformly distributed, partial, and changing samplings. In addition, graph convolutions are more efficient than spherical convolutions. Since equivariance is desired to take advantage of rotational symmetries, we explain how to achieve rotation equivariance using the graph neural network proposed in Defferrard et al. (2016). Our experiments demonstrate good performance on rotation-invariant learning tasks. The code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Make this fluent while doing minimal change": "Many applications use spherical data. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Graph convolutions are more efficient than spherical convolutions. To exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Refine this for me please": "Spherical data is found in many applications, and by modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere.",
        "Concise this for me and keep all the information": "Spherical data is found in many applications. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems.",
        "Improve this in GPT way": "We propose a new approach to spherical data that models the discretized sphere as a graph. This approach is more efficient than traditional methods, and can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, our approach is equivariant to rotations, which is desired in many applications. We provide code and examples at https://github.com/SwissDataScienceCenter/DeepSphere."
    },
    {
        "input": "We present graph wavelet neural network (GWNN), a novel graph convolutional\nneural network (CNN), leveraging graph wavelet transform to address the\nshortcomings of previous spectral graph CNN methods that depend on graph\nFourier transform. Different from graph Fourier transform, graph wavelet\ntransform can be obtained via a fast algorithm without requiring matrix\neigendecomposition with high computational cost. Moreover, graph wavelets are\nsparse and localized in vertex domain, offering high efficiency and good\ninterpretability for graph convolution. The proposed GWNN significantly\noutperforms previous spectral graph CNNs in the task of graph-based\nsemi-supervised classification on three benchmark datasets: Cora, Citeseer and\nPubmed.",
        "Revise this with your best effort": "We present the graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN) that uses graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.",
        "Help me polish this": "We present the graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN) that leverages graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.",
        "Rewrite this for me": "We introduce the graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN) that uses graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Unlike graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition, which is computationally expensive. In addition, graph wavelets are sparse and localized in the vertex domain, which makes them more efficient and interpretable for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer, and Pubmed.",
        "Make this fluent while doing minimal change": "We present the graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN) that leverages graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.",
        "Refine this for me please": "We present the graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN) that uses graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.",
        "Concise this for me and keep all the information": "We present the graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN) that uses graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.",
        "Improve this in GPT way": "We present the graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN) that leverages graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed."
    },
    {
        "input": "We propose a single neural probabilistic model based on variational\nautoencoder that can be conditioned on an arbitrary subset of observed features\nand then sample the remaining features in \"one shot\". The features may be both\nreal-valued and categorical. Training of the model is performed by stochastic\nvariational Bayes. The experimental evaluation on synthetic data, as well as\nfeature imputation and image inpainting problems, shows the effectiveness of\nthe proposed approach and diversity of the generated samples.",
        "Revise this with your best effort": "We propose a single neural probabilistic model based on a variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Help me polish this": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Rewrite this for me": "We propose a single neural probabilistic model, based on a variational autoencoder, that can be conditioned on an arbitrary subset of observed features, and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach, and the diversity of the generated samples.",
        "Make this fluent while doing minimal change": "We propose a single neural probabilistic model based on a variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Refine this for me please": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Concise this for me and keep all the information": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The model is trained by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
        "Improve this in GPT way": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples."
    },
    {
        "input": "We present the perceptor gradients algorithm -- a novel approach to learning\nsymbolic representations based on the idea of decomposing an agent's policy\ninto i) a perceptor network extracting symbols from raw observation data and\nii) a task encoding program which maps the input symbols to output actions. We\nshow that the proposed algorithm is able to learn representations that can be\ndirectly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A*\nplanner. Our experimental results confirm that the perceptor gradients\nalgorithm is able to efficiently learn transferable symbolic representations as\nwell as generate new observations according to a semantically meaningful\nspecification.",
        "Revise this with your best effort": "We present the perceptor gradients algorithm, a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into two parts: a perceptor network that extracts symbols from raw observation data, and a task encoding program that maps input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations and generate new observations according to a semantically meaningful specification.",
        "Help me polish this": "We present the perceptor gradients algorithm, a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.",
        "Rewrite this for me": "We introduce the perceptor gradients algorithm, a new approach to learning symbolic representations that is based on decomposing an agent's policy into two parts: a perceptor network that extracts symbols from raw observation data, and a task encoding program that maps input symbols to output actions. We demonstrate that the proposed algorithm is able to learn representations that can be directly used by a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations, as well as generate new observations according to a semantically meaningful specification.",
        "Make this fluent while doing minimal change": "We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.",
        "Refine this for me please": "We present the perceptor gradients algorithm, a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into two parts:\n\n1) a perceptor network that extracts symbols from raw observation data, and\n\n2) a task encoding program that maps input symbols to output actions.\n\nWe show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.",
        "Concise this for me and keep all the information": "We present the perceptor gradients algorithm, a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into a perceptor network extracting symbols from raw observation data and a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.",
        "Improve this in GPT way": "We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification."
    },
    {
        "input": "We study the robustness to symmetric label noise of GNNs training procedures.\nBy combining the nonlinear neural message-passing models (e.g. Graph\nIsomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present\na noise-tolerant approach for the graph classification task. Our experiments\nshow that test accuracy can be improved under the artificial symmetric noisy\nsetting.",
        "Revise this with your best effort": "We study the robustness of GNNs training procedures to symmetric label noise. By combining the nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present a noise-tolerant approach for the graph classification task. Our experiments show that test accuracy can be improved under the artificial symmetric noisy setting.",
        "Help me polish this": "We study the robustness of GNNs training procedures to symmetric label noise. By combining the nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present a noise-tolerant approach for the graph classification task. Our experiments show that test accuracy can be improved under the artificial symmetric noisy setting.",
        "Rewrite this for me": "We study the robustness of GNNs training procedures to symmetric label noise. By combining the nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present a noise-tolerant approach for the graph classification task. Our experiments show that test accuracy can be improved under the artificial symmetric noisy setting.",
        "Make this fluent while doing minimal change": "We study the robustness of GNNs training procedures to symmetric label noise. By combining the nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present a noise-tolerant approach for the graph classification task. Our experiments show that test accuracy can be improved under the artificial symmetric noisy setting.",
        "Refine this for me please": "We study the robustness of GNNs training procedures to symmetric label noise. By combining the nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present a noise-tolerant approach for the graph classification task. Our experiments show that test accuracy can be improved under the artificial symmetric noisy setting.",
        "Concise this for me and keep all the information": "We study the robustness of GNNs training procedures to symmetric label noise. By combining the nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present a noise-tolerant approach for the graph classification task. Our experiments show that test accuracy can be improved under the artificial symmetric noisy setting.",
        "Improve this in GPT way": "We study the robustness to symmetric label noise of GNNs training procedures.\nBy combining the nonlinear neural message-passing models (e.g. Graph\nIsomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present\na noise-tolerant approach for the graph classification task. Our experiments\nshow that test accuracy can be improved under the artificial symmetric noisy\nsetting."
    },
    {
        "input": "The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy above $90\\%$, which improves on previous\nsimilar work.",
        "Revise this with your best effort": "The recent use of Big Code with state-of-the-art deep learning methods offers promising avenues to ease program source code writing and correction. As a first step towards automatic code repair, we implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy of 90% or higher, which improves on previous similar work.",
        "Help me polish this": "The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy of $90\\%$ or higher, which improves on previous\nsimilar work.",
        "Rewrite this for me": "The recent use of Big Code with state-of-the-art deep learning methods offers promising avenues to ease program source code writing and correction. As a first step towards automatic code repair, we implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy above 90%, which improves on previous similar work.",
        "Make this fluent while doing minimal change": "The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy of $90\\%$ or higher, which improves on previous\nsimilar work.",
        "Refine this for me please": "The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy above $90\\%$, which is a significant improvement on previous\nsimilar work.",
        "Concise this for me and keep all the information": "We implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy above $90\\%$, which improves on previous similar work.",
        "Improve this in GPT way": "The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy above $90\\%$, which improves on previous\nsimilar work. We believe that this is just the beginning of what can be achieved with\ngraph neural networks for code prediction and repair, and we are excited to explore\nthis further in the future."
    },
    {
        "input": "In this paper we consider self-supervised representation learning to improve\nsample efficiency in reinforcement learning (RL). We propose a forward\nprediction objective for simultaneously learning embeddings of states and\naction sequences. These embeddings capture the structure of the environment's\ndynamics, enabling efficient policy learning. We demonstrate that our action\nembeddings alone improve the sample efficiency and peak performance of\nmodel-free RL on control from low-dimensional states. By combining state and\naction embeddings, we achieve efficient learning of high-quality policies on\ngoal-conditioned continuous control from pixel observations in only 1-2 million\nenvironment steps.",
        "Revise this with your best effort": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose an objective for learning embeddings of states and action sequences that captures the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Help me polish this": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Rewrite this for me": "In this paper, we explore how self-supervised representation learning can be used to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Make this fluent while doing minimal change": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Refine this for me please": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Concise this for me and keep all the information": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Improve this in GPT way": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps."
    },
    {
        "input": "We study the problem of learning permutation invariant representations that\ncan capture \"flexible\" notions of containment. We formalize this problem via a\nmeasure theoretic definition of multisets, and obtain a theoretically-motivated\nlearning model. We propose training this model on a novel task: predicting the\nsize of the symmetric difference (or intersection) between pairs of multisets.\nWe demonstrate that our model not only performs very well on predicting\ncontainment relations (and more effectively predicts the sizes of symmetric\ndifferences and intersections than DeepSets-based approaches with unconstrained\nobject representations), but that it also learns meaningful representations.",
        "Revise this with your best effort": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure-theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Help me polish this": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure-theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Rewrite this for me": "We investigate the issue of learning permutation invariant representations that can express \"flexible\" containment concepts. We formalize this problem through a measure-theoretic definition of multisets, and develop a learning model motivated by this definition. We train this model on a new task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We show that our model not only performs very well on containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but also learns useful representations.",
        "Make this fluent while doing minimal change": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Refine this for me please": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Concise this for me and keep all the information": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Improve this in GPT way": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations."
    },
    {
        "input": "One way to interpret trained deep neural networks (DNNs) is by inspecting\ncharacteristics that neurons in the model respond to, such as by iteratively\noptimising the model input (e.g., an image) to maximally activate specific\nneurons. However, this requires a careful selection of hyper-parameters to\ngenerate interpretable examples for each neuron of interest, and current\nmethods rely on a manual, qualitative evaluation of each setting, which is\nprohibitively slow. We introduce a new metric that uses Fr\\'echet Inception\nDistance (FID) to encourage similarity between model activations for real and\ngenerated data. This provides an efficient way to evaluate a set of generated\nexamples for each setting of hyper-parameters. We also propose a novel\nGAN-based method for generating explanations that enables an efficient search\nthrough the input space and imposes a strong prior favouring realistic outputs.\nWe apply our approach to a classification model trained to predict whether a\nmusic audio recording contains singing voice. Our results suggest that this\nproposed metric successfully selects hyper-parameters leading to interpretable\nexamples, avoiding the need for manual evaluation. Moreover, we see that\nexamples synthesised to maximise or minimise the predicted probability of\nsinging voice presence exhibit vocal or non-vocal characteristics,\nrespectively, suggesting that our approach is able to generate suitable\nexplanations for understanding concepts learned by a neural network.",
        "Revise this with your best effort": "One way to interpret trained deep neural networks (DNNs) is by inspecting the characteristics that neurons in the model respond to. For example, you can iteratively optimize the model input (e.g., an image) to maximally activate specific neurons. However, this requires a careful selection of hyper-parameters to generate interpretable examples for each neuron of interest. And current methods for doing this rely on a manual, qualitative evaluation of each setting, which can be prohibitively slow.\n\nWe introduce a new metric that uses Fr\\'echet Inception Distance (FID) to encourage similarity between model activations for real and generated data. This provides a more efficient way to evaluate a set of generated examples for each setting of hyper-parameters. We also propose a novel GAN-based method for generating explanations that enables an efficient search through the input space and imposes a strong prior favoring realistic outputs.\n\nWe apply our approach to a classification model trained to predict whether a music audio recording contains singing voice. Our results suggest that this proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Moreover, we see that examples synthesized to maximise or minimise the predicted probability of singing voice presence exhibit vocal or non-vocal characteristics, respectively, suggesting that our approach is able to generate suitable explanations for understanding concepts learned by a neural network.",
        "Help me polish this": "One way to interpret trained deep neural networks (DNNs) is by inspecting the characteristics that neurons in the model respond to. For example, you can iteratively optimize the model input (e.g., an image) to maximally activate specific neurons. However, this requires a careful selection of hyper-parameters to generate interpretable examples for each neuron of interest. And current methods for doing this rely on a manual, qualitative evaluation of each setting, which can be prohibitively slow.\n\nWe introduce a new metric that uses Fr\\'echet Inception Distance (FID) to encourage similarity between model activations for real and generated data. This provides a more efficient way to evaluate a set of generated examples for each setting of hyper-parameters. We also propose a novel GAN-based method for generating explanations that enables an efficient search through the input space and imposes a strong prior favoring realistic outputs.\n\nWe apply our approach to a classification model trained to predict whether a music audio recording contains singing voice. Our results suggest that this proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Moreover, we see that examples synthesized to maximise or minimise the predicted probability of singing voice presence exhibit vocal or non-vocal characteristics, respectively, suggesting that our approach is able to generate suitable explanations for understanding concepts learned by a neural network.",
        "Rewrite this for me": "One way to interpret a trained deep neural network is by looking at the characteristics that neurons in the model respond to. For example, you can iteratively optimize the model input (like an image) to maximally activate specific neurons. However, this process requires a careful selection of hyper-parameters to generate interpretable examples for each neuron of interest. And current methods for doing this rely on a manual, qualitative evaluation of each setting, which can be prohibitively slow.\n\nWe introduce a new metric that uses Fr\\'echet Inception Distance (FID) to encourage similarity between model activations for real and generated data. This provides a more efficient way to evaluate a set of generated examples for each setting of hyper-parameters. We also propose a novel GAN-based method for generating explanations that enables an efficient search through the input space and imposes a strong prior favouring realistic outputs.\n\nWe apply our approach to a classification model trained to predict whether a music audio recording contains singing voice. Our results suggest that this proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Moreover, we see that examples synthesised to maximise or minimise the predicted probability of singing voice presence exhibit vocal or non-vocal characteristics, respectively, suggesting that our approach is able to generate suitable explanations for understanding concepts learned by a neural network.",
        "Make this fluent while doing minimal change": "One way to interpret trained deep neural networks (DNNs) is by inspecting the characteristics that neurons in the model respond to, such as by iteratively optimising the model input (e.g., an image) to maximally activate specific neurons. However, this requires a careful selection of hyper-parameters to generate interpretable examples for each neuron of interest, and current methods rely on a manual, qualitative evaluation of each setting, which is prohibitively slow.\n\nWe introduce a new metric that uses Fr\\'echet Inception Distance (FID) to encourage similarity between model activations for real and generated data. This provides an efficient way to evaluate a set of generated examples for each setting of hyper-parameters. We also propose a novel GAN-based method for generating explanations that enables an efficient search through the input space and imposes a strong prior favouring realistic outputs.\n\nWe apply our approach to a classification model trained to predict whether a music audio recording contains singing voice. Our results suggest that this proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Moreover, we see that examples synthesised to maximise or minimise the predicted probability of singing voice presence exhibit vocal or non-vocal characteristics, respectively, suggesting that our approach is able to generate suitable explanations for understanding concepts learned by a neural network.",
        "Refine this for me please": "One way to interpret a trained deep neural network is by looking at the characteristics that neurons in the model respond to. For example, you can iteratively optimize the model input (like an image) to maximally activate specific neurons. However, this process requires a careful selection of hyper-parameters to generate interpretable examples for each neuron of interest. And current methods for doing this rely on a manual, qualitative evaluation of each setting, which can be prohibitively slow.\n\nWe introduce a new metric that uses Fr\\'echet Inception Distance (FID) to encourage similarity between model activations for real and generated data. This provides a more efficient way to evaluate a set of generated examples for each setting of hyper-parameters. We also propose a novel GAN-based method for generating explanations that enables an efficient search through the input space and imposes a strong prior favouring realistic outputs.\n\nWe apply our approach to a classification model trained to predict whether a music audio recording contains singing voice. Our results suggest that this proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Moreover, we see that examples synthesised to maximise or minimise the predicted probability of singing voice presence exhibit vocal or non-vocal characteristics, respectively, suggesting that our approach is able to generate suitable explanations for understanding concepts learned by a neural network.",
        "Concise this for me and keep all the information": "One way to interpret trained deep neural networks (DNNs) is by inspecting characteristics that neurons in the model respond to, such as by iteratively optimising the model input (e.g., an image) to maximally activate specific neurons. However, this requires a careful selection of hyper-parameters to generate interpretable examples for each neuron of interest, which current methods rely on a manual, qualitative evaluation of each setting, which is prohibitively slow.\n\nWe introduce a new metric that uses Fr\\'echet Inception Distance (FID) to encourage similarity between model activations for real and generated data. This provides an efficient way to evaluate a set of generated examples for each setting of hyper-parameters. We also propose a novel GAN-based method for generating explanations that enables an efficient search through the input space and imposes a strong prior favouring realistic outputs.\n\nWe apply our approach to a classification model trained to predict whether a music audio recording contains singing voice. Our results suggest that this proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Moreover, we see that examples synthesised to maximise or minimise the predicted probability of singing voice presence exhibit vocal or non-vocal characteristics, respectively, suggesting that our approach is able to generate suitable explanations for understanding concepts learned by a neural network.",
        "Improve this in GPT way": "One way to interpret trained deep neural networks (DNNs) is by inspecting the characteristics that neurons in the model respond to. For example, you can iteratively optimize the model input (e.g., an image) to maximally activate specific neurons. However, this requires a careful selection of hyper-parameters to generate interpretable examples for each neuron of interest. And current methods for doing this rely on a manual, qualitative evaluation of each setting, which can be prohibitively slow.\n\nWe introduce a new metric that uses Fr\\'echet Inception Distance (FID) to encourage similarity between model activations for real and generated data. This provides a more efficient way to evaluate a set of generated examples for each setting of hyper-parameters. We also propose a novel GAN-based method for generating explanations that enables an efficient search through the input space and imposes a strong prior favouring realistic outputs.\n\nWe apply our approach to a classification model trained to predict whether a music audio recording contains singing voice. Our results suggest that this proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Moreover, we see that examples synthesised to maximise or minimise the predicted probability of singing voice presence exhibit vocal or non-vocal characteristics, respectively, suggesting that our approach is able to generate suitable explanations for understanding concepts learned by a neural network."
    },
    {
        "input": "We characterize the singular values of the linear transformation associated\nwith a standard 2D multi-channel convolutional layer, enabling their efficient\ncomputation. This characterization also leads to an algorithm for projecting a\nconvolutional layer onto an operator-norm ball. We show that this is an\neffective regularizer; for example, it improves the test error of a deep\nresidual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Revise this with your best effort": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2% to 5.3%.",
        "Help me polish this": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Rewrite this for me": "We show that the singular values of a standard 2D multi-channel convolutional layer can be characterized efficiently, leading to an algorithm for projecting the layer onto an operator-norm ball. This is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Make this fluent while doing minimal change": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, which enables their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Refine this for me please": "We show that the singular values of a standard 2D multi-channel convolutional layer can be efficiently computed, and that this characterization can be used to improve the test error of a deep residual network using batch normalization on CIFAR-10.",
        "Concise this for me and keep all the information": "We show that the operator-norm ball is an effective regularizer for deep residual networks using batch normalization. This improves the test error on CIFAR-10 from 6.2\\% to 5.3\\%.",
        "Improve this in GPT way": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%."
    },
    {
        "input": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Revise this with your best effort": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Help me polish this": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Rewrite this for me": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Make this fluent while doing minimal change": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data and our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Refine this for me please": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem.",
        "Concise this for me and keep all the information": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits.",
        "Improve this in GPT way": "We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem."
    },
    {
        "input": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning\nalgorithms that capture the dynamics of physical systems from observed\ntrajectories. An SRNN models the Hamiltonian function of the system by a neural\nnetwork and furthermore leverages symplectic integration, multiple-step\ntraining and initial state optimization to address the challenging numerical\nissues associated with Hamiltonian systems. We show that SRNNs succeed reliably\non complex and noisy Hamiltonian systems. We also show how to augment the SRNN\nintegration scheme in order to handle stiff dynamical systems such as bouncing\nbilliards.",
        "Revise this with your best effort": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Help me polish this": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Rewrite this for me": "We propose that Symplectic Recurrent Neural Networks (SRNNs) are learning algorithms that can capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by using a neural network. Furthermore, SRNNs use symplectic integration, multiple-step training, and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We have shown that SRNNs can succeed reliably on complex and noisy Hamiltonian systems. We have also shown how to augment the SRNN integration scheme in order to handle stiff dynamical systems, such as bouncing billiards.",
        "Make this fluent while doing minimal change": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. SRNNs model the Hamiltonian function of the system by a neural network and furthermore leverage symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Refine this for me please": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Concise this for me and keep all the information": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems.",
        "Improve this in GPT way": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards."
    },
    {
        "input": "Spectral embedding is a popular technique for the representation of graph\ndata. Several regularization techniques have been proposed to improve the\nquality of the embedding with respect to downstream tasks like clustering. In\nthis paper, we explain on a simple block model the impact of the complete graph\nregularization, whereby a constant is added to all entries of the adjacency\nmatrix. Specifically, we show that the regularization forces the spectral\nembedding to focus on the largest blocks, making the representation less\nsensitive to noise or outliers. We illustrate these results on both on both\nsynthetic and real data, showing how regularization improves standard\nclustering scores.",
        "Revise this with your best effort": "Spectral embedding is a popular technique for representing graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain the impact of the complete graph regularization on a simple block model. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Help me polish this": "Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Rewrite this for me": "Spectral embedding is a popular technique for representing graph data. Adding a constant to all entries of the adjacency matrix (complete graph regularization) has been shown to improve the quality of the embedding with respect to downstream tasks like clustering. This paper explains the impact of complete graph regularization on a simple block model. Specifically, it shows that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. The results are illustrated on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Make this fluent while doing minimal change": "Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Refine this for me please": "Spectral embedding is a popular technique for representing graph data. In this paper, we explain how the regularization technique of adding a constant to all entries of the adjacency matrix affects the spectral embedding. We show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Concise this for me and keep all the information": "Spectral embedding is a popular technique for the representation of graph data. In this paper, we explain the impact of the complete graph regularization on the quality of the embedding. We show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Improve this in GPT way": "Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores."
    },
    {
        "input": "In this work we study locality and compositionality in the context of\nlearning representations for Zero Shot Learning (ZSL). In order to well-isolate\nthe importance of these properties in learned representations, we impose the\nadditional constraint that, differently from most recent work in ZSL, no\npre-training on different datasets (e.g. ImageNet) is performed. The results of\nour experiments show how locality, in terms of small parts of the input, and\ncompositionality, i.e. how well can the learned representations be expressed as\na function of a smaller vocabulary, are both deeply related to generalization\nand motivate the focus on more local-aware models in future research directions\nfor representation learning.",
        "Revise this with your best effort": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). To isolate the importance of these properties in learned representations, we impose the additional constraint that, unlike most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well the learned representations can be expressed as a function of a smaller vocabulary, are both deeply related to generalization. This motivates the focus on more local-aware models in future research directions for representation learning.",
        "Help me polish this": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization. This motivates the focus on more local-aware models in future research directions for representation learning.",
        "Rewrite this for me": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). To isolate the importance of these properties in learned representations, we impose the additional constraint that, unlike most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well the learned representations can be expressed as a function of a smaller vocabulary, are both deeply related to generalization. This motivates the focus on more local-aware models in future research directions for representation learning.",
        "Make this fluent while doing minimal change": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). To well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.",
        "Refine this for me please": "In this work, we study how locality and compositionality affect Zero Shot Learning (ZSL). We impose the constraint that no pre-training on different datasets (e.g. ImageNet) is performed, in order to isolate the importance of these properties in learned representations. Our experiments show that locality (in terms of small parts of the input) and compositionality (i.e. how well can the learned representations be expressed as a function of a smaller vocabulary) are both deeply related to generalization. These results motivate the focus on more local-aware models in future research directions for representation learning.",
        "Concise this for me and keep all the information": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). We impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization. These results motivate the focus on more local-aware models in future research directions for representation learning.",
        "Improve this in GPT way": "In this work, we study the importance of locality and compositionality in learned representations for Zero Shot Learning (ZSL). We find that these properties are deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning."
    },
    {
        "input": "We consider training machine learning models that are fair in the sense that\ntheir performance is invariant under certain sensitive perturbations to the\ninputs. For example, the performance of a resume screening system should be\ninvariant under changes to the gender and/or ethnicity of the applicant. We\nformalize this notion of algorithmic fairness as a variant of individual\nfairness and develop a distributionally robust optimization approach to enforce\nit during training. We also demonstrate the effectiveness of the approach on\ntwo ML tasks that are susceptible to gender and racial biases.",
        "Revise this with your best effort": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Help me polish this": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Rewrite this for me": "We believe that it is important to train machine learning models in a way that considers fairness - meaning that the performance of the models should not be affected by changes to sensitive input data, such as gender or ethnicity. We formalize this idea of fairness as a variant of individual fairness, and develop a method of distributional robust optimization to enforce it during the training process. We also show that this method is effective on two different machine learning tasks that are often susceptible to bias.",
        "Make this fluent while doing minimal change": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Refine this for me please": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Concise this for me and keep all the information": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training.",
        "Improve this in GPT way": "We consider training machine learning models that are fair in the sense that\ntheir performance is invariant under certain sensitive perturbations to the\ninputs. For example, the performance of a resume screening system should be\ninvariant under changes to the gender and/or ethnicity of the applicant. We\nformalize this notion of algorithmic fairness as a variant of individual\nfairness and develop a distributionally robust optimization approach to enforce\nit during training. We also demonstrate the effectiveness of the approach on\ntwo ML tasks that are susceptible to gender and racial biases."
    },
    {
        "input": "Neural message passing algorithms for semi-supervised classification on\ngraphs have recently achieved great success. However, for classifying a node\nthese methods only consider nodes that are a few propagation steps away and the\nsize of this utilized neighborhood is hard to extend. In this paper, we use the\nrelationship between graph convolutional networks (GCN) and PageRank to derive\nan improved propagation scheme based on personalized PageRank. We utilize this\npropagation procedure to construct a simple model, personalized propagation of\nneural predictions (PPNP), and its fast approximation, APPNP. Our model's\ntraining time is on par or faster and its number of parameters on par or lower\nthan previous models. It leverages a large, adjustable neighborhood for\nclassification and can be easily combined with any neural network. We show that\nthis model outperforms several recently proposed methods for semi-supervised\nclassification in the most thorough study done so far for GCN-like models. Our\nimplementation is available online.",
        "Revise this with your best effort": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Help me polish this": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Rewrite this for me": "Recently, neural message passing algorithms for semi-supervised classification on graphs have achieved great success. However, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Make this fluent while doing minimal change": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Refine this for me please": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.",
        "Concise this for me and keep all the information": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models.",
        "Improve this in GPT way": "Recently, neural message passing algorithms for semi-supervised classification on graphs have achieved great success. However, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online."
    },
    {
        "input": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg .",
        "Revise this with your best effort": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques used to train neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods. This may be because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Help me polish this": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness.\n\nWe hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Rewrite this for me": "Deep RL has been receiving more attention recently because of its encouraging performance on various control tasks. However, conventional regularization techniques used to train neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods. This may be because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness.\n\nWe hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Make this fluent while doing minimal change": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques used to train neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Refine this for me please": "Deep RL has been shown to be effective on a variety of control tasks, but has largely ignored conventional regularization techniques that are common in training neural networks. In this work, we study the effects of regularization on multiple policy optimization algorithms in continuous control tasks. We find that conventional regularization techniques can often improve performance, especially on harder tasks. Our findings are robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Concise this for me and keep all the information": "Deep RL has been receiving more attention recently, but conventional regularization techniques have been largely ignored. This study presents the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. The findings are shown to be robust against training hyperparameter variations. The study also compares these techniques with the more widely used entropy regularization.",
        "Improve this in GPT way": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best.\n\nWe further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg."
    },
    {
        "input": "We identify a class of over-parameterized deep neural networks with standard\nactivation functions and cross-entropy loss which provably have no bad local\nvalley, in the sense that from any point in parameter space there exists a\ncontinuous path on which the cross-entropy loss is non-increasing and gets\narbitrarily close to zero. This implies that these networks have no sub-optimal\nstrict local minima.",
        "Revise this with your best effort": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably do not have any bad local valleys. This means that, from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. Therefore, these networks have no sub-optimal strict local minima.",
        "Help me polish this": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.",
        "Rewrite this for me": "We identify a class of over-parameterized deep neural networks which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. Therefore, these networks have no sub-optimal strict local minima.",
        "Make this fluent while doing minimal change": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.",
        "Refine this for me please": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.",
        "Concise this for me and keep all the information": "We identify a class of deep neural networks which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero.",
        "Improve this in GPT way": "We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley. This implies that these networks have no sub-optimal strict local minima."
    },
    {
        "input": "Understanding theoretical properties of deep and locally connected nonlinear\nnetwork, such as deep convolutional neural network (DCNN), is still a hard\nproblem despite its empirical success. In this paper, we propose a novel\ntheoretical framework for such networks with ReLU nonlinearity. The framework\nexplicitly formulates data distribution, favors disentangled representations\nand is compatible with common regularization techniques such as Batch Norm. The\nframework is built upon teacher-student setting, by expanding the student\nforward/backward propagation onto the teacher's computational graph. The\nresulting model does not impose unrealistic assumptions (e.g., Gaussian inputs,\nindependence of activation, etc). Our framework could help facilitate\ntheoretical analysis of many practical issues, e.g. overfitting,\ngeneralization, disentangled representations in deep networks.",
        "Revise this with your best effort": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, e.g., overfitting, generalization, and disentangled representations in deep networks.",
        "Help me polish this": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Rewrite this for me": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Make this fluent while doing minimal change": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, e.g., overfitting, generalization, and disentangled representations in deep networks.",
        "Refine this for me please": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as Batch Norm. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Concise this for me and keep all the information": "Despite the empirical success of deep and locally connected nonlinear networks, such as deep convolutional neural networks (DCNNs), understanding their theoretical properties is still a hard problem. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations, and is compatible with common regularization techniques such as batch normalization. The framework is built upon a teacher-student setting, by expanding the student's forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc.). Our framework could help facilitate the theoretical analysis of many practical issues, such as overfitting, generalization, and disentangled representations in deep networks.",
        "Improve this in GPT way": "Our framework could help facilitate theoretical analysis of many practical issues in deep networks, such as overfitting, generalization, and disentangled representations."
    },
    {
        "input": "Generative adversarial networks (GANs) are able to model the complex\nhighdimensional distributions of real-world data, which suggests they could be\neffective for anomaly detection. However, few works have explored the use of\nGANs for the anomaly detection task. We leverage recently developed GAN models\nfor anomaly detection, and achieve state-of-the-art performance on image and\nnetwork intrusion datasets, while being several hundred-fold faster at test\ntime than the only published GAN-based method.",
        "Revise this with your best effort": "Generative adversarial networks (GANs) are able to model the complex high-dimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Help me polish this": "Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Rewrite this for me": "GANs have the ability to imitate real-world data distributions, which makes them good candidates for anomaly detection. However, few studies have looked into using GANs for anomaly detection. We use recently developed GAN models for anomaly detection and get better results than any other published method, while being much faster.",
        "Make this fluent while doing minimal change": "Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Refine this for me please": "GANs are effective for anomaly detection because they can model complex highdimensional distributions of real-world data.",
        "Concise this for me and keep all the information": "GANs can model complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
        "Improve this in GPT way": "We propose using GANs for anomaly detection, and demonstrate that our approach outperforms existing methods on image and network intrusion datasets. Our approach is also several hundred-fold faster at test time."
    },
    {
        "input": "Most state-of-the-art neural machine translation systems, despite being\ndifferent in architectural skeletons (e.g. recurrence, convolutional), share an\nindispensable feature: the Attention. However, most existing attention methods\nare token-based and ignore the importance of phrasal alignments, the key\ningredient for the success of phrase-based statistical machine translation. In\nthis paper, we propose novel phrase-based attention methods to model n-grams of\ntokens as attention entities. We incorporate our phrase-based attentions into\nthe recently proposed Transformer network, and demonstrate that our approach\nyields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for\nGerman-to-English translation tasks on WMT newstest2014 using WMT'16 training\ndata.",
        "Revise this with your best effort": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Help me polish this": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Rewrite this for me": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Make this fluent while doing minimal change": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Refine this for me please": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Concise this for me and keep all the information": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.",
        "Improve this in GPT way": "Most state-of-the-art neural machine translation systems share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data."
    },
    {
        "input": "We propose an algorithm combining calibrated prediction and generalization\nbounds from learning theory to construct confidence sets for deep neural\nnetworks with PAC guarantees---i.e., the confidence set for a given input\ncontains the true label with high probability. We demonstrate how our approach\ncan be used to construct PAC confidence sets on ResNet for ImageNet, a visual\nobject tracking model, and a dynamics model for the half-cheetah reinforcement\nlearning problem.",
        "Revise this with your best effort": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Help me polish this": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Rewrite this for me": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Make this fluent while doing minimal change": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Refine this for me please": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Concise this for me and keep all the information": "We propose an algorithm that uses calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Improve this in GPT way": "We propose an algorithm combining calibrated prediction and generalization\nbounds from learning theory to construct confidence sets for deep neural\nnetworks with PAC guarantees. This algorithm can be used to construct PAC confidence sets on ResNet for ImageNet, a visual\nobject tracking model, and a dynamics model for the half-cheetah reinforcement\nlearning problem."
    },
    {
        "input": "The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has\nemerged as a useful tool for thinking about realism and distortion of\nreconstructions in lossy compression. Unlike the rate-distortion function,\nhowever, it is unknown whether encoders and decoders exist that achieve the\nrate suggested by the RDPF. Building on results by Li and El Gamal (2018), we\nshow that the RDPF can indeed be achieved using stochastic, variable-length\ncodes. For this class of codes, we also prove that the RDPF lower-bounds the\nachievable rate",
        "Revise this with your best effort": "The rate-distortion-perception function (RDPF) has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal, we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Help me polish this": "The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Rewrite this for me": "The RDPF is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF, but it is possible to use stochastic, variable-length codes to achieve the RDPF. For this class of codes, the RDPF lower-bounds the achievable rate.",
        "Make this fluent while doing minimal change": "The RDPF has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Refine this for me please": "The RDPF is a tool for thinking about the realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. However, it is possible to achieve the RDPF using stochastic, variable-length codes. For this class of codes, the RDPF lower-bounds the achievable rate.",
        "Concise this for me and keep all the information": "The rate-distortion-perception function (RDPF) is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF, but it can be achieved using stochastic, variable-length codes.",
        "Improve this in GPT way": "The rate-distortion-perception function (RDPF) is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate."
    },
    {
        "input": "We address the problem of graph classification based only on structural\ninformation. Inspired by natural language processing techniques (NLP), our\nmodel sequentially embeds information to estimate class membership\nprobabilities. Besides, we experiment with NLP-like variational regularization\ntechniques, making the model predict the next node in the sequence as it reads\nit. We experimentally show that our model achieves state-of-the-art\nclassification results on several standard molecular datasets. Finally, we\nperform a qualitative analysis and give some insights on whether the node\nprediction helps the model better classify graphs.",
        "Revise this with your best effort": "We address the problem of graph classification by only using structural information. Our model is inspired by natural language processing techniques (NLP) and sequentially embeds information to estimate class membership probabilities. We experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",
        "Help me polish this": "We address the problem of graph classification by sequentially embedding information to estimate class membership probabilities, inspired by natural language processing techniques. We experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis to investigate whether the node prediction helps the model better classify graphs.",
        "Rewrite this for me": "We address the problem of graph classification by only using structural information. Our model is inspired by natural language processing techniques (NLP) and sequentially embeds information to estimate class membership probabilities. We also experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",
        "Make this fluent while doing minimal change": "We address the problem of graph classification by sequentially embedding information to estimate class membership probabilities, inspired by natural language processing techniques. We experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",
        "Refine this for me please": "We address the problem of graph classification by sequentially embedding information to estimate class membership probabilities, inspired by natural language processing techniques. We experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",
        "Concise this for me and keep all the information": "We address the problem of graph classification by sequentially embedding information to estimate class membership probabilities, inspired by natural language processing techniques. We experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",
        "Improve this in GPT way": "We address the problem of graph classification by sequentially embedding information to estimate class membership probabilities, similar to natural language processing techniques. We experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We find that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs."
    },
    {
        "input": "Neural network pruning techniques can reduce the parameter counts of trained\nnetworks by over 90%, decreasing storage requirements and improving\ncomputational performance of inference without compromising accuracy. However,\ncontemporary experience is that the sparse architectures produced by pruning\nare difficult to train from the start, which would similarly improve training\nperformance.\n  We find that a standard pruning technique naturally uncovers subnetworks\nwhose initializations made them capable of training effectively. Based on these\nresults, we articulate the \"lottery ticket hypothesis:\" dense,\nrandomly-initialized, feed-forward networks contain subnetworks (\"winning\ntickets\") that - when trained in isolation - reach test accuracy comparable to\nthe original network in a similar number of iterations. The winning tickets we\nfind have won the initialization lottery: their connections have initial\nweights that make training particularly effective.\n  We present an algorithm to identify winning tickets and a series of\nexperiments that support the lottery ticket hypothesis and the importance of\nthese fortuitous initializations. We consistently find winning tickets that are\nless than 10-20% of the size of several fully-connected and convolutional\nfeed-forward architectures for MNIST and CIFAR10. Above this size, the winning\ntickets that we find learn faster than the original network and reach higher\ntest accuracy.",
        "Revise this with your best effort": "Neural network pruning techniques can reduce the number of parameters in a trained network by more than 90%, which decreases storage requirements and improves computational performance without compromising accuracy. However, it is difficult to train a network from scratch with the same sparsity as a pruned network.\n\nWe find that a standard pruning technique naturally uncovers subnetworks with initializations that make them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Help me polish this": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\n\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Rewrite this for me": "Neural network pruning techniques can reduce the number of parameters in a trained network by more than 90%, which decreases storage requirements and improves computational performance without compromising accuracy. However, it is difficult to train a network from scratch with the same sparsity as a pruned network.\n\nWe find that a standard pruning technique naturally uncovers subnetworks with initializations that make them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Make this fluent while doing minimal change": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\n\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Refine this for me please": "We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Concise this for me and keep all the information": "Neural network pruning techniques can reduce the number of parameters in a trained network by over 90%, which decreases storage requirements and improves computational performance without compromising accuracy. However, it is difficult to train a network from the start with the sparse architecture produced by pruning. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
        "Improve this in GPT way": "We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy."
    },
    {
        "input": "Generative adversarial networks (GANs) form a generative modeling approach\nknown for producing appealing samples, but they are notably difficult to train.\nOne common way to tackle this issue has been to propose new formulations of the\nGAN objective. Yet, surprisingly few studies have looked at optimization\nmethods designed for this adversarial training. In this work, we cast GAN\noptimization problems in the general variational inequality framework. Tapping\ninto the mathematical programming literature, we counter some common\nmisconceptions about the difficulties of saddle point optimization and propose\nto extend techniques designed for variational inequalities to the training of\nGANs. We apply averaging, extrapolation and a computationally cheaper variant\nthat we call extrapolation from the past to the stochastic gradient method\n(SGD) and Adam.",
        "Revise this with your best effort": "GANs are notoriously difficult to train, with one common way to tackle this issue being to propose new formulations of the GAN objective. However, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Help me polish this": "Generative adversarial networks (GANs) are a generative modeling approach known for producing appealing samples, but they are notoriously difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Rewrite this for me": "GANs are difficult to train because they are a generative modeling approach that produces appealing samples. One way to tackle this issue is to propose new formulations of the GAN objective. However, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Make this fluent while doing minimal change": "GANs are notoriously difficult to train, with one common way to tackle this issue being to propose new formulations of the GAN objective. However, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Refine this for me please": "GANs are difficult to train because they are a generative modeling approach that produces appealing samples. One way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Concise this for me and keep all the information": "GANs are difficult to train because they are a generative modeling approach. One way to tackle this issue is to propose new formulations of the GAN objective. However, few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend techniques designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.",
        "Improve this in GPT way": "We propose a new method for training GANs that is based on the general variational inequality framework. This method is designed to address the common difficulties associated with training GANs, and we show that it can be applied to the training of GANs using stochastic gradient descent (SGD) and Adam."
    },
    {
        "input": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies.",
        "Revise this with your best effort": "In this paper, we introduce the Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Help me polish this": "In this paper, we introduce the Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Rewrite this for me": "In this paper, we introduce the Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. This framework is designed in a physics-informed manner, which allows for better generalization with fewer training samples. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way. This can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework offers interpretable, physically-consistent models for physical systems, which opens up new possibilities for synthesizing model-based control strategies.",
        "Make this fluent while doing minimal change": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Refine this for me please": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies.",
        "Concise this for me and keep all the information": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Improve this in GPT way": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies."
    },
    {
        "input": "Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods.",
        "Revise this with your best effort": "Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. \n\nIn this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding method to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. \n\nWe have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Help me polish this": "Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. \n\nIn this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding method to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. \n\nWe have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Rewrite this for me": "Graph embedding techniques have been used more and more in a variety of applications that involve learning from non-Euclidean data. However, many existing graph embedding models either don't take node attribute information into account during training, or they're not very accurate because of node attribute noise. Moreover, very few of them can handle large graphs because they're computationally complex and use a lot of memory. In this paper, we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first creates a new graph by merging the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding method to be applied to the coarsened graph, and then progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. We've evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to state-of-the-art unsupervised embedding methods.",
        "Make this fluent while doing minimal change": "Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods.",
        "Refine this for me please": "Graph embedding techniques are used to learn on non-Euclidean data and have been deployed in a multitude of different applications. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper, we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Concise this for me and keep all the information": "Graph embedding techniques are used to learn on non-Euclidean data and have been deployed in a multitude of different applications. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper, we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Improve this in GPT way": "Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods."
    },
    {
        "input": "Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance.",
        "Revise this with your best effort": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Help me polish this": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch.\n\nIn this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete.\n\nWe present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Rewrite this for me": "Distributed optimization is key to solving large-scale machine learning problems. A common feature of distributed optimization techniques is that all nodes must finish their assigned tasks in each computational epoch before the system can move on to the next epoch. In these types of settings, slow nodes, called stragglers, can severely slow down progress. To counteract the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a set amount of time to compute the gradients of as many data samples as possible. This results in a variable per-node minibatch size. Workers then have a set communication time to average their minibatch gradients through several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can actually finish. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Make this fluent while doing minimal change": "Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance.",
        "Refine this for me please": "Distributed optimization is a key tool for solving large-scale machine learning problems. A common feature of distributed optimization techniques is that all nodes must complete their assigned tasks in each computational epoch before the system can move on to the next epoch. This can be a problem if some nodes are slow, which can hold up the entire system. To address this issue, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Concise this for me and keep all the information": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Improve this in GPT way": "Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance."
    },
    {
        "input": "Scaling end-to-end reinforcement learning to control real robots from vision\npresents a series of challenges, in particular in terms of sample efficiency.\nAgainst end-to-end learning, state representation learning can help learn a\ncompact, efficient and relevant representation of states that speeds up policy\nlearning, reducing the number of samples needed, and that is easier to\ninterpret. We evaluate several state representation learning methods on goal\nbased robotics tasks and propose a new unsupervised model that stacks\nrepresentations and combines strengths of several of these approaches. This\nmethod encodes all the relevant features, performs on par or better than\nend-to-end learning with better sample efficiency, and is robust to\nhyper-parameters change.",
        "Revise this with your best effort": "Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. State representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",
        "Help me polish this": "Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. State representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",
        "Rewrite this for me": "End-to-end reinforcement learning can be difficult to scale up to control real robots, especially when it comes to being efficient with the number of samples needed. State representation learning can help create a more compact, efficient, and relevant representation of states. This can then speed up policy learning, and reduce the number overall samples needed. We compare several state representation learning methods against each other on goal-based robotics tasks. Our proposed unsupervised model stacks representations and combines the strengths of several existing approaches. This method encodes all relevant features, performs just as well or better than end-to-end learning, and is more robust to changes in hyper-parameters.",
        "Make this fluent while doing minimal change": "Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. State representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",
        "Refine this for me please": "End-to-end reinforcement learning can be difficult to scale up to control real robots, especially in terms of sample efficiency. State representation learning can help by learning a more efficient and relevant representation of states, which speeds up policy learning and requires fewer samples. We evaluate several state representation learning methods on goal-based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",
        "Concise this for me and keep all the information": "State representation learning can help learn a compact, efficient, and relevant representation of states that speeds up policy learning, reducing the number of samples needed. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",
        "Improve this in GPT way": "State representation learning can help learn a compact, efficient, and relevant representation of states that speeds up policy learning, reducing the number of samples needed. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change."
    },
    {
        "input": "A central challenge in reinforcement learning is discovering effective\npolicies for tasks where rewards are sparsely distributed. We postulate that in\nthe absence of useful reward signals, an effective exploration strategy should\nseek out {\\it decision states}. These states lie at critical junctions in the\nstate space from where the agent can transition to new, potentially unexplored\nregions. We propose to learn about decision states from prior experience. By\ntraining a goal-conditioned policy with an information bottleneck, we can\nidentify decision states by examining where the model actually leverages the\ngoal state. We find that this simple mechanism effectively identifies decision\nstates, even in partially observed settings. In effect, the model learns the\nsensory cues that correlate with potential subgoals. In new environments, this\nmodel can then identify novel subgoals for further exploration, guiding the\nagent through a sequence of potential decision states and through new regions\nof the state space.",
        "Revise this with your best effort": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",
        "Help me polish this": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.\n\nWe propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",
        "Rewrite this for me": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",
        "Make this fluent while doing minimal change": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",
        "Refine this for me please": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",
        "Concise this for me and keep all the information": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out decision states. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",
        "Improve this in GPT way": "A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.\n\nWe propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space."
    },
    {
        "input": "Multilingual machine translation, which translates multiple languages with a\nsingle model, has attracted much attention due to its efficiency of offline\ntraining and online serving. However, traditional multilingual translation\nusually yields inferior accuracy compared with the counterpart using individual\nmodels for each language pair, due to language diversity and model capacity\nlimitations. In this paper, we propose a distillation-based approach to boost\nthe accuracy of multilingual machine translation. Specifically, individual\nmodels are first trained and regarded as teachers, and then the multilingual\nmodel is trained to fit the training data and match the outputs of individual\nmodels simultaneously through knowledge distillation. Experiments on IWSLT, WMT\nand Ted talk translation datasets demonstrate the effectiveness of our method.\nParticularly, we show that one model is enough to handle multiple languages (up\nto 44 languages in our experiment), with comparable or even better accuracy\nthan individual models.",
        "Revise this with your best effort": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",
        "Help me polish this": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",
        "Rewrite this for me": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",
        "Make this fluent while doing minimal change": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",
        "Refine this for me please": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",
        "Concise this for me and keep all the information": "Multilingual machine translation, which translates multiple languages with a single model, is usually less accurate than using individual models for each language pair. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method.",
        "Improve this in GPT way": "Multilingual machine translation, which translates multiple languages with a\nsingle model, has attracted much attention due to its efficiency of offline\ntraining and online serving. However, traditional multilingual translation\nusually yields inferior accuracy compared with the counterpart using individual\nmodels for each language pair, due to language diversity and model capacity\nlimitations. In this paper, we propose a distillation-based approach to boost\nthe accuracy of multilingual machine translation. Specifically, individual\nmodels are first trained and regarded as teachers, and then the multilingual\nmodel is trained to fit the training data and match the outputs of individual\nmodels simultaneously through knowledge distillation. Experiments on IWSLT, WMT\nand Ted talk translation datasets demonstrate the effectiveness of our method.\nParticularly, we show that one model is enough to handle multiple languages (up\nto 44 languages in our experiment), with comparable or even better accuracy\nthan individual models."
    },
    {
        "input": "We introduce PyTorch Geometric, a library for deep learning on irregularly\nstructured input data such as graphs, point clouds and manifolds, built upon\nPyTorch. In addition to general graph data structures and processing methods,\nit contains a variety of recently published methods from the domains of\nrelational learning and 3D data processing. PyTorch Geometric achieves high\ndata throughput by leveraging sparse GPU acceleration, by providing dedicated\nCUDA kernels and by introducing efficient mini-batch handling for input\nexamples of different size. In this work, we present the library in detail and\nperform a comprehensive comparative study of the implemented methods in\nhomogeneous evaluation scenarios.",
        "Revise this with your best effort": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",
        "Help me polish this": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comparative study of the implemented methods in homogeneous evaluation scenarios.",
        "Rewrite this for me": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",
        "Make this fluent while doing minimal change": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",
        "Refine this for me please": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comparative study of the implemented methods in homogeneous evaluation scenarios.",
        "Concise this for me and keep all the information": "PyTorch Geometric is a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds. It contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size.",
        "Improve this in GPT way": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios."
    },
    {
        "input": "Although variational autoencoders (VAEs) represent a widely influential deep\ngenerative model, many aspects of the underlying energy function remain poorly\nunderstood. In particular, it is commonly believed that Gaussian\nencoder/decoder assumptions reduce the effectiveness of VAEs in generating\nrealistic samples. In this regard, we rigorously analyze the VAE objective,\ndifferentiating situations where this belief is and is not actually true. We\nthen leverage the corresponding insights to develop a simple VAE enhancement\nthat requires no additional hyperparameters or sensitive tuning.\nQuantitatively, this proposal produces crisp samples and stable FID scores that\nare actually competitive with a variety of GAN models, all while retaining\ndesirable attributes of the original VAE architecture. A shorter version of\nthis work will appear in the ICLR 2019 conference proceedings (Dai and Wipf,\n2019). The code for our model is available at https://github.com/daib13/\nTwoStageVAE.",
        "Revise this with your best effort": "Although variational autoencoders (VAEs) are a widely used deep generative model, many aspects of the underlying energy function are not well understood. It is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. However, we have found that this is not always the case. We have developed a simple VAE enhancement that does not require any additional hyperparameters or sensitive tuning. This proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, while retaining desirable attributes of the original VAE architecture.",
        "Help me polish this": "Although variational autoencoders (VAEs) are a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.\n\nIn this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.\n\nQuantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019).\n\nThe code for our model is available at https://github.com/daib13/TwoStageVAE.",
        "Rewrite this for me": "Although variational autoencoders (VAEs) are a widely used deep generative model, many aspects of the underlying energy function are not well understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. However, we have found that this is not always the case. We have developed a simple VAE enhancement that does not require any additional hyperparameters or sensitive tuning, and that produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models.",
        "Make this fluent while doing minimal change": "Although variational autoencoders (VAEs) are a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.\n\nIn this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.\n\nQuantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at https://github.com/daib13/TwoStageVAE.",
        "Refine this for me please": "The variational autoencoder (VAE) is a deep generative model that is widely used and influential. However, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.\n\nIn this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.\n\nQuantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture.",
        "Concise this for me and keep all the information": "Although variational autoencoders (VAEs) are widely used, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture.",
        "Improve this in GPT way": "Although variational autoencoders (VAEs) are a widely used deep generative model, many aspects of the underlying energy function are not well understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.\n\nQuantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at https://github.com/daib13/TwoStageVAE."
    },
    {
        "input": "Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nSurprisingly, several studies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. Although this phenomenon is interesting, there are only few\nworks that have offered an explanation. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off.",
        "Revise this with your best effort": "Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nStudies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. Although this phenomenon is interesting, there are only few\nworks that have offered an explanation. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off.",
        "Help me polish this": "Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nSurprisingly, several studies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. Although this phenomenon is interesting, there are only few\nworks that have offered an explanation. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off.",
        "Rewrite this for me": "Adversarial training is a training scheme designed to make a dataset more robust to adversarial attacks by adding in adversarial examples. Studies have found that the loss gradients from adversarially trained DNNs (Deep Neural Networks) are more interpretable to humans than those from standard DNNs. Although this is interesting, there is still a lack of understanding when it comes to why this is the case. \n\nIn this paper, we attempt to explain the relationship between adversarial robustness and gradient interpretability. We identify that loss gradients from adversarially trained DNNs align better with human perception because adversarial training restricts gradients to be closer to the image manifold. We then demonstrate that adversarial training causes loss gradients to be quantitatively meaningful. Finally, we show that under the adversarial training framework, there is a trade-off between test accuracy and loss gradient interpretability. We propose two potential approaches to resolving this trade-off.",
        "Make this fluent while doing minimal change": "Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nSurprisingly, several studies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. Although this phenomenon is interesting, there are only few\nworks that have offered an explanation. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off.",
        "Refine this for me please": "Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nAdversarial training causes loss gradients to be quantitatively meaningful and align better with human perception.\nThere is an empirical trade-off between test accuracy and loss gradient interpretability under the adversarial training framework.",
        "Concise this for me and keep all the information": "Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nSeveral studies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off.",
        "Improve this in GPT way": "Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nSurprisingly, several studies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. Although this phenomenon is interesting, there are only few\nworks that have offered an explanation. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off."
    },
    {
        "input": "This is the proceedings of the Computer Vision for Agriculture (CV4A)\nWorkshop that was held in conjunction with the International Conference on\nLearning Representations (ICLR) 2020.\n  The Computer Vision for Agriculture (CV4A) 2020 workshop was scheduled to be\nheld in Addis Ababa, Ethiopia, on April 26th, 2020. It was held virtually that\nsame day due to the COVID-19 pandemic. The workshop was held in conjunction\nwith the International Conference on Learning Representations (ICLR) 2020.",
        "Revise this with your best effort": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held virtually on April 26th, 2020 in conjunction with the International Conference on Learning Representations (ICLR) 2020. The workshop was originally scheduled to be held in Addis Ababa, Ethiopia, but was held online due to the COVID-19 pandemic.",
        "Help me polish this": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held virtually on April 26th, 2020 in conjunction with the International Conference on Learning Representations (ICLR) 2020.",
        "Rewrite this for me": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held virtually on April 26th, 2020 in conjunction with the International Conference on Learning Representations (ICLR) 2020. The workshop was originally scheduled to be held in Addis Ababa, Ethiopia, but was held online due to the COVID-19 pandemic.",
        "Make this fluent while doing minimal change": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held virtually on April 26th, 2020 in conjunction with the International Conference on Learning Representations (ICLR) 2020 due to the COVID-19 pandemic.",
        "Refine this for me please": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held virtually on April 26th, 2020 in conjunction with the International Conference on Learning Representations (ICLR) 2020 due to the COVID-19 pandemic.",
        "Concise this for me and keep all the information": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held in conjunction with the International Conference on Learning Representations (ICLR) 2020. The workshop was held virtually on April 26th, 2020 due to the COVID-19 pandemic.",
        "Improve this in GPT way": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held virtually on April 26th, 2020 in conjunction with the International Conference on Learning Representations (ICLR) 2020."
    },
    {
        "input": "Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR\n2020, Virtual Conference, Formerly Addis Ababa Ethiopia.",
        "Revise this with your best effort": "Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside the ICLR 2020 Virtual Conference, formerly in Addis Ababa, Ethiopia.",
        "Help me polish this": "Proceedings of the 1st AfricaNLP Workshop, held on 26th April alongside ICLR 2020. The conference was originally meant to be held in Addis Ababa, Ethiopia, but was moved to a virtual format due to the COVID-19 pandemic.",
        "Rewrite this for me": "\"The 1st AfricaNLP Workshop was held on 26th April 2020, alongside the ICLR Virtual Conference. It was formerly held in Addis Ababa, Ethiopia.\"",
        "Make this fluent while doing minimal change": "Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR 2020, Virtual Conference. (Formerly Addis Ababa, Ethiopia)",
        "Refine this for me please": "Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR 2020, Virtual Conference. The workshop was formerly held in Addis Ababa, Ethiopia.",
        "Concise this for me and keep all the information": "Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR 2020, Virtual Conference.",
        "Improve this in GPT way": "Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR 2020, Virtual Conference."
    },
    {
        "input": "In this work we show preliminary results of deep multi-task learning in the\narea of computational pathology. We combine 11 tasks ranging from patch-wise\noral cancer classification, one of the most prevalent cancers in the developing\nworld, to multi-tissue nuclei instance segmentation and classification.",
        "Revise this with your best effort": "In this work, we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks, ranging from patch-wise oral cancer classification (one of the most prevalent cancers in the developing world) to multi-tissue nuclei instance segmentation and classification.",
        "Help me polish this": "In this work, we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks, ranging from patch-wise oral cancer classification (one of the most prevalent cancers in the developing world) to multi-tissue nuclei instance segmentation and classification.",
        "Rewrite this for me": "In this work, we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks, ranging from patch-wise oral cancer classification (one of the most prevalent cancers in the developing world) to multi-tissue nuclei instance segmentation and classification.",
        "Make this fluent while doing minimal change": "In this work, we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks, ranging from patch-wise oral cancer classification\u2014one of the most prevalent cancers in the developing world\u2014to multi-tissue nuclei instance segmentation and classification.",
        "Refine this for me please": "In this work, we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks, ranging from patch-wise oral cancer classification (one of the most prevalent cancers in the developing world) to multi-tissue nuclei instance segmentation and classification.",
        "Concise this for me and keep all the information": "We show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks, including patch-wise oral cancer classification and multi-tissue nuclei instance segmentation and classification.",
        "Improve this in GPT way": "In this work, we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks ranging from patch-wise oral cancer classification, one of the most prevalent cancers in the developing world, to multi-tissue nuclei instance segmentation and classification."
    },
    {
        "input": "The principle of compositionality, which enables natural language to\nrepresent complex concepts via a structured combination of simpler ones, allows\nus to convey an open-ended set of messages using a limited vocabulary. If\ncompositionality is indeed a natural property of language, we may expect it to\nappear in communication protocols that are created by neural agents in language\ngames. In this paper, we propose an effective neural iterated learning (NIL)\nalgorithm that, when applied to interacting neural agents, facilitates the\nemergence of a more structured type of language. Indeed, these languages\nprovide learning speed advantages to neural agents during training, which can\nbe incrementally amplified via NIL. We provide a probabilistic model of NIL and\nan explanation of why the advantage of compositional language exist. Our\nexperiments confirm our analysis, and also demonstrate that the emerged\nlanguages largely improve the generalizing power of the neural agent\ncommunication.",
        "Revise this with your best effort": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games.\n\nIn this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL.\n\nWe provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.",
        "Help me polish this": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games.\n\nIn this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL.\n\nWe provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.",
        "Rewrite this for me": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games. In this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL. We provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.",
        "Make this fluent while doing minimal change": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games. In this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL. We provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.",
        "Refine this for me please": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games. In this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL. We provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.",
        "Concise this for me and keep all the information": "The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. In this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. These languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL.",
        "Improve this in GPT way": "We propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. This more structured language provides learning speed advantages to neural agents during training, which can be incrementally amplified via NIL."
    },
    {
        "input": "Text generation is ubiquitous in many NLP tasks, from summarization, to\ndialogue and machine translation. The dominant parametric approach is based on\nlocally normalized models which predict one word at a time. While these work\nremarkably well, they are plagued by exposure bias due to the greedy nature of\nthe generation process. In this work, we investigate un-normalized energy-based\nmodels (EBMs) which operate not at the token but at the sequence level. In\norder to make training tractable, we first work in the residual of a pretrained\nlocally normalized language model and second we train using noise contrastive\nestimation. Furthermore, since the EBM works at the sequence level, we can\nleverage pretrained bi-directional contextual representations, such as BERT and\nRoBERTa. Our experiments on two large language modeling datasets show that\nresidual EBMs yield lower perplexity compared to locally normalized baselines.\nMoreover, generation via importance sampling is very efficient and of higher\nquality than the baseline models according to human evaluation.",
        "Revise this with your best effort": "Text generation is a ubiquitous task in many natural language processing (NLP) applications, from summarization and dialogue to machine translation. The dominant parametric approach is based on locally normalized models, which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process.\n\nIn this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa.\n\nOur experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.",
        "Help me polish this": "Text generation is a ubiquitous task in many natural language processing (NLP) applications, from summarization and dialogue to machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process.\n\nIn this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa.\n\nOur experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.",
        "Rewrite this for me": "Text generation is a common task in many natural language processing applications, from summarization to dialogue and machine translation. The dominant approach uses locally normalized models, which predict one word at a time. While this approach works well, it is limited by exposure bias due to the greedy nature of the generation process.\n\nIn this work, we investigate un-normalized energy-based models (EBMs) which operate at the sequence level, rather than at the token level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model, and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa.\n\nOur experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.",
        "Make this fluent while doing minimal change": "Text generation is a ubiquitous task in many natural language processing (NLP) applications, from summarization and dialogue to machine translation. The dominant parametric approach is based on locally normalized models, which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process.\n\nIn this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa.\n\nOur experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.",
        "Refine this for me please": "In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.",
        "Concise this for me and keep all the information": "Text generation is a common task in natural language processing, typically done by predicting one word at a time. This can be improved by using an energy-based model, which predicts sequences of words instead of individual words. This is more efficient and of higher quality according to human evaluation.",
        "Improve this in GPT way": "Text generation is a ubiquitous task in many natural language processing (NLP) applications, from summarization and dialogue to machine translation. The dominant parametric approach is based on locally normalized models, which predict one word at a time. While these models work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process.\n\nIn this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa.\n\nOur experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation."
    },
    {
        "input": "We propose an energy-based model (EBM) of protein conformations that operates\nat atomic scale. The model is trained solely on crystallized protein data. By\ncontrast, existing approaches for scoring conformations use energy functions\nthat incorporate knowledge of physical principles and features that are the\ncomplex product of several decades of research and tuning. To evaluate the\nmodel, we benchmark on the rotamer recovery task, the problem of predicting the\nconformation of a side chain from its context within a protein structure, which\nhas been used to evaluate energy functions for protein design. The model\nachieves performance close to that of the Rosetta energy function, a\nstate-of-the-art method widely used in protein structure prediction and design.\nAn investigation of the model's outputs and hidden representations finds that\nit captures physicochemical properties relevant to protein energy.",
        "Revise this with your best effort": "We propose an energy-based model (EBM) of protein conformations that operates\nat atomic scale. The model is trained solely on crystallized protein data. By\ncontrast, existing approaches for scoring conformations use energy functions\nthat incorporate knowledge of physical principles and features that are the\ncomplex product of several decades of research and tuning. To evaluate the\nmodel, we benchmark on the rotamer recovery task, the problem of predicting the\nconformation of a side chain from its context within a protein structure, which\nhas been used to evaluate energy functions for protein design. The model\nachieves performance close to that of the Rosetta energy function, a\nstate-of-the-art method widely used in protein structure prediction and design.\nAn investigation of the model's outputs and hidden representations finds that\nit captures physicochemical properties relevant to protein energy.",
        "Help me polish this": "We propose an energy-based model (EBM) of protein conformations that operates at atomic scale. The model is trained solely on crystallized protein data. By contrast, existing approaches for scoring conformations use energy functions that incorporate knowledge of physical principles and features that are the complex product of several decades of research and tuning. \n\nTo evaluate the model, we benchmark on the rotamer recovery task, the problem of predicting the conformation of a side chain from its context within a protein structure, which has been used to evaluate energy functions for protein design. The model achieves performance close to that of the Rosetta energy function, a state-of-the-art method widely used in protein structure prediction and design. \n\nAn investigation of the model's outputs and hidden representations finds that it captures physicochemical properties relevant to protein energy.",
        "Rewrite this for me": "We propose an energy-based model (EBM) of protein conformations that operates\nat atomic scale. The model is trained solely on crystallized protein data. By\ncontrast, existing approaches for scoring conformations use energy functions\nthat incorporate knowledge of physical principles and features that are the\ncomplex product of several decades of research and tuning. To evaluate the\nmodel, we benchmark on the rotamer recovery task, the problem of predicting the\nconformation of a side chain from its context within a protein structure, which\nhas been used to evaluate energy functions for protein design. The model\nachieves performance close to that of the Rosetta energy function, a\nstate-of-the-art method widely used in protein structure prediction and design.\nAn investigation of the model's outputs and hidden representations finds that\nit captures physicochemical properties relevant to protein energy.",
        "Make this fluent while doing minimal change": "We propose an energy-based model (EBM) of protein conformations that operates\nat atomic scale. The model is trained solely on crystallized protein data, in contrast to existing approaches that use energy functions that incorporate knowledge of physical principles and features that are the complex product of several decades of research and tuning. To evaluate the model, we benchmark it on the rotamer recovery task, the problem of predicting the\nconformation of a side chain from its context within a protein structure, which has been used to evaluate energy functions for protein design. The model\nachieves performance close to that of the Rosetta energy function, a\nstate-of-the-art method widely used in protein structure prediction and design.\nAn investigation of the model's outputs and hidden representations finds that\nit captures physicochemical properties relevant to protein energy.",
        "Refine this for me please": "We propose an energy-based model (EBM) of protein conformations that operates\nat atomic scale. The model is trained solely on crystallized protein data. By\ncontrast, existing approaches for scoring conformations use energy functions\nthat incorporate knowledge of physical principles and features that are the\ncomplex product of several decades of research and tuning. To evaluate the\nmodel, we benchmark on the rotamer recovery task, the problem of predicting the\nconformation of a side chain from its context within a protein structure, which\nhas been used to evaluate energy functions for protein design. The model\nachieves performance close to that of the Rosetta energy function, a\nstate-of-the-art method widely used in protein structure prediction and design.\nAn investigation of the model's outputs and hidden representations finds that\nit captures physicochemical properties relevant to protein energy.",
        "Concise this for me and keep all the information": "We propose an energy-based model (EBM) of protein conformations that operates at atomic scale. The model is trained solely on crystallized protein data. To evaluate the model, we benchmark on the rotamer recovery task, the problem of predicting the conformation of a side chain from its context within a protein structure, which has been used to evaluate energy functions for protein design. The model achieves performance close to that of the Rosetta energy function, a state-of-the-art method widely used in protein structure prediction and design.",
        "Improve this in GPT way": "We propose an energy-based model (EBM) of protein conformations that operates\nat atomic scale. The model is trained solely on crystallized protein data. By\ncontrast, existing approaches for scoring conformations use energy functions\nthat incorporate knowledge of physical principles and features that are the\ncomplex product of several decades of research and tuning. To evaluate the\nmodel, we benchmark on the rotamer recovery task, the problem of predicting the\nconformation of a side chain from its context within a protein structure, which\nhas been used to evaluate energy functions for protein design. The model\nachieves performance close to that of the Rosetta energy function, a\nstate-of-the-art method widely used in protein structure prediction and design.\nAn investigation of the model's outputs and hidden representations finds that\nit captures physicochemical properties relevant to protein energy."
    },
    {
        "input": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural\ntangent kernel and the Laplace kernel include the same set of functions, when\nboth kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we\nprove that the exponential power kernel with a smaller power (making the kernel\nless smooth) leads to a larger RKHS, when it is restricted to the sphere\n$\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Revise this with your best effort": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Help me polish this": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Rewrite this for me": "We show that the set of functions that are reproducing kernel Hilbert spaces (RKHS) for both a deep neural tangent kernel and a Laplace kernel is the same when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. We also show that when an exponential power kernel is restricted to the sphere $\\mathbb{S}^{d-1}$, or defined on the entire $\\mathbb{R}^d$, it has a larger RKHS if the power is smaller (making the kernel less smooth).",
        "Make this fluent while doing minimal change": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Refine this for me please": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Concise this for me and keep all the information": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Improve this in GPT way": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$."
    },
    {
        "input": "We propose a novel node embedding of directed graphs to statistical\nmanifolds, which is based on a global minimization of pairwise relative entropy\nand graph geodesics in a non-linear way. Each node is encoded with a\nprobability density function over a measurable space. Furthermore, we analyze\nthe connection between the geometrical properties of such embedding and their\nefficient learning procedure. Extensive experiments show that our proposed\nembedding is better in preserving the global geodesic information of graphs, as\nwell as outperforming existing embedding models on directed graphs in a variety\nof evaluation metrics, in an unsupervised setting.",
        "Revise this with your best effort": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Help me polish this": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Rewrite this for me": "We propose a new node embedding for directed graphs onto statistical manifolds. This embedding is based on a global minimization of pairwise relative entropy, and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. We also analyze the connection between the geometrical properties of such an embedding and its efficient learning procedure. Extensive experiments show that our proposed embedding is better at preserving the global geodesic information of graphs, and outperforms existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Make this fluent while doing minimal change": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Refine this for me please": "We propose a new way of representing nodes in directed graphs as points on a statistical manifold. This is done by minimizing pairwise relative entropy and graph geodesics in a non-linear way. Each node is then represented by a probability density function over a measurable space. We also analyze the connection between the geometrical properties of such an embedding and its efficient learning procedure. Experiments show that our proposed embedding is better at preserving global geodesic information than existing embedding models, and outperforms them in a variety of evaluation metrics.",
        "Concise this for me and keep all the information": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.",
        "Improve this in GPT way": "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection between the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better in preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting."
    },
    {
        "input": "Euclidean geometry has historically been the typical \"workhorse\" for machine\nlearning applications due to its power and simplicity. However, it has recently\nbeen shown that geometric spaces with constant non-zero curvature improve\nrepresentations and performance on a variety of data types and downstream\ntasks. Consequently, generative models like Variational Autoencoders (VAEs)\nhave been successfully generalized to elliptical and hyperbolic latent spaces.\nWhile these approaches work well on data with particular kinds of biases e.g.\ntree-like data for a hyperbolic VAE, there exists no generic approach unifying\nand leveraging all three models. We develop a Mixed-curvature Variational\nAutoencoder, an efficient way to train a VAE whose latent space is a product of\nconstant curvature Riemannian manifolds, where the per-component curvature is\nfixed or learnable. This generalizes the Euclidean VAE to curved latent spaces\nand recovers it when curvatures of all latent space components go to 0.",
        "Revise this with your best effort": "Euclidean geometry has been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Help me polish this": "Euclidean geometry has been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Rewrite this for me": "Euclidean geometry is typically the \"workhorse\" for machine learning applications because of its power and simplicity. However, recent studies have shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases (e.g. tree-like data for a hyperbolic VAE), there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Make this fluent while doing minimal change": "Euclidean geometry has historically been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Refine this for me please": "Euclidean geometry has been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Concise this for me and keep all the information": "Euclidean geometry is typically used for machine learning applications because it is powerful and simple. However, recent studies have shown that geometric spaces with constant non-zero curvature can improve representations and performance on various data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases (e.g. tree-like data for a hyperbolic VAE), there is no generic approach unifying and leveraging all three models. The Mixed-curvature Variational Autoencoder is an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.",
        "Improve this in GPT way": "Euclidean geometry has historically been the typical \"workhorse\" for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0."
    },
    {
        "input": "We study training of Convolutional Neural Networks (CNNs) with ReLU\nactivations and introduce exact convex optimization formulations with a\npolynomial complexity with respect to the number of data samples, the number of\nneurons, and data dimension. More specifically, we develop a convex analytic\nframework utilizing semi-infinite duality to obtain equivalent convex\noptimization problems for several two- and three-layer CNN architectures. We\nfirst prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm\nregularized convex program. We then show that multi-layer circular CNN training\nproblems with a single ReLU layer are equivalent to an $\\ell_1$ regularized\nconvex program that encourages sparsity in the spectral domain. We also extend\nthese results to three-layer CNNs with two ReLU layers. Furthermore, we present\nextensions of our approach to different pooling methods, which elucidates the\nimplicit architectural bias as convex regularizers.",
        "Revise this with your best effort": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations, and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidate the implicit architectural bias as convex regularizers.",
        "Help me polish this": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Rewrite this for me": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Make this fluent while doing minimal change": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidate the implicit architectural bias as convex regularizers.",
        "Refine this for me please": "We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Concise this for me and keep all the information": "We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Improve this in GPT way": "We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers."
    },
    {
        "input": "We propose a new metric space of ReLU activation codes equipped with a\ntruncated Hamming distance which establishes an isometry between its elements\nand polyhedral bodies in the input space which have recently been shown to be\nstrongly related to safety, robustness, and confidence. This isometry allows\nthe efficient computation of adjacency relations between the polyhedral bodies.\nExperiments on MNIST and CIFAR-10 indicate that information besides accuracy\nmight be stored in the code space.",
        "Revise this with your best effort": "We propose a new metric space of ReLU activation codes equipped with a truncated Hamming distance. This establishes an isometry between the elements of the space and the polyhedral bodies in the input space. This isometry allows for the efficient computation of adjacency relations between the polyhedral bodies. Experiments on MNIST and CIFAR-10 suggest that this space might contain more information than just accuracy.",
        "Help me polish this": "We propose a new metric space of ReLU activation codes equipped with a truncated Hamming distance. This establishes an isometry between the codes and polyhedral bodies in the input space, which have been shown to be related to safety, robustness, and confidence. This isometry allows the efficient computation of adjacency relations between the polyhedral bodies. Experiments on MNIST and CIFAR-10 indicate that information besides accuracy might be stored in the code space.",
        "Rewrite this for me": "We propose a new metric space of ReLU activation codes which is equipped with a truncated Hamming distance. This establishes an isometry between the elements of the space and polyhedral bodies in the input space. These polyhedral bodies have been shown to be strongly related to safety, robustness, and confidence. This isometry allows for the efficient computation of adjacency relations between the polyhedral bodies. Experiments on MNIST and CIFAR-10 indicate that information besides accuracy might be stored in the code space.",
        "Make this fluent while doing minimal change": "We propose a new metric space of ReLU activation codes equipped with a truncated Hamming distance. This establishes an isometry between the codes and polyhedral bodies in the input space, which have been shown to be strongly related to safety, robustness, and confidence. This isometry allows the efficient computation of adjacency relations between the polyhedral bodies. Experiments on MNIST and CIFAR-10 indicate that information besides accuracy might be stored in the code space.",
        "Refine this for me please": "We propose a new metric space of ReLU activation codes equipped with a truncated Hamming distance. This establishes an isometry between the elements of the space and polyhedral bodies in the input space. This isometry allows for the efficient computation of adjacency relations between the polyhedral bodies. Experiments on MNIST and CIFAR-10 suggest that this space might be useful for storing information beyond accuracy.",
        "Concise this for me and keep all the information": "We propose a new metric space of ReLU activation codes equipped with a truncated Hamming distance. This isometry allows the efficient computation of adjacency relations between the polyhedral bodies. Experiments on MNIST and CIFAR-10 indicate that information besides accuracy might be stored in the code space.",
        "Improve this in GPT way": "We propose a new metric space of ReLU activation codes equipped with a truncated Hamming distance which establishes an isometry between its elements and polyhedral bodies in the input space. This isometry allows the efficient computation of adjacency relations between the polyhedral bodies. Experiments on MNIST and CIFAR-10 indicate that the code space might contain information beyond accuracy."
    },
    {
        "input": "This paper introduces the first dataset of satellite images labeled with\nforage quality by on-the-ground experts and provides proof of concept for\napplying computer vision methods to index-based drought insurance. We also\npresent the results of a collaborative benchmark tool used to crowdsource an\naccurate machine learning model on the dataset. Our methods significantly\noutperform the existing technology for an insurance program in Northern Kenya,\nsuggesting that a computer vision-based approach could substantially benefit\npastoralists, whose exposure to droughts is severe and worsening with climate\nchange.",
        "Revise this with your best effort": "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts. We provide proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change.",
        "Help me polish this": "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts and provides proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change.",
        "Rewrite this for me": "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts. We provide proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change.",
        "Make this fluent while doing minimal change": "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts. It provides proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change.",
        "Refine this for me please": "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts and provides proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change.",
        "Concise this for me and keep all the information": "This paper introduces a new dataset of satellite images labeled with forage quality by on-the-ground experts, and provides proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change.",
        "Improve this in GPT way": "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts and provides proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change."
    },
    {
        "input": "We propose a neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall.",
        "Revise this with your best effort": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder.\n\nThe encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers).\n\nExtensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Help me polish this": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder.\n\nThe encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers).\n\nExtensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Rewrite this for me": "We propose a neural network for unsupervised anomaly detection which uses a robust subspace recovery layer (RSR layer) in order to extract the underlying subspace from a latent representation of given data and remove outliers which lie away from this subspace. This layer is used within an autoencoder; the encoder maps data into a latent space from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Make this fluent while doing minimal change": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Refine this for me please": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Concise this for me and keep all the information": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder.\n\nThe encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \"manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers).\n\nExtensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.",
        "Improve this in GPT way": "We propose a GPT-based neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall."
    },
    {
        "input": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Revise this with your best effort": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Help me polish this": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Rewrite this for me": "Animal brains are able to learn throughout their lifetime primarily due to changes in synaptic connectivity that are plastic, meaning they can be changed. It's important to note that these changes are not passive, but are actively controlled by neuromodulation. This process is itself controlled by the brain, resulting in the brain's ability to modify itself, which plays an important role in learning and adaptation. This is also a major basis for biological reinforcement learning. Here, we show for the first time that artificial neural networks with this type of neuromodulated plasticity can be trained using gradient descent. We extend previous work on differentiable Hebbian plasticity by proposing a differentiable formulation for the neuromodulation of plasticity. We demonstrate that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic Long Short-Term Memory networks with millions of parameters outperform standard Long Short-Term Memory networks on a benchmark language modeling task. We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.",
        "Make this fluent while doing minimal change": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity that are actively controlled by neuromodulation. This neuromodulation is itself under\nthe control of the brain, resulting in self-modifying abilities that play an important role in learning and adaptation. This is a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks.",
        "Refine this for me please": "The ability of animals to learn throughout their lives is primarily due to the brain's ability to change the connections between neurons (synapses). These changes are not passive, but are actively controlled by neuromodulation, which itself is controlled by the brain. This allows the brain to constantly modify its own structure and function, which is essential for learning and adaptation.\n\nWe show that artificial neural networks can be trained using gradient descent to mimic this process of neuromodulated plasticity. This offers a powerful new framework for training neural networks that can improve performance on both reinforcement learning and supervised learning tasks.",
        "Concise this for me and keep all the information": "Animal brains are able to learn throughout their lifetime due to changes in synaptic connectivity, which are actively controlled by neuromodulation. This self-modifying ability of the brain is key to learning and adaptation, and forms the basis for reinforcement learning. Here, we show that artificial neural networks can be trained with gradient descent to mimic this process. By extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. This results in improved performance on both reinforcement learning and supervised learning tasks.",
        "Improve this in GPT way": "The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks."
    },
    {
        "input": "The inclusion of Computer Vision and Deep Learning technologies in\nAgriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects.",
        "Revise this with your best effort": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi, 2018) can help in detecting healthy apples\nfrom apples with defects.",
        "Help me polish this": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi, 2018) can help in detecting healthy apples\nfrom apples with defects.",
        "Rewrite this for me": "The use of computer vision and deep learning technologies in agriculture has the potential to increase the quality and productivity of farmers. These technologies can be used to sort fruits and vegetables, and to detect defects in apples.",
        "Make this fluent while doing minimal change": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture can help increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects.",
        "Refine this for me please": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper explores if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects.",
        "Concise this for me and keep all the information": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers. In particular, this paper explores if recent computer vision and deep learning methods such as the YOLOv3 can help in detecting healthy apples from apples with defects, in order to help farmers with post-harvest handling.",
        "Improve this in GPT way": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects."
    },
    {
        "input": "Recent advances in neural machine translation (NMT) have led to\nstate-of-the-art results for many European-based translation tasks. However,\ndespite these advances, there is has been little focus in applying these\nmethods to African languages. In this paper, we seek to address this gap by\ncreating an NMT benchmark BLEU score between English and the ten remaining\nofficial languages in South Africa.",
        "Revise this with your best effort": "Recent advances in neural machine translation (NMT) have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.",
        "Help me polish this": "Recent advances in neural machine translation (NMT) have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.",
        "Rewrite this for me": "Recent advances in neural machine translation have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there has been little focus on applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.",
        "Make this fluent while doing minimal change": "Recent advances in neural machine translation (NMT) have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.",
        "Refine this for me please": "Recent advances in neural machine translation (NMT) have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.",
        "Concise this for me and keep all the information": "Despite recent advances in neural machine translation, there has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa.",
        "Improve this in GPT way": "Recent advances in neural machine translation (NMT) have led to state-of-the-art results for many languages. However, despite these advances, there has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark BLEU score between English and the ten remaining official languages in South Africa."
    },
    {
        "input": "We propose an algorithm combining calibrated prediction and generalization\nbounds from learning theory to construct confidence sets for deep neural\nnetworks with PAC guarantees---i.e., the confidence set for a given input\ncontains the true label with high probability. We demonstrate how our approach\ncan be used to construct PAC confidence sets on ResNet for ImageNet, a visual\nobject tracking model, and a dynamics model for the half-cheetah reinforcement\nlearning problem.",
        "Revise this with your best effort": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Help me polish this": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Rewrite this for me": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Make this fluent while doing minimal change": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Refine this for me please": "We propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Concise this for me and keep all the information": "We propose an algorithm that uses calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees. This means that the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "Improve this in GPT way": "We propose an algorithm combining calibrated prediction and generalization\nbounds from learning theory to construct confidence sets for deep neural\nnetworks with PAC guarantees. This algorithm can be used to construct PAC confidence sets on ResNet for ImageNet, a visual\nobject tracking model, and a dynamics model for the half-cheetah reinforcement\nlearning problem."
    },
    {
        "input": "With the recent success and popularity of pre-trained language models (LMs)\nin natural language processing, there has been a rise in efforts to understand\ntheir inner workings. In line with such interest, we propose a novel method\nthat assists us in investigating the extent to which pre-trained LMs capture\nthe syntactic notion of constituency. Our method provides an effective way of\nextracting constituency trees from the pre-trained LMs without training. In\naddition, we report intriguing findings in the induced trees, including the\nfact that pre-trained LMs outperform other approaches in correctly demarcating\nadverb phrases in sentences.",
        "Revise this with your best effort": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.",
        "Help me polish this": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.",
        "Rewrite this for me": "Pre-trained language models (LMs) have been having a lot of success and popularity in natural language processing recently, so people are interested in understanding how they work. We have a new method that helps us see how well pre-trained LMs can capture the syntactic idea of constituency. Our method doesn't require any training, and it's very effective. We also found some interesting things in the trees we got from the pre-trained LMs. For example, pre-trained LMs seem to do a better job than other methods at correctly identifying adverb phrases in sentences.",
        "Make this fluent while doing minimal change": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.",
        "Refine this for me please": "We propose a new method for understanding how well pre-trained language models capture the syntactic notion of constituency. Our method provides a way of extracting constituency trees from pre-trained language models without training. We report findings in the induced trees, including the fact that pre-trained language models outperform other approaches in correctly demarcating adverb phrases in sentences.",
        "Concise this for me and keep all the information": "We propose a novel method of investigating the extent to which pre-trained language models (LMs) capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.",
        "Improve this in GPT way": "We propose a novel method for investigating the extent to which pre-trained language models (LMs) capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences."
    },
    {
        "input": "Magnitude-based pruning is one of the simplest methods for pruning neural\nnetworks. Despite its simplicity, magnitude-based pruning and its variants\ndemonstrated remarkable performances for pruning modern architectures. Based on\nthe observation that magnitude-based pruning indeed minimizes the Frobenius\ndistortion of a linear operator corresponding to a single layer, we develop a\nsimple pruning method, coined lookahead pruning, by extending the single layer\noptimization to a multi-layer optimization. Our experimental results\ndemonstrate that the proposed method consistently outperforms magnitude-based\npruning on various networks, including VGG and ResNet, particularly in the\nhigh-sparsity regime. See https://github.com/alinlab/lookahead_pruning for\ncodes.",
        "Revise this with your best effort": "Magnitude-based pruning is a simple method for pruning neural networks. Despite its simplicity, it has been shown to be effective in pruning modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we develop a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results demonstrate that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. See https://github.com/alinlab/lookahead_pruning for codes.",
        "Help me polish this": "Magnitude-based pruning is a simple but effective method for pruning neural networks. Its variants have demonstrated excellent performance on modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we develop a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results demonstrate that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. See https://github.com/alinlab/lookahead_pruning for codes.",
        "Rewrite this for me": "Magnitude-based pruning is a simple method for pruning neural networks. Despite its simplicity, magnitude-based pruning and its variants have demonstrated remarkable performance for pruning modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we have developed a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results demonstrate that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime.",
        "Make this fluent while doing minimal change": "Magnitude-based pruning is one of the simplest methods for pruning neural networks. Despite its simplicity, magnitude-based pruning and its variants have demonstrated remarkable performance for pruning modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we develop a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results demonstrate that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. See https://github.com/alinlab/lookahead_pruning for codes.",
        "Refine this for me please": "Magnitude-based pruning is a simple method for pruning neural networks. It is based on the observation that magnitude-based pruning minimizes the Frobenius distortion of a linear operator corresponding to a single layer. Lookahead pruning is a simple extension of magnitude-based pruning that is based on the observation that magnitude-based pruning also minimizes the Frobenius distortion of a multi-layer linear operator. Lookahead pruning consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime.",
        "Concise this for me and keep all the information": "Magnitude-based pruning is a simple method for pruning neural networks. It is based on the observation that magnitude-based pruning minimizes the Frobenius distortion of a linear operator corresponding to a single layer. Lookahead pruning is a simple pruning method that extends the single layer optimization to a multi-layer optimization. It outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime.",
        "Improve this in GPT way": "We propose a new pruning method, called lookahead pruning, which outperforms magnitude-based pruning on various networks, including VGG and ResNet. Lookahead pruning is based on the observation that magnitude-based pruning minimizes the Frobenius distortion of a linear operator corresponding to a single layer. We extend the single layer optimization to a multi-layer optimization, which results in better performance on high-sparsity regimes."
    },
    {
        "input": "As the share of renewable energy sources in the present electric energy mix\nrises, their intermittence proves to be the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation. We\npropose an implementation of a pricing agent with a reinforcement learning\napproach where the environment is represented by the customers, the electricity\ngeneration utilities and the weather conditions.",
        "Revise this with your best effort": "As the share of renewable energy sources in the present electric energy mix\nrises, their intermittency becomes the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation. We\npropose an implementation of a pricing agent with a reinforcement learning\napproach where the environment is represented by the customers, the electricity\ngeneration utilities and the weather conditions.",
        "Help me polish this": "As the share of renewable energy sources in the present electric energy mix\nrises, their intermittency becomes the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation. We\npropose an implementation of a pricing agent with a reinforcement learning\napproach where the environment is represented by the customers, the electricity\ngeneration utilities and the weather conditions.",
        "Rewrite this for me": "As renewable energy sources become a larger part of the current electric energy mix, their intermittency becomes the biggest challenge to carbon-free electricity generation. To address this challenge, we propose an electricity pricing agent, which sends price signals to customers and encourages them to shift their demand to periods of high renewable energy generation. We propose an implementation of a pricing agent using a reinforcement learning approach, where the environment is represented by the customers, the electricity generation utilities, and the weather conditions.",
        "Make this fluent while doing minimal change": "As the share of renewable energy sources in the present electric energy mix\nrises, their intermittence becomes the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation. We\npropose an implementation of a pricing agent with a reinforcement learning\napproach where the environment is represented by the customers, the electricity\ngeneration utilities and the weather conditions.",
        "Refine this for me please": "As the share of renewable energy sources in the present electric energy mix\nrises, their intermittency becomes the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation. We\npropose an implementation of a pricing agent with a reinforcement learning\napproach where the environment is represented by the customers, the electricity\ngeneration utilities and the weather conditions.",
        "Concise this for me and keep all the information": "As the share of renewable energy sources in the present electric energy mix\nrises, their intermittence proves to be the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation.",
        "Improve this in GPT way": "As the share of renewable energy sources in the present electric energy mix\nrises, their intermittency proves to be the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation. We\npropose an implementation of a pricing agent with a reinforcement learning\napproach where the environment is represented by the customers, the electricity\ngeneration utilities and the weather conditions. The agent is trained to\nmaximize a social welfare function that captures the benefits for the\ncustomers, the utilities, and the environment."
    },
    {
        "input": "We report our experiments in building a domain-specific Tigrinya-to-English\nneural machine translation system. We use transfer learning from other Ge'ez\nscript languages and report an improvement of 1.3 BLEU points over a classic\nneural baseline. We publish our development pipeline as an open-source library\nand also provide a demonstration application.",
        "Revise this with your best effort": "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library and also provide a demonstration application.",
        "Help me polish this": "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library and also provide a demonstration application.",
        "Rewrite this for me": "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library and also provide a demonstration application.",
        "Make this fluent while doing minimal change": "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library and also provide a demonstration application.",
        "Refine this for me please": "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library and also provide a demonstration application.",
        "Concise this for me and keep all the information": "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library.",
        "Improve this in GPT way": "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system. We use transfer learning from other Ge'ez script languages and report an improvement of 1.3 BLEU points over a classic neural baseline. We publish our development pipeline as an open-source library and also provide a demonstration application."
    },
    {
        "input": "Nigerian Pidgin is arguably the most widely spoken language in Nigeria.\nVariants of this language are also spoken across West and Central Africa,\nmaking it a very important language. This work aims to establish supervised and\nunsupervised neural machine translation (NMT) baselines between English and\nNigerian Pidgin. We implement and compare NMT models with different\ntokenization methods, creating a solid foundation for future works.",
        "Revise this with your best effort": "Nigerian Pidgin is a widely spoken language in Nigeria. Variants of this language are also spoken across West and Central Africa, making it a very important language. This work aims to establish supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. We implement and compare NMT models with different tokenization methods, creating a solid foundation for future works.",
        "Help me polish this": "Nigerian Pidgin is a widely spoken language in Nigeria and other parts of West and Central Africa. This work establishes supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. Different tokenization methods are implemented and compared, providing a solid foundation for future works.",
        "Rewrite this for me": "Nigerian Pidgin is a language spoken by many people in Nigeria. It is also spoken in other parts of West and Central Africa, making it a very important language. This work establishes supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. We implement and compare NMT models with different tokenization methods, creating a solid foundation for future works.",
        "Make this fluent while doing minimal change": "Nigerian Pidgin is widely spoken in Nigeria and other parts of West and Central Africa, making it a very important language. This work establishes supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. We implement and compare NMT models with different tokenization methods, creating a foundation for future works.",
        "Refine this for me please": "Nigerian Pidgin is a widely spoken language in Nigeria and other parts of West and Central Africa. This work establishes supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. Different tokenization methods are compared, providing a foundation for future works.",
        "Concise this for me and keep all the information": "Nigerian Pidgin is a widely spoken language in Nigeria and other parts of West and Central Africa. This work establishes supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. NMT models with different tokenization methods are compared, creating a foundation for future works.",
        "Improve this in GPT way": "Nigerian Pidgin is a very important language that is spoken by many people across West and Central Africa. This work establishes supervised and unsupervised neural machine translation (NMT) baselines between English and Nigerian Pidgin. Different tokenization methods are compared in order to create a solid foundation for future works."
    },
    {
        "input": "Estimating grape yield prior to harvest is important to commercial vineyard\nproduction as it informs many vineyard and winery decisions. Currently, the\nprocess of yield estimation is time consuming and varies in its accuracy from\n75-90\\% depending on the experience of the viticulturist. This paper proposes a\nmultiple task learning (MTL) convolutional neural network (CNN) approach that\nuses images captured by inexpensive smart phones secured in a simple tripod\narrangement. The CNN models use MTL transfer from autoencoders to achieve 85\\%\naccuracy from image data captured 6 days prior to harvest.",
        "Revise this with your best effort": "It is important to estimate grape yield prior to harvest in order to make informed decisions in commercial vineyards and wineries. The current process of yield estimation is time consuming and can be inaccurate, depending on the experience of the viticulturist. This paper proposes using a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones secured in a simple tripod arrangement. The CNN models use MTL transfer from autoencoders to achieve 85% accuracy from image data captured 6 days prior to harvest.",
        "Help me polish this": "Estimating grape yield prior to harvest is important to commercial vineyard production as it informs many vineyard and winery decisions. The current process of yield estimation is time consuming and varies in accuracy from 75-90%, depending on the experience of the viticulturist. This paper proposes a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones secured in a simple tripod arrangement. The CNN models use MTL transfer from autoencoders to achieve 85% accuracy from image data captured 6 days prior to harvest.",
        "Rewrite this for me": "It is important to estimate grape yield before harvest in order to make informed decisions in commercial vineyards and wineries. The current process of yield estimation is time-consuming and can be inaccurate 75-90% of the time, depending on the experience of the viticulturist. This paper proposes using a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones secured in a simple tripod arrangement. The CNN models use MTL transfer from autoencoders to achieve 85% accuracy from image data captured 6 days prior to harvest.",
        "Make this fluent while doing minimal change": "It is important to estimate grape yield prior to harvest in order to make informed decisions in commercial vineyard production. The current process of yield estimation is time consuming and can be inaccurate. This paper proposes using a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones. The CNN models use MTL transfer from autoencoders to achieve 85% accuracy from image data captured 6 days prior to harvest.",
        "Refine this for me please": "The paper proposes a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones secured in a simple tripod arrangement. The CNN models use MTL transfer from autoencoders to achieve 85% accuracy from image data captured 6 days prior to harvest.",
        "Concise this for me and keep all the information": "It is important to estimate grape yield prior to harvest in order to make informed decisions in the vineyard and winery. The current process of yield estimation is time consuming and can be inaccurate. This paper proposes a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones to achieve 85% accuracy from image data captured 6 days prior to harvest.",
        "Improve this in GPT way": "Estimating grape yield prior to harvest is important to commercial vineyard production as it informs many vineyard and winery decisions. Currently, the process of yield estimation is time consuming and varies in its accuracy from 75-90\\% depending on the experience of the viticulturist. This paper proposes a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones secured in a simple tripod arrangement. The CNN models use MTL transfer from autoencoders to achieve 85\\% accuracy from image data captured 6 days prior to harvest."
    },
    {
        "input": "Automatic change detection and disaster damage assessment are currently\nprocedures requiring a huge amount of labor and manual work by satellite\nimagery analysts. In the occurrences of natural disasters, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures which are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among top results on the xView2 challenge leaderboard. We release our\ncode used for the competition.",
        "Revise this with your best effort": "Automatic change detection and disaster damage assessment are currently\nprocedures that require a lot of labor and manual work by satellite\nimagery analysts. In the event of natural disasters, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures that are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among the top results on the xView2 challenge leaderboard. We release our\ncode used for the competition.",
        "Help me polish this": "Automatic change detection and disaster damage assessment are currently\nprocedures that require a lot of labor and manual work by satellite\nimagery analysts. In the event of a natural disaster, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures that are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among the top results on the xView2 challenge leaderboard. We release our\ncode used for the competition.",
        "Rewrite this for me": "Automatic change detection and disaster damage assessment are procedures that currently require a lot of labor and manual work by satellite imagery analysts. In the event of a natural disaster, timely change detection can save lives. In this work, we report findings on problem framing, data processing and training procedures which are specifically helpful for the task of building damage assessment using the newly released xBD dataset. Our insights lead to substantial improvement over the xBD baseline models, and we score among the top results on the xView2 challenge leaderboard. We release our code used for the competition.",
        "Make this fluent while doing minimal change": "Automatic change detection and disaster damage assessment are currently\nprocedures that require a lot of labor and manual work by satellite\nimagery analysts. In the event of natural disasters, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures that are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among the top results on the xView2 challenge leaderboard. We release our\ncode used for the competition.",
        "Refine this for me please": "Automatic change detection and disaster damage assessment are currently\nprocedures that require a lot of labor and manual work by satellite\nimagery analysts. In the case of natural disasters, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures which are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among the top results on the xView2 challenge leaderboard. We release our\ncode used for the competition.",
        "Concise this for me and keep all the information": "Automatic change detection and disaster damage assessment are procedures that currently require a lot of labor and manual work by satellite imagery analysts. In the case of natural disasters, timely change detection can save lives. In this work, we report findings on problem framing, data processing and training procedures which are specifically helpful for the task of building damage assessment using the newly released xBD dataset. Our insights lead to substantial improvement over the xBD baseline models, and we score among top results on the xView2 challenge leaderboard. We release our code used for the competition.",
        "Improve this in GPT way": "Automatic change detection and disaster damage assessment are currently\nprocedures that require a lot of labor and manual work by satellite\nimagery analysts. In the event of a natural disaster, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures that are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among the top results on the xView2 challenge leaderboard. We release our\ncode used for the competition."
    },
    {
        "input": "Recurrent neural networks (RNNs) are non-linear dynamic systems. Previous\nwork believes that RNN may suffer from the phenomenon of chaos, where the\nsystem is sensitive to initial states and unpredictable in the long run. In\nthis paper, however, we perform a systematic empirical analysis, showing that a\nvanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior\nalong the training process in real applications such as text generation. Our\nfindings suggest that future work in this direction should address the other\nside of non-linear dynamics for RNN.",
        "Revise this with your best effort": "Recurrent neural networks (RNNs) are non-linear dynamic systems. Previous\nwork has suggested that RNNs may suffer from the phenomenon of chaos, where the\nsystem is sensitive to initial states and unpredictable in the long run. In\nthis paper, however, we perform a systematic empirical analysis, showing that a\nvanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior\nalong the training process in real applications such as text generation. Our\nfindings suggest that future work in this direction should address the other\nside of non-linear dynamics for RNNs.",
        "Help me polish this": "Recurrent neural networks (RNNs) are non-linear dynamic systems that have been shown to be potentially chaotic. However, in this paper we show that, in real applications such as text generation, RNNs do not actually exhibit chaotic behavior. Our findings suggest that future work should focus on the other aspects of non-linear dynamics for RNNs.",
        "Rewrite this for me": "Recurrent neural networks (RNNs) are non-linear dynamic systems. Some previous\nwork has suggested that RNNs may suffer from the phenomenon of chaos, where the\nsystem is sensitive to initial states and unpredictable in the long run. However,\nin this paper we perform a systematic empirical analysis, showing that a\nvanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior\nalong the training process in real applications such as text generation. Our\nfindings suggest that future work in this direction should focus on the other\naspects of non-linear dynamics for RNNs.",
        "Make this fluent while doing minimal change": "Recurrent neural networks (RNNs) are non-linear dynamic systems. Previous\nwork has suggested that RNNs may suffer from the phenomenon of chaos, where the\nsystem is sensitive to initial states and unpredictable in the long run. In\nthis paper, however, we perform a systematic empirical analysis, showing that a\nvanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior\nalong the training process in real applications such as text generation. Our\nfindings suggest that future work in this direction should address the other\nside of non-linear dynamics for RNNs.",
        "Refine this for me please": "In this paper, we investigate whether recurrent neural networks (RNNs) suffer from the phenomenon of chaos. Our findings suggest that RNNs do not exhibit chaotic behavior during training in real applications.",
        "Concise this for me and keep all the information": "RNNs are non-linear dynamic systems that have been believed to suffer from the phenomenon of chaos. However, this paper shows that a vanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior along the training process in real applications such as text generation.",
        "Improve this in GPT way": "Recurrent neural networks (RNNs) are non-linear dynamic systems. Previous\nwork has suggested that RNNs may suffer from the phenomenon of chaos, where the\nsystem is sensitive to initial states and unpredictable in the long run. In\nthis paper, however, we perform a systematic empirical analysis, showing that a\nvanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior\nalong the training process in real applications such as text generation. Our\nfindings suggest that future work in this direction should focus on the other\naspects of non-linear dynamics for RNNs."
    },
    {
        "input": "Fine-tuning a pretrained BERT model is the state of the art method for\nextractive/abstractive text summarization, in this paper we showcase how this\nfine-tuning method can be applied to the Arabic language to both construct the\nfirst documented model for abstractive Arabic text summarization and show its\nperformance in Arabic extractive summarization. Our model works with\nmultilingual BERT (as Arabic language does not have a pretrained BERT of its\nown). We show its performance in English corpus first before applying it to\nArabic corpora in both extractive and abstractive tasks.",
        "Revise this with your best effort": "In this paper, we showcase how the state-of-the-art method for fine-tuning a pretrained BERT model can be applied to the Arabic language to construct the first documented model for abstractive Arabic text summarization. We demonstrate the performance of our model on an English corpus before applying it to Arabic corpora for both extractive and abstractive tasks.",
        "Help me polish this": "Fine-tuning a pretrained BERT model is the state of the art method for extractive/abstractive text summarization. In this paper, we showcase how this fine-tuning method can be applied to the Arabic language to both construct the first documented model for abstractive Arabic text summarization and show its performance in Arabic extractive summarization. Our model works with multilingual BERT (as Arabic language does not have a pretrained BERT of its own). We show its performance in English corpus first before applying it to Arabic corpora in both extractive and abstractive tasks.",
        "Rewrite this for me": "In this paper, we showcase how the state-of-the-art fine-tuning method for extractive/abstractive text summarization can be applied to the Arabic language. This is the first documented model for abstractive Arabic text summarization, and we show its performance in both extractive and abstractive tasks. Our model works with multilingual BERT (as Arabic language does not have a pretrained BERT of its own). We show its performance in an English corpus first before applying it to Arabic corpora.",
        "Make this fluent while doing minimal change": "Fine-tuning a pretrained BERT model is the state of the art method for extractive/abstractive text summarization. In this paper, we showcase how this fine-tuning method can be applied to the Arabic language to both construct the first documented model for abstractive Arabic text summarization and show its performance in Arabic extractive summarization. Our model works with multilingual BERT (as Arabic language does not have a pretrained BERT of its own). We show its performance in English corpus first before applying it to Arabic corpora in both extractive and abstractive tasks.",
        "Refine this for me please": "In this paper, we showcase how the state-of-the-art method for extractive/abstractive text summarization (fine-tuning a pretrained BERT model) can be applied to the Arabic language. We show that our model works with multilingual BERT (as Arabic language does not have a pretrained BERT of its own) and demonstrate its performance in both extractive and abstractive tasks on Arabic corpora.",
        "Concise this for me and keep all the information": "In this paper, we showcase how the fine-tuning method for pretrained BERT models can be applied to Arabic language for both extractive and abstractive text summarization tasks. Our model works with multilingual BERT (as Arabic language does not have a pretrained BERT of its own). We show its performance in English corpus first before applying it to Arabic corpora.",
        "Improve this in GPT way": "In this paper, we showcase how the fine-tuning method for pretrained BERT models can be applied to the Arabic language to construct the first documented model for abstractive Arabic text summarization. We show that our model works with multilingual BERT (as Arabic language does not have a pretrained BERT of its own) and demonstrate its performance in both extractive and abstractive tasks on English and Arabic corpora."
    },
    {
        "input": "During cluster analysis domain experts and visual analysis are frequently\nrelied on to identify the optimal clustering structure. This process tends to\nbe adhoc, subjective and difficult to reproduce. This work shows how competency\nquestions can be used to formalise expert knowledge and application\nrequirements for context specific evaluation of a clustering application in the\nresidential energy consumption sector.",
        "Revise this with your best effort": "Domain experts and visual analysis are frequently relied on to identify the optimal clustering structure during cluster analysis. However, this process tends to be ad hoc, subjective, and difficult to reproduce. This work shows how competency questions can be used to formalize expert knowledge and application requirements for context-specific evaluation of a clustering application in the residential energy consumption sector. This will help make the process more objective and reproducible.",
        "Help me polish this": "During cluster analysis, domain experts and visual analysis are frequently relied on to identify the optimal clustering structure. This process tends to be ad hoc, subjective, and difficult to reproduce. This work shows how competency questions can be used to formalize expert knowledge and application requirements for context-specific evaluation of a clustering application in the residential energy consumption sector.",
        "Rewrite this for me": "Domain experts and visual analysis are often relied on to identify the optimal clustering structure during cluster analysis. However, this process is usually ad hoc, subjective, and difficult to reproduce. This work demonstrates how competency questions can be used to formalize expert knowledge and application requirements for context-specific evaluation of a clustering application in the residential energy consumption sector.",
        "Make this fluent while doing minimal change": "Domain experts and visual analysis are frequently relied on to identify the optimal clustering structure. However, this process tends to be adhoc, subjective and difficult to reproduce. This work shows how competency questions can be used to formalise expert knowledge and application requirements for context specific evaluation of a clustering application in the residential energy consumption sector.",
        "Refine this for me please": "This work shows how competency questions can be used to formalise expert knowledge and application requirements for context specific evaluation of a clustering application in the residential energy consumption sector. This process tends to be adhoc, subjective and difficult to reproduce.",
        "Concise this for me and keep all the information": "This work shows how competency questions can be used to formalise expert knowledge and application requirements for context specific evaluation of a clustering application in the residential energy consumption sector. This process tends to be adhoc, subjective and difficult to reproduce.",
        "Improve this in GPT way": "During cluster analysis, domain experts and visual analysis are frequently relied on to identify the optimal clustering structure. However, this process tends to be adhoc, subjective and difficult to reproduce. This work shows how competency questions can be used to formalise expert knowledge and application requirements for context specific evaluation of a clustering application in the residential energy consumption sector. By doing so, it is possible to create a more objective and reproducible process for identifying the optimal clustering structure."
    },
    {
        "input": "Action and observation delays commonly occur in many Reinforcement Learning\napplications, such as remote control scenarios. We study the anatomy of\nrandomly delayed environments, and show that partially resampling trajectory\nfragments in hindsight allows for off-policy multi-step value estimation. We\napply this principle to derive Delay-Correcting Actor-Critic (DCAC), an\nalgorithm based on Soft Actor-Critic with significantly better performance in\nenvironments with delays. This is shown theoretically and also demonstrated\npractically on a delay-augmented version of the MuJoCo continuous control\nbenchmark.",
        "Revise this with your best effort": "Action and observation delays are a common problem in many Reinforcement Learning applications, such as remote control scenarios. We study the effect of these delays, and show that partially resampling trajectory fragments in hindsight allows for more accurate off-policy multi-step value estimation. We apply this principle to derive the Delay-Correcting Actor-Critic (DCAC) algorithm, which is based on Soft Actor-Critic and performs significantly better in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Help me polish this": "Action and observation delays are a common problem in many Reinforcement Learning applications, such as remote control scenarios. We study the effect of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Rewrite this for me": "We study the effects of randomly delayed environments on reinforcement learning, and show that partially resampling trajectory fragments in hindsight can improve value estimation in these environments. We apply this principle to derive the Delay-Correcting Actor-Critic (DCAC) algorithm, which is based on Soft Actor-Critic and performs significantly better in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Make this fluent while doing minimal change": "Action and observation delays are common in many Reinforcement Learning applications, such as remote control scenarios. We study the effect of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Refine this for me please": "We study the effects of randomly delayed environments on Reinforcement Learning, and show that partially resampling trajectory fragments in hindsight can improve value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic that performs significantly better in environments with delays. This is demonstrated theoretically and also practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Concise this for me and keep all the information": "We study the effects of randomly delayed environments on Reinforcement Learning, and show that partially resampling trajectory fragments in hindsight allows for better off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays.",
        "Improve this in GPT way": "We study the effect of randomly delayed environments on Reinforcement Learning, and show that partially resampling trajectory fragments in hindsight can improve off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark."
    },
    {
        "input": "We demonstrate that differentially private machine learning has not yet\nreached its \"AlexNet moment\" on many canonical vision tasks: linear models\ntrained on handcrafted features significantly outperform end-to-end deep neural\nnetworks for moderate privacy budgets. To exceed the performance of handcrafted\nfeatures, we show that private learning requires either much more private data,\nor access to features learned on public data from a similar domain. Our work\nintroduces simple yet strong baselines for differentially private learning that\ncan inform the evaluation of future progress in this area.",
        "Revise this with your best effort": "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Help me polish this": "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Rewrite this for me": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks, with linear models trained on handcrafted features significantly outperforming end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Make this fluent while doing minimal change": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks, according to a new study. The study found that linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, the study found that private learning requires either much more private data, or access to features learned on public data from a similar domain.",
        "Refine this for me please": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks, where linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Concise this for me and keep all the information": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks. Linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.",
        "Improve this in GPT way": "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area."
    },
    {
        "input": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies.",
        "Revise this with your best effort": "In this paper, we introduce the Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Help me polish this": "In this paper, we introduce the Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Rewrite this for me": "In this paper, we introduce the Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. This framework is designed in a physics-informed manner, which allows for better generalization with fewer training samples. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way. This can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework offers interpretable, physically-consistent models for physical systems, which opens up new possibilities for synthesizing model-based control strategies.",
        "Make this fluent while doing minimal change": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Refine this for me please": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies.",
        "Concise this for me and keep all the information": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Improve this in GPT way": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies."
    },
    {
        "input": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning\nalgorithms that capture the dynamics of physical systems from observed\ntrajectories. An SRNN models the Hamiltonian function of the system by a neural\nnetwork and furthermore leverages symplectic integration, multiple-step\ntraining and initial state optimization to address the challenging numerical\nissues associated with Hamiltonian systems. We show that SRNNs succeed reliably\non complex and noisy Hamiltonian systems. We also show how to augment the SRNN\nintegration scheme in order to handle stiff dynamical systems such as bouncing\nbilliards.",
        "Revise this with your best effort": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Help me polish this": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Rewrite this for me": "We propose that Symplectic Recurrent Neural Networks (SRNNs) are learning algorithms that can capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by using a neural network. Furthermore, SRNNs use symplectic integration, multiple-step training, and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We have shown that SRNNs can succeed reliably on complex and noisy Hamiltonian systems. We have also shown how to augment the SRNN integration scheme in order to handle stiff dynamical systems, such as bouncing billiards.",
        "Make this fluent while doing minimal change": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. SRNNs model the Hamiltonian function of the system by a neural network and furthermore leverage symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Refine this for me please": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
        "Concise this for me and keep all the information": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems.",
        "Improve this in GPT way": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards."
    },
    {
        "input": "Anomaly detection, finding patterns that substantially deviate from those\nseen previously, is one of the fundamental problems of artificial intelligence.\nRecently, classification-based methods were shown to achieve superior results\non this task. In this work, we present a unifying view and propose an open-set\nmethod, GOAD, to relax current generalization assumptions. Furthermore, we\nextend the applicability of transformation-based methods to non-image data\nusing random affine transformations. Our method is shown to obtain\nstate-of-the-art accuracy and is applicable to broad data types. The strong\nperformance of our method is extensively validated on multiple datasets from\ndifferent domains.",
        "Revise this with your best effort": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.",
        "Help me polish this": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.",
        "Rewrite this for me": "Anomaly detection, which involves finding patterns that substantially deviate from those seen previously, is a key challenge in artificial intelligence. Recently, classification-based methods have been shown to outperform other approaches in this domain. In this work, we present a unifying view and propose an open-set method, GOAD, that relaxes current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data by using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to a broad range of data types. The strong performance of our method is extensively validated on multiple datasets from different domains.",
        "Make this fluent while doing minimal change": "Anomaly detection, which involves finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.",
        "Refine this for me please": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.",
        "Concise this for me and keep all the information": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types.",
        "Improve this in GPT way": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains."
    },
    {
        "input": "We consider training machine learning models that are fair in the sense that\ntheir performance is invariant under certain sensitive perturbations to the\ninputs. For example, the performance of a resume screening system should be\ninvariant under changes to the gender and/or ethnicity of the applicant. We\nformalize this notion of algorithmic fairness as a variant of individual\nfairness and develop a distributionally robust optimization approach to enforce\nit during training. We also demonstrate the effectiveness of the approach on\ntwo ML tasks that are susceptible to gender and racial biases.",
        "Revise this with your best effort": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Help me polish this": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Rewrite this for me": "We believe that it is important to train machine learning models in a way that considers fairness - meaning that the performance of the models should not be affected by changes to sensitive input data, such as gender or ethnicity. We formalize this idea of fairness as a variant of individual fairness, and develop a method of distributional robust optimization to enforce it during the training process. We also show that this method is effective on two ML tasks that are often susceptible to gender and racial bias.",
        "Make this fluent while doing minimal change": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Refine this for me please": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
        "Concise this for me and keep all the information": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training.",
        "Improve this in GPT way": "We consider training machine learning models that are fair in the sense that\ntheir performance is invariant under certain sensitive perturbations to the\ninputs. For example, the performance of a resume screening system should be\ninvariant under changes to the gender and/or ethnicity of the applicant. We\nformalize this notion of algorithmic fairness as a variant of individual\nfairness and develop a distributionally robust optimization approach to enforce\nit during training. We also demonstrate the effectiveness of the approach on\ntwo ML tasks that are susceptible to gender and racial biases."
    },
    {
        "input": "In this paper we consider self-supervised representation learning to improve\nsample efficiency in reinforcement learning (RL). We propose a forward\nprediction objective for simultaneously learning embeddings of states and\naction sequences. These embeddings capture the structure of the environment's\ndynamics, enabling efficient policy learning. We demonstrate that our action\nembeddings alone improve the sample efficiency and peak performance of\nmodel-free RL on control from low-dimensional states. By combining state and\naction embeddings, we achieve efficient learning of high-quality policies on\ngoal-conditioned continuous control from pixel observations in only 1-2 million\nenvironment steps.",
        "Revise this with your best effort": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose an objective for learning embeddings of states and action sequences that captures the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Help me polish this": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Rewrite this for me": "In this paper, we explore how self-supervised representation learning can be used to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Make this fluent while doing minimal change": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Refine this for me please": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Concise this for me and keep all the information": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",
        "Improve this in GPT way": "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps."
    },
    {
        "input": "In this paper, we cast fair machine learning as invariant machine learning.\nWe first formulate a version of individual fairness that enforces invariance on\ncertain sensitive sets. We then design a transport-based regularizer that\nenforces this version of individual fairness and develop an algorithm to\nminimize the regularizer efficiently. Our theoretical results guarantee the\nproposed approach trains certifiably fair ML models. Finally, in the\nexperimental studies we demonstrate improved fairness metrics in comparison to\nseveral recent fair training procedures on three ML tasks that are susceptible\nto algorithmic bias.",
        "Revise this with your best effort": "In this paper, we explore the idea of fair machine learning through the lens of invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Help me polish this": "In this paper, we formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Rewrite this for me": "In this paper, we explore the idea of fair machine learning through the lens of invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Make this fluent while doing minimal change": "In this paper, we formulate fair machine learning as invariant machine learning. We first develop a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Refine this for me please": "In this paper, we explore the idea of fairness in machine learning from the perspective of invariance. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Concise this for me and keep all the information": "In this paper, we show that fair machine learning can be achieved by enforcing invariance on certain sensitive sets. We design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Improve this in GPT way": "In this paper, we formulate fair machine learning as invariant machine learning.\nWe first develop a version of individual fairness that enforces invariance on\ncertain sensitive sets. We then design a transport-based regularizer that\nenforces this version of individual fairness and develop an algorithm to\nminimize the regularizer efficiently. Our theoretical results guarantee the\nproposed approach trains certifiably fair ML models. Finally, in the\nexperimental studies we demonstrate improved fairness metrics in comparison to\nseveral recent fair training procedures on three ML tasks that are susceptible\nto algorithmic bias."
    },
    {
        "input": "Despite significant advances, continual learning models still suffer from\ncatastrophic forgetting when exposed to incrementally available data from\nnon-stationary distributions. Rehearsal approaches alleviate the problem by\nmaintaining and replaying a small episodic memory of previous samples, often\nimplemented as an array of independent memory slots. In this work, we propose\nto augment such an array with a learnable random graph that captures pairwise\nsimilarities between its samples, and use it not only to learn new tasks but\nalso to guard against forgetting. Empirical results on several benchmark\ndatasets show that our model consistently outperforms recently proposed\nbaselines for task-free continual learning.",
        "Revise this with your best effort": "Despite significant advances, continual learning models that rely on rehearsal approaches still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Help me polish this": "Despite significant advances, continual learning models that rely on rehearsal approaches still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Rewrite this for me": "Despite significant advances, continual learning models that are exposed to incrementally available data from non-stationary distributions still suffer from catastrophic forgetting. Rehearsal approaches that maintain and replay a small episodic memory of previous samples often alleviate the problem. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Make this fluent while doing minimal change": "Despite significant advances, continual learning models that are exposed to incrementally available data from non-stationary distributions still suffer from catastrophic forgetting. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Refine this for me please": "Despite significant advances, continual learning models that are exposed to incrementally available data from non-stationary distributions still suffer from catastrophic forgetting. Rehearsal approaches that maintain and replay a small episodic memory of previous samples often alleviate the problem. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Concise this for me and keep all the information": "Continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Improve this in GPT way": "Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning."
    },
    {
        "input": "We provide a general self-attention formulation to impose group equivariance\nto arbitrary symmetry groups. This is achieved by defining positional encodings\nthat are invariant to the action of the group considered. Since the group acts\non the positional encoding directly, group equivariant self-attention networks\n(GSA-Nets) are steerable by nature. Our experiments on vision benchmarks\ndemonstrate consistent improvements of GSA-Nets over non-equivariant\nself-attention networks.",
        "Revise this with your best effort": "We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Help me polish this": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Rewrite this for me": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Make this fluent while doing minimal change": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Refine this for me please": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Concise this for me and keep all the information": "We provide a self-attention formulation that imposes group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Improve this in GPT way": "We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks."
    },
    {
        "input": "We propose to study the problem of few shot graph classification in graph\nneural networks (GNNs) to recognize unseen classes, given limited labeled graph\nexamples. Despite several interesting GNN variants being proposed recently for\nnode and graph classification tasks, when faced with scarce labeled examples in\nthe few shot setting, these GNNs exhibit significant loss in classification\nperformance. Here, we present an approach where a probability measure is\nassigned to each graph based on the spectrum of the graphs normalized\nLaplacian. This enables us to accordingly cluster the graph base labels\nassociated with each graph into super classes, where the Lp Wasserstein\ndistance serves as our underlying distance metric. Subsequently, a super graph\nconstructed based on the super classes is then fed to our proposed GNN\nframework which exploits the latent inter class relationships made explicit by\nthe super graph to achieve better class label separation among the graphs. We\nconduct exhaustive empirical evaluations of our proposed method and show that\nit outperforms both the adaptation of state of the art graph classification\nmethods to few shot scenario and our naive baseline GNNs. Additionally, we also\nextend and study the behavior of our method to semi supervised and active\nlearning scenarios.",
        "Revise this with your best effort": "We propose to study the problem of few shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graphs normalized Laplacian. This enables us to accordingly cluster the graph base labels associated with each graph into super classes, where the Lp Wasserstein distance serves as our underlying distance metric. Subsequently, a super graph constructed based on the super classes is then fed to our proposed GNN framework which exploits the latent inter class relationships made explicit by the super graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state of the art graph classification methods to few shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi supervised and active learning scenarios.",
        "Help me polish this": "We propose to study the problem of few shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graphs normalized Laplacian. This enables us to accordingly cluster the graph base labels associated with each graph into super classes, where the Lp Wasserstein distance serves as our underlying distance metric. Subsequently, a super graph constructed based on the super classes is then fed to our proposed GNN framework which exploits the latent inter class relationships made explicit by the super graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state of the art graph classification methods to few shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi supervised and active learning scenarios.",
        "Rewrite this for me": "We want to study how well graph neural networks can classify unseen classes, given only a few labeled examples. Several variants of GNNs have been proposed for node and graph classification tasks, but when there are only a few labeled examples, they don't work well.\n\nOur approach assigns a probability measure to each graph based on the spectrum of the graph's normalized Laplacian. This lets us cluster the graph base labels into superclasses, using the Lp Wasserstein distance as our metric. Then we feed the supergraph into our GNN framework, which uses the latent inter-class relationships to achieve better class label separation.\n\nWe conduct exhaustive empirical evaluations of our proposed method, and it outperforms state-of-the-art graph classification methods and our naive baseline GNNs. Additionally, we extend and study the behavior of our method in semi-supervised and active learning scenarios.",
        "Make this fluent while doing minimal change": "We propose studying the problem of few shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graphs normalized Laplacian. This enables us to accordingly cluster the graph base labels associated with each graph into super classes, where the Lp Wasserstein distance serves as our underlying distance metric. Subsequently, a super graph constructed based on the super classes is then fed to our proposed GNN framework which exploits the latent inter class relationships made explicit by the super graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state of the art graph classification methods to few shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi supervised and active learning scenarios.",
        "Refine this for me please": "We propose to study the problem of few shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graphs normalized Laplacian. This enables us to accordingly cluster the graph base labels associated with each graph into super classes, where the Lp Wasserstein distance serves as our underlying distance metric. Subsequently, a super graph constructed based on the super classes is then fed to our proposed GNN framework which exploits the latent inter class relationships made explicit by the super graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state of the art graph classification methods to few shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi supervised and active learning scenarios.",
        "Concise this for me and keep all the information": "We propose a study of few shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graphs normalized Laplacian. This enables us to accordingly cluster the graph base labels associated with each graph into super classes, where the Lp Wasserstein distance serves as our underlying distance metric. Subsequently, a super graph constructed based on the super classes is then fed to our proposed GNN framework which exploits the latent inter class relationships made explicit by the super graph to achieve better class label separation among the graphs.",
        "Improve this in GPT way": "We propose to study the problem of few shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graphs normalized Laplacian. This enables us to accordingly cluster the graph base labels associated with each graph into super classes, where the Lp Wasserstein distance serves as our underlying distance metric. Subsequently, a super graph constructed based on the super classes is then fed to our proposed GNN framework which exploits the latent inter class relationships made explicit by the super graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state of the art graph classification methods to few shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi supervised and active learning scenarios."
    },
    {
        "input": "In this work, we investigate the positional encoding methods used in language\npre-training (e.g., BERT) and identify several problems in the existing\nformulations. First, we show that in the absolute positional encoding, the\naddition operation applied on positional embeddings and word embeddings brings\nmixed correlations between the two heterogeneous information resources. It may\nbring unnecessary randomness in the attention and further limit the\nexpressiveness of the model. Second, we question whether treating the position\nof the symbol \\texttt{[CLS]} the same as other words is a reasonable design,\nconsidering its special role (the representation of the entire sentence) in the\ndownstream tasks. Motivated from above analysis, we propose a new positional\nencoding method called \\textbf{T}ransformer with \\textbf{U}ntied\n\\textbf{P}ositional \\textbf{E}ncoding (TUPE). In the self-attention module,\nTUPE computes the word contextual correlation and positional correlation\nseparately with different parameterizations and then adds them together. This\ndesign removes the mixed and noisy correlations over heterogeneous embeddings\nand offers more expressiveness by using different projection matrices.\nFurthermore, TUPE unties the \\texttt{[CLS]} symbol from other positions, making\nit easier to capture information from all positions. Extensive experiments and\nablation studies on GLUE benchmark demonstrate the effectiveness of the\nproposed method. Codes and models are released at\nhttps://github.com/guolinke/TUPE.",
        "Revise this with your best effort": "In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol [CLS] the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called Transformer with Untied Positional Encoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the [CLS] symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at https://github.com/guolinke/TUPE.",
        "Help me polish this": "In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol [CLS] the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called Transformer with Untied Positional Encoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the [CLS] symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at https://github.com/guolinke/TUPE.",
        "Rewrite this for me": "We investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol [CLS] the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called Transformer with Untied Positional Encoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the [CLS] symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at https://github.com/guolinke/TUPE.",
        "Make this fluent while doing minimal change": "In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol \\texttt{[CLS]} the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called \\textbf{T}ransformer with \\textbf{U}ntied \\textbf{P}ositional \\textbf{E}ncoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the \\texttt{[CLS]} symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released",
        "Refine this for me please": "In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol [CLS] the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called Transformer with Untied Positional Encoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the [CLS] symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at https://github.com/guolinke/TUPE.",
        "Concise this for me and keep all the information": "In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. We show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. This may bring unnecessary randomness in the attention and further limit the expressiveness of the model. We question whether treating the position of the symbol [CLS] the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called Transformer with Untied Positional Encoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the [CLS] symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method.",
        "Improve this in GPT way": "In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol \\texttt{[CLS]} the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called \\textbf{T}ransformer with \\textbf{U}ntied \\textbf{P}ositional \\textbf{E}ncoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the \\texttt{[CLS]} symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released"
    },
    {
        "input": "Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods.",
        "Revise this with your best effort": "Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. \n\nIn this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding method to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. \n\nWe have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Help me polish this": "Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. \n\nIn this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding method to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. \n\nWe have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Rewrite this for me": "Graph embedding techniques have been used more and more in a variety of applications that involve learning from non-Euclidean data. However, many existing graph embedding models either don't take node attribute information into account during training, or they're not very accurate because of node attribute noise. Moreover, very few of them can handle large graphs because they're computationally complex and use a lot of memory. In this paper, we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first creates a new graph by merging the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding method to be applied to the coarsened graph, and then progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. We've evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to state-of-the-art unsupervised embedding methods.",
        "Make this fluent while doing minimal change": "Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods.",
        "Refine this for me please": "Graph embedding techniques are used to learn on non-Euclidean data and have been deployed in a multitude of different applications. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper, we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Concise this for me and keep all the information": "Graph embedding techniques are used to learn on non-Euclidean data and have been deployed in a multitude of different applications. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper, we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coarsened graph, before it progressively refines the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to 40.8x, when compared to the state-of-the-art unsupervised embedding methods.",
        "Improve this in GPT way": "Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods."
    },
    {
        "input": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control\nproblem with nonlinear dynamical systems has received considerable attention\nrecently, yet the algorithmic development remains relatively limited. In this\nwork, we make an attempt along this line by reformulating the training\nprocedure from the trajectory optimization perspective. We first show that most\nwidely-used algorithms for training DNNs can be linked to the Differential\nDynamic Programming (DDP), a celebrated second-order method rooted in the\nApproximate Dynamic Programming. In this vein, we propose a new class of\noptimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and\nconvolution networks. DDPNOpt features layer-wise feedback policies which\nimprove convergence and reduce sensitivity to hyper-parameter over existing\nmethods. It outperforms other optimal-control inspired training methods in both\nconvergence and complexity, and is competitive against state-of-the-art first\nand second order methods. We also observe DDPNOpt has surprising benefit in\npreventing gradient vanishing. Our work opens up new avenues for principled\nalgorithmic design built upon the optimal control theory.",
        "Revise this with your best effort": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
        "Help me polish this": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
        "Rewrite this for me": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
        "Make this fluent while doing minimal change": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently. However, the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
        "Refine this for me please": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
        "Concise this for me and keep all the information": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
        "Improve this in GPT way": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory."
    },
    {
        "input": "In this paper, we investigate the effects of releasing arXiv preprints of\npapers that are undergoing a double-blind review process. In particular, we ask\nthe following research question: What is the relation between de-anonymization\nof authors through arXiv preprints and acceptance of a research paper at a\n(nominally) double-blind venue? Under two conditions: papers that are released\non arXiv before the review phase and papers that are not, we examine the\ncorrelation between the reputation of their authors with the review scores and\nacceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019\nsubmissions (n=5050), we find statistically significant evidence of positive\ncorrelation between percentage acceptance and papers with high reputation\nreleased on arXiv. In order to understand this observed association better, we\nperform additional analyses based on self-specified confidence scores of\nreviewers and observe that less confident reviewers are more likely to assign\nhigh review scores to papers with well known authors and low review scores to\npapers with less known authors, where reputation is quantified in terms of\nnumber of Google Scholar citations. We emphasize upfront that our results are\npurely correlational and we neither can nor intend to make any causal claims. A\nblog post accompanying the paper and our scraping code will be linked in the\nproject website https://sites.google.com/view/deanon-arxiv/home",
        "Revise this with your best effort": "In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv. In order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations. We emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website https://sites.google.com/view/deanon-arxiv/home",
        "Help me polish this": "In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue?\n\nUnder two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions.\n\nBy analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv.\n\nIn order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations.\n\nWe emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website https://sites.google.com/view/deanon-arxiv/home",
        "Rewrite this for me": "We investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue?\n\nWe examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv.\n\nIn order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations.\n\nWe emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims.",
        "Make this fluent while doing minimal change": "In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv. In order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations. We emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website https://sites.google.com/view/deanon-arxiv/home",
        "Refine this for me please": "In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv. In order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations. We emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website https://sites.google.com/view/deanon-arxiv/home",
        "Concise this for me and keep all the information": "We investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? We examine the correlation between the reputation of their authors with the review scores and acceptance decisions. We find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv.",
        "Improve this in GPT way": "In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv. In order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations. We emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website https://sites.google.com/view/deanon-arxiv/home"
    },
    {
        "input": "Reinforcement learning (RL) has achieved impressive performance in a variety\nof online settings in which an agent's ability to query the environment for\ntransitions and rewards is effectively unlimited. However, in many practical\napplications, the situation is reversed: an agent may have access to large\namounts of undirected offline experience data, while access to the online\nenvironment is severely limited. In this work, we focus on this offline\nsetting. Our main insight is that, when presented with offline data composed of\na variety of behaviors, an effective way to leverage this data is to extract a\ncontinuous space of recurring and temporally extended primitive behaviors\nbefore using these primitives for downstream task learning. Primitives\nextracted in this way serve two purposes: they delineate the behaviors that are\nsupported by the data from those that are not, making them useful for avoiding\ndistributional shift in offline RL; and they provide a degree of temporal\nabstraction, which reduces the effective horizon yielding better learning in\ntheory, and improved offline RL in practice. In addition to benefiting offline\npolicy optimization, we show that performing offline primitive learning in this\nway can also be leveraged for improving few-shot imitation learning as well as\nexploration and transfer in online RL on a variety of benchmark domains.\nVisualizations are available at https://sites.google.com/view/opal-iclr",
        "Revise this with your best effort": "Reinforcement learning (RL) has been shown to be effective in online settings where an agent has unlimited access to transitions and rewards. However, in many practical applications, the agent only has access to large amounts of offline experience data. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains.",
        "Help me polish this": "Reinforcement learning (RL) has been shown to be effective in online settings where an agent has unlimited access to transitions and rewards. However, in many practical applications, the agent only has access to large amounts of offline experience data. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon and yields better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains.",
        "Rewrite this for me": "Reinforcement learning (RL) has been shown to be effective in online settings where an agent has unlimited access to transitions and rewards. However, in many practical applications, the agent only has access to large amounts of offline experience data. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains.",
        "Make this fluent while doing minimal change": "Reinforcement learning (RL) has been shown to be effective in online settings where an agent has unlimited access to transitions and rewards. However, in many practical applications, the agent only has access to large amounts of offline experience data. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains.",
        "Refine this for me please": "Reinforcement learning (RL) has been shown to be effective in online settings where an agent has unlimited access to the environment in order to query for transitions and rewards. However, in many practical applications, the agent only has access to large amounts of offline experience data, and access to the online environment is severely limited. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains.",
        "Concise this for me and keep all the information": "Reinforcement learning (RL) has been shown to be effective in online settings where an agent has unlimited access to transitions and rewards. However, in many practical applications, the agent only has access to large amounts of undirected offline experience data. In this work, we focus on this offline setting and show that an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. This has the benefit of delineating the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL, as well as providing a degree of temporal abstraction which reduces the effective horizon and yields better learning in theory and improved offline RL in practice. We also show that this approach can be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains.",
        "Improve this in GPT way": "We propose a method for offline RL that involves extracting a space of recurring and temporally extended primitive behaviors from data, which can then be used for downstream task learning. This approach has several benefits: it can help avoid distributional shift in offline RL, it provides a degree of temporal abstraction which can improve learning in theory, and it can improve offline RL in practice. In addition, we show that this approach can also be used to improve few-shot imitation learning, exploration, and transfer in online RL."
    },
    {
        "input": "Stochastic Gradient Descent (SGD) and its variants are mainstream methods for\ntraining deep networks in practice. SGD is known to find a flat minimum that\noften generalizes well. However, it is mathematically unclear how deep learning\ncan select a flat minimum among so many minima. To answer the question\nquantitatively, we develop a density diffusion theory (DDT) to reveal how\nminima selection quantitatively depends on the minima sharpness and the\nhyperparameters. To the best of our knowledge, we are the first to\ntheoretically and empirically prove that, benefited from the Hessian-dependent\ncovariance of stochastic gradient noise, SGD favors flat minima exponentially\nmore than sharp minima, while Gradient Descent (GD) with injected white noise\nfavors flat minima only polynomially more than sharp minima. We also reveal\nthat either a small learning rate or large-batch training requires\nexponentially many iterations to escape from minima in terms of the ratio of\nthe batch size and learning rate. Thus, large-batch training cannot search flat\nminima efficiently in a realistic computational time.",
        "Revise this with your best effort": "Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.",
        "Help me polish this": "Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.",
        "Rewrite this for me": "Stochastic Gradient Descent (SGD) is a mainstream method for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.",
        "Make this fluent while doing minimal change": "Stochastic Gradient Descent (SGD) and its variants are the most popular methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.",
        "Refine this for me please": "Stochastic Gradient Descent (SGD) is a mainstream method for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.",
        "Concise this for me and keep all the information": "Stochastic Gradient Descent (SGD) is a mainstream method for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.",
        "Improve this in GPT way": "We develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima."
    },
    {
        "input": "Spectral embedding is a popular technique for the representation of graph\ndata. Several regularization techniques have been proposed to improve the\nquality of the embedding with respect to downstream tasks like clustering. In\nthis paper, we explain on a simple block model the impact of the complete graph\nregularization, whereby a constant is added to all entries of the adjacency\nmatrix. Specifically, we show that the regularization forces the spectral\nembedding to focus on the largest blocks, making the representation less\nsensitive to noise or outliers. We illustrate these results on both on both\nsynthetic and real data, showing how regularization improves standard\nclustering scores.",
        "Revise this with your best effort": "Spectral embedding is a popular technique for representing graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain the impact of the complete graph regularization on a simple block model. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Help me polish this": "Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Rewrite this for me": "Spectral embedding is a popular technique for representing graph data. Adding a constant to all entries of the adjacency matrix (complete graph regularization) has been shown to improve the quality of the embedding with respect to downstream tasks like clustering. This paper explains the impact of complete graph regularization on a simple block model. Specifically, it shows that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. The results are illustrated on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Make this fluent while doing minimal change": "Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Refine this for me please": "Spectral embedding is a popular technique for representing graph data. In this paper, we explain how the regularization technique of adding a constant to all entries of the adjacency matrix affects the spectral embedding. We show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Concise this for me and keep all the information": "Spectral embedding is a popular technique for the representation of graph data. In this paper, we explain the impact of the complete graph regularization on the quality of the embedding. We show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores.",
        "Improve this in GPT way": "Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both synthetic and real data, showing how regularization improves standard clustering scores."
    },
    {
        "input": "In this work we study locality and compositionality in the context of\nlearning representations for Zero Shot Learning (ZSL). In order to well-isolate\nthe importance of these properties in learned representations, we impose the\nadditional constraint that, differently from most recent work in ZSL, no\npre-training on different datasets (e.g. ImageNet) is performed. The results of\nour experiments show how locality, in terms of small parts of the input, and\ncompositionality, i.e. how well can the learned representations be expressed as\na function of a smaller vocabulary, are both deeply related to generalization\nand motivate the focus on more local-aware models in future research directions\nfor representation learning.",
        "Revise this with your best effort": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). To isolate the importance of these properties in learned representations, we impose the additional constraint that, unlike most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well the learned representations can be expressed as a function of a smaller vocabulary, are both deeply related to generalization. This motivates the focus on more local-aware models in future research directions for representation learning.",
        "Help me polish this": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization. This motivates the focus on more local-aware models in future research directions for representation learning.",
        "Rewrite this for me": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). To isolate the importance of these properties in learned representations, we impose the additional constraint that, unlike most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well the learned representations can be expressed as a function of a smaller vocabulary, are both deeply related to generalization. This motivates the focus on more local-aware models in future research directions for representation learning.",
        "Make this fluent while doing minimal change": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). To well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.",
        "Refine this for me please": "In this work, we study how locality and compositionality affect Zero Shot Learning (ZSL). We impose the constraint that no pre-training on different datasets (e.g. ImageNet) is performed, in order to isolate the importance of these properties in learned representations. Our experiments show that locality (in terms of small parts of the input) and compositionality (i.e. how well can the learned representations be expressed as a function of a smaller vocabulary) are both deeply related to generalization. These results motivate the focus on more local-aware models in future research directions for representation learning.",
        "Concise this for me and keep all the information": "In this work, we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). We impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization. These results motivate the focus on more local-aware models in future research directions for representation learning.",
        "Improve this in GPT way": "In this work, we study the importance of locality and compositionality in learned representations for Zero Shot Learning (ZSL). We find that these properties are deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning."
    },
    {
        "input": "We study the problem of learning permutation invariant representations that\ncan capture \"flexible\" notions of containment. We formalize this problem via a\nmeasure theoretic definition of multisets, and obtain a theoretically-motivated\nlearning model. We propose training this model on a novel task: predicting the\nsize of the symmetric difference (or intersection) between pairs of multisets.\nWe demonstrate that our model not only performs very well on predicting\ncontainment relations (and more effectively predicts the sizes of symmetric\ndifferences and intersections than DeepSets-based approaches with unconstrained\nobject representations), but that it also learns meaningful representations.",
        "Revise this with your best effort": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure-theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Help me polish this": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure-theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Rewrite this for me": "We investigate the issue of learning permutation invariant representations that can express \"flexible\" containment concepts. We formalize this problem through a measure-theoretic definition of multisets, and develop a learning model motivated by this definition. We train this model on a new task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We show that our model not only performs very well on containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but also learns useful representations.",
        "Make this fluent while doing minimal change": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Refine this for me please": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Concise this for me and keep all the information": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",
        "Improve this in GPT way": "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations."
    },
    {
        "input": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg .",
        "Revise this with your best effort": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques used to train neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods. This may be because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Help me polish this": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness.\n\nWe hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Rewrite this for me": "Deep RL has been receiving more attention recently because it performs well on a variety of control tasks. However, conventional regularization techniques (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods. This may be because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. We find that conventional regularization techniques can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives: sample complexity, reward distribution, weight norm, and noise robustness.\n\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Make this fluent while doing minimal change": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques used to train neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Refine this for me please": "Deep RL has been shown to be effective on a variety of control tasks, but has largely ignored conventional regularization techniques that are common in training neural networks. In this work, we study the effects of regularization on multiple policy optimization algorithms in continuous control tasks. We find that conventional regularization techniques can often improve performance, especially on harder tasks. Our findings are robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Concise this for me and keep all the information": "Deep RL has been receiving more attention recently, but conventional regularization techniques have been largely ignored. This study presents the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. The findings are shown to be robust against training hyperparameter variations. The study also compares these techniques with the more widely used entropy regularization.",
        "Improve this in GPT way": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best.\n\nWe further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg."
    },
    {
        "input": "The Receptive Field (RF) size has been one of the most important factors for\nOne Dimensional Convolutional Neural Networks (1D-CNNs) on time series\nclassification tasks. Large efforts have been taken to choose the appropriate\nsize because it has a huge influence on the performance and differs\nsignificantly for each dataset. In this paper, we propose an Omni-Scale block\n(OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and\nuniversal rule. Particularly, it is a set of kernel sizes that can efficiently\ncover the best RF size across different datasets via consisting of multiple\nprime numbers according to the length of the time series. The experiment result\nshows that models with the OS-block can achieve a similar performance as models\nwith the searched optimal RF size and due to the strong optimal RF size capture\nability, simple 1D-CNN models with OS-block achieves the state-of-the-art\nperformance on four time series benchmarks, including both univariate and\nmultivariate data from multiple domains. Comprehensive analysis and discussions\nshed light on why the OS-block can capture optimal RF sizes across different\ndatasets. Code available [https://github.com/Wensi-Tang/OS-CNN]",
        "Revise this with your best effort": "The size of the receptive field (RF) has been one of the most important factors for one-dimensional convolutional neural networks (1D-CNNs) on time series classification tasks. Large efforts have been taken to choose the appropriate size because it has a huge influence on the performance and differs significantly for each dataset. In this paper, we propose an Omni-Scale block (OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and universal rule. Particularly, it is a set of kernel sizes that can efficiently cover the best RF size across different datasets by consisting of multiple prime numbers according to the length of the time series. The experiment result shows that models with the OS-block can achieve a similar performance as models with the searched optimal RF size and, due to the strong optimal RF size capture ability, simple 1D-CNN models with OS-block achieves the state-of-the-art performance on four time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and discussions shed light on why the OS-block can capture optimal RF sizes across different datasets. Code available [https://github.com/Wensi-Tang/OS-CNN].",
        "Help me polish this": "The size of the receptive field (RF) has been one of the most important factors for one-dimensional convolutional neural networks (1D-CNNs) on time series classification tasks. Large efforts have been taken to choose the appropriate size because it has a huge influence on the performance and differs significantly for each dataset. In this paper, we propose an Omni-Scale block (OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and universal rule. Particularly, it is a set of kernel sizes that can efficiently cover the best RF size across different datasets via consisting of multiple prime numbers according to the length of the time series. The experiment result shows that models with the OS-block can achieve a similar performance as models with the searched optimal RF size and, due to the strong optimal RF size capture ability, simple 1D-CNN models with OS-block achieves the state-of-the-art performance on four time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and discussions shed light on why the OS-block can capture optimal RF sizes across different datasets. Code available [https://github.com/Wensi-Tang/OS-CNN].",
        "Rewrite this for me": "The size of the receptive field (RF) is one of the most important factors for one-dimensional convolutional neural networks (1D-CNNs) in time series classification tasks. A lot of effort has been put into choosing the right size, because it has a huge impact on performance and varies significantly from one dataset to another. In this paper, we propose an Omni-Scale block (OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and universal rule. In particular, it is a set of kernel sizes that can efficiently cover the best RF size across different datasets, consisting of multiple prime numbers according to the length of the time series. The experiment results show that models with the OS-block can achieve similar performance to models with the searched optimal RF size, and due to the strong optimal RF size capture ability, simple 1D-CNN models with the OS-block achieve state-of-the-art performance on four time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and discussion shed light on why the OS-block can capture optimal RF sizes across different datasets. Code available [https://github.com/Wensi-Tang/OS-CNN].",
        "Make this fluent while doing minimal change": "The size of the Receptive Field (RF) has been one of the most important factors for\nOne Dimensional Convolutional Neural Networks (1D-CNNs) on time series\nclassification tasks. Large efforts have been taken to choose the appropriate\nsize because it has a huge influence on the performance and differs\nsignificantly for each dataset. In this paper, we propose an Omni-Scale block\n(OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and\nuniversal rule. Particularly, it is a set of kernel sizes that can efficiently\ncover the best RF size across different datasets via consisting of multiple\nprime numbers according to the length of the time series. The experiment result\nshows that models with the OS-block can achieve a similar performance as models\nwith the searched optimal RF size and due to the strong optimal RF size capture\nability, simple 1D-CNN models with OS-block achieves the state-of-the-art\nperformance on four time series benchmarks, including both univariate and\nmultivariate data from multiple domains. Comprehensive analysis and discussions\nshed light on why the OS-block can capture optimal RF sizes across different\ndatasets. Code available [https://github.com/Wensi-Tang/OS-CNN]",
        "Refine this for me please": "The proposed Omni-Scale block (OS-block) for 1D-CNNs is a set of kernel sizes that can efficiently cover the best RF size across different datasets via consisting of multiple prime numbers according to the length of the time series. The experiment result shows that models with the OS-block can achieve a similar performance as models with the searched optimal RF size and due to the strong optimal RF size capture ability, simple 1D-CNN models with OS-block achieves the state-of-the-art performance on four time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and discussions shed light on why the OS-block can capture optimal RF sizes across different datasets. Code available [https://github.com/Wensi-Tang/OS-CNN].",
        "Concise this for me and keep all the information": "The size of the receptive field (RF) is one of the most important factors for one-dimensional convolutional neural networks (1D-CNNs) on time series classification tasks. Large efforts have been taken to choose the appropriate size because it has a huge influence on the performance and differs significantly for each dataset. In this paper, we propose an Omni-Scale block (OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and universal rule. Particularly, it is a set of kernel sizes that can efficiently cover the best RF size across different datasets via consisting of multiple prime numbers according to the length of the time series. The experiment result shows that models with the OS-block can achieve a similar performance as models with the searched optimal RF size and due to the strong optimal RF size capture ability, simple 1D-CNN models with OS-block achieves the state-of-the-art performance on four time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and discussions shed light on why the OS-block can capture optimal RF sizes across different datasets.",
        "Improve this in GPT way": "The proposed Omni-Scale block (OS-block) for 1D-CNNs is a set of kernel sizes that can efficiently cover the best RF size across different datasets via consisting of multiple prime numbers according to the length of the time series. The experiment result shows that models with the OS-block can achieve a similar performance as models with the searched optimal RF size and due to the strong optimal RF size capture ability, simple 1D-CNN models with OS-block achieves the state-of-the-art performance on four time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and discussions shed light on why the OS-block can capture optimal RF sizes across different datasets. Code available [https://github.com/Wensi-Tang/OS-CNN]."
    },
    {
        "input": "Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance.",
        "Revise this with your best effort": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Help me polish this": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch.\n\nIn this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete.\n\nWe present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Rewrite this for me": "Distributed optimization is key to solving large-scale machine learning problems. A common feature of distributed optimization techniques is that all nodes must finish their assigned tasks in each computational epoch before the system can move on to the next epoch. In these types of settings, slow nodes, called stragglers, can severely slow down progress. To counteract the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a set amount of time to compute the gradients of as many data samples as possible. This results in a variable per-node minibatch size. Workers then have a set communication time to average their minibatch gradients through several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can actually finish. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Make this fluent while doing minimal change": "Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance.",
        "Refine this for me please": "Distributed optimization is a key tool for solving large-scale machine learning problems. A common feature of distributed optimization techniques is that all nodes must complete their assigned tasks in each computational epoch before the system can move on to the next epoch. This can be a problem if some nodes are slow, which can hold up the entire system. To address this issue, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Concise this for me and keep all the information": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. Our numerical results show that our approach is up to 1.5 times faster in Amazon EC2 and it is up to five times faster when there is greater variability in compute node performance.",
        "Improve this in GPT way": "Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance."
    },
    {
        "input": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,\nco-located with ICLR 2021. In this workshop, we want to advance theory, methods\nand tools for allowing experts to express prior coded knowledge for automatic\ndata annotations that can be used to train arbitrary deep neural networks for\nprediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing\nmethods that help modern machine-learning methods to generalize from knowledge\nprovided by experts, in interaction with observable (unlabeled) data. In total,\n15 papers were accepted. All the accepted contributions are listed in these\nProceedings.",
        "Revise this with your best effort": "Welcome to the WeaSuL 2021 workshop, the first ever workshop on weakly supervised learning! We want to bring together experts to discuss and advance theory, methods, and tools for using prior coded knowledge to automatically annotate data, which can then be used to train deep neural networks for prediction. The WeaSuL 2021 workshop aims to help modern machine-learning methods better generalize from the knowledge provided by experts, in interaction with observable (unlabeled) data. In total, 15 papers were accepted and are included in these proceedings.",
        "Help me polish this": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning. In this workshop, we want to advance theory, methods and tools for allowing experts to express prior coded knowledge for automatic data annotations that can be used to train arbitrary deep neural networks for prediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing methods that help modern machine-learning methods to generalize from knowledge provided by experts, in interaction with observable (unlabeled) data. In total, 15 papers were accepted. All the accepted contributions are listed in these Proceedings.",
        "Rewrite this for me": "Welcome to the ICLR 2021 Workshop on Weak Supervision, the first workshop on weakly supervised learning. In this workshop, we want to advance methods that help modern machine-learning methods to generalize from knowledge provided by experts, in interaction with observable (unlabeled) data. In total, 15 papers were accepted. All the accepted contributions are listed in these Proceedings.",
        "Make this fluent while doing minimal change": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning. Our goal is to advance theory, methods, and tools for allowing experts to express prior coded knowledge for automatic data annotations that can be used to train arbitrary deep neural networks for prediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing methods that help modern machine-learning methods to generalize from knowledge provided by experts, in interaction with observable (unlabeled) data. In total, 15 papers were accepted. All the accepted contributions are listed in these Proceedings.",
        "Refine this for me please": "Welcome to the ICLR 2021 Workshop on Weak Supervision, where we aim to advance methods that help modern machine-learning methods to generalize from knowledge provided by experts, in interaction with observable (unlabeled) data. In total, 15 papers were accepted, and all the accepted contributions are listed in these Proceedings.",
        "Concise this for me and keep all the information": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning. In this workshop, we want to advance theory, methods and tools for allowing experts to express prior coded knowledge for automatic data annotations that can be used to train arbitrary deep neural networks for prediction.",
        "Improve this in GPT way": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,\nco-located with ICLR 2021. In this workshop, we want to advance theory, methods\nand tools for allowing experts to express prior coded knowledge for automatic\ndata annotations that can be used to train arbitrary deep neural networks for\nprediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing\nmethods that help modern machine-learning methods to generalize from knowledge\nprovided by experts, in interaction with observable (unlabeled) data. In total,\n15 papers were accepted. All the accepted contributions are listed in these\nProceedings."
    },
    {
        "input": "Generative modeling has been used frequently in synthetic data generation.\nFairness and privacy are two big concerns for synthetic data. Although Recent\nGAN [\\cite{goodfellow2014generative}] based methods show good results in\npreserving privacy, the generated data may be more biased. At the same time,\nthese methods require high computation resources. In this work, we design a\nfast, fair, flexible and private data generation method. We show the\neffectiveness of our method theoretically and empirically. We show that models\ntrained on data generated by the proposed method can perform well (in inference\nstage) on real application scenarios.",
        "Revise this with your best effort": "Generative modeling is a powerful tool for synthetic data generation. However, fairness and privacy are two important concerns that must be addressed. Recent GAN-based methods have shown good results in preserving privacy, but the generated data may be more biased. These methods also require high computation resources. In this work, we propose a fast, fair, flexible, and private data generation method. We show the effectiveness of our method theoretically and empirically. We demonstrate that models trained on data generated by our method can perform well on real-world application scenarios.",
        "Help me polish this": "Generative modeling is a powerful tool for synthetic data generation. However, fairness and privacy are two big concerns that need to be addressed. Although recent GAN-based methods show good results in preserving privacy, the generated data may be more biased. At the same time, these methods require high computation resources. In this work, we design a fast, fair, flexible and private data generation method. We show the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the proposed method can perform well (in inference stage) on real application scenarios.",
        "Rewrite this for me": "Generative modeling is a popular technique for generating synthetic data. However, fairness and privacy are two major concerns with synthetic data. Recently, GAN-based methods have shown good results in preserving privacy, but the generated data may be more biased. These methods also require high computation resources. In this work, we propose a fast, fair, flexible, and private data generation method. We show the effectiveness of our method theoretically and empirically. We demonstrate that models trained on data generated by our method can perform well on real-world application scenarios.",
        "Make this fluent while doing minimal change": "Generative modeling is frequently used in synthetic data generation. However, fairness and privacy are two big concerns for synthetic data. Although recent GAN-based methods show good results in preserving privacy, the generated data may be more biased. At the same time, these methods require high computation resources. In this work, we design a fast, fair, flexible, and private data generation method. We show the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the proposed method can perform well (in inference stage) on real application scenarios.",
        "Refine this for me please": "Generative modeling is a powerful tool for synthetic data generation, but it has some important limitations to consider with regards to fairness and privacy. Some recent methods based on GANs (Goodfellow et al., 2014) have shown good results in preserving privacy, but the generated data may be more biased. These methods also tend to be computationally intensive. In this work, we propose a data generation method that is fast, fair, flexible, and private. We show that models trained on data generated by our method can perform well on real application scenarios.",
        "Concise this for me and keep all the information": "Generative modeling is a method of creating synthetic data. Fairness and privacy are two big concerns for synthetic data. GAN-based methods show good results in preserving privacy, but the generated data may be more biased. Our method is fast, fair, flexible, and private. We show that models trained on data generated by our method can perform well on real application scenarios.",
        "Improve this in GPT way": "Generative modeling is a powerful tool for synthetic data generation. However, fairness and privacy are two important concerns that must be addressed when using synthetic data. Recent GAN-based methods have shown good results in preserving privacy, but the generated data may be more biased. These methods also require high computation resources. In this work, we design a fast, fair, flexible, and private data generation method. We show the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the proposed method can perform well on real application scenarios."
    },
    {
        "input": "Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation.",
        "Revise this with your best effort": "It is challenging to learn from a limited number of samples, as the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution, so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on two datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.",
        "Help me polish this": "Learning from a limited number of samples can be challenging, as the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then sampling an adequate number of examples from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution, so that the mean and the variance of the distribution can be borrowed from similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on two datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.",
        "Rewrite this for me": "It is challenging to learn from a limited number of samples because the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on two datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.",
        "Make this fluent while doing minimal change": "It is challenging to learn from a limited number of samples, as the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution, so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on two datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.",
        "Refine this for me please": "In this paper, we propose a method for learning from a limited number of samples by transferring statistics from similar classes with more samples. Our method can be built on top of existing pretrained models and classifiers, and we show that it outperforms state-of-the-art accuracy on two datasets.",
        "Concise this for me and keep all the information": "It can be difficult to learn from a limited number of samples, as the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, the authors calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. The method can be built on top of existing pretrained feature extractors and classification models without extra parameters, and outperforms the state-of-the-art accuracy on two datasets.",
        "Improve this in GPT way": "Learning from a limited number of samples can be challenging, as the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then sampling an adequate number of examples from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution, so that the mean and the variance of the distribution can be borrowed from similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of existing pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on two datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation."
    },
    {
        "input": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs.",
        "Revise this with your best effort": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs.",
        "Help me polish this": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs.",
        "Rewrite this for me": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two models that are important for understanding the interface between statistical physics, machine learning, and neuroscience. Recently, there has been interest in the relationship between HNs and RBMs, due to their similarity under the statistical mechanics formalism. An exact mapping between HNs and RBMs has been previously noted for the special case of orthogonal (uncorrelated) encoded patterns. We present here an exact mapping in the case of correlated pattern HNs, which are more broadly applicable to existing datasets. Specifically, we show that any HN with $N$ binary variables and $p<N$ arbitrary binary patterns can be transformed into an RBM with $N$ binary visible variables and $p$ gaussian hidden variables. We outline the conditions under which the reverse mapping exists, and conduct experiments on the MNIST dataset which suggest the mapping provides a useful initialization to the RBM weights. We discuss extensions, the potential importance of this correspondence for the training of RBMs, and for understanding the performance of deep architectures which utilize RBMs.",
        "Make this fluent while doing minimal change": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs.",
        "Refine this for me please": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs.",
        "Concise this for me and keep all the information": "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two models that are similar under the statistical mechanics formalism. An exact mapping between HNs and RBMs has been noted for the special case of orthogonal (uncorrelated) encoded patterns. This paper presents an exact mapping in the case of correlated pattern HNs. The mapping provides a useful initialization to the RBM weights.",
        "Improve this in GPT way": "The Hopfield network (HN) is a powerful tool for machine learning and artificial intelligence. However, the HN has some limitations. For example, the HN is not very good at handling correlated patterns. The Restricted Boltzmann Machine (RBM) is a tool that is very good at handling correlated patterns. Recently, there has been interest in the relationship between the HN and the RBM because they are similar under the statistical mechanics formalism. An exact mapping between the HN and the RBM has been previously noted for the special case of orthogonal (uncorrelated) encoded patterns. We present here an exact mapping in the case of correlated pattern HNs, which are more broadly applicable to existing datasets. Specifically, we show that any HN with $N$ binary variables and $p<N$ arbitrary binary patterns can be transformed into an RBM with $N$ binary visible variables and $p$ gaussian hidden variables. We outline the conditions under which the reverse mapping exists, and conduct experiments on the MNIST dataset which suggest the mapping provides a useful initialization to the RBM weights. We discuss extensions, the potential importance of this correspondence for the training of RBMs, and for understanding the performance of deep architectures which utilize RBMs."
    },
    {
        "input": "Graph neural networks (GNNs) are a powerful inductive bias for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. For many tasks of\ninterest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. We introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.",
        "Revise this with your best effort": "Graph neural networks (GNNs) are a powerful tool for modelling\nalgorithmic reasoning procedures and data structures. Their ability was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. However, for many tasks of\ninterest, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. The Persistent Message Passing (PMP) mechanism endows GNNs with the capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.",
        "Help me polish this": "Graph neural networks (GNNs) are a powerful tool for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. For many tasks of\ninterest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. We introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with the capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.",
        "Rewrite this for me": "Graph neural networks (GNNs) are a powerful tool for modelling\nalgorithmic reasoning procedures and data structures. Their ability was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. However, for many tasks of\ninterest, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. The Persistent Message Passing (PMP) mechanism endows GNNs with the capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.",
        "Make this fluent while doing minimal change": "Graph neural networks (GNNs) are a powerful inductive bias for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. For many tasks of\ninterest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. We introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.",
        "Refine this for me please": "Graph neural networks (GNNs) are a powerful tool for modelling\nalgorithmic reasoning procedures and data structures. Their ability was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. However, for many tasks of\ninterest, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. The Persistent Message Passing (PMP) mechanism endows GNNs with the capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.",
        "Concise this for me and keep all the information": "Graph neural networks (GNNs) are a powerful inductive bias for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state.\n\nFor many tasks of interest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations.\n\nWe introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required.\n\nPMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.",
        "Improve this in GPT way": "Graph neural networks (GNNs) are a powerful inductive bias for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. For many tasks of\ninterest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations.\n\nWe introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states."
    },
    {
        "input": "A deep equilibrium model uses implicit layers, which are implicitly defined\nthrough an equilibrium point of an infinite sequence of computation. It avoids\nany explicit computation of the infinite sequence by finding an equilibrium\npoint directly via root-finding and by computing gradients via implicit\ndifferentiation. In this paper, we analyze the gradient dynamics of deep\nequilibrium models with nonlinearity only on weight matrices and non-convex\nobjective functions of weights for regression and classification. Despite\nnon-convexity, convergence to global optimum at a linear rate is guaranteed\nwithout any assumption on the width of the models, allowing the width to be\nsmaller than the output dimension and the number of data points. Moreover, we\nprove a relation between the gradient dynamics of the deep implicit layer and\nthe dynamics of trust region Newton method of a shallow explicit layer. This\nmathematically proven relation along with our numerical observation suggests\nthe importance of understanding implicit bias of implicit layers and an open\nproblem on the topic. Our proofs deal with implicit layers, weight tying and\nnonlinearity on weights, and differ from those in the related literature.",
        "Revise this with your best effort": "A deep equilibrium model uses implicit layers, which are defined\nthrough an equilibrium point of an infinite sequence of computation. It avoids\nany explicit computation of the infinite sequence by finding an equilibrium\npoint directly via root-finding and by computing gradients via implicit\ndifferentiation. In this paper, we analyze the gradient dynamics of deep\nequilibrium models with nonlinearity only on weight matrices and non-convex\nobjective functions of weights for regression and classification. Despite\nnon-convexity, convergence to global optimum at a linear rate is guaranteed\nwithout any assumption on the width of the models, allowing the width to be\nsmaller than the output dimension and the number of data points. Moreover, we\nprove a relation between the gradient dynamics of the deep implicit layer and\nthe dynamics of trust region Newton method of a shallow explicit layer. This\nmathematically proven relation along with our numerical observation suggests\nthe importance of understanding implicit bias of implicit layers and an open\nproblem on the topic. Our proofs deal with implicit layers, weight tying and\nnonlinearity on weights, and differ from those in the related literature.",
        "Help me polish this": "A deep equilibrium model uses implicit layers, which are implicitly defined\nthrough an equilibrium point of an infinite sequence of computation. It avoids\nany explicit computation of the infinite sequence by finding an equilibrium\npoint directly via root-finding and by computing gradients via implicit\ndifferentiation. In this paper, we analyze the gradient dynamics of deep\nequilibrium models with nonlinearity only on weight matrices and non-convex\nobjective functions of weights for regression and classification. Despite\nnon-convexity, convergence to global optimum at a linear rate is guaranteed\nwithout any assumption on the width of the models, allowing the width to be\nsmaller than the output dimension and the number of data points. Moreover, we\nprove a relation between the gradient dynamics of the deep implicit layer and\nthe dynamics of trust region Newton method of a shallow explicit layer. This\nmathematically proven relation along with our numerical observation suggests\nthe importance of understanding implicit bias of implicit layers and an open\nproblem on the topic. Our proofs deal with implicit layers, weight tying and\nnonlinearity on weights, and differ from those in the related literature.",
        "Rewrite this for me": "A deep equilibrium model uses implicit layers, which are defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.",
        "Make this fluent while doing minimal change": "A deep equilibrium model uses implicit layers, which are implicitly defined\nthrough an equilibrium point of an infinite sequence of computation. It avoids\nany explicit computation of the infinite sequence by finding an equilibrium\npoint directly via root-finding and by computing gradients via implicit\ndifferentiation. In this paper, we analyze the gradient dynamics of deep\nequilibrium models with nonlinearity only on weight matrices and non-convex\nobjective functions of weights for regression and classification. Despite\nnon-convexity, convergence to global optimum at a linear rate is guaranteed\nwithout any assumption on the width of the models, allowing the width to be\nsmaller than the output dimension and the number of data points. Moreover, we\nprove a relation between the gradient dynamics of the deep implicit layer and\nthe dynamics of trust region Newton method of a shallow explicit layer. This\nmathematically proven relation along with our numerical observation suggests\nthe importance of understanding implicit bias of implicit layers and an open\nproblem on the topic. Our proofs deal with implicit layers, weight tying and\nnonlinearity on weights, and differ from those in the related literature.",
        "Refine this for me please": "A deep equilibrium model uses implicit layers, which are defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.",
        "Concise this for me and keep all the information": "A deep equilibrium model uses implicit layers, which are defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic.",
        "Improve this in GPT way": "A deep equilibrium model uses implicit layers, which are implicitly defined\nthrough an equilibrium point of an infinite sequence of computation. It avoids\nany explicit computation of the infinite sequence by finding an equilibrium\npoint directly via root-finding and by computing gradients via implicit\ndifferentiation. In this paper, we analyze the gradient dynamics of deep\nequilibrium models with nonlinearity only on weight matrices and non-convex\nobjective functions of weights for regression and classification. Despite\nnon-convexity, convergence to global optimum at a linear rate is guaranteed\nwithout any assumption on the width of the models, allowing the width to be\nsmaller than the output dimension and the number of data points. Moreover, we\nprove a relation between the gradient dynamics of the deep implicit layer and\nthe dynamics of trust region Newton method of a shallow explicit layer. This\nmathematically proven relation along with our numerical observation suggests\nthe importance of understanding implicit bias of implicit layers and an open\nproblem on the topic. Our proofs deal with implicit layers, weight tying and\nnonlinearity on weights, and differ from those in the related literature."
    },
    {
        "input": "The ability to learn continually without forgetting the past tasks is a\ndesired attribute for artificial learning systems. Existing approaches to\nenable such learning in artificial neural networks usually rely on network\ngrowth, importance based weight update or replay of old data from the memory.\nIn contrast, we propose a novel approach where a neural network learns new\ntasks by taking gradient steps in the orthogonal direction to the gradient\nsubspaces deemed important for the past tasks. We find the bases of these\nsubspaces by analyzing network representations (activations) after learning\neach task with Singular Value Decomposition (SVD) in a single shot manner and\nstore them in the memory as Gradient Projection Memory (GPM). With qualitative\nand quantitative analyses, we show that such orthogonal gradient descent\ninduces minimum to no interference with the past tasks, thereby mitigates\nforgetting. We evaluate our algorithm on diverse image classification datasets\nwith short and long sequences of tasks and report better or on-par performance\ncompared to the state-of-the-art approaches.",
        "Revise this with your best effort": "The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.",
        "Help me polish this": "The ability to learn continually without forgetting the past tasks is a\ndesired attribute for artificial learning systems. Existing approaches to\nenable such learning in artificial neural networks usually rely on network\ngrowth, importance based weight update or replay of old data from the memory.\nIn contrast, we propose a novel approach where a neural network learns new\ntasks by taking gradient steps in the orthogonal direction to the gradient\nsubspaces deemed important for the past tasks. We find the bases of these\nsubspaces by analyzing network representations (activations) after learning\neach task with Singular Value Decomposition (SVD) in a single shot manner and\nstore them in the memory as Gradient Projection Memory (GPM). With qualitative\nand quantitative analyses, we show that such orthogonal gradient descent\ninduces minimum to no interference with the past tasks, thereby mitigates\nforgetting. We evaluate our algorithm on diverse image classification datasets\nwith short and long sequences of tasks and report better or on-par performance\ncompared to the state-of-the-art approaches.",
        "Rewrite this for me": "We propose a new approach for artificial neural networks to learn new tasks while retaining information from previous tasks. Our approach involves taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner, and store them in memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that this orthogonal gradient descent induces minimal interference with the past tasks, thereby mitigating forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks, and report better or on-par performance compared to state-of-the-art approaches.",
        "Make this fluent while doing minimal change": "The ability to learn continually without forgetting the past tasks is a\ndesired attribute for artificial learning systems. Existing approaches to\nenable such learning in artificial neural networks usually rely on network\ngrowth, importance based weight update or replay of old data from the memory.\nIn contrast, we propose a novel approach where a neural network learns new\ntasks by taking gradient steps in the orthogonal direction to the gradient\nsubspaces deemed important for the past tasks. We find the bases of these\nsubspaces by analyzing network representations (activations) after learning\neach task with Singular Value Decomposition (SVD) in a single shot manner and\nstore them in the memory as Gradient Projection Memory (GPM). With qualitative\nand quantitative analyses, we show that such orthogonal gradient descent\ninduces minimum to no interference with the past tasks, thereby mitigates\nforgetting. We evaluate our algorithm on diverse image classification datasets\nwith short and long sequences of tasks and report better or on-par performance\ncompared to the state-of-the-art approaches.",
        "Refine this for me please": "We propose a novel approach to continual learning where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.",
        "Concise this for me and keep all the information": "We propose a novel approach to continual learning where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting.",
        "Improve this in GPT way": "We propose a novel approach to continual learning where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches."
    },
    {
        "input": "In high-dimensional state spaces, the usefulness of Reinforcement Learning\n(RL) is limited by the problem of exploration. This issue has been addressed\nusing potential-based reward shaping (PB-RS) previously. In the present work,\nwe introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the\nstrict optimality guarantees of PB-RS to a guarantee of preserved long-term\nbehavior. Being less restrictive, FV-RS allows for reward shaping functions\nthat are even better suited for improving the sample efficiency of RL\nalgorithms. In particular, we consider settings in which the agent has access\nto an approximate plan. Here, we use examples of simulated robotic manipulation\ntasks to demonstrate that plan-based FV-RS can indeed significantly improve the\nsample efficiency of RL over plan-based PB-RS.",
        "Revise this with your best effort": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.",
        "Help me polish this": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.",
        "Rewrite this for me": "The usefulness of Reinforcement Learning (RL) is limited in high-dimensional state spaces due to the exploration problem. This issue has been addressed before using potential-based reward shaping (PB-RS). However, Final-Volume-Preserving Reward Shaping (FV-RS) relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. This allows for less restrictive reward shaping functions that are better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.",
        "Make this fluent while doing minimal change": "The usefulness of Reinforcement Learning (RL) is limited in high-dimensional state spaces by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.",
        "Refine this for me please": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.",
        "Concise this for me and keep all the information": "Reinforcement Learning (RL) is limited in high-dimensional state spaces due to the problem of exploration. Potential-based reward shaping (PB-RS) has been used to address this issue previously, but Final-Volume-Preserving Reward Shaping (FV-RS) relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. This allows for reward shaping functions that are better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.",
        "Improve this in GPT way": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS."
    },
    {
        "input": "Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. When no labeled\ndata is available, such networks can still be trained using reinforcement\nlearning. However, we demonstrate that this approach can suffer from an early\ncommitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression.",
        "Revise this with your best effort": "Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. However, when no labeled\ndata is available, such networks can still suffer from an early commitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression.",
        "Help me polish this": "Many machine learning strategies designed to automate mathematical tasks leverage neural networks to search large combinatorial spaces of mathematical symbols. In contrast to traditional evolutionary approaches, using a neural network at the core of the search allows learning higher-level symbolic patterns, providing an informed direction to guide the search. However, we demonstrate that this approach can suffer from an early commitment phenomenon and from initialization bias, both of which limit exploration. We present two exploration methods to tackle these issues, building upon ideas of entropy regularization and distribution initialization. We show that these techniques can improve the performance, increase sample efficiency, and lower the complexity of solutions for the task of symbolic regression.",
        "Rewrite this for me": "Many machine learning strategies that are designed to automate mathematical tasks use neural networks to search large combinatorial spaces of mathematical symbols. In contrast to traditional evolutionary approaches, using a neural network at the core of the search allows for learning of higher-level symbolic patterns, which provides an informed direction to guide the search. When no labeled data is available, such networks can still be trained using reinforcement learning. However, we demonstrate that this approach can suffer from an early commitment phenomenon and from initialization bias, which can limit exploration. We present two exploration methods that build upon ideas of entropy regularization and distribution initialization to tackle these issues. We show that these techniques can improve the performance, increase sample efficiency, and lower the complexity of solutions for the task of symbolic regression.",
        "Make this fluent while doing minimal change": "Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. However, we demonstrate that this approach can suffer from an early\ncommitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression.",
        "Refine this for me please": "Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. However, when no labeled\ndata is available, such networks can still suffer from an early commitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression.",
        "Concise this for me and keep all the information": "Many machine learning strategies designed to automate mathematical tasks use neural networks to search large combinatorial spaces of mathematical symbols. This allows the machine to learn higher-level symbolic patterns, providing an informed direction to guide the search. However, when no labeled data is available, this approach can still suffer from an early commitment phenomenon and from initialization bias, both of which limit exploration. We present two exploration methods to tackle these issues, building upon ideas of entropy regularization and distribution initialization. We show that these techniques can improve the performance, increase sample efficiency, and lower the complexity of solutions for the task of symbolic regression.",
        "Improve this in GPT way": "Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. However, we demonstrate that this approach can suffer from an early\ncommitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression."
    },
    {
        "input": "We study training of Convolutional Neural Networks (CNNs) with ReLU\nactivations and introduce exact convex optimization formulations with a\npolynomial complexity with respect to the number of data samples, the number of\nneurons, and data dimension. More specifically, we develop a convex analytic\nframework utilizing semi-infinite duality to obtain equivalent convex\noptimization problems for several two- and three-layer CNN architectures. We\nfirst prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm\nregularized convex program. We then show that multi-layer circular CNN training\nproblems with a single ReLU layer are equivalent to an $\\ell_1$ regularized\nconvex program that encourages sparsity in the spectral domain. We also extend\nthese results to three-layer CNNs with two ReLU layers. Furthermore, we present\nextensions of our approach to different pooling methods, which elucidates the\nimplicit architectural bias as convex regularizers.",
        "Revise this with your best effort": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations, and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidate the implicit architectural bias as convex regularizers.",
        "Help me polish this": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Rewrite this for me": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Make this fluent while doing minimal change": "We study the training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidate the implicit architectural bias as convex regularizers.",
        "Refine this for me please": "We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Concise this for me and keep all the information": "We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",
        "Improve this in GPT way": "We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers."
    },
    {
        "input": "We consider the problem of finding the best memoryless stochastic policy for\nan infinite-horizon partially observable Markov decision process (POMDP) with\nfinite state and action spaces with respect to either the discounted or mean\nreward criterion. We show that the (discounted) state-action frequencies and\nthe expected cumulative reward are rational functions of the policy, whereby\nthe degree is determined by the degree of partial observability. We then\ndescribe the optimization problem as a linear optimization problem in the space\nof feasible state-action frequencies subject to polynomial constraints that we\ncharacterize explicitly. This allows us to address the combinatorial and\ngeometric complexity of the optimization problem using recent tools from\npolynomial optimization. In particular, we estimate the number of critical\npoints and use the polynomial programming description of reward maximization to\nsolve a navigation problem in a grid world.",
        "Revise this with your best effort": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.",
        "Help me polish this": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.",
        "Rewrite this for me": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.",
        "Make this fluent while doing minimal change": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.",
        "Refine this for me please": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.",
        "Concise this for me and keep all the information": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.",
        "Improve this in GPT way": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world."
    },
    {
        "input": "Stochastic encoders have been used in rate-distortion theory and neural\ncompression because they can be easier to handle. However, in performance\ncomparisons with deterministic encoders they often do worse, suggesting that\nnoise in the encoding process may generally be a bad idea. It is poorly\nunderstood if and when stochastic encoders do better than deterministic\nencoders. In this paper we provide one illustrative example which shows that\nstochastic encoders can significantly outperform the best deterministic\nencoders. Our toy example suggests that stochastic encoders may be particularly\nuseful in the regime of \"perfect perceptual quality\".",
        "Revise this with your best effort": "Stochastic encoders have been used in rate-distortion theory and neural compression because they can be easier to handle. However, in performance comparisons with deterministic encoders they often do worse, suggesting that noise in the encoding process may generally be a bad idea. It is poorly understood if and when stochastic encoders do better than deterministic encoders. In this paper we provide one illustrative example which shows that stochastic encoders can significantly outperform the best deterministic encoders. Our toy example suggests that stochastic encoders may be particularly useful in the regime of \"perfect perceptual quality\".",
        "Help me polish this": "Stochastic encoders have been used in rate-distortion theory and neural compression because they can be easier to handle. However, in performance comparisons with deterministic encoders they often do worse, suggesting that noise in the encoding process may generally be a bad idea. It is poorly understood if and when stochastic encoders do better than deterministic encoders. In this paper we provide one illustrative example which shows that stochastic encoders can significantly outperform the best deterministic encoders. Our toy example suggests that stochastic encoders may be particularly useful in the regime of \"perfect perceptual quality\".",
        "Rewrite this for me": "Stochastic encoders are used in rate-distortion theory and neural compression because they can be easier to handle. However, in performance comparisons with deterministic encoders, they often do worse, suggesting that noise in the encoding process may generally be a bad idea. It is poorly understood if and when stochastic encoders do better than deterministic encoders. In this paper, we provide one illustrative example which shows that stochastic encoders can significantly outperform the best deterministic encoders. Our toy example suggests that stochastic encoders may be particularly useful in the regime of \"perfect perceptual quality.\"",
        "Make this fluent while doing minimal change": "Stochastic encoders are often used in rate-distortion theory and neural compression because they can be easier to handle. However, in performance comparisons with deterministic encoders, they often do worse, suggesting that noise in the encoding process may generally be a bad idea. It is poorly understood if and when stochastic encoders do better than deterministic encoders. In this paper, we provide one illustrative example which shows that stochastic encoders can significantly outperform the best deterministic encoders. Our toy example suggests that stochastic encoders may be particularly useful in the regime of \"perfect perceptual quality.\"",
        "Refine this for me please": "Stochastic encoders are used in rate-distortion theory and neural compression because they can be easier to handle. However, in performance comparisons with deterministic encoders, they often do worse, suggesting that noise in the encoding process may generally be a bad idea. It is poorly understood if and when stochastic encoders do better than deterministic encoders. In this paper, we provide one illustrative example which shows that stochastic encoders can significantly outperform the best deterministic encoders. Our toy example suggests that stochastic encoders may be particularly useful in the regime of \"perfect perceptual quality.\"",
        "Concise this for me and keep all the information": "Stochastic encoders are used in rate-distortion theory and neural compression because they can be easier to handle. However, in performance comparisons with deterministic encoders, they often do worse, suggesting that noise in the encoding process may generally be a bad idea. It is poorly understood if and when stochastic encoders do better than deterministic encoders. In this paper, we provide one illustrative example which shows that stochastic encoders can significantly outperform the best deterministic encoders. Our toy example suggests that stochastic encoders may be particularly useful in the regime of \"perfect perceptual quality.\"",
        "Improve this in GPT way": "Stochastic encoders have been shown to outperform deterministic encoders in many cases, suggesting that noise in the encoding process may not always be a bad idea. In this paper, we provide one illustrative example which shows that stochastic encoders can significantly outperform the best deterministic encoders. Our toy example suggests that stochastic encoders may be particularly useful in the regime of \"perfect perceptual quality\"."
    },
    {
        "input": "We consider the problem of learned transform compression where we learn both,\nthe transform as well as the probability distribution over the discrete codes.\nWe utilize a soft relaxation of the quantization operation to allow for\nback-propagation of gradients and employ vector (rather than scalar)\nquantization of the latent codes. Furthermore, we apply similar relaxation in\nthe code probability assignments enabling direct optimization of the code\nentropy. To the best of our knowledge, this approach is completely novel. We\nconduct a set of proof-of concept experiments confirming the potency of our\napproaches.",
        "Revise this with your best effort": "We consider the problem of learned transform compression, where we learn both the transform and the probability distribution over the discrete codes. We utilize a soft relaxation of the quantization operation to allow for back-propagation of gradients and employ vector (rather than scalar) quantization of the latent codes. Furthermore, we apply a similar relaxation in the code probability assignments, enabling direct optimization of the code entropy. To the best of our knowledge, this approach is completely novel. We conduct a set of proof-of concept experiments confirming the potency of our approaches.",
        "Help me polish this": "We consider the problem of learned transform compression, where we learn both the transform and the probability distribution over the discrete codes. We utilize a soft relaxation of the quantization operation to allow for back-propagation of gradients and employ vector (rather than scalar) quantization of the latent codes. Furthermore, we apply a similar relaxation in the code probability assignments, enabling direct optimization of the code entropy. To the best of our knowledge, this approach is completely novel. We conduct a set of proof-of concept experiments confirming the potency of our approaches.",
        "Rewrite this for me": "We are investigating the idea of learned transform compression, in which we learn both the transform and the probability distribution over the discrete codes. We use a soft relaxation of the quantization operation to allow for back-propagation of gradients and employ vector (rather than scalar) quantization of the latent codes. Furthermore, we apply a similar relaxation to the code probability assignments, enabling direct optimization of the code entropy. To the best of our knowledge, this approach is completely novel. We conduct a set of proof-of concept experiments confirming the potency of our approaches.",
        "Make this fluent while doing minimal change": "We consider the problem of learned transform compression, where we learn both the transform and the probability distribution over the discrete codes. We utilize a soft relaxation of the quantization operation to allow for back-propagation of gradients and employ vector (rather than scalar) quantization of the latent codes. Furthermore, we apply a similar relaxation in the code probability assignments, enabling direct optimization of the code entropy. To the best of our knowledge, this approach is completely novel. We conduct a set of proof-of concept experiments confirming the potency of our approaches.",
        "Refine this for me please": "We are trying to learn both the transform and the probability distribution over the discrete codes. We use a soft relaxation of the quantization operation to allow for back-propagation of gradients and employ vector (rather than scalar) quantization of the latent codes. Furthermore, we apply similar relaxation in the code probability assignments, which enables us to directly optimize the code entropy. To the best of our knowledge, this approach is completely novel. We conduct a set of proof-of concept experiments confirming the potency of our approaches.",
        "Concise this for me and keep all the information": "We learn both the transform and the probability distribution over the discrete codes, utilizing a soft relaxation of the quantization operation to allow for back-propagation of gradients. We apply similar relaxation in the code probability assignments, enabling direct optimization of the code entropy.",
        "Improve this in GPT way": "We consider the problem of learned transform compression where we learn both, the transform as well as the probability distribution over the discrete codes. We utilize a soft relaxation of the quantization operation to allow for back-propagation of gradients and employ vector (rather than scalar) quantization of the latent codes. Furthermore, we apply similar relaxation in the code probability assignments enabling direct optimization of the code entropy. To the best of our knowledge, this approach is completely novel. We conduct a set of proof-of concept experiments confirming the potency of our approaches."
    },
    {
        "input": "The dynamics of physical systems is often constrained to lower dimensional\nsub-spaces due to the presence of conserved quantities. Here we propose a\nmethod to learn and exploit such symmetry constraints building upon Hamiltonian\nNeural Networks. By enforcing cyclic coordinates with appropriate loss\nfunctions, we find that we can achieve improved accuracy on simple classical\ndynamics tasks. By fitting analytic formulae to the latent variables in our\nnetwork we recover that our networks are utilizing conserved quantities such as\n(angular) momentum.",
        "Revise this with your best effort": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network, we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
        "Help me polish this": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network, we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
        "Rewrite this for me": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
        "Make this fluent while doing minimal change": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network, we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
        "Refine this for me please": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
        "Concise this for me and keep all the information": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
        "Improve this in GPT way": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network, we recover that our networks are utilizing conserved quantities such as (angular) momentum."
    },
    {
        "input": "In this work, we study the behavior of standard models for community\ndetection under spectral manipulations. Through various ablation experiments,\nwe evaluate the impact of bandpass filtering on the performance of a GCN: we\nempirically show that most of the necessary and used information for nodes\nclassification is contained in the low-frequency domain, and thus contrary to\nimages, high frequencies are less crucial to community detection. In\nparticular, it is sometimes possible to obtain accuracies at a state-of-the-art\nlevel with simple classifiers that rely only on a few low frequencies.",
        "Revise this with your best effort": "In this work, we study the behavior of standard models for community detection under spectral manipulations. Through various ablation experiments, we evaluate the impact of bandpass filtering on the performance of a GCN. We empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to images, high frequencies are less crucial to community detection. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies.",
        "Help me polish this": "In this work, we study the behavior of standard models for community detection under spectral manipulations. Through various ablation experiments, we evaluate the impact of bandpass filtering on the performance of a GCN. We empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to images, high frequencies are less crucial to community detection. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies.",
        "Rewrite this for me": "In this work, we study the behavior of standard models for community detection under spectral manipulations. We evaluate the impact of bandpass filtering on the performance of a GCN through various ablation experiments. We show that most of the necessary and used information for nodes classification is contained in the low-frequency domain. High frequencies are less crucial to community detection. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies.",
        "Make this fluent while doing minimal change": "In this work, we study the behavior of standard models for community detection under spectral manipulations. Through various ablation experiments, we evaluate the impact of bandpass filtering on the performance of a GCN. We empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to images, high frequencies are less crucial to community detection. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies.",
        "Refine this for me please": "In this work, we study the behavior of standard models for community detection under spectral manipulations. Through various ablation experiments, we evaluate the impact of bandpass filtering on the performance of a GCN. We empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to images, high frequencies are less crucial to community detection. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies.",
        "Concise this for me and keep all the information": "In this work, we study the behavior of standard models for community detection under spectral manipulations. We evaluate the impact of bandpass filtering on the performance of a GCN and show that most of the necessary information for nodes classification is contained in the low-frequency domain. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies.",
        "Improve this in GPT way": "In this work, we study the behavior of standard models for community detection under spectral manipulations. Through various ablation experiments, we evaluate the impact of bandpass filtering on the performance of a GCN. We find that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to images, high frequencies are less crucial to community detection. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies."
    },
    {
        "input": "We propose a new framework of synthesizing data using deep generative models\nin a differentially private manner. Within our framework, sensitive data are\nsanitized with rigorous privacy guarantees in a one-shot fashion, such that\ntraining deep generative models is possible without re-using the original data.\nHence, no extra privacy costs or model constraints are incurred, in contrast to\npopular approaches such as Differentially Private Stochastic Gradient Descent\n(DP-SGD), which, among other issues, causes degradation in privacy guarantees\nas the training iteration increases. We demonstrate a realization of our\nframework by making use of the characteristic function and an adversarial\nre-weighting objective, which are of independent interest as well. Our proposal\nhas theoretical guarantees of performance, and empirical evaluations on\nmultiple datasets show that our approach outperforms other methods at\nreasonable levels of privacy.",
        "Revise this with your best effort": "We propose a new framework for synthesizing data using deep generative models in a differentially private manner. With our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.",
        "Help me polish this": "We propose a new framework for synthesizing data using deep generative models in a differentially private manner. With our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.",
        "Rewrite this for me": "We propose a new framework for synthesizing data using deep generative models in a differentially private manner. With our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. This means that no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.",
        "Make this fluent while doing minimal change": "We propose a new framework for synthesizing data using deep generative models in a differentially private manner. With our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.",
        "Refine this for me please": "We propose a new framework for synthesizing data using deep generative models in a differentially private manner. With our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. This means that no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.",
        "Concise this for me and keep all the information": "We propose a new framework for synthesizing data using deep generative models in a differentially private manner. Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.",
        "Improve this in GPT way": "We propose a new framework of synthesizing data using deep generative models in a differentially private manner. Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy."
    },
    {
        "input": "Self-supervised visual representation learning aims to learn useful\nrepresentations without relying on human annotations. Joint embedding approach\nbases on maximizing the agreement between embedding vectors from different\nviews of the same image. Various methods have been proposed to solve the\ncollapsing problem where all embedding vectors collapse to a trivial constant\nsolution. Among these methods, contrastive learning prevents collapse via\nnegative sample pairs. It has been shown that non-contrastive methods suffer\nfrom a lesser collapse problem of a different nature: dimensional collapse,\nwhereby the embedding vectors end up spanning a lower-dimensional subspace\ninstead of the entire available embedding space. Here, we show that dimensional\ncollapse also happens in contrastive learning. In this paper, we shed light on\nthe dynamics at play in contrastive learning that leads to dimensional\ncollapse. Inspired by our theory, we propose a novel contrastive learning\nmethod, called DirectCLR, which directly optimizes the representation space\nwithout relying on an explicit trainable projector. Experiments show that\nDirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
        "Revise this with your best effort": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. The joint embedding approach is based on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. However, it has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
        "Help me polish this": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. The joint embedding approach is based on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem, where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. However, it has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
        "Rewrite this for me": "Self-supervised visual representation learning is a way of learning useful representations without relying on human annotations. The joint embedding approach is based on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem, where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse by using negative sample pairs. However, it has been shown that non-contrastive methods suffer from a different kind of collapse problem, called dimensional collapse, where the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. In this paper, we show that dimensional collapse also happens in contrastive learning. We also shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
        "Make this fluent while doing minimal change": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. The joint embedding approach is based on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. However, it has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
        "Refine this for me please": "Self-supervised visual representation learning is a method of learning useful representations without relying on human annotations. The joint embedding approach is based on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem, where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
        "Concise this for me and keep all the information": "Self-supervised visual representation learning is a method of learning useful representations without relying on human annotations. The joint embedding approach is based on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem, where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. However, it has been shown that contrastive learning also suffers from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
        "Improve this in GPT way": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet."
    },
    {
        "input": "We provide a general self-attention formulation to impose group equivariance\nto arbitrary symmetry groups. This is achieved by defining positional encodings\nthat are invariant to the action of the group considered. Since the group acts\non the positional encoding directly, group equivariant self-attention networks\n(GSA-Nets) are steerable by nature. Our experiments on vision benchmarks\ndemonstrate consistent improvements of GSA-Nets over non-equivariant\nself-attention networks.",
        "Revise this with your best effort": "We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Help me polish this": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Rewrite this for me": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Make this fluent while doing minimal change": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Refine this for me please": "We provide a general self-attention formulation that imposes group equivariance on arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Concise this for me and keep all the information": "We provide a self-attention formulation that imposes group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks.",
        "Improve this in GPT way": "We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate the consistent improvements of GSA-Nets over non-equivariant self-attention networks."
    },
    {
        "input": "We propose the task of disambiguating symbolic expressions in informal STEM\ndocuments in the form of LaTeX files - that is, determining their precise\nsemantics and abstract syntax tree - as a neural machine translation task. We\ndiscuss the distinct challenges involved and present a dataset with roughly\n33,000 entries. We evaluated several baseline models on this dataset, which\nfailed to yield even syntactically valid LaTeX before overfitting.\nConsequently, we describe a methodology using a transformer language model\npre-trained on sources obtained from arxiv.org, which yields promising results\ndespite the small size of the dataset. We evaluate our model using a plurality\nof dedicated techniques, taking the syntax and semantics of symbolic\nexpressions into account.",
        "Revise this with your best effort": "We propose the task of disambiguating symbolic expressions in informal STEM documents in the form of LaTeX files - that is, determining their precise semantics and abstract syntax tree - as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset, which failed to yield even syntactically valid LaTeX before overfitting. Consequently, we describe a methodology using a transformer language model pre-trained on sources obtained from arxiv.org, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking the syntax and semantics of symbolic expressions into account.",
        "Help me polish this": "We propose the task of disambiguating symbolic expressions in informal STEM documents in the form of LaTeX files - that is, determining their precise semantics and abstract syntax tree - as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset, which failed to yield even syntactically valid LaTeX before overfitting. Consequently, we describe a methodology using a transformer language model pre-trained on sources obtained from arxiv.org, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking the syntax and semantics of symbolic expressions into account.",
        "Rewrite this for me": "We propose that the task of disambiguating symbolic expressions in informal STEM documents in the form of LaTeX files - that is, determining their precise semantics and abstract syntax tree - can be seen as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset, which failed to yield even syntactically valid LaTeX before overfitting. Consequently, we describe a methodology using a transformer language model pre-trained on sources obtained from arxiv.org, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking the syntax and semantics of symbolic expressions into account.",
        "Make this fluent while doing minimal change": "We propose the task of disambiguating symbolic expressions in informal STEM documents in the form of LaTeX files - that is, determining their precise semantics and abstract syntax tree - as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset, which failed to yield even syntactically valid LaTeX before overfitting. Consequently, we describe a methodology using a transformer language model pre-trained on sources obtained from arxiv.org, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking the syntax and semantics of symbolic expressions into account.",
        "Refine this for me please": "We propose that the task of disambiguating symbolic expressions in informal STEM documents in the form of LaTeX files - that is, determining their precise semantics and abstract syntax tree - can be seen as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset, which failed to yield even syntactically valid LaTeX before overfitting. Consequently, we describe a methodology using a transformer language model pre-trained on sources obtained from arxiv.org, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking the syntax and semantics of symbolic expressions into account.",
        "Concise this for me and keep all the information": "We propose that the task of disambiguating symbolic expressions in informal STEM documents in the form of LaTeX files - that is, determining their precise semantics and abstract syntax tree - can be seen as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset and found that they failed to yield even syntactically valid LaTeX before overfitting. Consequently, we describe a methodology using a transformer language model pre-trained on sources obtained from arxiv.org, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking the syntax and semantics of symbolic expressions into account.",
        "Improve this in GPT way": "We propose the task of disambiguating symbolic expressions in informal STEM\ndocuments in the form of LaTeX files - that is, determining their precise\nsemantics and abstract syntax tree - as a neural machine translation task. We\ndiscuss the distinct challenges involved and present a dataset with roughly\n33,000 entries. We evaluated several baseline models on this dataset, which\nfailed to yield even syntactically valid LaTeX before overfitting.\nConsequently, we describe a methodology using a transformer language model\npre-trained on sources obtained from arxiv.org, which yields promising results\ndespite the small size of the dataset. We evaluate our model using a plurality\nof dedicated techniques, taking the syntax and semantics of symbolic\nexpressions into account."
    },
    {
        "input": "Training classifiers under fairness constraints such as group fairness,\nregularizes the disparities of predictions between the groups. Nevertheless,\neven though the constraints are satisfied during training, they might not\ngeneralize at evaluation time. To improve the generalizability of fair\nclassifiers, we propose fair mixup, a new data augmentation strategy for\nimposing the fairness constraint. In particular, we show that fairness can be\nachieved by regularizing the models on paths of interpolated samples between\nthe groups. We use mixup, a powerful data augmentation strategy to generate\nthese interpolates. We analyze fair mixup and empirically show that it ensures\na better generalization for both accuracy and fairness measurement in tabular,\nvision, and language benchmarks.",
        "Revise this with your best effort": "Training classifiers under fairness constraints such as group fairness can help to reduce disparities in predictions between different groups. However, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it can help to ensure a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.",
        "Help me polish this": "Training classifiers under fairness constraints such as group fairness can help to reduce disparities in predictions between groups. However, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it can help to ensure better accuracy and fairness in tabular, vision, and language benchmarks.",
        "Rewrite this for me": "Training classifiers under fairness constraints such as group fairness can help to reduce disparities in predictions between different groups. However, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.",
        "Make this fluent while doing minimal change": "Training classifiers under fairness constraints such as group fairness regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.",
        "Refine this for me please": "Training classifiers under fairness constraints such as group fairness,\nregularizes the disparities of predictions between the groups. Nevertheless,\neven though the constraints are satisfied during training, they might not\ngeneralize at evaluation time. To improve the generalizability of fair\nclassifiers, we propose fair mixup, a new data augmentation strategy for\nimposing the fairness constraint. In particular, we show that fairness can be\nachieved by regularizing the models on paths of interpolated samples between\nthe groups. We use mixup, a powerful data augmentation strategy to generate\nthese interpolates. We analyze fair mixup and empirically show that it ensures\na better generalization for both accuracy and fairness measurement in tabular,\nvision, and language benchmarks.",
        "Concise this for me and keep all the information": "Training classifiers under fairness constraints such as group fairness regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, the authors propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, they show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. They use mixup, a powerful data augmentation strategy to generate these interpolates. They analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.",
        "Improve this in GPT way": "We propose a new data augmentation strategy, called fair mixup, for training classifiers under fairness constraints. This strategy regularizes the models on paths of interpolated samples between the groups, ensuring a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks."
    },
    {
        "input": "While autoregressive models excel at image compression, their sample quality\nis often lacking. Although not realistic, generated images often have high\nlikelihood according to the model, resembling the case of adversarial examples.\nInspired by a successful adversarial defense method, we incorporate randomized\nsmoothing into autoregressive generative modeling. We first model a smoothed\nversion of the data distribution, and then reverse the smoothing process to\nrecover the original data distribution. This procedure drastically improves the\nsample quality of existing autoregressive models on several synthetic and\nreal-world image datasets while obtaining competitive likelihoods on synthetic\ndatasets.",
        "Revise this with your best effort": "While autoregressive models are good at image compression, the quality of the samples they generate is often poor. The generated images often have high likelihood according to the model, resembling the case of adversarial examples. Inspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while still obtaining competitive likelihoods on synthetic datasets.",
        "Help me polish this": "While autoregressive models are good at image compression, the quality of the samples they produce is often poor. The generated images often have high likelihood according to the model, resembling the case of adversarial examples.\n\nInspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution.\n\nThis procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets.",
        "Rewrite this for me": "Autoregressive models are good at image compression, but the quality of the samples they produce is often poor. The generated images often look realistic, but they have a high likelihood of being adversarial examples. Inspired by a successful method of defending against adversarial examples, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets.",
        "Make this fluent while doing minimal change": "While autoregressive models are good at image compression, their sample quality is often lacking. This is because, although not realistic, generated images often have high likelihood according to the model, resembling the case of adversarial examples. Inspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets.",
        "Refine this for me please": "While autoregressive models are good at image compression, the quality of the images they generate is often poor. Inspired by a successful method for defending against adversarial examples, we incorporate randomized smoothing into autoregressive generative modeling. This improves the quality of the images generated by autoregressive models while still maintaining competitive likelihoods on synthetic datasets.",
        "Concise this for me and keep all the information": "Autoregressive models are good at image compression, but the quality of the samples they generate is often poor. This is because the images generated often have a high likelihood according to the model, resembling the case of adversarial examples. Inspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. This means that we first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets.",
        "Improve this in GPT way": "We propose a new method for autoregressive generative modeling that incorporates randomized smoothing. This method improves the sample quality of existing autoregressive models on synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets."
    },
    {
        "input": "We propose a simple method by which to choose sample weights for problems\nwith highly imbalanced or skewed traits. Rather than naively discretizing\nregression labels to find binned weights, we take a more principled approach --\nwe derive sample weights from the transfer function between an estimated source\nand specified target distributions. Our method outperforms both unweighted and\ndiscretely-weighted models on both regression and classification tasks. We also\nopen-source our implementation of this method\n(https://github.com/Daniel-Wu/Continuous-Weight-Balancing) to the scientific\ncommunity.",
        "Revise this with your best effort": "We propose a simple method for choosing sample weights for problems with highly imbalanced or skewed traits. Rather than naively discretizing regression labels to find binned weights, we take a more principled approach by deriving sample weights from the transfer function between an estimated source and specified target distributions. Our method outperforms both unweighted and discretely-weighted models on both regression and classification tasks. We also open-source our implementation of this method (https://github.com/Daniel-Wu/Continuous-Weight-Balancing) to the scientific community.",
        "Help me polish this": "We propose a simple, more effective method for choosing sample weights for problems with highly imbalanced or skewed traits. Rather than naively discretizing regression labels to find binned weights, we take a more principled approach -- we derive sample weights from the transfer function between an estimated source and specified target distributions. Our method outperforms both unweighted and discretely-weighted models on both regression and classification tasks. We also open-source our implementation of this method (https://github.com/Daniel-Wu/Continuous-Weight-Balancing) to the scientific community.",
        "Rewrite this for me": "We have developed a method for choosing sample weights for problems with highly imbalanced or skewed traits. This method outperforms both unweighted and discretely-weighted models on both regression and classification tasks. Our implementation of this method is available at https://github.com/Daniel-Wu/Continuous-Weight-Balancing.",
        "Make this fluent while doing minimal change": "We propose a simple, more effective method for choosing sample weights for problems with highly imbalanced or skewed traits. Rather than naively discretizing regression labels to find binned weights, we take a principled approach -- we derive sample weights from the transfer function between an estimated source and specified target distributions. Our method outperforms both unweighted and discretely-weighted models on both regression and classification tasks. We also open-source our implementation of this method (https://github.com/Daniel-Wu/Continuous-Weight-Balancing) to the scientific community.",
        "Refine this for me please": "We propose a method for choosing sample weights for problems with highly imbalanced or skewed traits. Rather than discretizing regression labels to find binned weights, we take a more principled approach -- we derive sample weights from the transfer function between an estimated source and specified target distributions. Our method outperforms both unweighted and discretely-weighted models on both regression and classification tasks. We also open-source our implementation of this method to the scientific community.",
        "Concise this for me and keep all the information": "We propose a method for choosing sample weights for problems with highly imbalanced or skewed traits. We derive sample weights from the transfer function between an estimated source and specified target distributions. Our method outperforms both unweighted and discretely-weighted models on both regression and classification tasks.",
        "Improve this in GPT way": "We propose a simple, more effective method for choosing sample weights for problems with highly imbalanced or skewed traits. Rather than naively discretizing regression labels to find binned weights, we take a principled approach -- we derive sample weights from the transfer function between an estimated source and specified target distributions. Our method outperforms both unweighted and discretely-weighted models on both regression and classification tasks. We also open-source our implementation of this method (https://github.com/Daniel-Wu/Continuous-Weight-Balancing) to the scientific community."
    },
    {
        "input": "In this work, we analyze the reinstatement mechanism introduced by Ritter et\nal. (2018) to reveal two classes of neurons that emerge in the agent's working\nmemory (an epLSTM cell) when trained using episodic meta-RL on an episodic\nvariant of the Harlow visual fixation task. Specifically, Abstract neurons\nencode knowledge shared across tasks, while Episodic neurons carry information\nrelevant for a specific episode's task.",
        "Revise this with your best effort": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Help me polish this": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Rewrite this for me": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Make this fluent while doing minimal change": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Refine this for me please": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.",
        "Concise this for me and keep all the information": "There are two types of neurons that are revealed when using the reinstatement mechanism- Abstract and Episodic neurons. Abstract neurons encode knowledge that is shared across tasks, while Episodic neurons carry information that is relevant for a specific episode's task.",
        "Improve this in GPT way": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task."
    },
    {
        "input": "Deep Neural Networks are known to be vulnerable to small, adversarially\ncrafted, perturbations. The current most effective defense methods against\nthese adversarial attacks are variants of adversarial training. In this paper,\nwe introduce a radically different defense trained only on clean images: a\nsparse coding based frontend which significantly attenuates adversarial attacks\nbefore they reach the classifier. We evaluate our defense on CIFAR-10 dataset\nunder a wide range of attack types (including Linf , L2, and L1 bounded\nattacks), demonstrating its promise as a general-purpose approach for defense.",
        "Revise this with your best effort": "Deep neural networks are known to be vulnerable to small, adversarially crafted perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on the CIFAR-10 dataset under a wide range of attack types (including Linf, L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense.",
        "Help me polish this": "Deep Neural Networks are known to be vulnerable to small, adversarially crafted, perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on CIFAR-10 dataset under a wide range of attack types (including Linf, L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense.",
        "Rewrite this for me": "Deep Neural Networks are known to be vulnerable to small, adversarially crafted, perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on CIFAR-10 dataset under a wide range of attack types (including Linf, L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense.",
        "Make this fluent while doing minimal change": "Deep Neural Networks are known to be vulnerable to small, adversarially crafted, perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on CIFAR-10 dataset under a wide range of attack types (including Linf, L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense.",
        "Refine this for me please": "Deep Neural Networks are known to be vulnerable to small, adversarially crafted, perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on CIFAR-10 dataset under a wide range of attack types (including Linf, L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense.",
        "Concise this for me and keep all the information": "Deep Neural Networks are known to be vulnerable to small, adversarially crafted, perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on CIFAR-10 dataset under a wide range of attack types (including Linf, L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense.",
        "Improve this in GPT way": "Deep Neural Networks are known to be vulnerable to small, adversarially crafted, perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on CIFAR-10 dataset under a wide range of attack types (including Linf, L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense."
    },
    {
        "input": "The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has\nemerged as a useful tool for thinking about realism and distortion of\nreconstructions in lossy compression. Unlike the rate-distortion function,\nhowever, it is unknown whether encoders and decoders exist that achieve the\nrate suggested by the RDPF. Building on results by Li and El Gamal (2018), we\nshow that the RDPF can indeed be achieved using stochastic, variable-length\ncodes. For this class of codes, we also prove that the RDPF lower-bounds the\nachievable rate",
        "Revise this with your best effort": "The rate-distortion-perception function (RDPF) has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal, we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Help me polish this": "The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Rewrite this for me": "The RDPF is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF, but it is possible to use stochastic, variable-length codes to achieve the RDPF. For this class of codes, the RDPF lower-bounds the achievable rate.",
        "Make this fluent while doing minimal change": "The RDPF has emerged as a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate.",
        "Refine this for me please": "The RDPF is a tool for thinking about the realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. However, it is possible to achieve the RDPF using stochastic, variable-length codes. For this class of codes, the RDPF lower-bounds the achievable rate.",
        "Concise this for me and keep all the information": "The rate-distortion-perception function (RDPF) is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. It is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF, but it can be achieved using stochastic, variable-length codes.",
        "Improve this in GPT way": "The rate-distortion-perception function (RDPF) is a useful tool for thinking about realism and distortion of reconstructions in lossy compression. Unlike the rate-distortion function, however, it is unknown whether encoders and decoders exist that achieve the rate suggested by the RDPF. Building on results by Li and El Gamal (2018), we show that the RDPF can indeed be achieved using stochastic, variable-length codes. For this class of codes, we also prove that the RDPF lower-bounds the achievable rate."
    },
    {
        "input": "Most graph neural network architectures work by message-passing node vector\nembeddings over the adjacency matrix, and it is assumed that they capture graph\ntopology by doing that. We design two synthetic tasks, focusing purely on\ntopological problems -- triangle detection and clique distance -- on which\ngraph neural networks perform surprisingly badly, failing to detect those\n\"bermuda\" triangles. Datasets and their generation scripts are publicly\navailable on github.com/FujitsuLaboratories/bermudatriangles and\ndataset.labs.fujitsu.com.",
        "Revise this with your best effort": "We design two synthetic tasks, focusing purely on topological problems -- triangle detection and clique distance -- on which graph neural networks perform surprisingly badly, failing to detect those \"bermuda\" triangles. Datasets and their generation scripts are publicly available on github.com/FujitsuLaboratories/bermudatriangles and dataset.labs.fujitsu.com.",
        "Help me polish this": "We design two synthetic tasks, focusing purely on topological problems -- triangle detection and clique distance -- on which graph neural networks perform surprisingly badly, failing to detect those \"bermuda\" triangles. Datasets and their generation scripts are publicly available on github.com/FujitsuLaboratories/bermudatriangles and dataset.labs.fujitsu.com.",
        "Rewrite this for me": "We designed two synthetic tasks, focusing purely on topological problems -- triangle detection and clique distance -- on which graph neural networks perform surprisingly badly, failing to detect those \"bermuda\" triangles. Datasets and their generation scripts are publicly available on github.com/FujitsuLaboratories/bermudatriangles and dataset.labs.fujitsu.com.",
        "Make this fluent while doing minimal change": "Most graph neural network architectures work by message-passing node vector\nembeddings over the adjacency matrix. It is assumed that they capture graph\ntopology by doing that. However, we design two synthetic tasks, focusing purely on\ntopological problems -- triangle detection and clique distance -- on which\ngraph neural networks perform surprisingly badly, failing to detect those\n\"bermuda\" triangles. Datasets and their generation scripts are publicly\navailable on github.com/FujitsuLaboratories/bermudatriangles and\ndataset.labs.fujitsu.com.",
        "Refine this for me please": "We design two synthetic tasks, focusing purely on topological problems -- triangle detection and clique distance -- on which graph neural networks perform surprisingly badly, failing to detect those \"bermuda\" triangles. Datasets and their generation scripts are publicly available on github.com/FujitsuLaboratories/bermudatriangles and dataset.labs.fujitsu.com.",
        "Concise this for me and keep all the information": "We designed two synthetic tasks, focusing purely on topological problems -- triangle detection and clique distance -- on which graph neural networks performed surprisingly badly, failing to detect those \"bermuda\" triangles.",
        "Improve this in GPT way": "We design two synthetic tasks, focusing purely on topological problems -- triangle detection and clique distance -- on which graph neural networks perform surprisingly badly, failing to detect those \"bermuda\" triangles. Datasets and their generation scripts are publicly available on github.com/FujitsuLaboratories/bermudatriangles and dataset.labs.fujitsu.com."
    },
    {
        "input": "Privacy and security-related concerns are growing as machine learning reaches\ndiverse application domains. The data holders want to train with private data\nwhile exploiting accelerators, such as GPUs, that are hosted in the cloud.\nHowever, Cloud systems are vulnerable to attackers that compromise the privacy\nof data and integrity of computations. This work presents DarKnight, a\nframework for large DNN training while protecting input privacy and computation\nintegrity. DarKnight relies on cooperative execution between trusted execution\nenvironments (TEE) and accelerators, where the TEE provides privacy and\nintegrity verification, while accelerators perform the computation heavy linear\nalgebraic operations.",
        "Revise this with your best effort": "As machine learning becomes increasingly prevalent in a variety of fields, concerns over privacy and security are mounting. Data holders want to be able to train using private data while taking advantage of cloud-based accelerators, such as GPUs, but such systems are vulnerable to attack. This work presents DarKnight, a framework for large DNN training that protects input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEEs) and accelerators, with the TEE providing privacy and integrity verification and the accelerator performing the computationally-intensive linear algebraic operations.",
        "Help me polish this": "Privacy and security concerns are growing as machine learning reaches diverse application domains. The data holders want to train with private data while exploiting accelerators, such as GPUs, that are hosted in the cloud. However, Cloud systems are vulnerable to attackers that compromise the privacy of data and integrity of computations. This work presents DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEE) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation heavy linear algebraic operations.",
        "Rewrite this for me": "As machine learning becomes increasingly prevalent in a variety of fields, concerns over privacy and security are mounting. Data holders want to be able to train using private data while taking advantage of cloud-based accelerators, such as GPUs, but such systems are vulnerable to attack. This work introduces DarKnight, a framework that allows for large-scale DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEEs) and accelerators, with the TEEs providing privacy and integrity verification and the accelerators performing the computationally-intensive linear algebraic operations.",
        "Make this fluent while doing minimal change": "The growing concerns over privacy and security as machine learning reaches diverse application domains has led data holders to want to train with private data while still being able to exploit accelerators, such as GPUs, that are hosted in the cloud. However, Cloud systems are vulnerable to attackers that compromise the privacy of data and integrity of computations. This work presents DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEE) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation heavy linear algebraic operations.",
        "Refine this for me please": "The data holders want to train with private data while exploiting accelerators, such as GPUs, that are hosted in the cloud. However, Cloud systems are vulnerable to attackers that compromise the privacy of data and integrity of computations. This work presents DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEE) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation heavy linear algebraic operations.",
        "Concise this for me and keep all the information": "The data holders want to train with private data while exploiting accelerators, such as GPUs, that are hosted in the cloud. However, Cloud systems are vulnerable to attackers that compromise the privacy of data and integrity of computations. This work presents DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEE) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation heavy linear algebraic operations.",
        "Improve this in GPT way": "Privacy and security concerns are growing as machine learning reaches diverse application domains. The data holders want to train with private data while exploiting accelerators, such as GPUs, that are hosted in the cloud. However, Cloud systems are vulnerable to attackers that compromise the privacy of data and integrity of computations.\n\nTo address these concerns, we present DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEEs) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation heavy linear algebraic operations.\n\nThis work demonstrates that DarKnight is effective in protecting both privacy and integrity of DNN training, while incurring only a moderate performance overhead."
    },
    {
        "input": "We generalize the Hamiltonian Monte Carlo algorithm with a stack of neural\nnetwork layers and evaluate its ability to sample from different topologies in\na two dimensional lattice gauge theory. We demonstrate that our model is able\nto successfully mix between modes of different topologies, significantly\nreducing the computational cost required to generated independent gauge field\nconfigurations. Our implementation is available at\nhttps://github.com/saforem2/l2hmc-qcd .",
        "Revise this with your best effort": "We generalize the Hamiltonian Monte Carlo algorithm by stacking neural network layers. We evaluate the ability of our model to sample from different topologies in a two dimensional lattice gauge theory. We demonstrate that our model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generate independent gauge field configurations. Our implementation is available at https://github.com/saforem2/l2hmc-qcd.",
        "Help me polish this": "We generalize the Hamiltonian Monte Carlo algorithm by stacking neural network layers and evaluate its ability to sample from different topologies in a two dimensional lattice gauge theory. We demonstrate that our model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generate independent gauge field configurations. Our implementation is available at https://github.com/saforem2/l2hmc-qcd.",
        "Rewrite this for me": "Our generalized Hamiltonian Monte Carlo algorithm uses a stack of neural network layers to sample from different topologies in a two dimensional lattice gauge theory. We demonstrate that our model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generate independent gauge field configurations. Our implementation is available at https://github.com/saforem2/l2hmc-qcd.",
        "Make this fluent while doing minimal change": "We generalize the Hamiltonian Monte Carlo algorithm by stacking neural network layers and evaluate its ability to sample from different topologies in a two dimensional lattice gauge theory. We demonstrate that our model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generate independent gauge field configurations. Our implementation is available at https://github.com/saforem2/l2hmc-qcd.",
        "Refine this for me please": "The Hamiltonian Monte Carlo algorithm can be generalized by adding a stack of neural network layers. This can be used to sample from different topologies in a two dimensional lattice gauge theory. The model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generate independent gauge field configurations.",
        "Concise this for me and keep all the information": "We generalize the Hamiltonian Monte Carlo algorithm with a stack of neural network layers. We demonstrate that our model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generate independent gauge field configurations.",
        "Improve this in GPT way": "We generalize the Hamiltonian Monte Carlo algorithm by stacking neural network layers and evaluate its ability to sample from different topologies in a two dimensional lattice gauge theory. We demonstrate that our model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generate independent gauge field configurations. Our implementation is available at https://github.com/saforem2/l2hmc-qcd."
    },
    {
        "input": "Concept bottleneck models map from raw inputs to concepts, and then from\nconcepts to targets. Such models aim to incorporate pre-specified, high-level\nconcepts into the learning procedure, and have been motivated to meet three\ndesiderata: interpretability, predictability, and intervenability. However, we\nfind that concept bottleneck models struggle to meet these goals. Using post\nhoc interpretability methods, we demonstrate that concepts do not correspond to\nanything semantically meaningful in input space, thus calling into question the\nusefulness of concept bottleneck models in their current form.",
        "Revise this with your best effort": "Concept bottleneck models map from raw inputs to concepts, and then from\nconcepts to targets. Such models aim to incorporate pre-specified, high-level\nconcepts into the learning procedure, and have been motivated to meet three\ndesiderata: interpretability, predictability, and intervenability. However, we\nfind that concept bottleneck models struggle to meet these goals. Using post\nhoc interpretability methods, we demonstrate that concepts do not correspond to\nanything semantically meaningful in input space, thus calling into question the\nusefulness of concept bottleneck models in their current form.",
        "Help me polish this": "We find that concept bottleneck models struggle to meet the goals of interpretability, predictability, and intervenability. Using post hoc interpretability methods, we demonstrate that concepts do not correspond to anything semantically meaningful in input space, thus calling into question the usefulness of concept bottleneck models in their current form.",
        "Rewrite this for me": "Concept bottleneck models are not effective in meeting the goals of interpretability, predictability, and intervenability.",
        "Make this fluent while doing minimal change": "Concept bottleneck models map from raw inputs to concepts, and then from\nconcepts to targets. Such models aim to incorporate pre-specified, high-level\nconcepts into the learning procedure, and have been motivated to meet three\ndesiderata: interpretability, predictability, and intervenability. However, we\nfind that concept bottleneck models struggle to meet these goals. Using post\nhoc interpretability methods, we demonstrate that concepts do not correspond to\nanything semantically meaningful in input space, thus calling into question the\nusefulness of concept bottleneck models.",
        "Refine this for me please": "Concept bottleneck models are not effective in achieving interpretability, predictability, or intervenability.",
        "Concise this for me and keep all the information": "Concept bottleneck models map from raw inputs to concepts, and then from concepts to targets. Such models aim to incorporate pre-specified, high-level concepts into the learning procedure, and have been motivated to meet three desiderata: interpretability, predictability, and intervenability. However, we find that concept bottleneck models struggle to meet these goals. Using post hoc interpretability methods, we demonstrate that concepts do not correspond to anything semantically meaningful in input space, thus calling into question the usefulness of concept bottleneck models in their current form.",
        "Improve this in GPT way": "We find that concept bottleneck models are not effective in meeting the goals of interpretability, predictability, and intervenability."
    },
    {
        "input": "In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models.",
        "Revise this with your best effort": "In this paper, we propose a new data poisoning attack and apply it to deep reinforcement learning agents. Our attack centers on what we call in-distribution triggers, which are triggers native to the data distributions the model will be trained on and deployed in. We outline a simple procedure for embedding these, and other, triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",
        "Help me polish this": "In this paper, we propose a new data poisoning attack that we apply to deep reinforcement learning agents. Our attack centers on what we call in-distribution triggers, which are triggers native to the data distributions the model will be trained on and deployed in. We outline a simple procedure for embedding these triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate the effectiveness of our attack in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",
        "Rewrite this for me": "In this paper, we introduce a new data poisoning attack that can be used on deep reinforcement learning agents. Our attack is based on what we call in-distribution triggers, which are triggers that are native to the data distributions that the model will be trained on and deployed in. We outline a simple procedure for embedding these triggers in deep reinforcement learning agents that are following a multi-task learning paradigm, and demonstrate the effectiveness of our attack in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",
        "Make this fluent while doing minimal change": "In this paper, we propose a new data poisoning attack and apply it to deep reinforcement learning agents. Our attack centers on what we call in-distribution triggers, which are triggers native to the data distributions the model will be trained on and deployed in. We outline a simple procedure for embedding these, and other, triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",
        "Refine this for me please": "In this paper, we propose a new data poisoning attack that targets deep reinforcement learning agents. Our attack centers on what we call in-distribution triggers, which are triggers native to the data distributions the model will be trained on and deployed in. We outline a simple procedure for embedding these triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate the effectiveness of our attack in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",
        "Concise this for me and keep all the information": "In this paper, we propose a new data poisoning attack that we call in-distribution triggers. This attack centers on triggers that are native to the data distributions that the model will be trained on and deployed in. We outline a simple procedure for embedding these triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",
        "Improve this in GPT way": "In this paper, we propose a new data poisoning attack and apply it to deep reinforcement learning agents. Our attack centers on what we call in-distribution triggers, which are triggers native to the data distributions the model will be trained on and deployed in. We outline a simple procedure for embedding these, and other, triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models."
    },
    {
        "input": "In this paper, we present a novel neuroevolutionary method to identify the\narchitecture and hyperparameters of convolutional autoencoders. Remarkably, we\nused a hypervolume indicator in the context of neural architecture search for\nautoencoders, for the first time to our current knowledge. Results show that\nimages were compressed by a factor of more than 10, while still retaining\nenough information to achieve image classification for the majority of the\ntasks. Thus, this new approach can be used to speed up the AutoML pipeline for\nimage compression.",
        "Revise this with your best effort": "In this paper, we present a new neuroevolutionary method to identify the architecture and hyperparameters of convolutional autoencoders. This is the first time, to our current knowledge, that a hypervolume indicator has been used in the context of neural architecture search for autoencoders. Results show that images can be compressed by a factor of more than 10, while still retaining enough information to achieve image classification for the majority of tasks. Thus, this new approach can be used to speed up the AutoML pipeline for image compression.",
        "Help me polish this": "In this paper, we present a new neuroevolutionary method to identify the architecture and hyperparameters of convolutional autoencoders. This is done by using a hypervolume indicator in the context of neural architecture search. Results show that images can be compressed by a factor of more than 10, while still retaining enough information to achieve image classification for the majority of the tasks. Thus, this new approach can be used to speed up the AutoML pipeline for image compression.",
        "Rewrite this for me": "In this paper, we introduce a new neuroevolutionary method to identify the architecture and hyperparameters of convolutional autoencoders. This is the first time, to our current knowledge, that a hypervolume indicator has been used in the context of neural architecture search for autoencoders. Results show that images can be compressed by a factor of more than 10 while still retaining enough information to achieve image classification for most tasks. Therefore, this new approach can be used to speed up the AutoML pipeline for image compression.",
        "Make this fluent while doing minimal change": "In this paper, we present a novel neuroevolutionary method for identifying the architecture and hyperparameters of convolutional autoencoders. To our current knowledge, this is the first time that a hypervolume indicator has been used in the context of neural architecture search for autoencoders. Results show that images can be compressed by a factor of more than 10 while still retaining enough information to achieve image classification for the majority of tasks. Thus, this new approach can be used to speed up the AutoML pipeline for image compression.",
        "Refine this for me please": "In this paper, we present a new neuroevolutionary method to identify the architecture and hyperparameters of convolutional autoencoders. This new approach can be used to speed up the AutoML pipeline for image compression by using a hypervolume indicator to compress images by a factor of more than 10 while still retaining enough information to achieve image classification for the majority of tasks.",
        "Concise this for me and keep all the information": "In this paper, we present a new neuroevolutionary method to identify the architecture and hyperparameters of convolutional autoencoders. This new approach can be used to speed up the AutoML pipeline for image compression.",
        "Improve this in GPT way": "In this paper, we present a novel neuroevolutionary method to identify the\narchitecture and hyperparameters of convolutional autoencoders. Remarkably, we\nused a hypervolume indicator in the context of neural architecture search for\nautoencoders, for the first time to our current knowledge. Results show that\nimages were compressed by a factor of more than 10, while still retaining\nenough information to achieve image classification for the majority of the\ntasks. Thus, this new approach can be used to speed up the AutoML pipeline for\nimage compression by a factor of 10."
    },
    {
        "input": "Model-based Reinforcement Learning estimates the true environment through a\nworld model in order to approximate the optimal policy. This family of\nalgorithms usually benefits from better sample efficiency than their model-free\ncounterparts. We investigate whether controllers learned in such a way are\nrobust and able to generalize under small perturbations of the environment. Our\nwork is inspired by the PILCO algorithm, a method for probabilistic policy\nsearch. We show that enforcing a lower bound to the likelihood noise in the\nGaussian Process dynamics model regularizes the policy updates and yields more\nrobust controllers. We demonstrate the empirical benefits of our method in a\nsimulation benchmark.",
        "Revise this with your best effort": "Model-based reinforcement learning algorithms estimate the true environment through a world model in order to approximate the optimal policy. This family of algorithms usually benefits from better sample efficiency than their model-free counterparts. We investigate whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark.",
        "Help me polish this": "Model-based reinforcement learning algorithms estimate the true environment through a world model in order to approximate the optimal policy. This family of algorithms usually benefits from better sample efficiency than their model-free counterparts. We investigate whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark.",
        "Rewrite this for me": "Model-based reinforcement learning algorithms estimate the true environment through a world model in order to approximate the optimal policy. These algorithms usually benefit from better sample efficiency than their model-free counterparts. This work investigates whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark.",
        "Make this fluent while doing minimal change": "Model-based Reinforcement Learning algorithms estimate the true environment through a world model in order to approximate the optimal policy. This family of algorithms usually benefits from better sample efficiency than their model-free counterparts. We investigate whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark.",
        "Refine this for me please": "Model-based reinforcement learning algorithms estimate the true environment through a world model in order to approximate the optimal policy. These algorithms usually benefit from better sample efficiency than their model-free counterparts. This work investigates whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark.",
        "Concise this for me and keep all the information": "Model-based reinforcement learning algorithms estimate the true environment through a world model in order to approximate the optimal policy. These algorithms usually benefit from better sample efficiency than their model-free counterparts. This work investigates whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark.",
        "Improve this in GPT way": "We investigate whether controllers learned using Model-based Reinforcement Learning are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark."
    },
    {
        "input": "The inputs and/or outputs of some neural nets are weight matrices of other\nneural nets. Indirect encodings or end-to-end compression of weight matrices\ncould help to scale such approaches. Our goal is to open a discussion on this\ntopic, starting with recurrent neural networks for character-level language\nmodelling whose weight matrices are encoded by the discrete cosine transform.\nOur fast weight version thereof uses a recurrent neural network to parameterise\nthe compressed weights. We present experimental results on the enwik8 dataset.",
        "Revise this with your best effort": "The weight matrices of some neural nets can be used as inputs and/or outputs for other neural nets. Indirect encodings or end-to-end compression of weight matrices could help to scale such approaches. Our goal is to open a discussion on this topic, starting with recurrent neural networks for character-level language modelling whose weight matrices are encoded by the discrete cosine transform. Our fast weight version thereof uses a recurrent neural network to parameterise the compressed weights. We present experimental results on the enwik8 dataset.",
        "Help me polish this": "The weight matrices of some neural nets can be used as inputs and/or outputs for other neural nets. This could help to scale such approaches. Our goal is to open a discussion on this topic, starting with recurrent neural networks for character-level language modelling. Our fast weight version uses a recurrent neural network to parameterise the compressed weights. We present experimental results on the enwik8 dataset.",
        "Rewrite this for me": "Some neural nets use weight matrices from other neural nets as inputs and/or outputs. Using indirect encodings or end-to-end compression on these weight matrices could help to make these approaches more scalable. Our goal is to open a discussion on this topic, starting with recurrent neural networks for character-level language modelling. Our fast weight version uses a recurrent neural network to parameterise the compressed weights. We present experimental results on the enwik8 dataset.",
        "Make this fluent while doing minimal change": "The weight matrices of some neural nets are inputs and/or outputs of other neural nets. Indirect encodings or end-to-end compression of weight matrices could help to scale such approaches. Our goal is to open a discussion on this topic, starting with recurrent neural networks for character-level language modelling whose weight matrices are encoded by the discrete cosine transform. Our fast weight version thereof uses a recurrent neural network to parameterise the compressed weights. We present experimental results on the enwik8 dataset.",
        "Refine this for me please": "The weight matrices of some neural nets can be compressed using the discrete cosine transform. Our goal is to open a discussion on this topic, starting with recurrent neural networks for character-level language modelling. Our fast weight version thereof uses a recurrent neural network to parameterise the compressed weights. We present experimental results on the enwik8 dataset.",
        "Concise this for me and keep all the information": "Some neural nets use weight matrices from other neural nets as inputs and/or outputs. This can cause problems when trying to scale the neural nets. Our goal is to discuss possible solutions to this problem, starting with recurrent neural networks for character-level language modelling. Our fast weight version uses a recurrent neural network to parameterise the compressed weights. We present experimental results on the enwik8 dataset.",
        "Improve this in GPT way": "The weight matrices of some neural nets can be indirectly encoded or compressed using other neural nets. This could help to scale such approaches. Our goal is to open a discussion on this topic, starting with recurrent neural networks for character-level language modelling. Our fast weight version uses a recurrent neural network to parameterise the compressed weights. We present experimental results on the enwik8 dataset."
    },
    {
        "input": "This paper presents the computational challenge on differential geometry and\ntopology that happened within the ICLR 2021 workshop \"Geometric and Topological\nRepresentation Learning\". The competition asked participants to provide\ncreative contributions to the fields of computational geometry and topology\nthrough the open-source repositories Geomstats and Giotto-TDA. The challenge\nattracted 16 teams in its two month duration. This paper describes the design\nof the challenge and summarizes its main findings.",
        "Revise this with your best effort": "This paper presents the computational challenge on differential geometry and topology that happened within the ICLR 2021 workshop \"Geometric and Topological Representation Learning\". The competition asked participants to provide creative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two month duration. This paper describes the design of the challenge and summarizes its main findings.",
        "Help me polish this": "This paper presents the computational challenge on differential geometry and topology that happened within the ICLR 2021 workshop \"Geometric and Topological Representation Learning\". The competition asked participants to provide creative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two month duration. This paper describes the design of the challenge and summarizes its main findings.",
        "Rewrite this for me": "The ICLR 2021 workshop \"Geometric and Topological Representation Learning\" presented a computational challenge in differential geometry and topology. The competition asked participants to contribute to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.",
        "Make this fluent while doing minimal change": "This paper presents the computational challenge on differential geometry and topology that happened at the ICLR 2021 workshop \"Geometric and Topological Representation Learning\". The competition asked participants to provide creative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two month duration. This paper describes the design of the challenge and summarizes its main findings.",
        "Refine this for me please": "This paper presents the computational challenge on differential geometry and topology that happened within the ICLR 2021 workshop \"Geometric and Topological Representation Learning\". The competition asked participants to provide creative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two month duration. This paper describes the design of the challenge and summarizes its main findings.",
        "Concise this for me and keep all the information": "This paper presents the computational challenge on differential geometry and topology that happened within the ICLR 2021 workshop \"Geometric and Topological Representation Learning\". The competition asked participants to provide creative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two month duration.",
        "Improve this in GPT way": "This paper presents the computational challenge on differential geometry and topology that happened within the ICLR 2021 workshop \"Geometric and Topological Representation Learning\". The competition asked participants to provide creative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two month duration. This paper describes the design of the challenge and summarizes its main findings."
    },
    {
        "input": "Training time budget and size of the dataset are among the factors affecting\nthe performance of a Deep Neural Network (DNN). This paper shows that Neural\nArchitecture Search (NAS), Hyper Parameters Optimization (HPO), and Data\nAugmentation help DNNs perform much better while these two factors are limited.\nHowever, searching for an optimal architecture and the best hyperparameter\nvalues besides a good combination of data augmentation techniques under low\nresources requires many experiments. We present our approach to achieving such\na goal in three steps: reducing training epoch time by compressing the model\nwhile maintaining the performance compared to the original model, preventing\nmodel overfitting when the dataset is small, and performing the hyperparameter\ntuning. We used NOMAD, which is a blackbox optimization software based on a\nderivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of\n86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware\nEfficient Training (HAET) Challenge and won second place in the competition.\nThe competition results can be found at haet2021.github.io/challenge and our\nsource code can be found at github.com/DouniaLakhmiri/ICLR\\_HAET2021.",
        "Revise this with your best effort": "Training time budget and size of the dataset are among the factors affecting the performance of a Deep Neural Network (DNN). This paper shows that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation help DNNs perform much better while these two factors are limited. However, searching for an optimal architecture and the best hyperparameter values besides a good combination of data augmentation techniques under low resources requires many experiments.\n\nWe present our approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. We used NOMAD, which is a blackbox optimization software based on a derivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of 86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition.\n\nThe competition results can be found at haet2021.github.io/challenge and our source code can be found at github.com/DouniaLakhmiri/ICLR_HAET2021.",
        "Help me polish this": "Training time budget and size of the dataset are among the factors that affect the performance of a Deep Neural Network (DNN). This paper shows that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation help DNNs perform much better while these two factors are limited. However, searching for an optimal architecture and the best hyperparameter values besides a good combination of data augmentation techniques under low resources requires many experiments. We present our approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. We used NOMAD, which is a blackbox optimization software based on a derivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of 86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition. The competition results can be found at haet2021.github.io/challenge and our source code can be found at github.com/DouniaLakhmiri/ICLR_HAET2021.",
        "Rewrite this for me": "The performance of a Deep Neural Network (DNN) is affected by factors such as the training time budget and the size of the dataset. This paper shows that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation can help DNNs perform much better while these two factors are limited. However, searching for an optimal architecture and the best hyperparameter values besides a good combination of data augmentation techniques under low resources requires many experiments.\n\nWe present our approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. We used NOMAD, which is a blackbox optimization software based on a derivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of 86.0% on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition.",
        "Make this fluent while doing minimal change": "Training time budget and size of the dataset are among the factors that affect the performance of a Deep Neural Network (DNN). This paper demonstrates that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation can help DNNs perform much better while these two factors are limited. However, searching for an optimal architecture and the best hyperparameter values, besides a good combination of data augmentation techniques, under low resources requires many experiments. We present our approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. We used NOMAD, which is a blackbox optimization software based on a derivative-free algorithm, to do NAS and HPO. Our work achieved an accuracy of 86.0% on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition. The competition results can be found at haet2021.github.io/challenge and our source code can be found at github.com/DouniaLakhmiri/ICLR_HAET2021.",
        "Refine this for me please": "The performance of a Deep Neural Network (DNN) is affected by the training time budget and size of the dataset. This paper shows that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation can help DNNs perform much better while these two factors are limited. However, searching for an optimal architecture and the best hyperparameter values besides a good combination of data augmentation techniques under low resources requires many experiments. The authors present their approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. They used NOMAD, which is a blackbox optimization software based on a derivative-free algorithm to do NAS and HPO. Their work achieved an accuracy of 86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition.",
        "Concise this for me and keep all the information": "This paper shows that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation help DNNs perform much better while these two factors are limited. We present our approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. Our work achieved an accuracy of 86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition.",
        "Improve this in GPT way": "We present a three-step approach to achieving good performance on Deep Neural Networks (DNNs) under limited resources. First, we reduce training epoch time by compressing the model while maintaining performance. Second, we prevent model overfitting when the dataset is small. Finally, we perform hyperparameter tuning. Our work achieved an accuracy of 86.0% on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition."
    },
    {
        "input": "In this paper, we cast fair machine learning as invariant machine learning.\nWe first formulate a version of individual fairness that enforces invariance on\ncertain sensitive sets. We then design a transport-based regularizer that\nenforces this version of individual fairness and develop an algorithm to\nminimize the regularizer efficiently. Our theoretical results guarantee the\nproposed approach trains certifiably fair ML models. Finally, in the\nexperimental studies we demonstrate improved fairness metrics in comparison to\nseveral recent fair training procedures on three ML tasks that are susceptible\nto algorithmic bias.",
        "Revise this with your best effort": "In this paper, we explore the idea of fair machine learning through the lens of invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Help me polish this": "In this paper, we formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Rewrite this for me": "In this paper, we explore the idea of fair machine learning through the lens of invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Make this fluent while doing minimal change": "In this paper, we formulate fair machine learning as invariant machine learning. We first develop a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Refine this for me please": "In this paper, we explore the idea of fairness in machine learning from the perspective of invariance. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Concise this for me and keep all the information": "In this paper, we show that fair machine learning can be achieved by enforcing invariance on certain sensitive sets. We design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
        "Improve this in GPT way": "In this paper, we formulate fair machine learning as invariant machine learning.\nWe first develop a version of individual fairness that enforces invariance on\ncertain sensitive sets. We then design a transport-based regularizer that\nenforces this version of individual fairness and develop an algorithm to\nminimize the regularizer efficiently. Our theoretical results guarantee the\nproposed approach trains certifiably fair ML models. Finally, in the\nexperimental studies we demonstrate improved fairness metrics in comparison to\nseveral recent fair training procedures on three ML tasks that are susceptible\nto algorithmic bias."
    },
    {
        "input": "Despite significant advances, continual learning models still suffer from\ncatastrophic forgetting when exposed to incrementally available data from\nnon-stationary distributions. Rehearsal approaches alleviate the problem by\nmaintaining and replaying a small episodic memory of previous samples, often\nimplemented as an array of independent memory slots. In this work, we propose\nto augment such an array with a learnable random graph that captures pairwise\nsimilarities between its samples, and use it not only to learn new tasks but\nalso to guard against forgetting. Empirical results on several benchmark\ndatasets show that our model consistently outperforms recently proposed\nbaselines for task-free continual learning.",
        "Revise this with your best effort": "Despite significant advances, continual learning models that rely on rehearsal approaches still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Help me polish this": "Despite significant advances, continual learning models that rely on rehearsal approaches still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Rewrite this for me": "Despite significant advances, continual learning models that are exposed to incrementally available data from non-stationary distributions still suffer from catastrophic forgetting. Rehearsal approaches that maintain and replay a small episodic memory of previous samples often alleviate the problem. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Make this fluent while doing minimal change": "Despite significant advances, continual learning models that are exposed to incrementally available data from non-stationary distributions still suffer from catastrophic forgetting. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Refine this for me please": "Despite significant advances, continual learning models that are exposed to incrementally available data from non-stationary distributions still suffer from catastrophic forgetting. Rehearsal approaches that maintain and replay a small episodic memory of previous samples often alleviate the problem. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Concise this for me and keep all the information": "Continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
        "Improve this in GPT way": "Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning."
    },
    {
        "input": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural\ntangent kernel and the Laplace kernel include the same set of functions, when\nboth kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we\nprove that the exponential power kernel with a smaller power (making the kernel\nless smooth) leads to a larger RKHS, when it is restricted to the sphere\n$\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Revise this with your best effort": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Help me polish this": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Rewrite this for me": "We show that the set of functions that are reproducing kernel Hilbert spaces (RKHS) for both a deep neural tangent kernel and a Laplace kernel is the same when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. We also show that when an exponential power kernel is restricted to the sphere $\\mathbb{S}^{d-1}$, or defined on the entire $\\mathbb{R}^d$, it has a larger RKHS if the power is smaller (making the kernel less smooth).",
        "Make this fluent while doing minimal change": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Refine this for me please": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Concise this for me and keep all the information": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$.",
        "Improve this in GPT way": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS when it is restricted to the sphere $\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$."
    },
    {
        "input": "Action and observation delays commonly occur in many Reinforcement Learning\napplications, such as remote control scenarios. We study the anatomy of\nrandomly delayed environments, and show that partially resampling trajectory\nfragments in hindsight allows for off-policy multi-step value estimation. We\napply this principle to derive Delay-Correcting Actor-Critic (DCAC), an\nalgorithm based on Soft Actor-Critic with significantly better performance in\nenvironments with delays. This is shown theoretically and also demonstrated\npractically on a delay-augmented version of the MuJoCo continuous control\nbenchmark.",
        "Revise this with your best effort": "Action and observation delays are a common problem in many Reinforcement Learning applications, such as remote control scenarios. We study the effect of these delays, and show that partially resampling trajectory fragments in hindsight allows for more accurate value estimation. We apply this principle to derive the Delay-Correcting Actor-Critic (DCAC) algorithm, which is based on Soft Actor-Critic and performs significantly better in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Help me polish this": "Action and observation delays are a common problem in many Reinforcement Learning applications, such as remote control scenarios. We study the effect of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Rewrite this for me": "We study the effects of randomly delayed environments on reinforcement learning, and show that partially resampling trajectory fragments in hindsight can improve value estimation in these environments. We apply this principle to derive the Delay-Correcting Actor-Critic (DCAC) algorithm, which is based on Soft Actor-Critic and performs significantly better in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Make this fluent while doing minimal change": "Action and observation delays are common in many Reinforcement Learning applications, such as remote control scenarios. We study the effect of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Refine this for me please": "We study the effects of randomly delayed environments on Reinforcement Learning, and show that partially resampling trajectory fragments in hindsight can improve value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic that performs significantly better in environments with delays. This is demonstrated theoretically and also practically on a delay-augmented version of the MuJoCo continuous control benchmark.",
        "Concise this for me and keep all the information": "We study the effects of randomly delayed environments on Reinforcement Learning, and show that partially resampling trajectory fragments in hindsight allows for better off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays.",
        "Improve this in GPT way": "We study the effect of randomly delayed environments on Reinforcement Learning, and show that partially resampling trajectory fragments in hindsight can improve off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark."
    },
    {
        "input": "We demonstrate that differentially private machine learning has not yet\nreached its \"AlexNet moment\" on many canonical vision tasks: linear models\ntrained on handcrafted features significantly outperform end-to-end deep neural\nnetworks for moderate privacy budgets. To exceed the performance of handcrafted\nfeatures, we show that private learning requires either much more private data,\nor access to features learned on public data from a similar domain. Our work\nintroduces simple yet strong baselines for differentially private learning that\ncan inform the evaluation of future progress in this area.",
        "Revise this with your best effort": "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Help me polish this": "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Rewrite this for me": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks, with linear models trained on handcrafted features significantly outperforming end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Make this fluent while doing minimal change": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks, according to a new study. The study found that linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, the study found that private learning requires either much more private data, or access to features learned on public data from a similar domain.",
        "Refine this for me please": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks, where linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "Concise this for me and keep all the information": "Differential private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks. Linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.",
        "Improve this in GPT way": "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area."
    },
    {
        "input": "We develop an algorithm to train individually fair learning-to-rank (LTR)\nmodels. The proposed approach ensures items from minority groups appear\nalongside similar items from majority groups. This notion of fair ranking is\nbased on the definition of individual fairness from supervised learning and is\nmore nuanced than prior fair LTR approaches that simply ensure the ranking\nmodel provides underrepresented items with a basic level of exposure. The crux\nof our method is an optimal transport-based regularizer that enforces\nindividual fairness and an efficient algorithm for optimizing the regularizer.\nWe show that our approach leads to certifiably individually fair LTR models and\ndemonstrate the efficacy of our method on ranking tasks subject to demographic\nbiases.",
        "Revise this with your best effort": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.",
        "Help me polish this": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.",
        "Rewrite this for me": "We develop an algorithm that trains learning-to-rank (LTR) models in a way that is individually fair. This approach ensures that items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The key to our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.",
        "Make this fluent while doing minimal change": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.",
        "Refine this for me please": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.",
        "Concise this for me and keep all the information": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure.",
        "Improve this in GPT way": "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases."
    },
    {
        "input": "We consider the task of enforcing individual fairness in gradient boosting.\nGradient boosting is a popular method for machine learning from tabular data,\nwhich arise often in applications where algorithmic fairness is a concern. At a\nhigh level, our approach is a functional gradient descent on a\n(distributionally) robust loss function that encodes our intuition of\nalgorithmic fairness for the ML task at hand. Unlike prior approaches to\nindividual fairness that only work with smooth ML models, our approach also\nworks with non-smooth models such as decision trees. We show that our algorithm\nconverges globally and generalizes. We also demonstrate the efficacy of our\nalgorithm on three ML problems susceptible to algorithmic bias.",
        "Revise this with your best effort": "We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which often arises in applications where algorithmic fairness is a concern. Our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",
        "Help me polish this": "We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which often arises in applications where algorithmic fairness is a concern. Our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",
        "Rewrite this for me": "We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which often arise in applications where algorithmic fairness is a concern. Our approach is a functional gradient descent on a robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",
        "Make this fluent while doing minimal change": "We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which often arise in applications where algorithmic fairness is a concern. Our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",
        "Refine this for me please": "We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which often arise in applications where algorithmic fairness is a concern. Our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",
        "Concise this for me and keep all the information": "We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which arise often in applications where algorithmic fairness is a concern.\n\nOur approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees.\n\nWe show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.",
        "Improve this in GPT way": "We consider the task of enforcing individual fairness in gradient boosting.\nGradient boosting is a popular method for machine learning from tabular data,\nwhich often arise in applications where algorithmic fairness is a concern. Our approach is a functional gradient descent on a\n(distributionally) robust loss function that encodes our intuition of\nalgorithmic fairness for the ML task at hand. Unlike prior approaches to\nindividual fairness that only work with smooth ML models, our approach also\nworks with non-smooth models such as decision trees. We show that our algorithm\nconverges globally and generalizes. We also demonstrate the efficacy of our\nalgorithm on three ML problems susceptible to algorithmic bias."
    },
    {
        "input": "The amount of data, manpower and capital required to understand, evaluate and\nagree on a group of symptoms for the elementary prognosis of pandemic diseases\nis enormous. In this paper, we present FedPandemic, a novel noise\nimplementation algorithm integrated with cross-device Federated learning for\nElementary symptom prognosis during a pandemic, taking COVID-19 as a case\nstudy. Our results display consistency and enhance robustness in recovering the\ncommon symptoms displayed by the disease, paving a faster and cheaper path\ntowards symptom retrieval while also preserving the privacy of patient's\nsymptoms via Federated learning.",
        "Revise this with your best effort": "The amount of data, manpower and capital required to understand, evaluate and agree on a group of symptoms for the elementary prognosis of pandemic diseases is enormous. In this paper, we present FedPandemic, a novel noise implementation algorithm integrated with cross-device Federated learning for Elementary symptom prognosis during a pandemic, taking COVID-19 as a case study. Our results display consistency and enhance robustness in recovering the common symptoms displayed by the disease, paving a faster and cheaper path towards symptom retrieval while also preserving the privacy of patient's symptoms via Federated learning.",
        "Help me polish this": "The amount of data, manpower and capital required to understand, evaluate and agree on a group of symptoms for the elementary prognosis of pandemic diseases is enormous. In this paper, we present FedPandemic, a novel noise implementation algorithm integrated with cross-device Federated learning for Elementary symptom prognosis during a pandemic, taking COVID-19 as a case study. Our results display consistency and enhance robustness in recovering the common symptoms displayed by the disease, paving a faster and cheaper path towards symptom retrieval while also preserving the privacy of patient's symptoms via Federated learning.",
        "Rewrite this for me": "In this paper, we introduce FedPandemic, a novel noise implementation algorithm integrated with cross-device Federated learning for Elementary symptom prognosis during a pandemic, taking COVID-19 as a case study. Our results display consistency and enhance robustness in recovering the common symptoms displayed by the disease, paving a faster and cheaper path towards symptom retrieval while also preserving the privacy of patient's symptoms via Federated learning.",
        "Make this fluent while doing minimal change": "The amount of data, manpower and capital required to understand, evaluate and agree on a group of symptoms for the elementary prognosis of pandemic diseases is enormous. In this paper, we present FedPandemic, a novel noise implementation algorithm integrated with cross-device Federated learning for Elementary symptom prognosis during a pandemic, taking COVID-19 as a case study. Our results display consistency and enhance robustness in recovering the common symptoms displayed by the disease, paving a faster and cheaper path towards symptom retrieval while also preserving the privacy of patient's symptoms via Federated learning.",
        "Refine this for me please": "In this paper, we present FedPandemic, a novel noise implementation algorithm integrated with cross-device Federated learning for Elementary symptom prognosis during a pandemic, taking COVID-19 as a case study. Our results display consistency and enhance robustness in recovering the common symptoms displayed by the disease, paving a faster and cheaper path towards symptom retrieval while also preserving the privacy of patient's symptoms via Federated learning.",
        "Concise this for me and keep all the information": "In this paper, we present FedPandemic, a novel noise implementation algorithm integrated with cross-device Federated learning for Elementary symptom prognosis during a pandemic, taking COVID-19 as a case study. Our results display consistency and enhance robustness in recovering the common symptoms displayed by the disease, paving a faster and cheaper path towards symptom retrieval while also preserving the privacy of patient's symptoms via Federated learning.",
        "Improve this in GPT way": "The amount of data, manpower and capital required to understand, evaluate and agree on a group of symptoms for the elementary prognosis of pandemic diseases is enormous. In this paper, we present FedPandemic, a novel noise implementation algorithm integrated with cross-device Federated learning for Elementary symptom prognosis during a pandemic, taking COVID-19 as a case study. Our results display consistency and enhance robustness in recovering the common symptoms displayed by the disease, paving a faster and cheaper path towards symptom retrieval while also preserving the privacy of patient's symptoms via Federated learning."
    },
    {
        "input": "Ontologies comprising of concepts, their attributes, and relationships are\nused in many knowledge based AI systems. While there have been efforts towards\npopulating domain specific ontologies, we examine the role of document\nstructure in learning ontological relationships between concepts in any\ndocument corpus. Inspired by ideas from hypernym discovery and explainability,\nour method performs about 15 points more accurate than a stand-alone R-GCN\nmodel for this task.",
        "Revise this with your best effort": "Ontologies that comprise concepts, their attributes, and relationships are used in many knowledge-based AI systems. While there have been efforts to populate domain-specific ontologies, we examine the role of document structure in learning ontological relationships between concepts in any document corpus. Inspired by ideas from hypernym discovery and explainability, our method performs about 15 points more accurately than a stand-alone R-GCN model for this task.",
        "Help me polish this": "Ontologies that comprise concepts, their attributes, and relationships are\nused in many knowledge-based AI systems. While there have been efforts towards\npopulating domain-specific ontologies, we examine the role of document\nstructure in learning ontological relationships between concepts in any\ndocument corpus. Inspired by ideas from hypernym discovery and explainability,\nour method performs about 15 points more accurately than a stand-alone R-GCN\nmodel for this task.",
        "Rewrite this for me": "Ontologies that include concepts, their attributes, and relationships are used in many knowledge-based AI systems. Although there have been efforts to create domain-specific ontologies, we wanted to examine the role of document structure in learning ontological relationships between concepts in any document corpus. We were inspired by ideas from hypernym discovery and explainability when we created our method, which is about 15 points more accurate than a stand-alone R-GCN model for this task.",
        "Make this fluent while doing minimal change": "Ontologies that comprise concepts, their attributes, and relationships are\nused in many knowledge-based AI systems. While there have been efforts towards\npopulating domain-specific ontologies, we examine the role of document\nstructure in learning ontological relationships between concepts in any\ndocument corpus. Inspired by ideas from hypernym discovery and explainability,\nour method performs about 15 points more accurately than a stand-alone R-GCN\nmodel for this task.",
        "Refine this for me please": "Our method for learning ontological relationships between concepts in any document corpus outperforms a stand-alone R-GCN model by 15 points.",
        "Concise this for me and keep all the information": "Ontologies are used in many knowledge based AI systems. They comprise of concepts, their attributes, and relationships. We examine the role of document structure in learning ontological relationships between concepts in any document corpus. Our method is inspired by ideas from hypernym discovery and explainability. It is 15 points more accurate than a stand-alone R-GCN model for this task.",
        "Improve this in GPT way": "We examine the role of document structure in learning ontological relationships between concepts in any document corpus. Inspired by ideas from hypernym discovery and explainability, our method performs about 15 points more accurate than a stand-alone R-GCN model for this task."
    },
    {
        "input": "Imitation learning algorithms learn a policy from demonstrations of expert\nbehavior. We show that, for deterministic experts, imitation learning can be\ndone by reduction to reinforcement learning with a stationary reward. Our\ntheoretical analysis both certifies the recovery of expert reward and bounds\nthe total variation distance between the expert and the imitation learner,\nshowing a link to adversarial imitation learning. We conduct experiments which\nconfirm that our reduction works well in practice for continuous control tasks.",
        "Revise this with your best effort": "Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. We conduct experiments which confirm that our reduction works well in practice for continuous control tasks.",
        "Help me polish this": "Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. We conduct experiments which confirm that our reduction works well in practice for continuous control tasks.",
        "Rewrite this for me": "Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. We conduct experiments which confirm that our reduction works well in practice for continuous control tasks.",
        "Make this fluent while doing minimal change": "Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. We conduct experiments which confirm that our reduction works well in practice for continuous control tasks.",
        "Refine this for me please": "Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. We conduct experiments which confirm that our reduction works well in practice for continuous control tasks.",
        "Concise this for me and keep all the information": "Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning.",
        "Improve this in GPT way": "We show that imitation learning can be reduced to reinforcement learning with a stationary reward, and provide a theoretical analysis which both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner. We conduct experiments which confirm that our reduction works well in practice for continuous control tasks."
    },
    {
        "input": "Black-box optimization formulations for biological sequence design have drawn\nrecent attention due to their promising potential impact on the pharmaceutical\nindustry. In this work, we propose to unify two seemingly distinct worlds:\nlikelihood-free inference and black-box optimization, under one probabilistic\nframework. In tandem, we provide a recipe for constructing various sequence\ndesign methods based on this framework. We show how previous optimization\napproaches can be \"reinvented\" in our framework, and further propose new\nprobabilistic black-box optimization algorithms. Extensive experiments on\nsequence design application illustrate the benefits of the proposed\nmethodology.",
        "Revise this with your best effort": "Black-box optimization formulations for biological sequence design have drawn\nrecent attention due to their promising potential impact on the pharmaceutical\nindustry. In this work, we propose to unify two seemingly distinct worlds:\nlikelihood-free inference and black-box optimization, under one probabilistic\nframework. In tandem, we provide a recipe for constructing various sequence\ndesign methods based on this framework. We show how previous optimization\napproaches can be \"reinvented\" in our framework, and further propose new\nprobabilistic black-box optimization algorithms. Extensive experiments on\nsequence design application illustrate the benefits of the proposed\nmethodology.",
        "Help me polish this": "Black-box optimization formulations for biological sequence design have drawn\nrecent attention due to their promising potential impact on the pharmaceutical\nindustry. In this work, we propose to unify two seemingly distinct worlds:\nlikelihood-free inference and black-box optimization, under one probabilistic\nframework. In tandem, we provide a recipe for constructing various sequence\ndesign methods based on this framework. We show how previous optimization\napproaches can be \"reinvented\" in our framework, and further propose new\nprobabilistic black-box optimization algorithms. Extensive experiments on\nsequence design applications illustrate the benefits of the proposed\nmethodology.",
        "Rewrite this for me": "In this work, we propose a unified framework for likelihood-free inference and black-box optimization. We provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be \"reinvented\" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design applications illustrate the benefits of the proposed methodology.",
        "Make this fluent while doing minimal change": "Black-box optimization formulations for biological sequence design have recently drawn attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be \"reinvented\" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design applications illustrate the benefits of the proposed methodology.",
        "Refine this for me please": "In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be \"reinvented\" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design application illustrate the benefits of the proposed methodology.",
        "Concise this for me and keep all the information": "In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be \"reinvented\" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design application illustrate the benefits of the proposed methodology.",
        "Improve this in GPT way": "We propose a new framework for black-box optimization that unifies likelihood-free inference and black-box optimization. This framework provides a recipe for constructing various sequence design methods, and we show how previous optimization approaches can be \"reinvented\" in our framework. We also propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design applications illustrate the benefits of the proposed methodology."
    },
    {
        "input": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg .",
        "Revise this with your best effort": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques used to train neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods. This may be because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Help me polish this": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness.\n\nWe hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Rewrite this for me": "Deep RL has been receiving more attention recently because it performs well on a variety of control tasks. However, conventional regularization techniques (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods. This may be because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. We find that conventional regularization techniques can often bring large improvements, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization.\n\nIn addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives: sample complexity, reward distribution, weight norm, and noise robustness.\n\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Make this fluent while doing minimal change": "Deep RL has been receiving more attention recently due to its encouraging performance on various control tasks. However, conventional regularization techniques used to train neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find that conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Refine this for me please": "Deep RL has been shown to be effective on a variety of control tasks, but has largely ignored conventional regularization techniques that are common in training neural networks. In this work, we study the effects of regularization on multiple policy optimization algorithms in continuous control tasks. We find that conventional regularization techniques can often improve performance, especially on harder tasks. Our findings are robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg.",
        "Concise this for me and keep all the information": "Deep RL has been receiving more attention recently, but conventional regularization techniques have been largely ignored. This study presents the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. The findings are shown to be robust against training hyperparameter variations. The study also compares these techniques with the more widely used entropy regularization.",
        "Improve this in GPT way": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs.\n\nIn this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best.\n\nWe further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg."
    },
    {
        "input": "Although neural module networks have an architectural bias towards\ncompositionality, they require gold standard layouts to generalize\nsystematically in practice. When instead learning layouts and modules jointly,\ncompositionality does not arise automatically and an explicit pressure is\nnecessary for the emergence of layouts exhibiting the right structure. We\npropose to address this problem using iterated learning, a cognitive science\ntheory of the emergence of compositional languages in nature that has primarily\nbeen applied to simple referential games in machine learning. Considering the\nlayouts of module networks as samples from an emergent language, we use\niterated learning to encourage the development of structure within this\nlanguage. We show that the resulting layouts support systematic generalization\nin neural agents solving the more complex task of visual question-answering.\nOur regularized iterated learning method can outperform baselines without\niterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a\nnew split of the SHAPES dataset we introduce to evaluate systematic\ngeneralization, and on CLOSURE, an extension of CLEVR also designed to test\nsystematic generalization. We demonstrate superior performance in recovering\nground-truth compositional program structure with limited supervision on both\nSHAPES-SyGeT and CLEVR.",
        "Revise this with your best effort": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.",
        "Help me polish this": "Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.",
        "Rewrite this for me": "Although neural module networks have a bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure.\n\nWe propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language.\n\nWe show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization.\n\nWe demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.",
        "Make this fluent while doing minimal change": "Although neural module networks have an architectural bias towards\ncompositionality, they require gold standard layouts to generalize\nsystematically in practice. When instead learning layouts and modules jointly,\ncompositionality does not arise automatically. We\npropose to address this problem using iterated learning, a cognitive science\ntheory of the emergence of compositional languages in nature that has primarily\nbeen applied to simple referential games in machine learning. Considering the\nlayouts of module networks as samples from an emergent language, we use\niterated learning to encourage the development of structure within this\nlanguage. We show that the resulting layouts support systematic generalization\nin neural agents solving the more complex task of visual question-answering.\nOur regularized iterated learning method can outperform baselines without\niterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a\nnew split of the SHAPES dataset we introduce to evaluate systematic\ngeneralization, and on CLOSURE, an extension of CLEVR also designed to test\nsystematic generalization. We demonstrate superior performance in recovering\nground-truth compositional program structure with limited supervision on both\nSHAPES-SyGeT and CLEVR.",
        "Refine this for me please": "We propose a method of using iterated learning to encourage the development of structure within an emergent language, in order to support systematic generalization in neural agents. We show that the resulting layouts can outperform baselines without iterated learning on tasks designed to test systematic generalization.",
        "Concise this for me and keep all the information": "Neural module networks have an architectural bias towards compositionality, but they require gold standard layouts to generalize systematically in practice. When learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization.",
        "Improve this in GPT way": "Although neural module networks have an architectural bias towards\ncompositionality, they require gold standard layouts to generalize\nsystematically in practice. When instead learning layouts and modules jointly,\ncompositionality does not arise automatically and an explicit pressure is\nnecessary for the emergence of layouts exhibiting the right structure. We\npropose to address this problem using iterated learning, a cognitive science\ntheory of the emergence of compositional languages in nature that has primarily\nbeen applied to simple referential games in machine learning. Considering the\nlayouts of module networks as samples from an emergent language, we use\niterated learning to encourage the development of structure within this\nlanguage. We show that the resulting layouts support systematic generalization\nin neural agents solving the more complex task of visual question-answering.\nOur regularized iterated learning method can outperform baselines without\niterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a\nnew split of the SHAPES dataset we introduce to evaluate systematic\ngeneralization, and on CLOSURE, an extension of CLEVR also designed to test\nsystematic generalization. We demonstrate superior performance in recovering\nground-truth compositional program structure with limited supervision on both\nSHAPES-SyGeT and CLEVR."
    },
    {
        "input": "Knowledge Distillation (KD) is a widely used technique to transfer knowledge\nfrom pre-trained teacher models to (usually more lightweight) student models.\nHowever, in certain situations, this technique is more of a curse than a\nblessing. For instance, KD poses a potential risk of exposing intellectual\nproperties (IPs): even if a trained machine learning model is released in\n'black boxes' (e.g., as executable software or APIs without open-sourcing\ncode), it can still be replicated by KD through imitating input-output\nbehaviors. To prevent this unwanted effect of KD, this paper introduces and\ninvestigates a concept called Nasty Teacher: a specially trained teacher\nnetwork that yields nearly the same performance as a normal one, but would\nsignificantly degrade the performance of student models learned by imitating\nit. We propose a simple yet effective algorithm to build the nasty teacher,\ncalled self-undermining knowledge distillation. Specifically, we aim to\nmaximize the difference between the output of the nasty teacher and a normal\npre-trained network. Extensive experiments on several datasets demonstrate that\nour method is effective on both standard KD and data-free KD, providing the\ndesirable KD-immunity to model owners for the first time. We hope our\npreliminary study can draw more awareness and interest in this new practical\nproblem of both social and legal importance.",
        "Revise this with your best effort": "Knowledge Distillation (KD) is a technique used to transfer knowledge from pre-trained teacher models to (usually more lightweight) student models. However, in certain situations, this technique can be more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in 'black boxes' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called self-undermining knowledge distillation. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance.",
        "Help me polish this": "Knowledge Distillation (KD) is a widely used technique to transfer knowledge from pre-trained teacher models to (usually more lightweight) student models. However, in certain situations, this technique is more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in 'black boxes' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called self-undermining knowledge distillation. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance.",
        "Rewrite this for me": "Knowledge Distillation (KD) is a technique used to transfer knowledge from pre-trained teacher models to student models. However, in certain situations, this technique can be more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in 'black boxes' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called self-undermining knowledge distillation. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance.",
        "Make this fluent while doing minimal change": "Knowledge Distillation (KD) is a technique used to transfer knowledge from pre-trained teacher models to (usually more lightweight) student models. However, in certain situations, this technique can be more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in 'black boxes' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called self-undermining knowledge distillation. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance.",
        "Refine this for me please": "Knowledge Distillation (KD) is a technique used to transfer knowledge from pre-trained teacher models to student models. However, this technique can pose a risk of exposing intellectual properties (IPs). To prevent this, a concept called Nasty Teacher has been introduced. This is a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it.",
        "Concise this for me and keep all the information": "Knowledge Distillation (KD) is a technique used to transfer knowledge from pre-trained teacher models to student models. However, KD poses a potential risk of exposing intellectual properties (IPs). To prevent this, this paper introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called self-undermining knowledge distillation. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time.",
        "Improve this in GPT way": "Knowledge Distillation (KD) is a widely used technique to transfer knowledge from pre-trained teacher models to (usually more lightweight) student models. However, in certain situations, this technique is more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in 'black boxes' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors.\n\nTo prevent this unwanted effect of KD, this paper introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called self-undermining knowledge distillation. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network.\n\nExtensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance."
    },
    {
        "input": "To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions.",
        "Revise this with your best effort": "To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions.",
        "Help me polish this": "To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions.",
        "Rewrite this for me": "Differentiable probabilistic models can output a variety of explanations for a single input due to the lack of constraints placed on the explanation. The original CLUE approach generates Counterfactual Latent Uncertainty Explanations (CLUEs) to interpret uncertainty estimates. CLUEs indicate one way to change an input, while remaining on the data manifold, such that the model becomes more confident about its prediction. The $\\delta$-CLUE approach returns a set of plausible CLUEs: multiple, diverse inputs that are within a $\\delta$ ball of the original input in latent space, all yielding confident predictions.",
        "Make this fluent while doing minimal change": "To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. $\\delta$-CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions.",
        "Refine this for me please": "The original CLUE approach proposed generating Counterfactual Latent Uncertainty Explanations (CLUEs) in order to interpret uncertainty estimates from differentiable probabilistic models. However, for a single input, such approaches could output a variety of explanations due to the lack of constraints placed on the explanation. The $\\delta$-CLUE approach augments the original CLUE approach by providing a set of plausible CLUEs, rather than a single CLUE. This set of CLUEs consists of multiple, diverse inputs that are within a $\\delta$ ball of the original input in latent space, all yielding confident predictions.",
        "Concise this for me and keep all the information": "To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating Counterfactual Latent Uncertainty Explanations (CLUEs). However, for a single input, such approaches could output a variety of explanations due to the lack of constraints placed on the explanation. Here we augment the original CLUE approach, to provide what we call $\\delta$-CLUE. CLUE indicates one way to change an input, while remaining on the data manifold, such that the model becomes more confident about its prediction. We instead return a set of plausible CLUEs: multiple, diverse inputs that are within a $\\delta$ ball of the original input in latent space, all yielding confident predictions.",
        "Improve this in GPT way": "To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions."
    }
]