[
    {
        "abs": "The title is Theano-based Large-Scale Visual Recognition with Multiple GPUs, start with In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012)\nimplementation and its naive data parallelism on multiple GPUs. Our performance\non 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014)\nrun on 1 GPU. To the best of our knowledge, this is the first open-source\nPython-based AlexNet implementation to-date., write a short concise abstract based on this: \nIn this report, we present a Theano-based implementation of the AlexNet deep learning model for large-scale visual recognition, which achieves comparable performance to the state-of-the-art Caffe library on multiple GPUs. This is the first open-source Python-based AlexNet implementation to date, and demonstrates the potential of Theano for",
        "title": "Theano-based Large-Scale Visual Recognition with Multiple GPUs"
    },
    {
        "abs": "The title is Deep Narrow Boltzmann Machines are Universal Approximators, start with We show that deep narrow Boltzmann machines are universal approximators of\nprobability distributions on the activities of their visible units, provided\nthey have sufficiently many hidden layers, each containing the same number of\nunits as the visible layer. We show that, within certain parameter domains,\ndeep Boltzmann machines can be studied as feedforward networks. We provide\nupper and lower bounds on the sufficient depth and width of universal\napproximators. These results settle various intuitions regarding undirected\nnetworks and, in particular, they show that deep narrow Boltzmann machines are\nat least as compact universal approximators as narrow sigmoid belief networks\nand restricted Boltzmann machines, with respect to the currently available\nbounds for those models., write a short concise abstract based on this:  Deep narrow Boltzmann machines are shown to be universal approximators of probability distributions on the activities of their visible units, provided they have sufficient hidden layers. This result settles intuitions regarding undirected networks and shows that deep narrow Boltzmann machines are at least as compact as other universal approximators.",
        "title": "Deep Narrow Boltzmann Machines are Universal Approximators"
    },
    {
        "abs": "The title is Learning Stochastic Recurrent Networks, start with Leveraging advances in variational inference, we propose to enhance recurrent\nneural networks with latent variables, resulting in Stochastic Recurrent\nNetworks (STORNs). The model i) can be trained with stochastic gradient\nmethods, ii) allows structured and multi-modal conditionals at each time step,\niii) features a reliable estimator of the marginal likelihood and iv) is a\ngeneralisation of deterministic recurrent neural networks. We evaluate the\nmethod on four polyphonic musical data sets and motion capture data., write a short concise abstract based on this: \nLearning Stochastic Recurrent Networks (STORNs) leverages advances in variational inference to enhance recurrent neural networks with latent variables, leading to a more flexible and reliable model. STORNs can be trained with stochastic gradient methods, allow for structured and multi-modal conditionals at each time step, and feature a reliable estimator of the marginal likelihood. As a generalization",
        "title": "Learning Stochastic Recurrent Networks"
    },
    {
        "abs": "The title is Hot Swapping for Online Adaptation of Optimization Hyperparameters, start with We describe a general framework for online adaptation of optimization\nhyperparameters by `hot swapping' their values during learning. We investigate\nthis approach in the context of adaptive learning rate selection using an\nexplore-exploit strategy from the multi-armed bandit literature. Experiments on\na benchmark neural network show that the hot swapping approach leads to\nconsistently better solutions compared to well-known alternatives such as\nAdaDelta and stochastic gradient with exhaustive hyperparameter search., write a short concise abstract based on this: \nIn this paper, we propose a novel approach for adaptively adjusting optimization hyperparameters during training deep neural networks. Our approach, called \"hot swapping,\" involves dynamically updating the hyperparameters during training, rather than relying on predefined schedules or exhaustive search. We demonstrate the effectiveness of hot swapping in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature",
        "title": "Hot Swapping for Online Adaptation of Optimization Hyperparameters"
    },
    {
        "abs": "The title is Fast Label Embeddings for Extremely Large Output Spaces, start with Many modern multiclass and multilabel problems are characterized by\nincreasingly large output spaces. For these problems, label embeddings have\nbeen shown to be a useful primitive that can improve computational and\nstatistical efficiency. In this work we utilize a correspondence between rank\nconstrained estimation and low dimensional label embeddings that uncovers a\nfast label embedding algorithm which works in both the multiclass and\nmultilabel settings. The result is a randomized algorithm for partial least\nsquares, whose running time is exponentially faster than naive algorithms. We\ndemonstrate our techniques on two large-scale public datasets, from the Large\nScale Hierarchical Text Challenge and the Open Directory Project, where we\nobtain state of the art results., write a short concise abstract based on this:  In this paper, we present a fast label embedding algorithm for large output spaces, which can be used for multiclass and multilabel problems. The algorithm is based on a correspondence between rank constrained estimation and label embeddings, and it is much faster than traditional methods. We demonstrate the effectiveness of our algorithm on two large-scale public datasets, achieving state-of-the-art results.",
        "title": "Fast Label Embeddings for Extremely Large Output Spaces"
    },
    {
        "abs": "The title is Dynamic Adaptive Network Intelligence, start with Accurate representational learning of both the explicit and implicit\nrelationships within data is critical to the ability of machines to perform\nmore complex and abstract reasoning tasks. We describe the efficient weakly\nsupervised learning of such inferences by our Dynamic Adaptive Network\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\nquestion answering tasks in the bAbI dataset that have proved difficult for\ncontemporary approaches to learning representation (Weston et al., 2015)., write a short concise abstract based on this: \nAccurate representation learning is crucial for machines to perform complex reasoning tasks. Our Dynamic Adaptive Network Intelligence (DANI) model efficiently learns such representations through weakly supervised learning, achieving state-of-the-art results on question answering tasks in the bAbI dataset.",
        "title": "Dynamic Adaptive Network Intelligence"
    },
    {
        "abs": "The title is Learning linearly separable features for speech recognition using convolutional neural networks, start with Automatic speech recognition systems usually rely on spectral-based features,\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\nas, speech perception or/and speech production. Recently, convolutional neural\nnetworks have been shown to be able to estimate phoneme conditional\nprobabilities in a completely data-driven manner, i.e. using directly temporal\nraw speech signal as input. This system was shown to yield similar or better\nperformance than HMM/ANN based system on phoneme recognition task and on large\nscale continuous speech recognition task, using less parameters. Motivated by\nthese studies, we investigate the use of simple linear classifier in the\nCNN-based framework. Thus, the network learns linearly separable features from\nraw speech. We show that such system yields similar or better performance than\nMLP based system using cepstral-based features as input., write a short concise abstract based on this: \nIn this paper, we explore the use of convolutional neural networks (CNNs) to learn linearly separable features for speech recognition. By using raw speech as input, the CNN learns to extract features that are directly related to speech perception, rather than relying on prior knowledge of speech perception or production. We show that this approach yields similar or better performance than traditional MLP-based systems using cepstral-based features, and demonstrates the potential of using CNNs for speech recognition tasks.",
        "title": "Learning linearly separable features for speech recognition using convolutional neural networks"
    },
    {
        "abs": "The title is Parallel training of DNNs with Natural Gradient and Parameter Averaging, start with We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine., write a short concise abstract based on this: \nIn this paper, we describe a parallel training framework for deep neural networks (DNNs) using multiple GPU-equipped or multi-core machines. Our approach involves periodically averaging the neural network parameters and redistributing the averaged parameters to the machines for further training. We also introduce an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which improves the convergence of SGD on a single machine and enables the periodic-averaging method to work well in a parallel setting. Our approach allows for efficient and scalable training of DNNs with large amounts of training data using multiple machines without generating excessive network traffic.",
        "title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging"
    },
    {
        "abs": "The title is Geodesics of learned representations, start with We develop a new method for visualizing and refining the invariances of\nlearned representations. Specifically, we test for a general form of\ninvariance, linearization, in which the action of a transformation is confined\nto a low-dimensional subspace. Given two reference images (typically, differing\nby some transformation), we synthesize a sequence of images lying on a path\nbetween them that is of minimal length in the space of the representation (a\n\"representational geodesic\"). If the transformation relating the two reference\nimages is linearized by the representation, this sequence should follow the\ngradual evolution of this transformation. We use this method to assess the\ninvariance properties of a state-of-the-art image classification network and\nfind that geodesics generated for image pairs differing by translation,\nrotation, and dilation do not evolve according to their associated\ntransformations. Our method also suggests a remedy for these failures, and\nfollowing this prescription, we show that the modified representation is able\nto linearize a variety of geometric image transformations., write a short concise abstract based on this:  We present a new method for visualizing and refining the invariances of learned representations in deep neural networks. Our approach involves testing for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. However, we also show that a remedy for these failures exists, and following this prescription, we demonstrate that the modified representation is able to linearize a variety of geometric image transformations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "title": "Geodesics of learned representations"
    },
    {
        "abs": "The title is A Group Theoretic Perspective on Unsupervised Deep Learning, start with Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called {\\em pretraining}: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper., write a short concise abstract based on this:  A new perspective on deep learning is provided by group theory, which helps explain why it works and what representations it captures. The key step of pretraining is connected to the interplay of orbits and stabilizers of group actions, leading to the existence of \"shadow groups\" whose elements serve as good approximations. This connection explains why deep learning networks learn simple features first and why representation complexity increases as layers get deeper.",
        "title": "A Group Theoretic Perspective on Unsupervised Deep Learning"
    },
    {
        "abs": "The title is Stacked What-Where Auto-encoders, start with We present a novel architecture, the \"stacked what-where auto-encoders\"\n(SWWAE), which integrates discriminative and generative pathways and provides a\nunified approach to supervised, semi-supervised and unsupervised learning\nwithout relying on sampling during training. An instantiation of SWWAE uses a\nconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and\nemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the\nreconstruction. The objective function includes reconstruction terms that\ninduce the hidden states in the Deconvnet to be similar to those of the\nConvnet. Each pooling layer produces two sets of variables: the \"what\" which\nare fed to the next layer, and its complementary variable \"where\" that are fed\nto the corresponding layer in the generative decoder., write a short concise abstract based on this: \nIn this paper, we propose a novel architecture called Stacked What-Where Autoencoders (SWWAE) that integrates both discriminative and generative pathways. Unlike traditional autoencoders that rely on sampling during training, SWWAE does not require any sampling and can handle supervised, semi-supervised, and unsupervised learning tasks. The proposed architecture uses a Convolutional Neural Network (Convnet) to encode the input and a Deconvolutional Neural Network (Deconvnet) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconv",
        "title": "Stacked What-Where Auto-encoders"
    },
    {
        "abs": "The title is Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison, start with We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks., write a short concise abstract based on this: \nIn this paper, we explore the idea of tailoring word embeddings for a specific bilexical relation. Our approach compresses an existing lexical vector space to create task-specific embeddings that are better predictors for the target relation. We evaluate the effectiveness of this approach through experiments and show that it can improve both the quality and efficiency of lexical prediction tasks.",
        "title": "Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison"
    },
    {
        "abs": "The title is A Generative Model for Deep Convolutional Learning, start with A generative model is developed for deep (multi-layered) convolutional\ndictionary learning. A novel probabilistic pooling operation is integrated into\nthe deep model, yielding efficient bottom-up (pretraining) and top-down\n(refinement) probabilistic learning. Experimental results demonstrate powerful\ncapabilities of the model to learn multi-layer features from images, and\nexcellent classification results are obtained on the MNIST and Caltech 101\ndatasets., write a short concise abstract based on this: \nA novel generative model for deep convolutional learning is proposed, which integrates a probabilistic pooling operation to enable efficient bottom-up and top-down learning. Experimental results show that the model can effectively learn multi-layer features from images and achieve excellent classification results on the MNIST and Caltech 101 datasets.",
        "title": "A Generative Model for Deep Convolutional Learning"
    },
    {
        "abs": "The title is Generating Images from Captions with Attention, start with Motivated by the recent progress in generative models, we introduce a model\nthat generates images from natural language descriptions. The proposed model\niteratively draws patches on a canvas, while attending to the relevant words in\nthe description. After training on Microsoft COCO, we compare our model with\nseveral baseline generative models on image generation and retrieval tasks. We\ndemonstrate that our model produces higher quality samples than other\napproaches and generates images with novel scene compositions corresponding to\npreviously unseen captions in the dataset., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to generating images from natural language descriptions, leveraging recent advances in generative models. Our model iteratively draws patches on a canvas, attending to relevant words in the description. We train the model on Microsoft COCO and compare it to other baseline generative models on image generation and retrieval tasks. Our model produces higher quality samples and generates novel scene compositions corresponding to unseen captions in the dataset.",
        "title": "Generating Images from Captions with Attention"
    },
    {
        "abs": "The title is Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference, start with Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10., write a short concise abstract based on this: \nIn this paper, we propose a Bayesian Convolutional Neural Network (CNN) that leverages Bernoulli approximate variational inference to improve the robustness of CNNs on small datasets. Our approach places a probability distribution over the CNN's kernels, allowing for efficient inference without additional model parameters. We also demonstrate that dropout network training can be viewed as approximate inference in Bayesian neural networks, enabling the use of existing deep learning tools without increasing time complexity. Our experiments show a significant improvement in classification accuracy compared to standard techniques and outperform published state-of-the-art results on the CIFAR-10 dataset.",
        "title": "Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference"
    },
    {
        "abs": "The title is Training CNNs with Low-Rank Filters for Efficient Image Classification, start with We propose a new method for creating computationally efficient convolutional\nneural networks (CNNs) by using low-rank representations of convolutional\nfilters. Rather than approximating filters in previously-trained networks with\nmore efficient versions, we learn a set of small basis filters from scratch;\nduring training, the network learns to combine these basis filters into more\ncomplex filters that are discriminative for image classification. To train such\nnetworks, a novel weight initialization scheme is used. This allows effective\ninitialization of connection weights in convolutional layers composed of groups\nof differently-shaped filters. We validate our approach by applying it to\nseveral existing CNN architectures and training these networks from scratch\nusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or\nhigher accuracy than conventional CNNs with much less compute. Applying our\nmethod to an improved version of VGG-11 network using global max-pooling, we\nachieve comparable validation accuracy using 41% less compute and only 24% of\nthe original VGG-11 model parameters; another variant of our method gives a 1\npercentage point increase in accuracy over our improved VGG-11 model, giving a\ntop-5 center-crop validation accuracy of 89.7% while reducing computation by\n16% relative to the original VGG-11 model. Applying our method to the GoogLeNet\narchitecture for ILSVRC, we achieved comparable accuracy with 26% less compute\nand 41% fewer model parameters. Applying our method to a near state-of-the-art\nnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and\n55% fewer parameters., write a short concise abstract based on this:  We propose a new method for training CNNs using low-rank filters, which reduces computational cost and model parameters without sacrificing accuracy. Our method learns a set of basis filters from scratch and combines them into more complex filters during training, leading to more efficient and accurate CNNs. We validate our approach on several datasets and show that it achieves similar or higher accuracy with much less compute than conventional CNNs.",
        "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification"
    },
    {
        "abs": "The title is A Simple and Efficient Method To Generate Word Sense Representations, start with Distributed representations of words have boosted the performance of many\nNatural Language Processing tasks. However, usually only one representation per\nword is obtained, not acknowledging the fact that some words have multiple\nmeanings. This has a negative effect on the individual word representations and\nthe language model as a whole. In this paper we present a simple model that\nenables recent techniques for building word vectors to represent distinct\nsenses of polysemic words. In our assessment of this model we show that it is\nable to effectively discriminate between words' senses and to do so in a\ncomputationally efficient manner., write a short concise abstract based on this: \nIn this paper, we propose a novel method for generating word sense representations that takes into account the multiple meanings of words, known as polysemy. Our approach leverages recent advances in word embedding techniques to represent distinct senses of polysemic words in a computationally efficient manner. We evaluate the effectiveness of our method through experiments on several language datasets and show that it outperforms existing methods in discriminating between word senses. Our approach has important implications for improving the performance of natural language processing tasks that rely on word sense disambiguation.",
        "title": "A Simple and Efficient Method To Generate Word Sense Representations"
    },
    {
        "abs": "The title is Diverse Embedding Neural Network Language Models, start with We propose Diverse Embedding Neural Network (DENN), a novel architecture for\nlanguage models (LMs). A DENNLM projects the input word history vector onto\nmultiple diverse low-dimensional sub-spaces instead of a single\nhigher-dimensional sub-space as in conventional feed-forward neural network\nLMs. We encourage these sub-spaces to be diverse during network training\nthrough an augmented loss function. Our language modeling experiments on the\nPenn Treebank data set show the performance benefit of using a DENNLM., write a short concise abstract based on this: \nIn this paper, we propose a novel architecture for language models called DENN, which projects input word histories onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space. This encourages the sub-spaces to be diverse during training, leading to improved performance on language modeling tasks. Our experiments on the Penn Treebank data set show the effectiveness of DENN compared to conventional feed-forward",
        "title": "Diverse Embedding Neural Network Language Models"
    },
    {
        "abs": "The title is Representation Learning for cold-start recommendation, start with A standard approach to Collaborative Filtering (CF), i.e. prediction of user\nratings on items, relies on Matrix Factorization techniques. Representations\nfor both users and items are computed from the observed ratings and used for\nprediction. Unfortunatly, these transductive approaches cannot handle the case\nof new users arriving in the system, with no known rating, a problem known as\nuser cold-start. A common approach in this context is to ask these incoming\nusers for a few initialization ratings. This paper presents a model to tackle\nthis twofold problem of (i) finding good questions to ask, (ii) building\nefficient representations from this small amount of information. The model can\nalso be used in a more standard (warm) context. Our approach is evaluated on\nthe classical CF problem and on the cold-start problem on four different\ndatasets showing its ability to improve baseline performance in both cases., write a short concise abstract based on this:  In this paper, we propose a novel approach to address the cold-start problem in recommendation systems, where new users arrive with no known ratings. Our approach leverages representation learning to efficiently build user and item representations from a small number of initialization ratings. We also propose a method to select informative questions to ask incoming users, which can help improve the quality of their ratings. The proposed model can be used in both warm and cold-start contexts and is evaluated on several datasets, demonstrating its effectiveness in improving baseline performance.",
        "title": "Representation Learning for cold-start recommendation"
    },
    {
        "abs": "The title is NICE: Non-linear Independent Components Estimation, start with We propose a deep learning framework for modeling complex high-dimensional\ndensities called Non-linear Independent Component Estimation (NICE). It is\nbased on the idea that a good representation is one in which the data has a\ndistribution that is easy to model. For this purpose, a non-linear\ndeterministic transformation of the data is learned that maps it to a latent\nspace so as to make the transformed data conform to a factorized distribution,\ni.e., resulting in independent latent variables. We parametrize this\ntransformation so that computing the Jacobian determinant and inverse transform\nis trivial, yet we maintain the ability to learn complex non-linear\ntransformations, via a composition of simple building blocks, each based on a\ndeep neural network. The training criterion is simply the exact log-likelihood,\nwhich is tractable. Unbiased ancestral sampling is also easy. We show that this\napproach yields good generative models on four image datasets and can be used\nfor inpainting., write a short concise abstract based on this: \nIn this paper, we propose a deep learning framework called NICE (Non-linear Independent Component Estimation) for modeling complex high-dimensional densities. Our approach is based on the idea that a good representation is one in which the data has a distribution that is easy to model. We learn a non-linear deterministic transformation of the data that maps it to a latent space with independent latent variables, while maintaining the ability to learn complex non-linear transformations via a composition of simple building blocks. We show that our approach yields good generative models on four image datasets and can be used for inpainting. The training criterion is the exact log-likelihood, which is tractable, and unbiased ancestral sampling is also easy.",
        "title": "NICE: Non-linear Independent Components Estimation"
    },
    {
        "abs": "The title is Deep Linear Discriminant Analysis, start with We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns\nlinearly separable latent representations in an end-to-end fashion. Classic LDA\nextracts features which preserve class separability and is used for\ndimensionality reduction for many classification problems. The central idea of\nthis paper is to put LDA on top of a deep neural network. This can be seen as a\nnon-linear extension of classic LDA. Instead of maximizing the likelihood of\ntarget labels for individual samples, we propose an objective function that\npushes the network to produce feature distributions which: (a) have low\nvariance within the same class and (b) high variance between different classes.\nOur objective is derived from the general LDA eigenvalue problem and still\nallows to train with stochastic gradient descent and back-propagation. For\nevaluation we test our approach on three different benchmark datasets (MNIST,\nCIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and\nCIFAR-10 and outperforms a network trained with categorical cross entropy (same\narchitecture) on a supervised setting of STL-10., write a short concise abstract based on this:  Deep Linear Discriminant Analysis (DeepLDA) is introduced, which combines the classic LDA method with deep neural networks to learn linearly separable latent representations in an end-to-end fashion. DeepLDA improves upon classic LDA by maximizing the likelihood of feature distributions that have low variance within classes and high variance between classes, leading to improved classification performance on several benchmark datasets.",
        "title": "Deep Linear Discriminant Analysis"
    },
    {
        "abs": "The title is All you need is a good init, start with Layer-sequential unit-variance (LSUV) initialization - a simple method for\nweight initialization for deep net learning - is proposed. The method consists\nof the two steps. First, pre-initialize weights of each convolution or\ninner-product layer with orthonormal matrices. Second, proceed from the first\nto the final layer, normalizing the variance of the output of each layer to be\nequal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh)\nshow that the proposed initialization leads to learning of very deep nets that\n(i) produces networks with test accuracy better or equal to standard methods\nand (ii) is at least as fast as the complex schemes proposed specifically for\nvery deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava\net al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets\nand the state-of-the-art, or very close to it, is achieved on the MNIST,\nCIFAR-10/100 and ImageNet datasets., write a short concise abstract based on this: \nAll you need is a good init: Layer-sequential unit-variance initialization for deep neural networks.\nIn this paper, the authors propose a simple method for weight initialization in deep neural networks, called Layer-sequential unit-variance (LSUV) initialization. The method consists of two steps: pre-initializing weights with orthonormal matrices, and normalizing the variance of the output of each layer to be equal to one. The authors show that this initialization leads to learning of very deep nets with test accuracy that is competitive with state-of-the-art methods, and is at least as fast as more complex schemes. The performance of the proposed initialization is evaluated on several benchmark datasets, and the authors achieve state-of-the-art or near-state-of-the-",
        "title": "All you need is a good init"
    },
    {
        "abs": "The title is Density Modeling of Images using a Generalized Normalization Transformation, start with We introduce a parametric nonlinear transformation that is well-suited for\nGaussianizing data from natural images. The data are linearly transformed, and\neach component is then normalized by a pooled activity measure, computed by\nexponentiating a weighted sum of rectified and exponentiated components and a\nconstant. We optimize the parameters of the full transformation (linear\ntransform, exponents, weights, constant) over a database of natural images,\ndirectly minimizing the negentropy of the responses. The optimized\ntransformation substantially Gaussianizes the data, achieving a significantly\nsmaller mutual information between transformed components than alternative\nmethods including ICA and radial Gaussianization. The transformation is\ndifferentiable and can be efficiently inverted, and thus induces a density\nmodel on images. We show that samples of this model are visually similar to\nsamples of natural image patches. We demonstrate the use of the model as a\nprior probability density that can be used to remove additive noise. Finally,\nwe show that the transformation can be cascaded, with each layer optimized\nusing the same Gaussianization objective, thus offering an unsupervised method\nof optimizing a deep network architecture., write a short concise abstract based on this:  In this paper, we propose a new method for density modeling of images using a generalized normalization transformation. The transformation is designed to Gaussianize the data, resulting in a significantly smaller mutual information between transformed components compared to alternative methods. The transformation is differentiable and can be efficiently inverted, allowing us to induce a density model on images. We demonstrate the use of the model as a prior probability density to remove additive noise and show that it can be cascaded with other layers to optimize a deep network architecture.",
        "title": "Density Modeling of Images using a Generalized Normalization Transformation"
    },
    {
        "abs": "The title is Flattened Convolutional Neural Networks for Feedforward Acceleration, start with We present flattened convolutional neural networks that are designed for fast\nfeedforward execution. The redundancy of the parameters, especially weights of\nthe convolutional filters in convolutional neural networks has been extensively\nstudied and different heuristics have been proposed to construct a low rank\nbasis of the filters after training. In this work, we train flattened networks\nthat consist of consecutive sequence of one-dimensional filters across all\ndirections in 3D space to obtain comparable performance as conventional\nconvolutional networks. We tested flattened model on different datasets and\nfound that the flattened layer can effectively substitute for the 3D filters\nwithout loss of accuracy. The flattened convolution pipelines provide around\ntwo times speed-up during feedforward pass compared to the baseline model due\nto the significant reduction of learning parameters. Furthermore, the proposed\nmethod does not require efforts in manual tuning or post processing once the\nmodel is trained., write a short concise abstract based on this: \nIn this paper, we propose a new method for accelerating convolutional neural networks (CNNs) called flattened convolutional neural networks (FCNNs). FCNNs are designed to reduce the computational complexity of CNNs by flattening the convolutional layers into one-dimensional filters across all directions in 3D space. We demonstrate that FCNNs can achieve comparable performance to conventional CNNs while providing a significant speed-up during feedforward execution. Our results show that FCNNs can achieve around two times faster execution compared to the baseline model, without requiring manual tuning or post-processing. This work has important implications for deploying CNNs on resource-constrained devices, such as smartphones or embedded systems, where computational resources are limited.",
        "title": "Flattened Convolutional Neural Networks for Feedforward Acceleration"
    },
    {
        "abs": "The title is Purine: A bi-graph based deep learning framework, start with In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration., write a short concise abstract based on this:  In this paper, we introduce Purine, a novel deep learning framework that represents a deep network as a bipartite graph (bi-graph). The bi-graph abstraction enables the use of an event-driven task dispatcher, which simplifies the process of parallelizing the network across multiple GPUs or CPUs. We demonstrate that different parallelization schemes can be implemented by graph composition, eliminating the need for researchers to code for various parallelization schemes. The same dispatcher can be used for solving different graphs, and memory transfers are overlapped with other computations to reduce communication overhead and achieve linear acceleration.",
        "title": "Purine: A bi-graph based deep learning framework"
    },
    {
        "abs": "The title is Variational Recurrent Auto-Encoders, start with In this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state., write a short concise abstract based on this: \nIn this paper, we propose a novel model called Variational Recurrent Autoencoder (VRAE) that combines the strengths of Recurrent Neural Networks (RNNs) and Variational Autoencoders (VAEs). The VRAE model can learn to represent time series data in a compact and meaningful way, and can be used for efficient and large-scale unsupervised learning. Additionally, the VRAE model can make use of unlabeled data to facilitate supervised training of",
        "title": "Variational Recurrent Auto-Encoders"
    },
    {
        "abs": "The title is Word Representations via Gaussian Embedding, start with Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation., write a short concise abstract based on this:  In this paper, we propose a new approach to word embeddings that represents each word as a density in a low-dimensional space. This approach provides several advantages over traditional dot product or cosine similarity-based methods, including better capturing uncertainty and expressing asymmetries more naturally. We present a method for learning these density-based distributed embeddings and demonstrate their effectiveness on various word embedding benchmarks. We also explore the ability of these embeddings to model entailment and other asymmetric relationships, and uncover novel properties of the representation.",
        "title": "Word Representations via Gaussian Embedding"
    },
    {
        "abs": "The title is Training deep neural networks with low precision multiplications, start with Multipliers are the most space and power-hungry arithmetic operators of the\ndigital implementation of deep neural networks. We train a set of\nstate-of-the-art neural networks (Maxout networks) on three benchmark datasets:\nMNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:\nfloating point, fixed point and dynamic fixed point. For each of those datasets\nand for each of those formats, we assess the impact of the precision of the\nmultiplications on the final error after training. We find that very low\nprecision is sufficient not just for running trained networks but also for\ntraining them. For example, it is possible to train Maxout networks with 10\nbits multiplications., write a short concise abstract based on this: \nTraining deep neural networks with low precision multiplications can significantly reduce the memory and computational requirements of the digital implementation. We demonstrate this by training state-of-the-art neural networks on three benchmark datasets using three different formats with varying precision of multiplications. Our results show that very low precision is sufficient for both running and training deep neural networks, with examples of training networks with 10-bit multiplications. This finding has important implications for deploying deep learning models on resource-constrained devices.",
        "title": "Training deep neural networks with low precision multiplications"
    },
    {
        "abs": "The title is Fully Convolutional Multi-Class Multiple Instance Learning, start with Multiple instance learning (MIL) can reduce the need for costly annotation in\ntasks such as semantic segmentation by weakening the required degree of\nsupervision. We propose a novel MIL formulation of multi-class semantic\nsegmentation learning by a fully convolutional network. In this setting, we\nseek to learn a semantic segmentation model from just weak image-level labels.\nThe model is trained end-to-end to jointly optimize the representation while\ndisambiguating the pixel-image label assignment. Fully convolutional training\naccepts inputs of any size, does not need object proposal pre-processing, and\noffers a pixelwise loss map for selecting latent instances. Our multi-class MIL\nloss exploits the further supervision given by images with multiple labels. We\nevaluate this approach through preliminary experiments on the PASCAL VOC\nsegmentation challenge., write a short concise abstract based on this: \nIn this paper, we propose a novel formulation of multi-class multiple instance learning (MIL) for semantic segmentation, which can reduce the need for costly annotation. Our approach uses a fully convolutional network to learn a semantic segmentation model from weak image-level labels, and jointly optimizes the representation while disambiguating the pixel-image label assignment. The fully convolutional training accepts inputs of any size, eliminates the need for object proposal pre-processing, and provides a pixelwise loss map for selecting latent instances. We evaluate our approach on the PASCAL VOC segmentation challenge and show its effectiveness.",
        "title": "Fully Convolutional Multi-Class Multiple Instance Learning"
    },
    {
        "abs": "The title is Learning Compact Convolutional Neural Networks with Nested Dropout, start with Recently, nested dropout was proposed as a method for ordering representation\nunits in autoencoders by their information content, without diminishing\nreconstruction cost. However, it has only been applied to training\nfully-connected autoencoders in an unsupervised setting. We explore the impact\nof nested dropout on the convolutional layers in a CNN trained by\nbackpropagation, investigating whether nested dropout can provide a simple and\nsystematic way to determine the optimal representation size with respect to the\ndesired accuracy and desired task and data complexity., write a short concise abstract based on this: \nNested dropout is a method for ordering representation units in autoencoders based on their information content, without diminishing reconstruction cost. While it has been applied to fully-connected autoencoders in an unsupervised setting, its application to convolutional neural networks (CNNs) has been limited. In this work, we explore the impact of nested dropout on CNNs trained by backpropagation, examining whether it can provide a simple and systematic way to determine the",
        "title": "Learning Compact Convolutional Neural Networks with Nested Dropout"
    },
    {
        "abs": "The title is ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient, start with Stochastic gradient algorithms have been the main focus of large-scale\nlearning problems and they led to important successes in machine learning. The\nconvergence of SGD depends on the careful choice of learning rate and the\namount of the noise in stochastic estimates of the gradients. In this paper, we\npropose a new adaptive learning rate algorithm, which utilizes curvature\ninformation for automatically tuning the learning rates. The information about\nthe element-wise curvature of the loss function is estimated from the local\nstatistics of the stochastic first order gradients. We further propose a new\nvariance reduction technique to speed up the convergence. In our preliminary\nexperiments with deep neural networks, we obtained better performance compared\nto the popular stochastic gradient algorithms., write a short concise abstract based on this: \nIn this paper, we propose a new adaptive learning rate algorithm for stochastic gradient methods, which utilizes curvature information to automatically tune the learning rates. The algorithm estimates the element-wise curvature of the loss function from local statistics of stochastic first-order gradients and proposes a new variance reduction technique to speed up the convergence. Our preliminary experiments with deep neural networks show better performance compared to popular stochastic gradient algorithms.",
        "title": "ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient"
    },
    {
        "abs": "The title is Transformation Properties of Learned Visual Representations, start with When a three-dimensional object moves relative to an observer, a change\noccurs on the observer's image plane and in the visual representation computed\nby a learned model. Starting with the idea that a good visual representation is\none that transforms linearly under scene motions, we show, using the theory of\ngroup representations, that any such representation is equivalent to a\ncombination of the elementary irreducible representations. We derive a striking\nrelationship between irreducibility and the statistical dependency structure of\nthe representation, by showing that under restricted conditions, irreducible\nrepresentations are decorrelated. Under partial observability, as induced by\nthe perspective projection of a scene onto the image plane, the motion group\ndoes not have a linear action on the space of images, so that it becomes\nnecessary to perform inference over a latent representation that does transform\nlinearly. This idea is demonstrated in a model of rotating NORB objects that\nemploys a latent representation of the non-commutative 3D rotation group SO(3)., write a short concise abstract based on this:  When a 3D object moves relative to an observer, the image it projects onto the observer's image plane undergoes a transformation. We investigate the properties of learned visual representations that transform linearly under scene motions, using the theory of group representations. We show that any such representation is equivalent to a combination of elementary irreducible representations, and derive a relationship between irreducibility and the statistical dependency structure of the representation. Under partial observability, we demonstrate the need to perform inference over a latent representation that transforms linearly, and demonstrate this idea in a model of rotating NORB objects using a latent representation of the non-commutative 3D rotation group SO(3).",
        "title": "Transformation Properties of Learned Visual Representations"
    },
    {
        "abs": "The title is Clustering is Efficient for Approximate Maximum Inner Product Search, start with Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise., write a short concise abstract based on this: \nClustering is Efficient for Approximate Maximum Inner Product Search, a new approach for solving approximate Maximum Inner Product Search (MIPS) is proposed based on variants of the k-means clustering algorithm. The proposed method, called spherical k-means, reduces the MIPS problem to a Maximum Cosine Similarity Search (MCSS) and yields higher speedups than current state-of-the-art hashing-based and tree-based methods for the same retrieval precision. Additionally, the proposed method yields more robust retrievals when the query is corrupted by noise.",
        "title": "Clustering is Efficient for Approximate Maximum Inner Product Search"
    },
    {
        "abs": "The title is Importance Weighted Autoencoders, start with The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\nproposed generative model pairing a top-down generative network with a\nbottom-up recognition network which approximates posterior inference. It\ntypically makes strong assumptions about posterior inference, for instance that\nthe posterior distribution is approximately factorial, and that its parameters\ncan be approximated with nonlinear regression from the observations. As we show\nempirically, the VAE objective can lead to overly simplified representations\nwhich fail to use the network's entire modeling capacity. We present the\nimportance weighted autoencoder (IWAE), a generative model with the same\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\nbound derived from importance weighting. In the IWAE, the recognition network\nuses multiple samples to approximate the posterior, giving it increased\nflexibility to model complex posteriors which do not fit the VAE modeling\nassumptions. We show empirically that IWAEs learn richer latent space\nrepresentations than VAEs, leading to improved test log-likelihood on density\nestimation benchmarks., write a short concise abstract based on this: \nThe Importance Weighted Autoencoder (IWAE) is a generative model that improves upon the Variational Autoencoder (VAE) by using a tighter log-likelihood lower bound derived from importance weighting. The IWAE uses multiple samples to approximate the posterior, allowing it to model complex posteriors that do not fit the VAE's assumptions. Empirical results show that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
        "title": "Importance Weighted Autoencoders"
    },
    {
        "abs": "The title is Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets, start with This work investigates how using reduced precision data in Convolutional\nNeural Networks (CNNs) affects network accuracy during classification. More\nspecifically, this study considers networks where each layer may use different\nprecision data. Our key result is the observation that the tolerance of CNNs to\nreduced precision data not only varies across networks, a well established\nobservation, but also within networks. Tuning precision per layer is appealing\nas it could enable energy and performance improvements. In this paper we study\nhow error tolerance across layers varies and propose a method for finding a low\nprecision configuration for a network while maintaining high accuracy. A\ndiverse set of CNNs is analyzed showing that compared to a conventional\nimplementation using a 32-bit floating-point representation for all layers, and\nwith less than 1% loss in relative accuracy, the data footprint required by\nthese networks can be reduced by an average of 74% and up to 92%., write a short concise abstract based on this: \nThis paper investigates the impact of reduced precision data on the accuracy of Convolutional Neural Networks (CNNs). The authors find that the tolerance of CNNs to reduced precision data varies across networks and within networks, and propose a method for finding a low precision configuration that maintains high accuracy. The analysis shows that using reduced precision data can significantly reduce the data footprint required by CNNs, with an average reduction of 74% and up to 92%.",
        "title": "Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets"
    },
    {
        "abs": "The title is Metric learning approach for graph-based label propagation, start with The efficiency of graph-based semi-supervised algorithms depends on the graph\nof instances on which they are applied. The instances are often in a vectorial\nform before a graph linking them is built. The construction of the graph relies\non a metric over the vectorial space that help define the weight of the\nconnection between entities. The classic choice for this metric is usually a\ndistance measure or a similarity measure based on the euclidean norm. We claim\nthat in some cases the euclidean norm on the initial vectorial space might not\nbe the more appropriate to solve the task efficiently. We propose an algorithm\nthat aims at learning the most appropriate vectorial representation for\nbuilding a graph on which the task at hand is solved efficiently., write a short concise abstract based on this:  In this paper, we propose a novel approach to graph-based semi-supervised learning that learns a metric over the vectorial space of instances before constructing the graph. This allows us to choose the most appropriate distance measure or similarity measure for the task at hand, leading to improved efficiency and accuracy. Our approach is based on a novel algorithm that learns the most appropriate vectorial representation for building a graph that enables efficient solution of the task. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms traditional methods in terms of efficiency and accuracy.",
        "title": "Metric learning approach for graph-based label propagation"
    },
    {
        "abs": "The title is Order-Embeddings of Images and Language, start with Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval., write a short concise abstract based on this: \nIn this paper, we propose a new method for modeling the relationships between words, sentences, and images using a partial order structure. We show that this approach improves performance on tasks such as hypernym prediction and image-caption retrieval. Our method represents a significant departure from traditional approaches, which rely on flat representations of visual and semantic information. By explicitly modeling the order in which these elements appear, we are able to capture more complex relationships and improve overall performance.",
        "title": "Order-Embeddings of Images and Language"
    },
    {
        "abs": "The title is Distributional Smoothing with Virtual Adversarial Training, start with We propose local distributional smoothness (LDS), a new notion of smoothness\nfor statistical model that can be used as a regularization term to promote the\nsmoothness of the model distribution. We named the LDS based regularization as\nvirtual adversarial training (VAT). The LDS of a model at an input datapoint is\ndefined as the KL-divergence based robustness of the model distribution against\nlocal perturbation around the datapoint. VAT resembles adversarial training,\nbut distinguishes itself in that it determines the adversarial direction from\nthe model distribution alone without using the label information, making it\napplicable to semi-supervised learning. The computational cost for VAT is\nrelatively low. For neural network, the approximated gradient of the LDS can be\ncomputed with no more than three pairs of forward and back propagations. When\nwe applied our technique to supervised and semi-supervised learning for the\nMNIST dataset, it outperformed all the training methods other than the current\nstate of the art method, which is based on a highly advanced generative model.\nWe also applied our method to SVHN and NORB, and confirmed our method's\nsuperior performance over the current state of the art semi-supervised method\napplied to these datasets., write a short concise abstract based on this: \nIn this paper, we propose a new regularization term called local distributional smoothness (LDS) and apply it to virtual adversarial training (VAT) to improve the smoothness of the model distribution. VAT resembles adversarial training but determines the adversarial direction from the model distribution alone without using label information, making it applicable to semi-supervised learning. We demonstrate the effectiveness of VAT on several benchmark datasets, outperforming state-of-the-art methods in supervised and semi-supervised learning.",
        "title": "Distributional Smoothing with Virtual Adversarial Training"
    },
    {
        "abs": "The title is Training Convolutional Networks with Noisy Labels, start with The availability of large labeled datasets has allowed Convolutional Network\nmodels to achieve impressive recognition results. However, in many settings\nmanual annotation of the data is impractical; instead our data has noisy\nlabels, i.e. there is some freely available label for each image which may or\nmay not be accurate. In this paper, we explore the performance of\ndiscriminatively-trained Convnets when trained on such noisy data. We introduce\nan extra noise layer into the network which adapts the network outputs to match\nthe noisy label distribution. The parameters of this noise layer can be\nestimated as part of the training process and involve simple modifications to\ncurrent training infrastructures for deep networks. We demonstrate the\napproaches on several datasets, including large scale experiments on the\nImageNet classification benchmark., write a short concise abstract based on this:  In this paper, we investigate the performance of training Convolutional Neural Networks (ConvNets) on noisy labeled data. We introduce an additional noise layer in the network to adapt the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated during training, and we demonstrate the approach on several datasets, including large-scale experiments on the ImageNet classification benchmark. Our results show that training ConvNets on noisy data can lead to improved performance compared to training on clean data, and the proposed approach can effectively handle noisy labels.",
        "title": "Training Convolutional Networks with Noisy Labels"
    },
    {
        "abs": "The title is Provable Methods for Training Neural Networks with Sparse Connectivity, start with We provide novel guaranteed approaches for training feedforward neural\nnetworks with sparse connectivity. We leverage on the techniques developed\npreviously for learning linear networks and show that they can also be\neffectively adopted to learn non-linear networks. We operate on the moments\ninvolving label and the score function of the input, and show that their\nfactorization provably yields the weight matrix of the first layer of a deep\nnetwork under mild conditions. In practice, the output of our method can be\nemployed as effective initializers for gradient descent., write a short concise abstract based on this: \nIn this paper, we propose novel methods for training feedforward neural networks with sparse connectivity. By leveraging techniques developed for learning linear networks, we show that these methods can be applied to non-linear networks as well. Our approach involves factorizing moments involving the label and score function of the input, which provably yields the weight matrix of the first layer of a deep network under mild conditions. We demonstrate the effectiveness of our method through practical experiments, showing that the output can be used as effective initializers for gradient descent.",
        "title": "Provable Methods for Training Neural Networks with Sparse Connectivity"
    },
    {
        "abs": "The title is Entity-Augmented Distributional Semantics for Discourse Relations, start with Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked sentences. A more subtle\nchallenge is that it is not enough to represent the meaning of each sentence of\na discourse relation, because the relation may depend on links between\nlower-level elements, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted not only\nfrom the distributional representations of the sentences, but also of their\ncoreferent entity mentions. The resulting system obtains substantial\nimprovements over the previous state-of-the-art in predicting implicit\ndiscourse relations in the Penn Discourse Treebank., write a short concise abstract based on this:  In this paper, we present a novel approach to identifying discourse relations in text, which leverages the power of distributional semantics to capture the meaning of sentences and their relationships. Unlike previous work, we also consider the role of entity mentions in forming discourse relations, using a novel downward compositional pass to compute their representations. The resulting system achieves substantial improvements in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "title": "Entity-Augmented Distributional Semantics for Discourse Relations"
    },
    {
        "abs": "The title is Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations, start with In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage., write a short concise abstract based on this:  In this paper, we propose a new method for inducing semantic representations from text by jointly predicting and factorizing relations. Our model consists of two components: an encoding component that predicts roles given syntactic and lexical features, and a reconstruction component that uses roles to predict argument fillers. When the components are estimated jointly, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with the most accurate role induction methods on English, despite not incorporating any prior linguistic knowledge about the language.",
        "title": "Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations"
    },
    {
        "abs": "The title is Algorithmic Robustness for Learning via $(\u03b5, \u03b3, \u03c4)$-Good Similarity Functions, start with The notion of metric plays a key role in machine learning problems such as\nclassification, clustering or ranking. However, it is worth noting that there\nis a severe lack of theoretical guarantees that can be expected on the\ngeneralization capacity of the classifier associated to a given metric. The\ntheoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions\n(Balcan et al., 2008) has been one of the first attempts to draw a link between\nthe properties of a similarity function and those of a linear classifier making\nuse of it. In this paper, we extend and complete this theory by providing a new\ngeneralization bound for the associated classifier based on the algorithmic\nrobustness framework., write a short concise abstract based on this:  In this paper, we explore the connection between the properties of a similarity function and the generalization capacity of the associated linear classifier. We introduce a new framework for analyzing the robustness of similarity functions, which allows us to derive a generalization bound for the classifier based on the algorithmic robustness of the similarity function. Our results provide a theoretical guarantee on the generalization capacity of the classifier, and demonstrate the importance of considering the robustness of the similarity function in the design of machine learning algorithms.",
        "title": "Algorithmic Robustness for Learning via $(\u03b5, \u03b3, \u03c4)$-Good Similarity Functions"
    },
    {
        "abs": "The title is Modeling Compositionality with Multiplicative Recurrent Neural Networks, start with We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees., write a short concise abstract based on this:  In this paper, we propose a new model for compositional meaning in language, called the multiplicative recurrent neural network (MRNN). We evaluate the MRNN on the task of fine-grained sentiment analysis and show that it performs comparably or better than other state-of-the-art models. Additionally, we demonstrate that the MRNN is a generalization of previous matrix-space models for compositionality, and outperforms these models on a standard sentiment analysis corpus. Finally, we show that the MRNN can be used to perform fine-grained sentiment analysis",
        "title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks"
    },
    {
        "abs": "The title is Explorations on high dimensional landscapes, start with Finding minima of a real valued non-convex function over a high dimensional\nspace is a major challenge in science. We provide evidence that some such\nfunctions that are defined on high dimensional domains have a narrow band of\nvalues whose pre-image contains the bulk of its critical points. This is in\ncontrast with the low dimensional picture in which this band is wide. Our\nsimulations agree with the previous theoretical work on spin glasses that\nproves the existence of such a band when the dimension of the domain tends to\ninfinity. Furthermore our experiments on teacher-student networks with the\nMNIST dataset establish a similar phenomenon in deep networks. We finally\nobserve that both the gradient descent and the stochastic gradient descent\nmethods can reach this level within the same number of steps., write a short concise abstract based on this:  Finding minima of high dimensional non-convex functions is a challenging task in science. Our research shows that some such functions have a narrow band of values whose pre-image contains the bulk of their critical points, in contrast to the low dimensional picture. We provide simulations and experiments on teacher-student networks with the MNIST dataset that support this phenomenon, and show that both gradient descent and stochastic gradient descent methods can reach the minimum within the same number of steps.",
        "title": "Explorations on high dimensional landscapes"
    },
    {
        "abs": "The title is The local low-dimensionality of natural images, start with We develop a new statistical model for photographic images, in which the\nlocal responses of a bank of linear filters are described as jointly Gaussian,\nwith zero mean and a covariance that varies slowly over spatial position. We\noptimize sets of filters so as to minimize the nuclear norms of matrices of\ntheir local activations (i.e., the sum of the singular values), thus\nencouraging a flexible form of sparsity that is not tied to any particular\ndictionary or coordinate system. Filters optimized according to this objective\nare oriented and bandpass, and their responses exhibit substantial local\ncorrelation. We show that images can be reconstructed nearly perfectly from\nestimates of the local filter response covariances alone, and with minimal\ndegradation (either visual or MSE) from low-rank approximations of these\ncovariances. As such, this representation holds much promise for use in\napplications such as denoising, compression, and texture representation, and\nmay form a useful substrate for hierarchical decompositions., write a short concise abstract based on this: \nIn this paper, we propose a new statistical model for photographic images, which represents the local responses of a bank of linear filters as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters to minimize the nuclear norms of their local activations, encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. The resulting filters are oriented and bandpass, and their responses exhibit substantial local correlation. We demonstrate that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, with minimal degradation from low-rank approximations of these covariances. This representation holds much promise for applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarch",
        "title": "The local low-dimensionality of natural images"
    },
    {
        "abs": "The title is Striving for Simplicity: The All Convolutional Net, start with Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches., write a short concise abstract based on this: \nIn this paper, we challenge the conventional wisdom of using max-pooling in convolutional neural networks (CNNs) for object recognition tasks. We find that max-pooling can be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Building on this discovery, we propose a new architecture that consists solely of convolutional layers and achieves competitive or state-of-the-art performance on several datasets. To better understand the features learned by the network, we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs. Our findings demonstrate the simplicity and effectiveness of this new architecture, and highlight the potential for future research in this area.",
        "title": "Striving for Simplicity: The All Convolutional Net"
    },
    {
        "abs": "The title is Learning Activation Functions to Improve Deep Neural Networks, start with Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes., write a short concise abstract based on this: \nLearning Activation Functions to Improve Deep Neural Networks: A novel approach to improve deep neural networks by adaptively learning the activation function for each neuron using gradient descent. The proposed method achieves state-of-the-art performance on various benchmarks, including CIFAR-10, CIFAR-100, and a high-energy physics dataset.",
        "title": "Learning Activation Functions to Improve Deep Neural Networks"
    },
    {
        "abs": "The title is Joint RNN-Based Greedy Parsing and Word Composition, start with This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper., write a short concise abstract based on this: \nThis paper presents a novel approach to greedy parsing and word composition, which leverages the power of neural networks to achieve state-of-the-art performance in parsing and composition tasks. The proposed method combines a greedy parser and a compositional procedure, which are jointly trained and rely on each other for optimal performance. The compositional procedure generates a vector representation that captures both syntactic and semantic information of the input, and the parser uses this representation to generate parse trees. The proposed method achieves competitive performance with existing parsers while offering the advantage of faster processing",
        "title": "Joint RNN-Based Greedy Parsing and Word Composition"
    },
    {
        "abs": "The title is Denoising autoencoder with modulated lateral connections learns invariant representations of natural images, start with Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings., write a short concise abstract based on this: \nA new type of denoising autoencoder (dAE) is proposed that learns more robust and invariant representations of natural images by incorporating modulated lateral connections between the encoder and decoder. The modulated connections allow the higher layers of the dAE to focus on invariant representations, while the additive connections help to preserve the spatial information. The proposed dAE model was tested on real-world images and showed improved denoising performance and the formation of more robust and diverse invariant poolings compared to regular autoencoders.",
        "title": "Denoising autoencoder with modulated lateral connections learns invariant representations of natural images"
    },
    {
        "abs": "The title is Geodesics of learned representations, start with We develop a new method for visualizing and refining the invariances of\nlearned representations. Specifically, we test for a general form of\ninvariance, linearization, in which the action of a transformation is confined\nto a low-dimensional subspace. Given two reference images (typically, differing\nby some transformation), we synthesize a sequence of images lying on a path\nbetween them that is of minimal length in the space of the representation (a\n\"representational geodesic\"). If the transformation relating the two reference\nimages is linearized by the representation, this sequence should follow the\ngradual evolution of this transformation. We use this method to assess the\ninvariance properties of a state-of-the-art image classification network and\nfind that geodesics generated for image pairs differing by translation,\nrotation, and dilation do not evolve according to their associated\ntransformations. Our method also suggests a remedy for these failures, and\nfollowing this prescription, we show that the modified representation is able\nto linearize a variety of geometric image transformations., write a short concise abstract based on this:  We present a new method for visualizing and refining the invariances of learned representations in deep neural networks. Our approach involves testing for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. However, we also show that a remedy for these failures exists, and following this prescription, we demonstrate that the modified representation is able to linearize a variety of geometric image transformations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "title": "Geodesics of learned representations"
    },
    {
        "abs": "The title is Learning Genomic Representations to Predict Clinical Outcomes in Cancer, start with Genomics are rapidly transforming medical practice and basic biomedical\nresearch, providing insights into disease mechanisms and improving therapeutic\nstrategies, particularly in cancer. The ability to predict the future course of\na patient's disease from high-dimensional genomic profiling will be essential\nin realizing the promise of genomic medicine, but presents significant\nchallenges for state-of-the-art survival analysis methods. In this abstract we\npresent an investigation in learning genomic representations with neural\nnetworks to predict patient survival in cancer. We demonstrate the advantages\nof this approach over existing survival analysis methods using brain tumor\ndata., write a short concise abstract based on this:  Learning genomic representations to predict clinical outcomes in cancer is essential for realizing the promise of genomic medicine. While state-of-the-art survival analysis methods are limited in their ability to handle high-dimensional genomic data, neural networks can learn complex patterns in these data to predict patient survival. This approach offers significant advantages over existing methods, including improved accuracy and the ability to handle complex genomic data. By leveraging these advances, we can better understand the underlying mechanisms of cancer and develop more effective therapeutic strateg",
        "title": "Learning Genomic Representations to Predict Clinical Outcomes in Cancer"
    },
    {
        "abs": "The title is A Differentiable Transition Between Additive and Multiplicative Neurons, start with Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. However, this leads to an\nextensive increase in the computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to combine additive and multiplicative neural units in a way that allows for a smooth and differentiable transition between the two. Unlike existing methods that require discrete optimization or fixed assignments of operations, our approach uses a parameterizable transfer function based on non-integer functional iteration. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure, resulting in a more efficient and flexible neural network architecture.",
        "title": "A Differentiable Transition Between Additive and Multiplicative Neurons"
    },
    {
        "abs": "The title is Scale Normalization, start with One of the difficulties of training deep neural networks is caused by\nimproper scaling between layers. Scaling issues introduce exploding / gradient\nproblems, and have typically been addressed by careful scale-preserving\ninitialization. We investigate the value of preserving scale, or isometry,\nbeyond the initial weights. We propose two methods of maintaing isometry, one\nexact and one stochastic. Preliminary experiments show that for both\ndeterminant and scale-normalization effectively speeds up learning. Results\nsuggest that isometry is important in the beginning of learning, and\nmaintaining it leads to faster learning., write a short concise abstract based on this: \nMaintaining scale isometry in deep neural networks during training can significantly improve learning speed, according to new research. The study investigates the importance of preserving scale beyond initial weights and proposes two methods to achieve this. Experimental results show that maintaining isometry in the beginning of learning and throughout the training process leads to faster learning.",
        "title": "Scale Normalization"
    },
    {
        "abs": "The title is Stick-Breaking Variational Autoencoders, start with We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's., write a short concise abstract based on this: \nWe introduce a new framework for Bayesian nonparametric inference in variational autoencoders, called Stick-Breaking Variational Autoencoders (SB-VAE). By extending Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes, we are able to define a SB-VAE with a lat",
        "title": "Stick-Breaking Variational Autoencoders"
    },
    {
        "abs": "The title is Unsupervised Learning with Imbalanced Data via Structure Consolidation Latent Variable Model, start with Unsupervised learning on imbalanced data is challenging because, when given\nimbalanced data, current model is often dominated by the major category and\nignores the categories with small amount of data. We develop a latent variable\nmodel that can cope with imbalanced data by dividing the latent space into a\nshared space and a private space. Based on Gaussian Process Latent Variable\nModels, we propose a new kernel formulation that enables the separation of\nlatent space and derives an efficient variational inference method. The\nperformance of our model is demonstrated with an imbalanced medical image\ndataset., write a short concise abstract based on this: \nUnsupervised learning on imbalanced data is challenging due to the dominance of the majority class, leading to poor performance on minority classes. To address this issue, we propose a latent variable model that divides the latent space into a shared and private space, enabling the model to focus on minority classes. Our approach is based on Gaussian Process Latent Variable Models and derives an efficient variational inference method. We evaluate our model on an imbalanced medical image dataset, demonstrating its effectiveness in handling imbalanced data.",
        "title": "Unsupervised Learning with Imbalanced Data via Structure Consolidation Latent Variable Model"
    },
    {
        "abs": "The title is Generative Adversarial Nets from a Density Ratio Estimation Perspective, start with Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful., write a short concise abstract based on this:  Generative adversarial networks (GANs) have been successful in generating realistic samples, but their objective function has been modified to obtain stronger gradients during training. Our proposed algorithm repeats density ratio estimation and f-divergence minimization, offering a new perspective on GANs and making use of multiple viewpoints from density ratio estimation research.",
        "title": "Generative Adversarial Nets from a Density Ratio Estimation Perspective"
    },
    {
        "abs": "The title is Learning to SMILE(S), start with This paper shows how one can directly apply natural language processing (NLP)\nmethods to classification problems in cheminformatics. Connection between these\nseemingly separate fields is shown by considering standard textual\nrepresentation of compound, SMILES. The problem of activity prediction against\na target protein is considered, which is a crucial part of computer aided drug\ndesign process. Conducted experiments show that this way one can not only\noutrank state of the art results of hand crafted representations but also gets\ndirect structural insights into the way decisions are made., write a short concise abstract based on this: \nThis paper explores the application of natural language processing (NLP) techniques to classification problems in cheminformatics, specifically in the context of activity prediction against a target protein. The authors show that by using standard textual representation of compounds, SMILES, and NLP methods, they can outrank state-of-the-art results of hand-crafted representations and gain direct structural insights into the decision-making process. The results demonstrate the potential of combining NLP and cheminformatics",
        "title": "Learning to SMILE(S)"
    },
    {
        "abs": "The title is Understanding Visual Concepts with Continuation Learning, start with We introduce a neural network architecture and a learning algorithm to\nproduce factorized symbolic representations. We propose to learn these concepts\nby observing consecutive frames, letting all the components of the hidden\nrepresentation except a small discrete set (gating units) be predicted from the\nprevious frame, and let the factors of variation in the next frame be\nrepresented entirely by these discrete gated units (corresponding to symbolic\nrepresentations). We demonstrate the efficacy of our approach on datasets of\nfaces undergoing 3D transformations and Atari 2600 games., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to understanding visual concepts using continuation learning. Our method involves learning a neural network architecture that factorizes symbolic representations into a small set of gating units, which are used to predict the components of the hidden representation from consecutive frames. We demonstrate the effectiveness of our approach on two challenging datasets: faces undergoing 3D transformations and Atari 2600 games. Our results show that our method can learn to recognize and generalize to complex visual concepts",
        "title": "Understanding Visual Concepts with Continuation Learning"
    },
    {
        "abs": "The title is Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond, start with We look at the eigenvalues of the Hessian of a loss function before and after\ntraining. The eigenvalue distribution is seen to be composed of two parts, the\nbulk which is concentrated around zero, and the edges which are scattered away\nfrom zero. We present empirical evidence for the bulk indicating how\nover-parametrized the system is, and for the edges that depend on the input\ndata., write a short concise abstract based on this: \nAbstract: We investigate the eigenvalues of the Hessian matrix of a deep neural network's loss function before and after training. We find that the eigenvalue distribution is composed of two parts: a bulk around zero and edges scattered away from zero. The bulk indicates over-parameterization, while the edges depend on the input data. Our results provide insight into the behavior of deep neural networks during training and their relationship to the underlying data.",
        "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond"
    },
    {
        "abs": "The title is Density Modeling of Images using a Generalized Normalization Transformation, start with We introduce a parametric nonlinear transformation that is well-suited for\nGaussianizing data from natural images. The data are linearly transformed, and\neach component is then normalized by a pooled activity measure, computed by\nexponentiating a weighted sum of rectified and exponentiated components and a\nconstant. We optimize the parameters of the full transformation (linear\ntransform, exponents, weights, constant) over a database of natural images,\ndirectly minimizing the negentropy of the responses. The optimized\ntransformation substantially Gaussianizes the data, achieving a significantly\nsmaller mutual information between transformed components than alternative\nmethods including ICA and radial Gaussianization. The transformation is\ndifferentiable and can be efficiently inverted, and thus induces a density\nmodel on images. We show that samples of this model are visually similar to\nsamples of natural image patches. We demonstrate the use of the model as a\nprior probability density that can be used to remove additive noise. Finally,\nwe show that the transformation can be cascaded, with each layer optimized\nusing the same Gaussianization objective, thus offering an unsupervised method\nof optimizing a deep network architecture., write a short concise abstract based on this:  In this paper, we propose a new method for density modeling of images using a generalized normalization transformation. The transformation is designed to Gaussianize the data, resulting in a significantly smaller mutual information between transformed components compared to alternative methods. The transformation is differentiable and can be efficiently inverted, allowing us to induce a density model on images. We demonstrate the use of the model as a prior probability density to remove additive noise and show that it can be cascaded with other layers to optimize a deep network architecture.",
        "title": "Density Modeling of Images using a Generalized Normalization Transformation"
    },
    {
        "abs": "The title is Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series, start with Approximate variational inference has shown to be a powerful tool for\nmodeling unknown complex probability distributions. Recent advances in the\nfield allow us to learn probabilistic models of sequences that actively exploit\nspatial and temporal structure. We apply a Stochastic Recurrent Network (STORN)\nto learn robot time series data. Our evaluation demonstrates that we can\nrobustly detect anomalies both off- and on-line., write a short concise abstract based on this: \nIn this paper, we propose a novel approach for on-line anomaly detection in high-dimensional time series data using approximate variational inference. Our approach leverages recent advances in probabilistic modeling to learn a Stochastic Recurrent Network (STORN) that actively exploits spatial and temporal structure in the data. We demonstrate the effectiveness of our approach through experiments on real-world robot time",
        "title": "Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series"
    },
    {
        "abs": "The title is Towards Information-Seeking Agents, start with We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired., write a short concise abstract based on this: \nIn this paper, we propose a new approach to training agents to gather information efficiently in a partially-observed environment. We present a collection of tasks that require the agents to search for fragments of information to achieve various goals. We combine deep architectures with reinforcement learning techniques to develop agents that can solve these tasks. We use a combination of extrinsic and intrinsic rewards to shape the behavior of these agents, and demonstrate through empirical results that they learn to search actively and intelligently to reduce their uncertainty and exploit the information they have already acquired.",
        "title": "Towards Information-Seeking Agents"
    },
    {
        "abs": "The title is Improving Neural Language Models with a Continuous Cache, start with We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks., write a short concise abstract based on this: \nIn this paper, we propose an extension to neural network language models that improves their ability to adapt to recent input. Our approach, called a continuous cache, stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is efficient and scalable, and we show that it outperforms recent memory augmented networks on several language model datasets. Our work demonstrates the potential of using external memory in neural network language models, and provides a new perspective on the use of cache models in natural language processing.",
        "title": "Improving Neural Language Models with a Continuous Cache"
    },
    {
        "abs": "The title is Generating Images from Captions with Attention, start with Motivated by the recent progress in generative models, we introduce a model\nthat generates images from natural language descriptions. The proposed model\niteratively draws patches on a canvas, while attending to the relevant words in\nthe description. After training on Microsoft COCO, we compare our model with\nseveral baseline generative models on image generation and retrieval tasks. We\ndemonstrate that our model produces higher quality samples than other\napproaches and generates images with novel scene compositions corresponding to\npreviously unseen captions in the dataset., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to generating images from natural language descriptions, leveraging recent advances in generative models. Our model iteratively draws patches on a canvas, attending to relevant words in the description. We train the model on Microsoft COCO and compare it to other baseline generative models on image generation and retrieval tasks. Our model produces higher quality samples and generates novel scene compositions corresponding to unseen captions in the dataset.",
        "title": "Generating Images from Captions with Attention"
    },
    {
        "abs": "The title is Trace Norm Regularised Deep Multi-Task Learning, start with We propose a framework for training multiple neural networks simultaneously.\nThe parameters from all models are regularised by the tensor trace norm, so\nthat each neural network is encouraged to reuse others' parameters if possible\n-- this is the main motivation behind multi-task learning. In contrast to many\ndeep multi-task learning models, we do not predefine a parameter sharing\nstrategy by specifying which layers have tied parameters. Instead, our\nframework considers sharing for all shareable layers, and the sharing strategy\nis learned in a data-driven way., write a short concise abstract based on this: \nIn this paper, we propose a deep multi-task learning framework that leverages the power of trace norm regularization to encourage models to share parameters. Unlike traditional methods that predefine a sharing strategy, our approach learns the sharing strategy from the data, allowing the models to adapt to different tasks and datasets. By doing so, we can improve the generalization ability of the models and reduce the computational cost of training. Our experiments show that our proposed framework achieves better performance than traditional deep multi-task learning",
        "title": "Trace Norm Regularised Deep Multi-Task Learning"
    },
    {
        "abs": "The title is Sample Efficient Actor-Critic with Experience Replay, start with This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method., write a short concise abstract based on this: \nThis paper presents a novel actor-critic deep reinforcement learning agent that leverages experience replay, truncated importance sampling, stochastic dueling network architectures, and a new trust region policy optimization method to achieve sample efficiency and high performance in challenging environments. The proposed method is tested on various domains, including the discrete 57-game Atari domain and continuous control problems,",
        "title": "Sample Efficient Actor-Critic with Experience Replay"
    },
    {
        "abs": "The title is Song From PI: A Musically Plausible Network for Pop Music Generation, start with We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing., write a short concise abstract based on this: \nIn this paper, we propose a novel framework for generating pop music using a hierarchical Recurrent Neural Network (RNN). The model is designed to capture the structure and composition of pop music, with bottom layers generating melody, and higher levels producing drums and chords. We conduct human studies that show our generated music is preferred over recent methods by Google. We also demonstrate two applications of our framework: neural dancing and karaoke, as well as neural story singing. Our approach has the potential to revolutionize the music industry by enabling the creation of high",
        "title": "Song From PI: A Musically Plausible Network for Pop Music Generation"
    },
    {
        "abs": "The title is Early Methods for Detecting Adversarial Images, start with Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix., write a short concise abstract based on this: \nAbstract: Many machine learning classifiers are vulnerable to adversarial perturbations, which modify an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images, which adversaries must bypass to succeed. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA.",
        "title": "Early Methods for Detecting Adversarial Images"
    },
    {
        "abs": "The title is Training CNNs with Low-Rank Filters for Efficient Image Classification, start with We propose a new method for creating computationally efficient convolutional\nneural networks (CNNs) by using low-rank representations of convolutional\nfilters. Rather than approximating filters in previously-trained networks with\nmore efficient versions, we learn a set of small basis filters from scratch;\nduring training, the network learns to combine these basis filters into more\ncomplex filters that are discriminative for image classification. To train such\nnetworks, a novel weight initialization scheme is used. This allows effective\ninitialization of connection weights in convolutional layers composed of groups\nof differently-shaped filters. We validate our approach by applying it to\nseveral existing CNN architectures and training these networks from scratch\nusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or\nhigher accuracy than conventional CNNs with much less compute. Applying our\nmethod to an improved version of VGG-11 network using global max-pooling, we\nachieve comparable validation accuracy using 41% less compute and only 24% of\nthe original VGG-11 model parameters; another variant of our method gives a 1\npercentage point increase in accuracy over our improved VGG-11 model, giving a\ntop-5 center-crop validation accuracy of 89.7% while reducing computation by\n16% relative to the original VGG-11 model. Applying our method to the GoogLeNet\narchitecture for ILSVRC, we achieved comparable accuracy with 26% less compute\nand 41% fewer model parameters. Applying our method to a near state-of-the-art\nnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and\n55% fewer parameters., write a short concise abstract based on this:  We propose a new method for training CNNs using low-rank filters, which reduces computational cost and model parameters without sacrificing accuracy. Our method learns a set of basis filters from scratch and combines them into more complex filters during training, leading to more efficient and accurate CNNs. We validate our approach on several datasets and show that it achieves similar or higher accuracy with much less compute than conventional CNNs.",
        "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification"
    },
    {
        "abs": "The title is All you need is a good init, start with Layer-sequential unit-variance (LSUV) initialization - a simple method for\nweight initialization for deep net learning - is proposed. The method consists\nof the two steps. First, pre-initialize weights of each convolution or\ninner-product layer with orthonormal matrices. Second, proceed from the first\nto the final layer, normalizing the variance of the output of each layer to be\nequal to one.\n  Experiment with different activation functions (maxout, ReLU-family, tanh)\nshow that the proposed initialization leads to learning of very deep nets that\n(i) produces networks with test accuracy better or equal to standard methods\nand (ii) is at least as fast as the complex schemes proposed specifically for\nvery deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava\net al. (2015)).\n  Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets\nand the state-of-the-art, or very close to it, is achieved on the MNIST,\nCIFAR-10/100 and ImageNet datasets., write a short concise abstract based on this: \nAll you need is a good init: Layer-sequential unit-variance initialization for deep neural networks.\nIn this paper, the authors propose a simple method for weight initialization in deep neural networks, called Layer-sequential unit-variance (LSUV) initialization. The method consists of two steps: pre-initializing weights with orthonormal matrices, and normalizing the variance of the output of each layer to be equal to one. The authors show that this initialization leads to learning of very deep nets with test accuracy that is competitive with state-of-the-art methods, and is at least as fast as more complex schemes. The performance of the proposed initialization is evaluated on several benchmark datasets, and the authors achieve state-of-the-art or near-state-of-the-",
        "title": "All you need is a good init"
    },
    {
        "abs": "The title is Deep Biaffine Attention for Neural Dependency Parsing, start with This paper builds off recent work from Kiperwasser & Goldberg (2016) using\nneural attention in a simple graph-based dependency parser. We use a larger but\nmore thoroughly regularized parser than other recent BiLSTM-based approaches,\nwith biaffine classifiers to predict arcs and labels. Our parser gets state of\nthe art or near state of the art performance on standard treebanks for six\ndifferent languages, achieving 95.7% UAS and 94.1% LAS on the most popular\nEnglish PTB dataset. This makes it the highest-performing graph-based parser on\nthis benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and\n2.2%---and comparable to the highest performing transition-based parser\n(Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show\nwhich hyperparameter choices had a significant effect on parsing accuracy,\nallowing us to achieve large gains over other graph-based approaches., write a short concise abstract based on this:  This paper presents a deep biaffine attention neural dependency parser that achieves state-of-the-art performance on six different languages. The parser uses a larger and more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. The results show that the parser achieves 95.7% UAS and 94.1% LAS on the most popular English PTB dataset, outperforming previous approaches by 1.8% and 2.2%. The paper also discusses which hyperparameter choices had a significant effect on parsing accuracy, allowing for further improvements.",
        "title": "Deep Biaffine Attention for Neural Dependency Parsing"
    },
    {
        "abs": "The title is Dynamic Adaptive Network Intelligence, start with Accurate representational learning of both the explicit and implicit\nrelationships within data is critical to the ability of machines to perform\nmore complex and abstract reasoning tasks. We describe the efficient weakly\nsupervised learning of such inferences by our Dynamic Adaptive Network\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\nquestion answering tasks in the bAbI dataset that have proved difficult for\ncontemporary approaches to learning representation (Weston et al., 2015)., write a short concise abstract based on this: \nAccurate representation learning is crucial for machines to perform complex reasoning tasks. Our Dynamic Adaptive Network Intelligence (DANI) model efficiently learns such representations through weakly supervised learning, achieving state-of-the-art results on question answering tasks in the bAbI dataset.",
        "title": "Dynamic Adaptive Network Intelligence"
    },
    {
        "abs": "The title is DeepSphere: towards an equivariant graph-based spherical CNN, start with Spherical data is found in many applications. By modeling the discretized\nsphere as a graph, we can accommodate non-uniformly distributed, partial, and\nchanging samplings. Moreover, graph convolutions are computationally more\nefficient than spherical convolutions. As equivariance is desired to exploit\nrotational symmetries, we discuss how to approach rotation equivariance using\nthe graph neural network introduced in Defferrard et al. (2016). Experiments\nshow good performance on rotation-invariant learning problems. Code and\nexamples are available at https://github.com/SwissDataScienceCenter/DeepSphere, write a short concise abstract based on this: \nIn this paper, we propose a novel approach to graph-based convolutional neural networks (CNNs) for spherical data, called DeepSphere. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. To ensure equivariance, we discuss how to approach rotation equivar",
        "title": "DeepSphere: towards an equivariant graph-based spherical CNN"
    },
    {
        "abs": "The title is Hardware-oriented Approximation of Convolutional Neural Networks, start with High computational complexity hinders the widespread usage of Convolutional\nNeural Networks (CNNs), especially in mobile devices. Hardware accelerators are\narguably the most promising approach for reducing both execution time and power\nconsumption. One of the most important steps in accelerator development is\nhardware-oriented model approximation. In this paper we present Ristretto, a\nmodel approximation framework that analyzes a given CNN with respect to\nnumerical resolution used in representing weights and outputs of convolutional\nand fully connected layers. Ristretto can condense models by using fixed point\narithmetic and representation instead of floating point. Moreover, Ristretto\nfine-tunes the resulting fixed point network. Given a maximum error tolerance\nof 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit.\nThe code for Ristretto is available., write a short concise abstract based on this:  Hardware-oriented Approximation of Convolutional Neural Networks (CNNs) is crucial for reducing computational complexity and power consumption in mobile devices. Ristretto is a model approximation framework that condenses CNNs using fixed point arithmetic and representation, resulting in 8-bit models of CaffeNet and SqueezeNet with a maximum error tolerance of 1%. The Ristretto code is available for use.",
        "title": "Hardware-oriented Approximation of Convolutional Neural Networks"
    },
    {
        "abs": "The title is A Learned Representation For Artistic Style, start with The diversity of painting styles represents a rich visual vocabulary for the\nconstruction of an image. The degree to which one may learn and parsimoniously\ncapture this visual vocabulary measures our understanding of the higher level\nfeatures of paintings, if not images in general. In this work we investigate\nthe construction of a single, scalable deep network that can parsimoniously\ncapture the artistic style of a diversity of paintings. We demonstrate that\nsuch a network generalizes across a diversity of artistic styles by reducing a\npainting to a point in an embedding space. Importantly, this model permits a\nuser to explore new painting styles by arbitrarily combining the styles learned\nfrom individual paintings. We hope that this work provides a useful step\ntowards building rich models of paintings and offers a window on to the\nstructure of the learned representation of artistic style., write a short concise abstract based on this: \nA deep learning model that can capture the artistic style of a wide range of paintings and generalize across different styles. The model reduces a painting to a point in an embedding space, allowing users to explore new styles by combining the learned styles from individual paintings. This work provides a step towards building rich models of paintings and offers insights into the structure of the learned representation of artistic style.",
        "title": "A Learned Representation For Artistic Style"
    },
    {
        "abs": "The title is A Minimalistic Approach to Sum-Product Network Learning for Real Applications, start with Sum-Product Networks (SPNs) are a class of expressive yet tractable\nhierarchical graphical models. LearnSPN is a structure learning algorithm for\nSPNs that uses hierarchical co-clustering to simultaneously identifying similar\nentities and similar features. The original LearnSPN algorithm assumes that all\nthe variables are discrete and there is no missing data. We introduce a\npractical, simplified version of LearnSPN, MiniSPN, that runs faster and can\nhandle missing data and heterogeneous features common in real applications. We\ndemonstrate the performance of MiniSPN on standard benchmark datasets and on\ntwo datasets from Google's Knowledge Graph exhibiting high missingness rates\nand a mix of discrete and continuous features., write a short concise abstract based on this:  A new algorithm called MiniSPN is proposed to address the limitations of the original LearnSPN algorithm for structure learning in Sum-Product Networks (SPNs). MiniSPN is a simplified version of LearnSPN that can handle missing data and heterogeneous features, making it more practical for real-world applications. The performance of MiniSPN is evaluated on several benchmark datasets and on two Google Knowledge Graph datasets with high missingness rates and mixed discrete and continuous features. The results show that MiniSPN outperforms the original LearnSPN in terms of computational efficiency and accuracy,",
        "title": "A Minimalistic Approach to Sum-Product Network Learning for Real Applications"
    },
    {
        "abs": "The title is SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size, start with Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet, write a short concise abstract based on this: \nSqueezeNet is a deep neural network architecture that achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters and less than 0.5MB model size. This makes it a highly efficient and compact alternative to traditional deep neural networks, with potential applications in areas such as autonomous vehicles and edge computing.",
        "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size"
    },
    {
        "abs": "The title is Query-Reduction Networks for Question Answering, start with In this paper, we study the problem of question answering when reasoning over\nmultiple facts is required. We propose Query-Reduction Network (QRN), a variant\nof Recurrent Neural Network (RNN) that effectively handles both short-term\n(local) and long-term (global) sequential dependencies to reason over multiple\nfacts. QRN considers the context sentences as a sequence of state-changing\ntriggers, and reduces the original query to a more informed query as it\nobserves each trigger (context sentence) through time. Our experiments show\nthat QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and\nin a real goal-oriented dialog dataset. In addition, QRN formulation allows\nparallelization on RNN's time axis, saving an order of magnitude in time\ncomplexity for training and inference., write a short concise abstract based on this:  In this paper, we propose a new approach to question answering called Query-Reduction Networks (QRN) that leverages the power of Recurrent Neural Networks (RNNs) to handle complex, multi-step reasoning tasks. Unlike traditional RNNs, QRNs can efficiently process long-term dependencies by reducing the original query to a more informed query as it observes each context sentence through time. Experimental results show that QRNs achieve state-of-the-art performance in both bAbI QA and dialog tasks, and can even be parallelized on the time axis, making them much faster to train and",
        "title": "Query-Reduction Networks for Question Answering"
    },
    {
        "abs": "The title is Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations, start with We propose a language-agnostic way of automatically generating sets of\nsemantically similar clusters of entities along with sets of \"outlier\"\nelements, which may then be used to perform an intrinsic evaluation of word\nembeddings in the outlier detection task. We used our methodology to create a\ngold-standard dataset, which we call WikiSem500, and evaluated multiple\nstate-of-the-art embeddings. The results show a correlation between performance\non this dataset and performance on sentiment analysis., write a short concise abstract based on this: \nIn this paper, we propose a novel method for generating multilingual clusters of semantically similar entities, along with outliers, to evaluate the performance of word embeddings in the outlier detection task. Our approach is language-agnostic and uses a combination of clustering and dimensionality reduction techniques to create a gold-standard dataset, WikiSem500. We evaluate multiple state-of-the-art embeddings on this dataset and show a",
        "title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations"
    },
    {
        "abs": "The title is Surprisal-Driven Feedback in Recurrent Networks, start with Recurrent neural nets are widely used for predicting temporal data. Their\ninherent deep feedforward structure allows learning complex sequential\npatterns. It is believed that top-down feedback might be an important missing\ningredient which in theory could help disambiguate similar patterns depending\non broader context. In this paper we introduce surprisal-driven recurrent\nnetworks, which take into account past error information when making new\npredictions. This is achieved by continuously monitoring the discrepancy\nbetween most recent predictions and the actual observations. Furthermore, we\nshow that it outperforms other stochastic and fully deterministic approaches on\nenwik8 character level prediction task achieving 1.37 BPC on the test portion\nof the text., write a short concise abstract based on this: \nIn this paper, we propose a new approach to improving the performance of recurrent neural networks (RNNs) on temporal data. By incorporating top-down feedback into the network, we are able to better disambiguate similar patterns and improve overall performance. Our approach, called surprisal-driven recurrent networks, continuously monitors the discrepancy between the network's predictions and the actual observations, and uses this information to make more accurate predictions. We demonstrate the effectiveness of our approach on the enwik8 character level prediction task, achieving a BPC of 1",
        "title": "Surprisal-Driven Feedback in Recurrent Networks"
    },
    {
        "abs": "The title is Mode Regularized Generative Adversarial Networks, start with Although Generative Adversarial Networks achieve state-of-the-art results on\na variety of generative tasks, they are regarded as highly unstable and prone\nto miss modes. We argue that these bad behaviors of GANs are due to the very\nparticular functional shape of the trained discriminators in high dimensional\nspaces, which can easily make training stuck or push probability mass in the\nwrong direction, towards that of higher concentration than that of the data\ngenerating distribution. We introduce several ways of regularizing the\nobjective, which can dramatically stabilize the training of GAN models. We also\nshow that our regularizers can help the fair distribution of probability mass\nacross the modes of the data generating distribution, during the early phases\nof training and thus providing a unified solution to the missing modes problem., write a short concise abstract based on this: \nAlthough Generative Adversarial Networks (GANs) have achieved state-of-the-art results on various generative tasks, they are prone to missing modes during training. We identify the root cause of this issue as the functional shape of the trained discriminators in high-dimensional spaces, which can lead to training instability and biased probability mass. To address this problem, we propose several regularizers that stabilize GAN training and promote fair distribution of probability mass across modes. Our approach provides a unified solution to the missing modes problem and improves the overall performance of GANs.",
        "title": "Mode Regularized Generative Adversarial Networks"
    },
    {
        "abs": "The title is EPOpt: Learning Robust Neural Network Policies Using Model Ensembles, start with Sample complexity and safety are major challenges when learning policies with\nreinforcement learning for real-world tasks, especially when the policies are\nrepresented using rich function approximators like deep neural networks.\nModel-based methods where the real-world target domain is approximated using a\nsimulated source domain provide an avenue to tackle the above challenges by\naugmenting real data with simulated data. However, discrepancies between the\nsimulated source domain and the target domain pose a challenge for simulated\ntraining. We introduce the EPOpt algorithm, which uses an ensemble of simulated\nsource domains and a form of adversarial training to learn policies that are\nrobust and generalize to a broad range of possible target domains, including\nunmodeled effects. Further, the probability distribution over source domains in\nthe ensemble can be adapted using data from target domain and approximate\nBayesian methods, to progressively make it a better approximation. Thus,\nlearning on a model ensemble, along with source domain adaptation, provides the\nbenefit of both robustness and learning/adaptation., write a short concise abstract based on this:  Learning policies for real-world tasks using reinforcement learning can be challenging due to sample complexity and safety issues. Model-based methods that use simulated data can help address these challenges, but the discrepancies between the simulated and real-world domains can still pose a problem. The EPOpt algorithm addresses this by using an ensemble of simulated domains and adversarial training to learn robust policies that generalize to a broad range of target domains, including unmodeled effects. Additionally, the algorithm can adapt the probability distribution over the source domains using data from the target domain, allowing for better approximation. This approach combines the benefits of both robustness and learning/adaptation.",
        "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles"
    },
    {
        "abs": "The title is Diversity Networks: Neural Network Compression Using Determinantal Point Processes, start with We introduce Divnet, a flexible technique for learning networks with diverse\nneurons. Divnet models neuronal diversity by placing a Determinantal Point\nProcess (DPP) over neurons in a given layer. It uses this DPP to select a\nsubset of diverse neurons and subsequently fuses the redundant neurons into the\nselected ones. Compared with previous approaches, Divnet offers a more\nprincipled, flexible technique for capturing neuronal diversity and thus\nimplicitly enforcing regularization. This enables effective auto-tuning of\nnetwork architecture and leads to smaller network sizes without hurting\nperformance. Moreover, through its focus on diversity and neuron fusing, Divnet\nremains compatible with other procedures that seek to reduce memory footprints\nof networks. We present experimental results to corroborate our claims: for\npruning neural networks, Divnet is seen to be notably superior to competing\napproaches., write a short concise abstract based on this: \nTitle: Diversity Networks: Neural Network Compression Using Determinantal Point Processes\nAbstract: We introduce Divnet, a novel technique for learning neural networks with diverse neurons. Divnet leverages Determinantal Point Processes (DPP) to model neuronal diversity and select a subset of diverse neurons. The redundant neurons are then fused into the selected ones, leading to more principled and flexible network compression. Experimental results demonstrate Divnet's superiority in pruning neural networks compared to existing approaches, achieving smaller network sizes without compromising performance. Divnet's focus on diversity and neuron fusing makes it compatible with other memory-reduction techniques, making it a promising",
        "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes"
    },
    {
        "abs": "The title is Metric learning approach for graph-based label propagation, start with The efficiency of graph-based semi-supervised algorithms depends on the graph\nof instances on which they are applied. The instances are often in a vectorial\nform before a graph linking them is built. The construction of the graph relies\non a metric over the vectorial space that help define the weight of the\nconnection between entities. The classic choice for this metric is usually a\ndistance measure or a similarity measure based on the euclidean norm. We claim\nthat in some cases the euclidean norm on the initial vectorial space might not\nbe the more appropriate to solve the task efficiently. We propose an algorithm\nthat aims at learning the most appropriate vectorial representation for\nbuilding a graph on which the task at hand is solved efficiently., write a short concise abstract based on this:  In this paper, we propose a novel approach to graph-based semi-supervised learning that learns a metric over the vectorial space of instances before constructing the graph. This allows us to choose the most appropriate distance measure or similarity measure for the task at hand, leading to improved efficiency and accuracy. Our approach is based on a novel algorithm that learns the most appropriate vectorial representation for building a graph that enables efficient solution of the task. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms traditional methods in terms of efficiency and accuracy.",
        "title": "Metric learning approach for graph-based label propagation"
    },
    {
        "abs": "The title is Reducing Overfitting in Deep Networks by Decorrelating Representations, start with One major challenge in training Deep Neural Networks is preventing\noverfitting. Many techniques such as data augmentation and novel regularizers\nsuch as Dropout have been proposed to prevent overfitting without requiring a\nmassive amount of training data. In this work, we propose a new regularizer\ncalled DeCov which leads to significantly reduced overfitting (as indicated by\nthe difference between train and val performance), and better generalization.\nOur regularizer encourages diverse or non-redundant representations in Deep\nNeural Networks by minimizing the cross-covariance of hidden activations. This\nsimple intuition has been explored in a number of past works but surprisingly\nhas never been applied as a regularizer in supervised learning. Experiments\nacross a range of datasets and network architectures show that this loss always\nreduces overfitting while almost always maintaining or increasing\ngeneralization performance and often improving performance over Dropout., write a short concise abstract based on this: \nOverfitting is a major challenge in training deep neural networks, and existing techniques such as data augmentation and regularizers like Dropout can be insufficient. In this work, we propose a new regularizer called DeCov, which encourages diverse or non-redundant representations in deep neural networks by minimizing the cross-covariance of hidden activations. Our experiments show that DeCov reduces overfitting while maintaining or improving generalization performance compared to Dropout.",
        "title": "Reducing Overfitting in Deep Networks by Decorrelating Representations"
    },
    {
        "abs": "The title is Online Batch Selection for Faster Training of Neural Networks, start with Deep neural networks are commonly trained using stochastic non-convex\noptimization procedures, which are driven by gradient information estimated on\nfractions (batches) of the dataset. While it is commonly accepted that batch\nsize is an important parameter for offline tuning, the benefits of online\nselection of batches remain poorly understood. We investigate online batch\nselection strategies for two state-of-the-art methods of stochastic\ngradient-based optimization, AdaDelta and Adam. As the loss function to be\nminimized for the whole dataset is an aggregation of loss functions of\nindividual datapoints, intuitively, datapoints with the greatest loss should be\nconsidered (selected in a batch) more frequently. However, the limitations of\nthis intuition and the proper control of the selection pressure over time are\nopen questions. We propose a simple strategy where all datapoints are ranked\nw.r.t. their latest known loss value and the probability to be selected decays\nexponentially as a function of rank. Our experimental results on the MNIST\ndataset suggest that selecting batches speeds up both AdaDelta and Adam by a\nfactor of about 5., write a short concise abstract based on this:  Online Batch Selection for Faster Training of Neural Networks. Deep neural networks are trained using stochastic optimization methods, which rely on gradients estimated from fractions of the dataset. While batch size is crucial for offline tuning, the benefits of online batch selection remain unclear. We investigate online batch selection strategies for AdaDelta and Adam, and propose a simple strategy that ranks datapoints based on their loss values and decays the selection probability exponentially with rank. Experimental results on MNIST show that online batch selection speeds up training by a factor of 5.",
        "title": "Online Batch Selection for Faster Training of Neural Networks"
    },
    {
        "abs": "The title is Semi-Supervised Classification with Graph Convolutional Networks, start with We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin., write a short concise abstract based on this:  In this paper, we propose a scalable approach for semi-supervised learning on graph-structured data using graph convolutional networks. Our approach leverages a localized first-order approximation of spectral graph convolutions, which enables efficient and accurate learning of node representations that capture both local graph structure and features of nodes. We evaluate our approach on several datasets and show that it outperforms related methods significantly.",
        "title": "Semi-Supervised Classification with Graph Convolutional Networks"
    },
    {
        "abs": "The title is Energy-based Generative Adversarial Network, start with We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN)\nwhich views the discriminator as an energy function that attributes low\nenergies to the regions near the data manifold and higher energies to other\nregions. Similar to the probabilistic GANs, a generator is seen as being\ntrained to produce contrastive samples with minimal energies, while the\ndiscriminator is trained to assign high energies to these generated samples.\nViewing the discriminator as an energy function allows to use a wide variety of\narchitectures and loss functionals in addition to the usual binary classifier\nwith logistic output. Among them, we show one instantiation of EBGAN framework\nas using an auto-encoder architecture, with the energy being the reconstruction\nerror, in place of the discriminator. We show that this form of EBGAN exhibits\nmore stable behavior than regular GANs during training. We also show that a\nsingle-scale architecture can be trained to generate high-resolution images., write a short concise abstract based on this: \nIn this paper, we introduce the Energy-based Generative Adversarial Network (EBGAN), a new approach to generative adversarial networks (GANs) that views the discriminator as an energy function. This allows for a wider range of architectures and loss functionals, and we demonstrate that using an auto-encoder architecture with the energy being the reconstruction error leads to more stable training. Additionally, we show that a single-scale architecture can be trained to generate high-resolution images.",
        "title": "Energy-based Generative Adversarial Network"
    },
    {
        "abs": "The title is Deep Convolutional Neural Network Design Patterns, start with Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work., write a short concise abstract based on this: \nDeep Convolutional Neural Network Design Patterns: Recent research has led to a surge of new deep learning architectures, but inexperienced practitioners often struggle to choose the best one. This paper aims to bridge this gap by mining recent research to identify underlying principles for designing neural network architectures. We also introduce several innovative architectures, including Fractal of FractalNet, Stagewise Boosting Networks, and Taylor Series Networks. Our code and prototxt files are available for others to build upon.",
        "title": "Deep Convolutional Neural Network Design Patterns"
    },
    {
        "abs": "The title is Bidirectional Attention Flow for Machine Comprehension, start with Machine comprehension (MC), answering a query about a given context\nparagraph, requires modeling complex interactions between the context and the\nquery. Recently, attention mechanisms have been successfully extended to MC.\nTypically these methods use attention to focus on a small portion of the\ncontext and summarize it with a fixed-size vector, couple attentions\ntemporally, and/or often form a uni-directional attention. In this paper we\nintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage\nhierarchical process that represents the context at different levels of\ngranularity and uses bi-directional attention flow mechanism to obtain a\nquery-aware context representation without early summarization. Our\nexperimental evaluations show that our model achieves the state-of-the-art\nresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze\ntest., write a short concise abstract based on this: \nIn this paper, the authors propose a new approach to machine comprehension (MC) called Bi-Directional Attention Flow (BIDAF) network. Unlike traditional methods that use attention to focus on a small portion of the context and summarize it with a fixed-size vector, BIDAF uses a multi-stage hierarchical process to represent the context at different levels of granularity. The bi-directional attention flow mechanism allows the model to obtain a query-aware context representation without early summarization, leading to improved performance in MC tasks. The authors demonstrate this through experimental evaluations on two benchmark datasets, achieving state-of-the-art results",
        "title": "Bidirectional Attention Flow for Machine Comprehension"
    },
    {
        "abs": "The title is Joint Stochastic Approximation learning of Helmholtz Machines, start with Though with progress, model learning and performing posterior inference still\nremains a common challenge for using deep generative models, especially for\nhandling discrete hidden variables. This paper is mainly concerned with\nalgorithms for learning Helmholz machines, which is characterized by pairing\nthe generative model with an auxiliary inference model. A common drawback of\nprevious learning algorithms is that they indirectly optimize some bounds of\nthe targeted marginal log-likelihood. In contrast, we successfully develop a\nnew class of algorithms, based on stochastic approximation (SA) theory of the\nRobbins-Monro type, to directly optimize the marginal log-likelihood and\nsimultaneously minimize the inclusive KL-divergence. The resulting learning\nalgorithm is thus called joint SA (JSA). Moreover, we construct an effective\nMCMC operator for JSA. Our results on the MNIST datasets demonstrate that the\nJSA's performance is consistently superior to that of competing algorithms like\nRWS, for learning a range of difficult models., write a short concise abstract based on this:  Learning of Helmholtz machines, a generative model with discrete hidden variables, remains challenging due to posterior inference. This paper introduces a new class of algorithms, based on stochastic approximation, to directly optimize the marginal log-likelihood and minimize the inclusive KL-divergence. The resulting learning algorithm, called joint stochastic approximation (JSA), is shown to outperform competing methods on the MNIST dataset.",
        "title": "Joint Stochastic Approximation learning of Helmholtz Machines"
    },
    {
        "abs": "The title is On-the-fly Network Pruning for Object Detection, start with Object detection with deep neural networks is often performed by passing a\nfew thousand candidate bounding boxes through a deep neural network for each\nimage. These bounding boxes are highly correlated since they originate from the\nsame image. In this paper we investigate how to exploit feature occurrence at\nthe image scale to prune the neural network which is subsequently applied to\nall bounding boxes. We show that removing units which have near-zero activation\nin the image allows us to significantly reduce the number of parameters in the\nnetwork. Results on the PASCAL 2007 Object Detection Challenge demonstrate that\nup to 40% of units in some fully-connected layers can be entirely eliminated\nwith little change in the detection result., write a short concise abstract based on this: \nIn this paper, we propose a novel approach for object detection using deep neural networks, called On-the-fly Network Pruning. By exploiting the feature occurrence at the image scale, we prune the neural network and significantly reduce the number of parameters, while maintaining the detection result almost unchanged. Our approach can be applied to any object detection algorithm and can be used to reduce the computational cost and memory usage of the network. Experimental results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be eliminated with little change in the detection result.",
        "title": "On-the-fly Network Pruning for Object Detection"
    },
    {
        "abs": "The title is Exponential Machines, start with Modeling interactions between features improves the performance of machine\nlearning solutions in many domains (e.g. recommender systems or sentiment\nanalysis). In this paper, we introduce Exponential Machines (ExM), a predictor\nthat models all interactions of every order. The key idea is to represent an\nexponentially large tensor of parameters in a factorized format called Tensor\nTrain (TT). The Tensor Train format regularizes the model and lets you control\nthe number of underlying parameters. To train the model, we develop a\nstochastic Riemannian optimization procedure, which allows us to fit tensors\nwith 2^160 entries. We show that the model achieves state-of-the-art\nperformance on synthetic data with high-order interactions and that it works on\npar with high-order factorization machines on a recommender system dataset\nMovieLens 100K., write a short concise abstract based on this: \nAbstract: In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. By representing an exponentially large tensor of parameters in a factorized format called Tensor Train (TT), ExM regularizes the model and allows for control over the number of underlying parameters. We develop a stochastic Riemannian optimization procedure to train the model, which enables us to fit tensors with 2^160 entries. ExM achieves state-of-the-art performance on synthetic data with high-order interactions and works on par with high-order factorization machines on a recommender system dataset MovieLens ",
        "title": "Exponential Machines"
    },
    {
        "abs": "The title is Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data, start with We introduce Deep Variational Bayes Filters (DVBF), a new method for\nunsupervised learning and identification of latent Markovian state space\nmodels. Leveraging recent advances in Stochastic Gradient Variational Bayes,\nDVBF can overcome intractable inference distributions via variational\ninference. Thus, it can handle highly nonlinear input data with temporal and\nspatial dependencies such as image sequences without domain knowledge. Our\nexperiments show that enabling backpropagation through transitions enforces\nstate space assumptions and significantly improves information content of the\nlatent embedding. This also enables realistic long-term prediction., write a short concise abstract based on this:  Deep Variational Bayes Filters (DVBF) is a new method for unsupervised learning and identification of latent Markovian state space models from raw data. It leverages recent advances in Stochastic Gradient Variational Bayes to overcome intractable inference distributions and handle highly nonlinear input data with temporal and spatial dependencies. Experimental results show that DVBF can significantly improve the information content of the latent embedding and enable realistic long-term prediction.",
        "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data"
    },
    {
        "abs": "The title is Learning End-to-End Goal-Oriented Dialog, start with Traditional dialog systems used in goal-oriented applications require a lot\nof domain-specific handcrafting, which hinders scaling up to new domains.\nEnd-to-end dialog systems, in which all components are trained from the dialogs\nthemselves, escape this limitation. But the encouraging success recently\nobtained in chit-chat dialog may not carry over to goal-oriented settings. This\npaper proposes a testbed to break down the strengths and shortcomings of\nend-to-end dialog systems in goal-oriented applications. Set in the context of\nrestaurant reservation, our tasks require manipulating sentences and symbols,\nso as to properly conduct conversations, issue API calls and use the outputs of\nsuch calls. We show that an end-to-end dialog system based on Memory Networks\ncan reach promising, yet imperfect, performance and learn to perform\nnon-trivial operations. We confirm those results by comparing our system to a\nhand-crafted slot-filling baseline on data from the second Dialog State\nTracking Challenge (Henderson et al., 2014a). We show similar result patterns\non data extracted from an online concierge service., write a short concise abstract based on this: \nTraditional goal-oriented dialog systems are limited by the need for domain-specific handcrafting, which hinders their ability to scale to new domains. End-to-end dialog systems, which train all components from the dialogs themselves, offer a potential solution. However, recent success in chit-chat dialog may not generalize to goal-oriented settings. This paper proposes a testbed to evaluate the strengths and weaknesses of end-to-end dialog systems in goal-oriented applications, using restaurant reservation as a context. The system based on Memory Networks reaches promising yet imperfect performance and learns to perform non-trivial operations. The results are confirmed by comparing the system to a hand-crafted slot-filling baseline on data from the Dialog State Tracking Challenge. Similar result patterns are observed on data",
        "title": "Learning End-to-End Goal-Oriented Dialog"
    },
    {
        "abs": "The title is Adversarial Training Methods for Semi-Supervised Text Classification, start with Adversarial training provides a means of regularizing supervised learning\nalgorithms while virtual adversarial training is able to extend supervised\nlearning algorithms to the semi-supervised setting. However, both methods\nrequire making small perturbations to numerous entries of the input vector,\nwhich is inappropriate for sparse high-dimensional inputs such as one-hot word\nrepresentations. We extend adversarial and virtual adversarial training to the\ntext domain by applying perturbations to the word embeddings in a recurrent\nneural network rather than to the original input itself. The proposed method\nachieves state of the art results on multiple benchmark semi-supervised and\npurely supervised tasks. We provide visualizations and analysis showing that\nthe learned word embeddings have improved in quality and that while training,\nthe model is less prone to overfitting. Code is available at\nhttps://github.com/tensorflow/models/tree/master/research/adversarial_text., write a short concise abstract based on this: \nAdversarial training is a regularization technique that can be used to improve the performance of supervised learning algorithms. In this paper, we extend adversarial training to the semi-supervised text classification task, where the input data is sparse and high-dimensional. We propose a method that applies perturbations to the word embeddings in a recurrent neural network, rather than to the original input itself. Our approach achieves state-of-the-art results on multiple benchmark tasks and provides visualizations and analysis showing improved quality of the learned word embeddings and reduced overfitting during training.",
        "title": "Adversarial Training Methods for Semi-Supervised Text Classification"
    },
    {
        "abs": "The title is Density estimation using Real NVP, start with Unsupervised learning of probabilistic models is a central yet challenging\nproblem in machine learning. Specifically, designing models with tractable\nlearning, sampling, inference and evaluation is crucial in solving this task.\nWe extend the space of such models using real-valued non-volume preserving\n(real NVP) transformations, a set of powerful invertible and learnable\ntransformations, resulting in an unsupervised learning algorithm with exact\nlog-likelihood computation, exact sampling, exact inference of latent\nvariables, and an interpretable latent space. We demonstrate its ability to\nmodel natural images on four datasets through sampling, log-likelihood\nevaluation and latent variable manipulations., write a short concise abstract based on this: \nUnsupervised learning of probabilistic models is a fundamental problem in machine learning, but designing models with tractable learning, sampling, inference, and evaluation is challenging. We propose a new algorithm that extends the space of probabilistic models using real-valued non-volume preserving (real NVP) transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets",
        "title": "Density estimation using Real NVP"
    },
    {
        "abs": "The title is Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance, start with This paper is focused on studying the view-manifold structure in the feature\nspaces implied by the different layers of Convolutional Neural Networks (CNN).\nThere are several questions that this paper aims to answer: Does the learned\nCNN representation achieve viewpoint invariance? How does it achieve viewpoint\ninvariance? Is it achieved by collapsing the view manifolds, or separating them\nwhile preserving them? At which layer is view invariance achieved? How can the\nstructure of the view manifold at each layer of a deep convolutional neural\nnetwork be quantified experimentally? How does fine-tuning of a pre-trained CNN\non a multi-view dataset affect the representation at each layer of the network?\nIn order to answer these questions we propose a methodology to quantify the\ndeformation and degeneracy of view manifolds in CNN layers. We apply this\nmethodology and report interesting results in this paper that answer the\naforementioned questions., write a short concise abstract based on this: \nThis paper investigates the view-manifold structure in the feature spaces of Convolutional Neural Networks (CNNs) by analyzing the different layers of the network. The authors aim to answer questions such as whether the learned CNN representation achieves viewpoint invariance, how it achieves it, and at which layer it is achieved. They propose a method to quantify the deformation and degeneracy of view manifolds in CNN layers and apply it to report interesting results. The study provides insights into how CNNs achieve view invariance and how fine-tuning a pre-trained CNN on a multi-view dataset affects the representation at each layer of the network.",
        "title": "Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance"
    },
    {
        "abs": "The title is Hadamard Product for Low-rank Bilinear Pooling, start with Bilinear models provide rich representations compared with linear models.\nThey have been applied in various visual tasks, such as object recognition,\nsegmentation, and visual question-answering, to get state-of-the-art\nperformances taking advantage of the expanded representations. However,\nbilinear representations tend to be high-dimensional, limiting the\napplicability to computationally complex tasks. We propose low-rank bilinear\npooling using Hadamard product for an efficient attention mechanism of\nmultimodal learning. We show that our model outperforms compact bilinear\npooling in visual question-answering tasks with the state-of-the-art results on\nthe VQA dataset, having a better parsimonious property., write a short concise abstract based on this: \nBilinear models provide rich representations for visual tasks, but their high dimensionality limits their applicability. We propose a low-rank bilinear pooling method using Hadamard product for efficient attention in multimodal learning. Our model outperforms compact bilinear pooling in visual question-answering tasks, achieving state-of-the-art results on the VQA dataset while maintaining a better parsimonious property.",
        "title": "Hadamard Product for Low-rank Bilinear Pooling"
    },
    {
        "abs": "The title is Reinterpreting Importance-Weighted Autoencoders, start with The standard interpretation of importance-weighted autoencoders is that they\nmaximize a tighter lower bound on the marginal likelihood than the standard\nevidence lower bound. We give an alternate interpretation of this procedure:\nthat it optimizes the standard variational lower bound, but using a more\ncomplex distribution. We formally derive this result, present a tighter lower\nbound, and visualize the implicit importance-weighted distribution., write a short concise abstract based on this: \nThe standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood than the standard evidence lower bound. However, we provide an alternative interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution. We formally derive this result, present a tighter lower bound,",
        "title": "Reinterpreting Importance-Weighted Autoencoders"
    },
    {
        "abs": "The title is A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks, start with We present a generalization bound for feedforward neural networks in terms of\nthe product of the spectral norm of the layers and the Frobenius norm of the\nweights. The generalization bound is derived using a PAC-Bayes analysis., write a short concise abstract based on this: \nIn this paper, we present a new approach to deriving generalization bounds for feedforward neural networks using a PAC-Bayesian analysis. Our approach involves bounding the expected generalization error in terms of the product of the spectral norm of the layers and the Frobenius norm of",
        "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"
    },
    {
        "abs": "The title is Calibrating Energy-based Generative Adversarial Networks, start with In this paper, we propose to equip Generative Adversarial Networks with the\nability to produce direct energy estimates for samples.Specifically, we propose\na flexible adversarial training framework, and prove this framework not only\nensures the generator converges to the true data distribution, but also enables\nthe discriminator to retain the density information at the global optimal. We\nderive the analytic form of the induced solution, and analyze the properties.\nIn order to make the proposed framework trainable in practice, we introduce two\neffective approximation techniques. Empirically, the experiment results closely\nmatch our theoretical analysis, verifying the discriminator is able to recover\nthe energy of data distribution., write a short concise abstract based on this:  In this paper, we propose a new framework for training Generative Adversarial Networks (GANs) that enables the generator to produce direct energy estimates for samples. We prove that this framework ensures the generator converges to the true data distribution and enables the discriminator to retain the density information at the global optimal. We also introduce two effective approximation techniques to make the proposed framework trainable in practice. Our experimental results show that the discriminator is able to recover the energy of the data distribution, demonstrating the effectiveness of our proposed framework.",
        "title": "Calibrating Energy-based Generative Adversarial Networks"
    },
    {
        "abs": "The title is Efficient variational Bayesian neural network ensembles for outlier detection, start with In this work we perform outlier detection using ensembles of neural networks\nobtained by variational approximation of the posterior in a Bayesian neural\nnetwork setting. The variational parameters are obtained by sampling from the\ntrue posterior by gradient descent. We show our outlier detection results are\ncomparable to those obtained using other efficient ensembling methods., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to outlier detection using ensembles of neural networks obtained through variational Bayesian methods. By approximating the true posterior distribution over the neural network weights, we are able to efficiently obtain a large number of samples from the posterior distribution, which are then used to form an ensemble. We demonstrate that our approach is comparable in performance",
        "title": "Efficient variational Bayesian neural network ensembles for outlier detection"
    },
    {
        "abs": "The title is Factorization tricks for LSTM networks, start with We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster to the near state-of the art\nperplexity while using significantly less RNN parameters., write a short concise abstract based on this: \nIn this paper, we present two novel techniques for reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks. The first approach involves factorizing the LSTM matrix into the product of two smaller matrices, while the second approach involves partitioning the LSTM matrix, its inputs, and states into independent groups. Both techniques allow for faster training of large LSTM networks with significantly fewer parameters, bringing them closer to state-",
        "title": "Factorization tricks for LSTM networks"
    },
    {
        "abs": "The title is Exploring loss function topology with cyclical learning rates, start with We present observations and discussion of previously unreported phenomena\ndiscovered while training residual networks. The goal of this work is to better\nunderstand the nature of neural networks through the examination of these new\nempirical results. These behaviors were identified through the application of\nCyclical Learning Rates (CLR) and linear network interpolation. Among these\nbehaviors are counterintuitive increases and decreases in training loss and\ninstances of rapid training. For example, we demonstrate how CLR can produce\ngreater testing accuracy than traditional training despite using large learning\nrates. Files to replicate these results are available at\nhttps://github.com/lnsmith54/exploring-loss, write a short concise abstract based on this: \nAbstract: In this work, we investigate the topology of loss functions in deep neural networks through the application of cyclical learning rates. We observe previously unreported phenomena, including counterintuitive increases and decreases in training loss, and instances of rapid training. Our findings suggest that the traditional view of loss function topology may not be complete, and that the use of cyclical learning rates can reveal new insights into the behavior of neural networks.",
        "title": "Exploring loss function topology with cyclical learning rates"
    },
    {
        "abs": "The title is Changing Model Behavior at Test-Time Using Reinforcement Learning, start with Machine learning models are often used at test-time subject to constraints\nand trade-offs not present at training-time. For example, a computer vision\nmodel operating on an embedded device may need to perform real-time inference,\nor a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a\nmixture-of-experts model and show how to change its test-time resource-usage on\na per-input basis using reinforcement learning. We test our method on a small\nMNIST-based example., write a short concise abstract based on this: \nIn this paper, we propose a novel approach for adapting the behavior of a machine learning model at test-time using reinforcement learning. Our approach allows for changing the model's resource usage on a per-input basis, enabling it to operate more efficiently in real-time applications. We demonstrate the effectiveness of our method on a small MNIST-based example, showing that it can adapt to different input sizes and compute times while maintaining high accuracy. This work has important implications for deploying machine learning models",
        "title": "Changing Model Behavior at Test-Time Using Reinforcement Learning"
    },
    {
        "abs": "The title is Delving into adversarial attacks on deep policies, start with Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples., write a short concise abstract based on this: \nAdversarial attacks on deep reinforcement learning policies have shown to be effective in fooling the agent. In this paper, we investigate the effectiveness of adversarial attacks on deep reinforcement learning policies and compare the results with random noise. We also propose a novel method to reduce the number of times adversarial examples need to be injected for a successful attack. Our results show that adversarial attacks are more effective than random noise and our proposed method can significantly reduce the number of attacks required.",
        "title": "Delving into adversarial attacks on deep policies"
    },
    {
        "abs": "The title is Variational Continual Learning, start with This paper develops variational continual learning (VCL), a simple but\ngeneral framework for continual learning that fuses online variational\ninference (VI) and recent advances in Monte Carlo VI for neural networks. The\nframework can successfully train both deep discriminative models and deep\ngenerative models in complex continual learning settings where existing tasks\nevolve over time and entirely new tasks emerge. Experimental results show that\nVCL outperforms state-of-the-art continual learning methods on a variety of\ntasks, avoiding catastrophic forgetting in a fully automatic way., write a short concise abstract based on this: \nThis paper introduces Variational Continual Learning (VCL), a framework that combines online variational inference and recent advances in Monte Carlo VI for neural networks to successfully train both discriminative and generative models in complex continual learning settings. VCL outperforms state-of-the-art methods in avoiding catastrophic forgetting and can train both deep discriminative models and deep generative models.",
        "title": "Variational Continual Learning"
    },
    {
        "abs": "The title is Nonparametric Neural Networks, start with Automatically determining the optimal size of a neural network for a given\ntask without prior information currently requires an expensive global search\nand training many networks from scratch. In this paper, we address the problem\nof automatically finding a good network size during a single training cycle. We\nintroduce *nonparametric neural networks*, a non-probabilistic framework for\nconducting optimization over all possible network sizes and prove its soundness\nwhen network growth is limited via an L_p penalty. We train networks under this\nframework by continuously adding new units while eliminating redundant units\nvia an L_2 penalty. We employ a novel optimization algorithm, which we term\n*adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising\nresults., write a short concise abstract based on this: \nIn this paper, we propose a new approach to automatically determine the optimal size of a neural network for a given task without prior knowledge of the network size. Our approach, called nonparametric neural networks, is a non-probabilistic framework that optimizes over all possible network sizes and proves to be sound when network growth is limited via an L_p penalty. We train networks using an adaptive optimization algorithm called AdaRad, which continuously adds new units while eliminating redundant units via an L_2 penalty. Our results show promising performance gains compared to training many networks from scratch.",
        "title": "Nonparametric Neural Networks"
    },
    {
        "abs": "The title is Natural Language Inference over Interaction Space, start with Natural Language Inference (NLI) task requires an agent to determine the\nlogical relationship between a natural language premise and a natural language\nhypothesis. We introduce Interactive Inference Network (IIN), a novel class of\nneural network architectures that is able to achieve high-level understanding\nof the sentence pair by hierarchically extracting semantic features from\ninteraction space. We show that an interaction tensor (attention weight)\ncontains semantic information to solve natural language inference, and a denser\ninteraction tensor contains richer semantic information. One instance of such\narchitecture, Densely Interactive Inference Network (DIIN), demonstrates the\nstate-of-the-art performance on large scale NLI copora and large-scale NLI\nalike corpus. It's noteworthy that DIIN achieve a greater than 20% error\nreduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to\nthe strongest published system., write a short concise abstract based on this: \nNatural Language Inference (NLI) is a fundamental task in natural language processing (NLP) that requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. In this paper, we introduce Interactive Inference Network (IIN), a novel class of neural network architectures that can achieve high-level understanding of sentence pairs by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve NLI, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on",
        "title": "Natural Language Inference over Interaction Space"
    },
    {
        "abs": "The title is Provably Minimally-Distorted Adversarial Examples, start with The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken. We propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2., write a short concise abstract based on this: \nThe ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples, but most of these have been quickly shown to be vulnerable to future attacks. We propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples, given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct",
        "title": "Provably Minimally-Distorted Adversarial Examples"
    },
    {
        "abs": "The title is Stick-Breaking Variational Autoencoders, start with We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's., write a short concise abstract based on this: \nWe introduce a new framework for Bayesian nonparametric inference in variational autoencoders, called Stick-Breaking Variational Autoencoders (SB-VAE). By extending Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes, we are able to define a SB-VAE with a lat",
        "title": "Stick-Breaking Variational Autoencoders"
    },
    {
        "abs": "The title is Trace Norm Regularised Deep Multi-Task Learning, start with We propose a framework for training multiple neural networks simultaneously.\nThe parameters from all models are regularised by the tensor trace norm, so\nthat each neural network is encouraged to reuse others' parameters if possible\n-- this is the main motivation behind multi-task learning. In contrast to many\ndeep multi-task learning models, we do not predefine a parameter sharing\nstrategy by specifying which layers have tied parameters. Instead, our\nframework considers sharing for all shareable layers, and the sharing strategy\nis learned in a data-driven way., write a short concise abstract based on this: \nIn this paper, we propose a deep multi-task learning framework that leverages the power of trace norm regularization to encourage models to share parameters. Unlike traditional methods that predefine a sharing strategy, our approach learns the sharing strategy from the data, allowing the models to adapt to different tasks and datasets. By doing so, we can improve the generalization ability of the models and reduce the computational cost of training. Our experiments show that our proposed framework achieves better performance than traditional deep multi-task learning",
        "title": "Trace Norm Regularised Deep Multi-Task Learning"
    },
    {
        "abs": "The title is Sample Efficient Actor-Critic with Experience Replay, start with This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method., write a short concise abstract based on this: \nThis paper presents a novel actor-critic deep reinforcement learning agent that leverages experience replay, truncated importance sampling, stochastic dueling network architectures, and a new trust region policy optimization method to achieve sample efficiency and high performance in challenging environments. The proposed method is tested on various domains, including the discrete 57-game Atari domain and continuous control problems,",
        "title": "Sample Efficient Actor-Critic with Experience Replay"
    },
    {
        "abs": "The title is Early Methods for Detecting Adversarial Images, start with Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix., write a short concise abstract based on this: \nAbstract: Many machine learning classifiers are vulnerable to adversarial perturbations, which modify an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images, which adversaries must bypass to succeed. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA.",
        "title": "Early Methods for Detecting Adversarial Images"
    },
    {
        "abs": "The title is Not-So-Random Features, start with We propose a principled method for kernel learning, which relies on a\nFourier-analytic characterization of translation-invariant or\nrotation-invariant kernels. Our method produces a sequence of feature maps,\niteratively refining the SVM margin. We provide rigorous guarantees for\noptimality and generalization, interpreting our algorithm as online\nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations\non synthetic and real-world datasets demonstrate scalability and consistent\nimprovements over related random features-based methods., write a short concise abstract based on this:  In this paper, we propose a new method for kernel learning that leverages the Fourier transform to produce a sequence of feature maps that iteratively refine the SVM margin. Our approach provides rigorous guarantees for optimality and generalization, and can be interpreted as an online equilibrium-finding dynamics in a two-player min-max game. We evaluate our method on both synthetic and real-world datasets",
        "title": "Not-So-Random Features"
    },
    {
        "abs": "The title is Fast Reading Comprehension with ConvNets, start with State-of-the-art deep reading comprehension models are dominated by recurrent\nneural nets. Their sequential nature is a natural fit for language, but it also\nprecludes parallelization within an instances and often becomes the bottleneck\nfor deploying such models to latency critical scenarios. This is particularly\nproblematic for longer texts. Here we present a convolutional architecture as\nan alternative to these recurrent architectures. Using simple dilated\nconvolutional units in place of recurrent ones, we achieve results comparable\nto the state of the art on two question answering tasks, while at the same time\nachieving up to two orders of magnitude speedups for question answering., write a short concise abstract based on this: \nDeep learning models have shown great promise in improving reading comprehension, but current state-of-the-art models rely on recurrent neural networks (RNNs) which can be slow and sequential. In this paper, we propose a convolutional architecture as an alternative to RNNs, using dilated convolutional units to achieve comparable results while providing faster processing speeds. We demonstrate the effectiveness of our approach on two question answering tasks, achieving up to two orders of magnitude speedups compared to RNNs.",
        "title": "Fast Reading Comprehension with ConvNets"
    },
    {
        "abs": "The title is On reproduction of On the regularization of Wasserstein GANs, start with This report has several purposes. First, our report is written to investigate\nthe reproducibility of the submitted paper On the regularization of Wasserstein\nGANs (2018). Second, among the experiments performed in the submitted paper,\nfive aspects were emphasized and reproduced: learning speed, stability,\nrobustness against hyperparameter, estimating the Wasserstein distance, and\nvarious sampling method. Finally, we identify which parts of the contribution\ncan be reproduced, and at what cost in terms of resources. All source code for\nreproduction is open to the public., write a short concise abstract based on this: \nThis report investigates the reproducibility of the paper \"On the regularization of Wasserstein GANs\" (2018) and reproduces five key experiments from the paper. The report assesses the learning speed, stability, robustness against hyperparameter, estimation of Wasserstein distance, and various sampling methods of the original paper. The authors also identify which parts of the contribution can be reproduced and at what cost in terms of resources. All source code for reproduction is made publicly available",
        "title": "On reproduction of On the regularization of Wasserstein GANs"
    },
    {
        "abs": "The title is Trading Information between Latents in Hierarchical Variational Autoencoders, start with Variational Autoencoders (VAEs) were originally motivated (Kingma & Welling,\n2014) as probabilistic generative models in which one performs approximate\nBayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks\nthis interpretation and generalizes VAEs to application domains beyond\ngenerative modeling (e.g., representation learning, clustering, or lossy data\ncompression) by introducing an objective function that allows practitioners to\ntrade off between the information content (\"bit rate\") of the latent\nrepresentation and the distortion of reconstructed data (Alemi et al., 2018).\nIn this paper, we reconsider this rate/distortion trade-off in the context of\nhierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We\nidentify a general class of inference models for which one can split the rate\ninto contributions from each layer, which can then be tuned independently. We\nderive theoretical bounds on the performance of downstream tasks as functions\nof the individual layers' rates and verify our theoretical findings in\nlarge-scale experiments. Our results provide guidance for practitioners on\nwhich region in rate-space to target for a given application., write a short concise abstract based on this:  In this paper, we investigate the rate/distortion trade-off in hierarchical Variational Autoencoders (VAEs), which are VAEs with multiple layers of latent variables. We identify a general class of inference models that allow us to split the rate into contributions from each layer, which can be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "title": "Trading Information between Latents in Hierarchical Variational Autoencoders"
    },
    {
        "abs": "The title is Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking, start with Methods that learn representations of nodes in a graph play a critical role\nin network analysis since they enable many downstream learning tasks. We\npropose Graph2Gauss - an approach that can efficiently learn versatile node\nembeddings on large scale (attributed) graphs that show strong performance on\ntasks such as link prediction and node classification. Unlike most approaches\nthat represent nodes as point vectors in a low-dimensional continuous space, we\nembed each node as a Gaussian distribution, allowing us to capture uncertainty\nabout the representation. Furthermore, we propose an unsupervised method that\nhandles inductive learning scenarios and is applicable to different types of\ngraphs: plain/attributed, directed/undirected. By leveraging both the network\nstructure and the associated node attributes, we are able to generalize to\nunseen nodes without additional training. To learn the embeddings we adopt a\npersonalized ranking formulation w.r.t. the node distances that exploits the\nnatural ordering of the nodes imposed by the network structure. Experiments on\nreal world networks demonstrate the high performance of our approach,\noutperforming state-of-the-art network embedding methods on several different\ntasks. Additionally, we demonstrate the benefits of modeling uncertainty - by\nanalyzing it we can estimate neighborhood diversity and detect the intrinsic\nlatent dimensionality of a graph., write a short concise abstract based on this:  Deep Gaussian Embedding of Graphs (DGEG) is a novel approach for learning node representations on large-scale graphs. Unlike traditional methods that represent nodes as point vectors, DGEG embeds each node as a Gaussian distribution, capturing uncertainty in the representation. DGEG is an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs. By leveraging both network structure and node attributes, DGEG generalizes to unseen nodes without additional training. Experiments on real-world networks demonstrate the high performance of DGEG, outperforming state-of-the-art methods on several tasks. Additionally, DGEG provides insights into neighborhood diversity and intrinsic dimensionality of a graph.",
        "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking"
    },
    {
        "abs": "The title is Self-ensembling for visual domain adaptation, start with This paper explores the use of self-ensembling for visual domain adaptation\nproblems. Our technique is derived from the mean teacher variant (Tarvainen et\nal., 2017) of temporal ensembling (Laine et al;, 2017), a technique that\nachieved state of the art results in the area of semi-supervised learning. We\nintroduce a number of modifications to their approach for challenging domain\nadaptation scenarios and evaluate its effectiveness. Our approach achieves\nstate of the art results in a variety of benchmarks, including our winning\nentry in the VISDA-2017 visual domain adaptation challenge. In small image\nbenchmarks, our algorithm not only outperforms prior art, but can also achieve\naccuracy that is close to that of a classifier trained in a supervised fashion., write a short concise abstract based on this: \nThis paper presents a novel approach to visual domain adaptation using self-ensembling, a technique derived from temporal ensembling. Our modifications to the original approach enable it to perform well in challenging domain adaptation scenarios, achieving state-of-the-art results in various benchmarks, including the winning entry in the VISDA-2017 challenge. Our algorithm not only outperforms prior art but can also achieve accuracy close to that of a supervised classifier in small image datasets.",
        "title": "Self-ensembling for visual domain adaptation"
    },
    {
        "abs": "The title is A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples, start with Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust., write a short concise abstract based on this:  A new theoretical framework for understanding the robustness of deep classifiers against adversarial examples is proposed. The framework leverages concepts from topology to analyze the relationship between the predictor and its oracle, and to develop necessary and sufficient conditions for robustness. The results show that a single unnecessary feature can make a classifier not robust, and that feature representation learning is crucial for obtaining a classifier that is both accurate and robust.",
        "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples"
    },
    {
        "abs": "The title is Towards Information-Seeking Agents, start with We develop a general problem setting for training and testing the ability of\nagents to gather information efficiently. Specifically, we present a collection\nof tasks in which success requires searching through a partially-observed\nenvironment, for fragments of information which can be pieced together to\naccomplish various goals. We combine deep architectures with techniques from\nreinforcement learning to develop agents that solve our tasks. We shape the\nbehavior of these agents by combining extrinsic and intrinsic rewards. We\nempirically demonstrate that these agents learn to search actively and\nintelligently for new information to reduce their uncertainty, and to exploit\ninformation they have already acquired., write a short concise abstract based on this: \nIn this paper, we propose a new approach to training agents to gather information efficiently in a partially-observed environment. We present a collection of tasks that require the agents to search for fragments of information to achieve various goals. We combine deep architectures with reinforcement learning techniques to develop agents that can solve these tasks. We use a combination of extrinsic and intrinsic rewards to shape the behavior of these agents, and demonstrate through empirical results that they learn to search actively and intelligently to reduce their uncertainty and exploit the information they have already acquired.",
        "title": "Towards Information-Seeking Agents"
    },
    {
        "abs": "The title is Improving Neural Language Models with a Continuous Cache, start with We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks., write a short concise abstract based on this: \nIn this paper, we propose an extension to neural network language models that improves their ability to adapt to recent input. Our approach, called a continuous cache, stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is efficient and scalable, and we show that it outperforms recent memory augmented networks on several language model datasets. Our work demonstrates the potential of using external memory in neural network language models, and provides a new perspective on the use of cache models in natural language processing.",
        "title": "Improving Neural Language Models with a Continuous Cache"
    },
    {
        "abs": "The title is Generative Adversarial Nets from a Density Ratio Estimation Perspective, start with Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful., write a short concise abstract based on this:  Generative adversarial networks (GANs) have been successful in generating realistic samples, but their objective function has been modified to obtain stronger gradients during training. Our proposed algorithm repeats density ratio estimation and f-divergence minimization, offering a new perspective on GANs and making use of multiple viewpoints from density ratio estimation research.",
        "title": "Generative Adversarial Nets from a Density Ratio Estimation Perspective"
    },
    {
        "abs": "The title is Song From PI: A Musically Plausible Network for Pop Music Generation, start with We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing., write a short concise abstract based on this: \nIn this paper, we propose a novel framework for generating pop music using a hierarchical Recurrent Neural Network (RNN). The model is designed to capture the structure and composition of pop music, with bottom layers generating melody, and higher levels producing drums and chords. We conduct human studies that show our generated music is preferred over recent methods by Google. We also demonstrate two applications of our framework: neural dancing and karaoke, as well as neural story singing. Our approach has the potential to revolutionize the music industry by enabling the creation of high",
        "title": "Song From PI: A Musically Plausible Network for Pop Music Generation"
    },
    {
        "abs": "The title is Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond, start with We look at the eigenvalues of the Hessian of a loss function before and after\ntraining. The eigenvalue distribution is seen to be composed of two parts, the\nbulk which is concentrated around zero, and the edges which are scattered away\nfrom zero. We present empirical evidence for the bulk indicating how\nover-parametrized the system is, and for the edges that depend on the input\ndata., write a short concise abstract based on this: \nAbstract: We investigate the eigenvalues of the Hessian matrix of a deep neural network's loss function before and after training. We find that the eigenvalue distribution is composed of two parts: a bulk around zero and edges scattered away from zero. The bulk indicates over-parameterization, while the edges depend on the input data. Our results provide insight into the behavior of deep neural networks during training and their relationship to the underlying data.",
        "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond"
    },
    {
        "abs": "The title is Semantic embeddings for program behavior patterns, start with In this paper, we propose a new feature extraction technique for program\nexecution logs. First, we automatically extract complex patterns from a\nprogram's behavior graph. Then, we embed these patterns into a continuous space\nby training an autoencoder. We evaluate the proposed features on a real-world\nmalicious software detection task. We also find that the embedding space\ncaptures interpretable structures in the space of pattern parts., write a short concise abstract based on this: \nIn this paper, we propose a new technique for extracting meaningful features from program execution logs. Our approach involves automatically identifying complex patterns in a program's behavior graph, and then embedding these patterns into a continuous space using an autoencoder. We evaluate the effectiveness of our features on a real-world malicious software detection task, and find that they capture interpretable structures in the space of pattern parts",
        "title": "Semantic embeddings for program behavior patterns"
    },
    {
        "abs": "The title is Vision-based route following by an embodied insect-inspired sparse neural network, start with We compared the efficiency of the FlyHash model, an insect-inspired sparse\nneural network (Dasgupta et al., 2017), to similar but non-sparse models in an\nembodied navigation task. This requires a model to control steering by\ncomparing current visual inputs to memories stored along a training route. We\nconcluded the FlyHash model is more efficient than others, especially in terms\nof data encoding., write a short concise abstract based on this: \nIn this study, we compared the efficiency of the FlyHash model, an insect-inspired sparse neural network, to similar but non-sparse models in an embodied navigation task. Our results show that the FlyHash model is more efficient in terms of data encoding, allowing it to navigate more efficiently than other models. This is particularly important for embodied navigation tasks, where the model must compare current",
        "title": "Vision-based route following by an embodied insect-inspired sparse neural network"
    },
    {
        "abs": "The title is Integrating Rankings into Quantized Scores in Peer Review, start with In peer review, reviewers are usually asked to provide scores for the papers.\nThe scores are then used by Area Chairs or Program Chairs in various ways in\nthe decision-making process. The scores are usually elicited in a quantized\nform to accommodate the limited cognitive ability of humans to describe their\nopinions in numerical values. It has been found that the quantized scores\nsuffer from a large number of ties, thereby leading to a significant loss of\ninformation. To mitigate this issue, conferences have started to ask reviewers\nto additionally provide a ranking of the papers they have reviewed. There are\nhowever two key challenges. First, there is no standard procedure for using\nthis ranking information and Area Chairs may use it in different ways\n(including simply ignoring them), thereby leading to arbitrariness in the\npeer-review process. Second, there are no suitable interfaces for judicious use\nof this data nor methods to incorporate it in existing workflows, thereby\nleading to inefficiencies. We take a principled approach to integrate the\nranking information into the scores. The output of our method is an updated\nscore pertaining to each review that also incorporates the rankings. Our\napproach addresses the two aforementioned challenges by: (i) ensuring that\nrankings are incorporated into the updates scores in the same manner for all\npapers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use\nexisting interfaces and workflows designed for scores. We empirically evaluate\nour method on synthetic datasets as well as on peer reviews from the ICLR 2017\nconference, and find that it reduces the error by approximately 30% as compared\nto the best performing baseline on the ICLR 2017 data., write a short concise abstract based on this:  In peer review, reviewers are asked to provide scores to help area chairs make decisions. However, these scores are often quantized, leading to a loss of information. To address this, reviewers are asked to provide rankings as well. However, there are challenges in incorporating these rankings into the scores. Our approach addresses these challenges by ensuring that rankings are incorporated in the same manner for all papers and allowing for seamless use of existing interfaces and workflows. We evaluate our approach on synthetic and real-world data and find that it reduces error by approximately 30%.",
        "title": "Integrating Rankings into Quantized Scores in Peer Review"
    },
    {
        "abs": "The title is Association between author metadata and acceptance: A feature-rich, matched observational study of a corpus of ICLR submissions between 2017-2022, start with Many recent studies have probed status bias in the peer-review process of\nacademic journals and conferences. In this article, we investigated the\nassociation between author metadata and area chairs' final decisions\n(Accept/Reject) using our compiled database of 5,313 borderline submissions to\nthe International Conference on Learning Representations (ICLR) from 2017 to\n2022. We carefully defined elements in a cause-and-effect analysis, including\nthe treatment and its timing, pre-treatment variables, potential outcomes and\ncausal null hypothesis of interest, all in the context of study units being\ntextual data and under Neyman and Rubin's potential outcomes (PO) framework. We\nfound some weak evidence that author metadata was associated with articles'\nfinal decisions. We also found that, under an additional stability assumption,\nborderline articles from high-ranking institutions (top-30% or top-20%) were\nless favored by area chairs compared to their matched counterparts. The results\nwere consistent in two different matched designs (odds ratio = 0.82 [95% CI:\n0.67 to 1.00] in a first design and 0.83 [95% CI: 0.64 to 1.07] in a\nstrengthened design). We discussed how to interpret these results in the\ncontext of multiple interactions between a study unit and different agents\n(reviewers and area chairs) in the peer-review system., write a short concise abstract based on this:  The relationship between author metadata and acceptance at ICLR conferences was investigated in a large-scale observational study. The study found weak evidence that author metadata was associated with final decisions, and that articles from high-ranking institutions were less favored by area chairs. These findings suggest that multiple interactions between study units and different agents in the peer-review system may influence acceptance decisions.",
        "title": "Association between author metadata and acceptance: A feature-rich, matched observational study of a corpus of ICLR submissions between 2017-2022"
    },
    {
        "abs": "The title is Deep Variational Information Bottleneck, start with We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack., write a short concise abstract based on this: \nDeep Variational Information Bottleneck (Deep VIB) is a new approach to regularizing deep neural networks that leverages the reparameterization trick for efficient training. By parameterizing the information bottleneck model using a neural network, we can train Deep VIB models that outperform those trained with other forms of regularization in terms of generalization performance and robustness to adversarial attacks.",
        "title": "Deep Variational Information Bottleneck"
    },
    {
        "abs": "The title is Structured Attention Networks, start with Attention networks have proven to be an effective approach for embedding\ncategorical inference within a deep neural network. However, for many tasks we\nmay want to model richer structural dependencies without abandoning end-to-end\ntraining. In this work, we experiment with incorporating richer structural\ndistributions, encoded using graphical models, within deep networks. We show\nthat these structured attention networks are simple extensions of the basic\nattention procedure, and that they allow for extending attention beyond the\nstandard soft-selection approach, such as attending to partial segmentations or\nto subtrees. We experiment with two different classes of structured attention\nnetworks: a linear-chain conditional random field and a graph-based parsing\nmodel, and describe how these models can be practically implemented as neural\nnetwork layers. Experiments show that this approach is effective for\nincorporating structural biases, and structured attention networks outperform\nbaseline attention models on a variety of synthetic and real tasks: tree\ntransduction, neural machine translation, question answering, and natural\nlanguage inference. We further find that models trained in this way learn\ninteresting unsupervised hidden representations that generalize simple\nattention., write a short concise abstract based on this: \nIn this paper, we propose a new approach to incorporating structural information into deep neural networks, called structured attention networks. We show that by adding a simple structural component to the attention mechanism, we can model richer structural dependencies without sacrificing the end-to-end training capabilities of attention. We experiment with two different classes of structured attention networks and show that they outperform baseline attention models on a variety of tasks. Our approach learns interesting unsupervised hidden representations that generalize beyond simple attention.",
        "title": "Structured Attention Networks"
    },
    {
        "abs": "The title is Robustness to Adversarial Examples through an Ensemble of Specialists, start with We are proposing to use an ensemble of diverse specialists, where speciality\nis defined according to the confusion matrix. Indeed, we observed that for\nadversarial instances originating from a given class, labeling tend to be done\ninto a small subset of (incorrect) classes. Therefore, we argue that an\nensemble of specialists should be better able to identify and reject fooling\ninstances, with a high entropy (i.e., disagreement) over the decisions in the\npresence of adversaries. Experimental results obtained confirm that\ninterpretation, opening a way to make the system more robust to adversarial\nexamples through a rejection mechanism, rather than trying to classify them\nproperly at any cost., write a short concise abstract based on this: \nAn ensemble of diverse specialists can improve the robustness of a machine learning model to adversarial examples by identifying and rejecting instances that are likely to be misclassified. By defining specialty based on the confusion matrix, the ensemble can better identify instances that are likely to be fooled by adversaries, leading to a higher entropy in the decisions made by the ensemble. Experimental results confirm the effectiveness of this approach, providing a way to make the system more robust to adversarial examples through a rejection mechanism rather than trying to classify them properly at any cost.",
        "title": "Robustness to Adversarial Examples through an Ensemble of Specialists"
    },
    {
        "abs": "The title is Towards Neural Phrase-based Machine Translation, start with In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our\nmethod explicitly models the phrase structures in output sequences using\nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence\nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we\nintroduce a new layer to perform (soft) local reordering of input sequences.\nDifferent from existing neural machine translation (NMT) approaches, NPMT does\nnot use attention-based decoding mechanisms. Instead, it directly outputs\nphrases in a sequential order and can decode in linear time. Our experiments\nshow that NPMT achieves superior performances on IWSLT 2014\nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine\ntranslation tasks compared with strong NMT baselines. We also observe that our\nmethod produces meaningful phrases in output languages., write a short concise abstract based on this:  In this paper, the authors propose a new neural machine translation method called Neural Phrase-based Machine Translation (NPMT). Unlike traditional NMT approaches, NPMT does not use attention-based decoding mechanisms and instead directly outputs phrases in a sequential order. The authors use Sleep-WAke Networks (SWAN) to model the phrase structures in output sequences and introduce a new layer to perform (soft) local reordering of input sequences to mitigate the monotonic alignment requirement of SWAN. The authors show that NPMT achieves superior performances on several machine translation tasks compared to strong NMT baselines and",
        "title": "Towards Neural Phrase-based Machine Translation"
    },
    {
        "abs": "The title is LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation, start with We present LR-GAN: an adversarial image generation model which takes scene\nstructure and context into account. Unlike previous generative adversarial\nnetworks (GANs), the proposed GAN learns to generate image background and\nforegrounds separately and recursively, and stitch the foregrounds on the\nbackground in a contextually relevant manner to produce a complete natural\nimage. For each foreground, the model learns to generate its appearance, shape\nand pose. The whole model is unsupervised, and is trained in an end-to-end\nmanner with gradient descent methods. The experiments demonstrate that LR-GAN\ncan generate more natural images with objects that are more human recognizable\nthan DCGAN., write a short concise abstract based on this: \nIn this paper, we propose LR-GAN, a novel image generation model that takes into account the scene structure and context when generating images. Unlike traditional GANs, LR-GAN learns to generate image foregrounds and backgrounds separately and recursively, resulting in more natural and human-recognizable images. The model is trained in an end-to-end manner using gradient descent methods, and produces better results than DCGAN in terms of image quality and object recognition.",
        "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation"
    },
    {
        "abs": "The title is Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play, start with We describe a simple scheme that allows an agent to learn about its\nenvironment in an unsupervised manner. Our scheme pits two versions of the same\nagent, Alice and Bob, against one another. Alice proposes a task for Bob to\ncomplete; and then Bob attempts to complete the task. In this work we will\nfocus on two kinds of environments: (nearly) reversible environments and\nenvironments that can be reset. Alice will \"propose\" the task by doing a\nsequence of actions and then Bob must undo or repeat them, respectively. Via an\nappropriate reward structure, Alice and Bob automatically generate a curriculum\nof exploration, enabling unsupervised training of the agent. When Bob is\ndeployed on an RL task within the environment, this unsupervised training\nreduces the number of supervised episodes needed to learn, and in some cases\nconverges to a higher reward., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to unsupervised learning in reinforcement learning (RL) environments. Our method, called Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play (IMACASP), leverages the inherent competition between two versions of the same agent, Alice and Bob, to learn about the environment. In nearly reversible environments, Alice proposes a task for Bob to complete, while in environments that can be reset, Alice \"proposes\" by doing a sequence of actions and then Bob must undo or repeat them. Through an appropriate reward structure, Alice and Bob generate a curriculum of exploration, enabling unsupervised training of the agent. Our approach reduces the number of supervised episodes needed",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
    },
    {
        "abs": "The title is Maximum Entropy Flow Networks, start with Maximum entropy modeling is a flexible and popular framework for formulating\nstatistical models given partial knowledge. In this paper, rather than the\ntraditional method of optimizing over the continuous density directly, we learn\na smooth and invertible transformation that maps a simple distribution to the\ndesired maximum entropy distribution. Doing so is nontrivial in that the\nobjective being maximized (entropy) is a function of the density itself. By\nexploiting recent developments in normalizing flow networks, we cast the\nmaximum entropy problem into a finite-dimensional constrained optimization, and\nsolve the problem by combining stochastic optimization with the augmented\nLagrangian method. Simulation results demonstrate the effectiveness of our\nmethod, and applications to finance and computer vision show the flexibility\nand accuracy of using maximum entropy flow networks., write a short concise abstract based on this:  In this paper, we propose a new method for maximum entropy modeling that leverages recent advances in normalizing flow networks. Instead of directly optimizing the density function, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. We cast the maximum entropy problem into a finite-dimensional constrained optimization problem and solve it using a combination of stochastic optimization and the augmented Lagrangian method. Our approach is effective and flexible, and we demonstrate its application in finance and computer vision.",
        "title": "Maximum Entropy Flow Networks"
    },
    {
        "abs": "The title is CommAI: Evaluating the first steps towards a useful general AI, start with With machine learning successfully applied to new daunting problems almost\nevery day, general AI starts looking like an attainable goal. However, most\ncurrent research focuses instead on important but narrow applications, such as\nimage classification or machine translation. We believe this to be largely due\nto the lack of objective ways to measure progress towards broad machine\nintelligence. In order to fill this gap, we propose here a set of concrete\ndesiderata for general AI, together with a platform to test machines on how\nwell they satisfy such desiderata, while keeping all further complexities to a\nminimum., write a short concise abstract based on this: \nWith machine learning making rapid progress in tackling new challenges, the dream of general AI (AGI) is gaining traction. However, most research focuses on narrow applications, neglecting the need for objective measures of progress towards true machine intelligence. To address this gap, we propose a set of desiderata for AGI and a platform to evaluate machines on their ability to meet these criteria. By keeping complexity to a minimum, we aim to accelerate the development of AGI.",
        "title": "CommAI: Evaluating the first steps towards a useful general AI"
    },
    {
        "abs": "The title is Deep Learning with Dynamic Computation Graphs, start with Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference. They are also difficult to implement in popular\ndeep learning libraries, which are based on static data-flow graphs. We\nintroduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size. We further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature., write a short concise abstract based on this:  Deep learning models that operate on graph structures are useful in various domains, but their dynamic nature poses challenges for batched training and inference. We propose a technique called dynamic batching to overcome these challenges and create static graphs that mimic dynamic computation graphs of arbitrary shape and size. Our approach allows for the creation of a high-level library of compositional blocks that simplifies the development of dynamic graph models. We demonstrate the effectiveness of our approach by implementing various models from the literature in a concise and batch-wise parallel manner.",
        "title": "Deep Learning with Dynamic Computation Graphs"
    },
    {
        "abs": "The title is Automatic Rule Extraction from Long Short Term Memory Networks, start with Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM., write a short concise abstract based on this: \nAlthough deep learning models have proven effective in natural language processing, their internal workings remain unclear. This paper presents a new approach for identifying the most important input patterns in LSTMs, which are then used to construct a simple, rule-based classifier that approximates the output of the LSTM. The approach is validated through quantitative evaluation, demonstrating the effectiveness of the extracted phrases in approximating the output of the LSTM.",
        "title": "Automatic Rule Extraction from Long Short Term Memory Networks"
    },
    {
        "abs": "The title is Stochastic Neural Networks for Hierarchical Reinforcement Learning, start with Deep reinforcement learning has achieved many impressive results in recent\nyears. However, tasks with sparse rewards or long horizons continue to pose\nsignificant challenges. To tackle these important problems, we propose a\ngeneral framework that first learns useful skills in a pre-training\nenvironment, and then leverages the acquired skills for learning faster in\ndownstream tasks. Our approach brings together some of the strengths of\nintrinsic motivation and hierarchical methods: the learning of useful skill is\nguided by a single proxy reward, the design of which requires very minimal\ndomain knowledge about the downstream tasks. Then a high-level policy is\ntrained on top of these skills, providing a significant improvement of the\nexploration and allowing to tackle sparse rewards in the downstream tasks. To\nefficiently pre-train a large span of skills, we use Stochastic Neural Networks\ncombined with an information-theoretic regularizer. Our experiments show that\nthis combination is effective in learning a wide span of interpretable skills\nin a sample-efficient way, and can significantly boost the learning performance\nuniformly across a wide range of downstream tasks., write a short concise abstract based on this:  In this paper, we propose a framework for hierarchical reinforcement learning that combines the strengths of intrinsic motivation and hierarchical methods. Our approach uses stochastic neural networks and an information-theoretic regularizer to efficiently pre-train a large span of skills, which are then leveraged for learning faster in downstream tasks with sparse rewards. The proposed method shows significant improvement in exploration and learning performance across a wide range of tasks, demonstrating its effectiveness in tackling important problems in reinforcement learning.",
        "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"
    },
    {
        "abs": "The title is On Unifying Deep Generative Models, start with Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques., write a short concise abstract based on this:  Deep generative models have achieved impressive success in recent years, with GANs and VAEs emerging as two distinct paradigms. This paper unifies these models by establishing formal connections between them, interpreting sample generation in GANs as posterior inference and showing that GANs and VAEs involve minimizing KL divergences with opposite directions. This unified view provides a powerful tool to analyze existing model variants and transfer techniques across research lines in a principled way. Experiments demonstrate the generality and effectiveness of the transferred techniques.",
        "title": "On Unifying Deep Generative Models"
    },
    {
        "abs": "The title is Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks, start with We consider the problem of detecting out-of-distribution images in neural\nnetworks. We propose ODIN, a simple and effective method that does not require\nany change to a pre-trained neural network. Our method is based on the\nobservation that using temperature scaling and adding small perturbations to\nthe input can separate the softmax score distributions between in- and\nout-of-distribution images, allowing for more effective detection. We show in a\nseries of experiments that ODIN is compatible with diverse network\narchitectures and datasets. It consistently outperforms the baseline approach\nby a large margin, establishing a new state-of-the-art performance on this\ntask. For example, ODIN reduces the false positive rate from the baseline 34.7%\nto 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is\n95%., write a short concise abstract based on this: \nIn this paper, we propose ODIN, a simple and effective method for detecting out-of-distribution images in neural networks without modifying the pre-trained model. ODIN uses temperature scaling and small perturbations to separate the softmax score distributions between in- and out-of-distribution images, leading to more accurate detection. Our experiments show that ODIN consistently outperforms the baseline approach across diverse architectures and datasets, establishing a new state-of-the-art performance on this task.",
        "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks"
    },
    {
        "abs": "The title is An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax, start with A framework is presented for unsupervised learning of representations based\non infomax principle for large-scale neural populations. We use an asymptotic\napproximation to the Shannon's mutual information for a large neural population\nto demonstrate that a good initial approximation to the global\ninformation-theoretic optimum can be obtained by a hierarchical infomax method.\nStarting from the initial solution, an efficient algorithm based on gradient\ndescent of the final objective function is proposed to learn representations\nfrom the input datasets, and the method works for complete, overcomplete, and\nundercomplete bases. As confirmed by numerical experiments, our method is\nrobust and highly efficient for extracting salient features from input\ndatasets. Compared with the main existing methods, our algorithm has a distinct\nadvantage in both the training speed and the robustness of unsupervised\nrepresentation learning. Furthermore, the proposed method is easily extended to\nthe supervised or unsupervised model for training deep structure networks., write a short concise abstract based on this:  A new framework for unsupervised learning of representations based on the infomax principle is proposed. The method uses an asymptotic approximation of Shannon's mutual information for a large neural population to find a good initial approximation of the global information-theoretic optimum. An efficient algorithm based on gradient descent of the final objective function is then used to learn representations from input datasets. The method is robust and efficient for extracting salient features from input datasets and can be used for complete, overcomplete, and undercomplete bases. The proposed method is also easily extended to supervised or unsupervised model for training deep structure networks.",
        "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax"
    },
    {
        "abs": "The title is Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks, start with Recurrent Neural Networks (RNNs) continue to show outstanding performance in\nsequence modeling tasks. However, training RNNs on long sequences often face\nchallenges like slow inference, vanishing gradients and difficulty in capturing\nlong term dependencies. In backpropagation through time settings, these issues\nare tightly coupled with the large, sequential computational graph resulting\nfrom unfolding the RNN in time. We introduce the Skip RNN model which extends\nexisting RNN models by learning to skip state updates and shortens the\neffective size of the computational graph. This model can also be encouraged to\nperform fewer state updates through a budget constraint. We evaluate the\nproposed model on various tasks and show how it can reduce the number of\nrequired RNN updates while preserving, and sometimes even improving, the\nperformance of the baseline RNN models. Source code is publicly available at\nhttps://imatge-upc.github.io/skiprnn-2017-telecombcn/ ., write a short concise abstract based on this:  Skip RNN is a new model that extends traditional RNNs by learning to skip state updates, reducing the computational graph size and the number of required updates. It can also be constrained to perform fewer updates, while preserving or even improving performance. The proposed model is evaluated on various tasks and shows promising results.",
        "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks"
    },
    {
        "abs": "The title is SGDR: Stochastic Gradient Descent with Warm Restarts, start with Restart techniques are common in gradient-free optimization to deal with\nmultimodal functions. Partial warm restarts are also gaining popularity in\ngradient-based optimization to improve the rate of convergence in accelerated\ngradient schemes to deal with ill-conditioned functions. In this paper, we\npropose a simple warm restart technique for stochastic gradient descent to\nimprove its anytime performance when training deep neural networks. We\nempirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where\nwe demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively.\nWe also demonstrate its advantages on a dataset of EEG recordings and on a\ndownsampled version of the ImageNet dataset. Our source code is available at\nhttps://github.com/loshchil/SGDR, write a short concise abstract based on this: \nAbstract: In this paper, we propose a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. Our technique, called SGDR, combines the advantages of restart techniques and accelerated gradient methods to achieve better convergence rates and improved generalization. We empirically study the performance of SGDR on several benchmark datasets and demonstrate new state-of-the-art results. Our technique is simple to implement and can be easily integrated into existing SGD algorithms, making it a promising approach for improving the efficiency and accuracy of deep learning training.",
        "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"
    },
    {
        "abs": "The title is Action-depedent Control Variates for Policy Optimization via Stein's Identity, start with Policy gradient methods have achieved remarkable successes in solving\nchallenging reinforcement learning problems. However, it still often suffers\nfrom the large variance issue on policy gradient estimation, which leads to\npoor sample efficiency during training. In this work, we propose a control\nvariate method to effectively reduce variance for policy gradient methods.\nMotivated by the Stein's identity, our method extends the previous control\nvariate methods used in REINFORCE and advantage actor-critic by introducing\nmore general action-dependent baseline functions. Empirical studies show that\nour method significantly improves the sample efficiency of the state-of-the-art\npolicy gradient approaches., write a short concise abstract based on this: \nIn this paper, we propose a new method for reducing variance in policy gradient methods, called Action-Dependent Control Variates for Policy Optimization via Stein's Identity. By leveraging Stein's identity, we introduce more general action-dependent baseline functions, leading to improved sample efficiency in policy gradient training. Our method outperforms state-of-the-art policy gradient approaches in empirical studies, demonstrating its effectiveness in reducing variance and improving training speed.",
        "title": "Action-depedent Control Variates for Policy Optimization via Stein's Identity"
    },
    {
        "abs": "The title is Skip Connections Eliminate Singularities, start with Skip connections made the training of very deep networks possible and have\nbecome an indispensable component in a variety of neural architectures. A\ncompletely satisfactory explanation for their success remains elusive. Here, we\npresent a novel explanation for the benefits of skip connections in training\nvery deep networks. The difficulty of training deep networks is partly due to\nthe singularities caused by the non-identifiability of the model. Several such\nsingularities have been identified in previous works: (i) overlap singularities\ncaused by the permutation symmetry of nodes in a given layer, (ii) elimination\nsingularities corresponding to the elimination, i.e. consistent deactivation,\nof nodes, (iii) singularities generated by the linear dependence of the nodes.\nThese singularities cause degenerate manifolds in the loss landscape that slow\ndown learning. We argue that skip connections eliminate these singularities by\nbreaking the permutation symmetry of nodes, by reducing the possibility of node\nelimination and by making the nodes less linearly dependent. Moreover, for\ntypical initializations, skip connections move the network away from the\n\"ghosts\" of these singularities and sculpt the landscape around them to\nalleviate the learning slow-down. These hypotheses are supported by evidence\nfrom simplified models, as well as from experiments with deep networks trained\non real-world datasets., write a short concise abstract based on this: \nSkip connections in deep neural networks have been instrumental in enabling the training of very deep networks, but their mechanism of action remains unclear. We propose a novel explanation for the benefits of skip connections, which lies in their ability to eliminate singularities that arise due to permutation symmetry, node elimination, and linear dependence of nodes. By breaking permutation symmetry, reducing node elimination, and making nodes less linearly dependent, skip connections alleviate the learning slow-down caused by these singularities. Our hypotheses are supported by evidence from simplified models and real-world datasets.",
        "title": "Skip Connections Eliminate Singularities"
    },
    {
        "abs": "The title is Natural Language Inference over Interaction Space: ICLR 2018 Reproducibility Report, start with We have tried to reproduce the results of the paper \"Natural Language\nInference over Interaction Space\" submitted to ICLR 2018 conference as part of\nthe ICLR 2018 Reproducibility Challenge. Initially, we were not aware that the\ncode was available, so we started to implement the network from scratch. We\nhave evaluated our version of the model on Stanford NLI dataset and reached\n86.38% accuracy on the test set, while the paper claims 88.0% accuracy. The\nmain difference, as we understand it, comes from the optimizers and the way\nmodel selection is performed., write a short concise abstract based on this:  We attempted to reproduce the results of a paper on Natural Language Inference over Interaction Space submitted to ICLR 2018. We initially attempted to implement the network from scratch, but were able to reproduce the results after discovering the authors had made their code available. Our version of the model achieved an accuracy of 86.38% on the test set, which is lower than the 88.0% claimed in the paper. The main difference appears to be in the optimizers and model selection methods used.",
        "title": "Natural Language Inference over Interaction Space: ICLR 2018 Reproducibility Report"
    },
    {
        "abs": "The title is Reproduction Report on \"Learn to Pay Attention\", start with We have successfully implemented the \"Learn to Pay Attention\" model of\nattention mechanism in convolutional neural networks, and have replicated the\nresults of the original paper in the categories of image classification and\nfine-grained recognition., write a short concise abstract based on this: \nWe have successfully implemented and replicated the results of the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks. This model has shown improved performance in image classification and fine-grained recognition tasks compared to traditional attention mechanisms. Our",
        "title": "Reproduction Report on \"Learn to Pay Attention\""
    },
    {
        "abs": "The title is SufiSent - Universal Sentence Representations Using Suffix Encodings, start with Computing universal distributed representations of sentences is a fundamental\ntask in natural language processing. We propose a method to learn such\nrepresentations by encoding the suffixes of word sequences in a sentence and\ntraining on the Stanford Natural Language Inference (SNLI) dataset. We\ndemonstrate the effectiveness of our approach by evaluating it on the SentEval\nbenchmark, improving on existing approaches on several transfer tasks., write a short concise abstract based on this:  In this paper, we propose a novel approach to learning universal sentence representations using suffix encodings. By encoding the suffixes of word sequences in a sentence, we are able to learn distributed representations that capture the meaning of sentences in a way that is effective for a wide range of natural language processing tasks. We evaluate our approach on the Stanford Natural Language Inference (SNLI) dataset and demonstrate its superiority to existing",
        "title": "SufiSent - Universal Sentence Representations Using Suffix Encodings"
    },
    {
        "abs": "The title is On the scaling of polynomial features for representation matching, start with In many neural models, new features as polynomial functions of existing ones\nare used to augment representations. Using the natural language inference task\nas an example, we investigate the use of scaled polynomials of degree 2 and\nabove as matching features. We find that scaling degree 2 features has the\nhighest impact on performance, reducing classification error by 5% in the best\nmodels., write a short concise abstract based on this: \nIn this paper, we investigate the use of scaled polynomial features for representation matching in neural models. Specifically, we focus on the natural language inference task and find that scaling degree 2 features has the highest impact on performance, reducing classification error by 5% in the best models. Our results demonstrate the effectiveness of using scaled polynomial features for representation matching and highlight the importance of degree 2 features in neural models",
        "title": "On the scaling of polynomial features for representation matching"
    },
    {
        "abs": "The title is A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks, start with We present a generalization bound for feedforward neural networks in terms of\nthe product of the spectral norm of the layers and the Frobenius norm of the\nweights. The generalization bound is derived using a PAC-Bayes analysis., write a short concise abstract based on this: \nIn this paper, we present a new approach to deriving generalization bounds for feedforward neural networks using a PAC-Bayesian analysis. Our approach involves bounding the expected generalization error in terms of the product of the spectral norm of the layers and the Frobenius norm of",
        "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"
    },
    {
        "abs": "The title is Uncertainty Estimation via Stochastic Batch Normalization, start with In this work, we investigate Batch Normalization technique and propose its\nprobabilistic interpretation. We propose a probabilistic model and show that\nBatch Normalization maximazes the lower bound of its marginalized\nlog-likelihood. Then, according to the new probabilistic model, we design an\nalgorithm which acts consistently during train and test. However, inference\nbecomes computationally inefficient. To reduce memory and computational cost,\nwe propose Stochastic Batch Normalization -- an efficient approximation of\nproper inference procedure. This method provides us with a scalable uncertainty\nestimation technique. We demonstrate the performance of Stochastic Batch\nNormalization on popular architectures (including deep convolutional\narchitectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets., write a short concise abstract based on this:  In this paper, we propose a new technique for uncertainty estimation in deep learning called Stochastic Batch Normalization. This method provides a scalable and efficient way to estimate uncertainty in deep neural networks by approximating the proper inference procedure. We show that Stochastic Batch Normalization maximizes the lower bound of the marginalized log-likelihood of the probabilistic model, and demonstrate its performance on popular architectures for MNIST and CIFAR-10 datasets.",
        "title": "Uncertainty Estimation via Stochastic Batch Normalization"
    },
    {
        "abs": "The title is i-RevNet: Deep Invertible Networks, start with It is widely believed that the success of deep convolutional networks is\nbased on progressively discarding uninformative variability about the input\nwith respect to the problem at hand. This is supported empirically by the\ndifficulty of recovering images from their hidden representations, in most\ncommonly used network architectures. In this paper we show via a one-to-one\nmapping that this loss of information is not a necessary condition to learn\nrepresentations that generalize well on complicated problems, such as ImageNet.\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can\nbe fully inverted up to the final projection onto the classes, i.e. no\ninformation is discarded. Building an invertible architecture is difficult, for\none, because the local inversion is ill-conditioned, we overcome this by\nproviding an explicit inverse. An analysis of i-RevNets learned representations\nsuggests an alternative explanation for the success of deep networks by a\nprogressive contraction and linear separation with depth. To shed light on the\nnature of the model learned by the i-RevNet we reconstruct linear\ninterpolations between natural image representations., write a short concise abstract based on this: \nIn this paper, we introduce i-RevNet, a deep neural network architecture that learns invertible representations. Unlike traditional networks that discard information as they process the input, i-RevNet preserves all the information and provides a one-to-one mapping between the input and the output. We show that this approach leads to better generalization on complex tasks such as ImageNet. Our analysis suggests that the success of deep networks is not due to discarding information but rather due to a progressive contraction and linear separation with depth. We demonstrate this by reconstructing linear interpolations between natural image representations.",
        "title": "i-RevNet: Deep Invertible Networks"
    },
    {
        "abs": "The title is Learning Sparse Latent Representations with the Deep Copula Information Bottleneck, start with Deep latent variable models are powerful tools for representation learning.\nIn this paper, we adopt the deep information bottleneck model, identify its\nshortcomings and propose a model that circumvents them. To this end, we apply a\ncopula transformation which, by restoring the invariance properties of the\ninformation bottleneck method, leads to disentanglement of the features in the\nlatent space. Building on that, we show how this transformation translates to\nsparsity of the latent space in the new model. We evaluate our method on\nartificial and real data., write a short concise abstract based on this: \nIn this paper, we propose a new method for learning sparse latent representations using the deep information bottleneck model. We identify the limitations of the original model and propose a new approach that incorporates a copula transformation to restore invariance properties and promote disentanglement of features in the latent space. Our approach leads to sparsity of the latent space and improves performance on both artificial and real-world data.",
        "title": "Learning Sparse Latent Representations with the Deep Copula Information Bottleneck"
    },
    {
        "abs": "The title is On transfer learning using a MAC model variant, start with We introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with\na simplified set of equations that achieves comparable accuracy, while training\nfaster. We evaluate both models on CLEVR and CoGenT, and show that, transfer\nlearning with fine-tuning results in a 15 point increase in accuracy, matching\nthe state of the art. Finally, in contrast, we demonstrate that improper\nfine-tuning can actually reduce a model's accuracy as well., write a short concise abstract based on this: \nIn this paper, we introduce a simplified variant of the MAC model (Hudson and Manning, ICLR 2018) that achieves comparable accuracy to the original model while training faster. We evaluate both models on two benchmark datasets and show that transfer learning with fine-tuning results in a 15 point increase in accuracy, matching the state of the art. Additionally, we demonstrate that improper fine-tuning",
        "title": "On transfer learning using a MAC model variant"
    },
    {
        "abs": "The title is Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks, start with Adaptive Computation Time for Recurrent Neural Networks (ACT) is one of the\nmost promising architectures for variable computation. ACT adapts to the input\nsequence by being able to look at each sample more than once, and learn how\nmany times it should do it. In this paper, we compare ACT to Repeat-RNN, a\nnovel architecture based on repeating each sample a fixed number of times. We\nfound surprising results, where Repeat-RNN performs as good as ACT in the\nselected tasks. Source code in TensorFlow and PyTorch is publicly available at\nhttps://imatge-upc.github.io/danifojo-2018-repeatrnn/, write a short concise abstract based on this: \nAdaptive computation time for Recurrent Neural Networks (ACT) is a promising architecture for variable computation. In this paper, we compare ACT to Repeat-RNN, a novel architecture based on repeating each sample a fixed number of times. Surprisingly, we found that Repeat-RNN performs as good as ACT in selected tasks, despite its simpler implementation. The source code is publicly available in TensorFlow and PyTorch.",
        "title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks"
    },
    {
        "abs": "The title is Efficient GAN-Based Anomaly Detection, start with Generative adversarial networks (GANs) are able to model the complex\nhighdimensional distributions of real-world data, which suggests they could be\neffective for anomaly detection. However, few works have explored the use of\nGANs for the anomaly detection task. We leverage recently developed GAN models\nfor anomaly detection, and achieve state-of-the-art performance on image and\nnetwork intrusion datasets, while being several hundred-fold faster at test\ntime than the only published GAN-based method., write a short concise abstract based on this: \nEfficient GAN-Based Anomaly Detection: Leveraging Recent Advances in GANs for State-of-the-Art Performance and Speed.",
        "title": "Efficient GAN-Based Anomaly Detection"
    },
    {
        "abs": "The title is Natural Language Inference over Interaction Space, start with Natural Language Inference (NLI) task requires an agent to determine the\nlogical relationship between a natural language premise and a natural language\nhypothesis. We introduce Interactive Inference Network (IIN), a novel class of\nneural network architectures that is able to achieve high-level understanding\nof the sentence pair by hierarchically extracting semantic features from\ninteraction space. We show that an interaction tensor (attention weight)\ncontains semantic information to solve natural language inference, and a denser\ninteraction tensor contains richer semantic information. One instance of such\narchitecture, Densely Interactive Inference Network (DIIN), demonstrates the\nstate-of-the-art performance on large scale NLI copora and large-scale NLI\nalike corpus. It's noteworthy that DIIN achieve a greater than 20% error\nreduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to\nthe strongest published system., write a short concise abstract based on this: \nNatural Language Inference (NLI) is a fundamental task in natural language processing (NLP) that requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. In this paper, we introduce Interactive Inference Network (IIN), a novel class of neural network architectures that can achieve high-level understanding of sentence pairs by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve NLI, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates state-of-the-art performance on",
        "title": "Natural Language Inference over Interaction Space"
    },
    {
        "abs": "The title is Provably Minimally-Distorted Adversarial Examples, start with The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken. We propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2., write a short concise abstract based on this: \nThe ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples, but most of these have been quickly shown to be vulnerable to future attacks. We propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples, given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct",
        "title": "Provably Minimally-Distorted Adversarial Examples"
    },
    {
        "abs": "The title is Hierarchical interpretations for neural network predictions, start with Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise., write a short concise abstract based on this:  Deep neural networks (DNNs) are powerful predictors but their complex relationships are difficult to interpret. We propose a method, agglomerative contextual decomposition (ACD), to explain DNN predictions by producing a hierarchical clustering of input features and their contribution to the final prediction. We demonstrate its effectiveness through examples from sentiment analysis and image classification, showing that ACD can identify incorrect predictions and dataset bias. Additionally, ACD enables users to choose the more accurate of two DNNs and to better trust the outputs. Finally, we show that ACD's hierarchy is robust to adversarial perturbations, indicating that it captures fundamental aspects of the input.",
        "title": "Hierarchical interpretations for neural network predictions"
    },
    {
        "abs": "The title is TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer, start with In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples., write a short concise abstract based on this:  In this paper, we present TimbreTron, a novel approach for musical timbre transfer that leverages the power of WaveNet and CycleGAN. TimbreTron first converts the audio signal into a time-frequency representation using the Constant Q Transform (CQT), and then applies image-domain style transfer to manipulate the timbre of the signal. Finally, a conditional WaveNet synthesizer is used to generate a high-quality waveform that preserves the original musical content. Our experiments show that TimbreTron can effectively transfer the timbre of an audio signal while preserving other musical aspects, and outperform existing methods in terms of perceptual quality.",
        "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer"
    },
    {
        "abs": "The title is Meta-Learning a Dynamical Language Model, start with We consider the task of word-level language modeling and study the\npossibility of combining hidden-states-based short-term representations with\nmedium-term representations encoded in dynamical weights of a language model.\nOur work extends recent experiments on language models with dynamically\nevolving weights by casting the language modeling problem into an online\nlearning-to-learn framework in which a meta-learner is trained by\ngradient-descent to continuously update a language model weights., write a short concise abstract based on this: \nIn this paper, we explore the combination of short-term and medium-term representations in a language model, using a dynamical weights framework. We cast the language modeling task as an online learning-to-learn problem, where a meta-learner is trained to continuously update the language model weights using gradient descent. Our approach allows for more flexible and efficient language modeling, as the model can adapt",
        "title": "Meta-Learning a Dynamical Language Model"
    },
    {
        "abs": "The title is Semi-Supervised Learning with GANs: Revisiting Manifold Regularization, start with GANS are powerful generative models that are able to model the manifold of\nnatural images. We leverage this property to perform manifold regularization by\napproximating the Laplacian norm using a Monte Carlo approximation that is\neasily computed with the GAN. When incorporated into the feature-matching GAN\nof Improved GAN, we achieve state-of-the-art results for GAN-based\nsemi-supervised learning on the CIFAR-10 dataset, with a method that is\nsignificantly easier to implement than competing methods., write a short concise abstract based on this: \nGANs are powerful generative models that can model the manifold of natural images. By leveraging this property, we propose a new method for semi-supervised learning with GANs, which approximates the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. Our method achieves state-of-the-art results on the CIFAR-10 dataset and is significantly easier to implement than competing methods",
        "title": "Semi-Supervised Learning with GANs: Revisiting Manifold Regularization"
    },
    {
        "abs": "The title is On the loss landscape of a class of deep neural networks with no bad local valleys, start with We identify a class of over-parameterized deep neural networks with standard\nactivation functions and cross-entropy loss which provably have no bad local\nvalley, in the sense that from any point in parameter space there exists a\ncontinuous path on which the cross-entropy loss is non-increasing and gets\narbitrarily close to zero. This implies that these networks have no sub-optimal\nstrict local minima., write a short concise abstract based on this: \nAbstract: We introduce a class of deep neural networks with standard activation functions and cross-entropy loss that have no bad local valleys. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This result implies that these networks have no sub-optimal strict local minima, and thus are guaranteed to converge to",
        "title": "On the loss landscape of a class of deep neural networks with no bad local valleys"
    },
    {
        "abs": "The title is Learning to Count Objects in Natural Images for Visual Question Answering, start with Visual Question Answering (VQA) models have struggled with counting objects\nin natural images so far. We identify a fundamental problem due to soft\nattention in these models as a cause. To circumvent this problem, we propose a\nneural network component that allows robust counting from object proposals.\nExperiments on a toy task show the effectiveness of this component and we\nobtain state-of-the-art accuracy on the number category of the VQA v2 dataset\nwithout negatively affecting other categories, even outperforming ensemble\nmodels with our single model. On a difficult balanced pair metric, the\ncomponent gives a substantial improvement in counting over a strong baseline by\n6.6%., write a short concise abstract based on this: \nA new neural network component is proposed to address the challenge of counting objects in natural images for visual question answering (VQA) models. The proposed component allows robust counting from object proposals and improves accuracy on a difficult balanced pair metric by 6.6%. Experiments on a toy task show the effectiveness of the component and state-of-the-art accuracy is achieved on the number category of the VQA v2 dataset without negatively affecting other categories.",
        "title": "Learning to Count Objects in Natural Images for Visual Question Answering"
    },
    {
        "abs": "The title is Spectral Normalization for Generative Adversarial Networks, start with One of the challenges in the study of generative adversarial networks is the\ninstability of its training. In this paper, we propose a novel weight\nnormalization technique called spectral normalization to stabilize the training\nof the discriminator. Our new normalization technique is computationally light\nand easy to incorporate into existing implementations. We tested the efficacy\nof spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we\nexperimentally confirmed that spectrally normalized GANs (SN-GANs) is capable\nof generating images of better or equal quality relative to the previous\ntraining stabilization techniques., write a short concise abstract based on this:  In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of generative adversarial networks (GANs). Our technique is computationally light and easy to incorporate into existing implementations, and we demonstrate its efficacy on several datasets. Our results show that spectrally normalized GANs (SN-GANs) are capable of generating images of better or equal quality compared to previous training stabilization techniques.",
        "title": "Spectral Normalization for Generative Adversarial Networks"
    },
    {
        "abs": "The title is Node Centralities and Classification Performance for Characterizing Node Embedding Algorithms, start with Embedding graph nodes into a vector space can allow the use of machine\nlearning to e.g. predict node classes, but the study of node embedding\nalgorithms is immature compared to the natural language processing field\nbecause of a diverse nature of graphs. We examine the performance of node\nembedding algorithms with respect to graph centrality measures that\ncharacterize diverse graphs, through systematic experiments with four node\nembedding algorithms, four or five graph centralities, and six datasets.\nExperimental results give insights into the properties of node embedding\nalgorithms, which can be a basis for further research on this topic., write a short concise abstract based on this: \nNode embedding algorithms are used to map graph nodes into a vector space, enabling machine learning tasks such as node classification. However, the study of node embedding algorithms is still in its infancy compared to natural language processing due to the diverse nature of graphs. This paper investigates the performance of node embedding algorithms in relation to graph centrality measures that characterize diverse graphs through systematic experiments with four algorithms, four centralities, and six datasets. The findings provide insights into the properties of node embedding algorithms, which can serve as a foundation for future research in this area.",
        "title": "Node Centralities and Classification Performance for Characterizing Node Embedding Algorithms"
    },
    {
        "abs": "The title is Can Neural Networks Understand Logical Entailment?, start with We introduce a new dataset of logical entailments for the purpose of\nmeasuring models' ability to capture and exploit the structure of logical\nexpressions against an entailment prediction task. We use this task to compare\na series of architectures which are ubiquitous in the sequence-processing\nliterature, in addition to a new model class---PossibleWorldNets---which\ncomputes entailment as a \"convolution over possible worlds\". Results show that\nconvolutional networks present the wrong inductive bias for this class of\nproblems relative to LSTM RNNs, tree-structured neural networks outperform LSTM\nRNNs due to their enhanced ability to exploit the syntax of logic, and\nPossibleWorldNets outperform all benchmarks., write a short concise abstract based on this: \nIn this paper, we introduce a new dataset of logical entailments and use it to compare various neural network architectures in terms of their ability to capture and exploit the structure of logical expressions for an entailment prediction task. Our results show that convolutional networks are not well-suited for this task, while tree-structured networks and a new class of models called PossibleWorldNets perform better. PossibleWorldNets, which compute entailment as a \"convolution over possible worlds,\" outperform all other models, demonstrating their effectiveness in capturing the structure",
        "title": "Can Neural Networks Understand Logical Entailment?"
    },
    {
        "abs": "The title is The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, start with Neural network pruning techniques can reduce the parameter counts of trained\nnetworks by over 90%, decreasing storage requirements and improving\ncomputational performance of inference without compromising accuracy. However,\ncontemporary experience is that the sparse architectures produced by pruning\nare difficult to train from the start, which would similarly improve training\nperformance.\n  We find that a standard pruning technique naturally uncovers subnetworks\nwhose initializations made them capable of training effectively. Based on these\nresults, we articulate the \"lottery ticket hypothesis:\" dense,\nrandomly-initialized, feed-forward networks contain subnetworks (\"winning\ntickets\") that - when trained in isolation - reach test accuracy comparable to\nthe original network in a similar number of iterations. The winning tickets we\nfind have won the initialization lottery: their connections have initial\nweights that make training particularly effective.\n  We present an algorithm to identify winning tickets and a series of\nexperiments that support the lottery ticket hypothesis and the importance of\nthese fortuitous initializations. We consistently find winning tickets that are\nless than 10-20% of the size of several fully-connected and convolutional\nfeed-forward architectures for MNIST and CIFAR10. Above this size, the winning\ntickets that we find learn faster than the original network and reach higher\ntest accuracy., write a short concise abstract based on this:  A new\nhypothesis, the lottery ticket hypothesis, suggests that randomly initialized neural networks contain subnetworks, or winning tickets, that can be trained to achieve high accuracy without the computational cost of training the entire network. These winning tickets are found to be less than 10-20% of the size of fully connected and convolutional feed-forward architectures for MNIST and CIFAR10, and learn faster than the original network while reaching higher test accuracy.",
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
    },
    {
        "abs": "The title is The Singular Values of Convolutional Layers, start with We characterize the singular values of the linear transformation associated\nwith a standard 2D multi-channel convolutional layer, enabling their efficient\ncomputation. This characterization also leads to an algorithm for projecting a\nconvolutional layer onto an operator-norm ball. We show that this is an\neffective regularizer; for example, it improves the test error of a deep\nresidual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%., write a short concise abstract based on this: \nThe singular values of convolutional layers are crucial for understanding their behavior and designing efficient algorithms for training deep neural networks. In this paper, we characterize the singular values of standard 2D multi-channel convolutional layers and propose an algorithm for projecting them onto an operator-norm ball. We demonstrate the effectiveness of this regularizer by applying it to a deep residual network using batch normalization on",
        "title": "The Singular Values of Convolutional Layers"
    },
    {
        "abs": "The title is A theoretical framework for deep locally connected ReLU network, start with Understanding theoretical properties of deep and locally connected nonlinear\nnetwork, such as deep convolutional neural network (DCNN), is still a hard\nproblem despite its empirical success. In this paper, we propose a novel\ntheoretical framework for such networks with ReLU nonlinearity. The framework\nexplicitly formulates data distribution, favors disentangled representations\nand is compatible with common regularization techniques such as Batch Norm. The\nframework is built upon teacher-student setting, by expanding the student\nforward/backward propagation onto the teacher's computational graph. The\nresulting model does not impose unrealistic assumptions (e.g., Gaussian inputs,\nindependence of activation, etc). Our framework could help facilitate\ntheoretical analysis of many practical issues, e.g. overfitting,\ngeneralization, disentangled representations in deep networks., write a short concise abstract based on this: \nA new theoretical framework for deep and locally connected ReLU networks is proposed, which explicitly formulates the data distribution, favors disentangled representations, and is compatible with common regularization techniques. The framework is built upon the teacher-student setting, expanding the student's forward/backward propagation onto the teacher's computational graph, without imposing unrealistic assumptions. This could help facilitate theoretical analysis of practical issues in deep learning, such as overfitting, generalization, and disentangled representations.",
        "title": "A theoretical framework for deep locally connected ReLU network"
    },
    {
        "abs": "The title is Neural Program Search: Solving Programming Tasks from Description and Examples, start with We present a Neural Program Search, an algorithm to generate programs from\nnatural language description and a small number of input/output examples. The\nalgorithm combines methods from Deep Learning and Program Synthesis fields by\ndesigning rich domain-specific language (DSL) and defining efficient search\nalgorithm guided by a Seq2Tree model on it. To evaluate the quality of the\napproach we also present a semi-synthetic dataset of descriptions with test\nexamples and corresponding programs. We show that our algorithm significantly\noutperforms a sequence-to-sequence model with attention baseline., write a short concise abstract based on this: \nIn this paper, we propose a novel algorithm called Neural Program Search (NPS) that can generate programs from natural language descriptions and a small number of input/output examples. NPS combines techniques from deep learning and program synthesis to create a rich domain-specific language (DSL) and an efficient search algorithm. We evaluate the quality of NPS using a semi-synthetic dataset of descriptions with test examples and show that it significantly outperforms a baseline sequence-to-sequence model with attention",
        "title": "Neural Program Search: Solving Programming Tasks from Description and Examples"
    },
    {
        "abs": "The title is Phrase-Based Attentions, start with Most state-of-the-art neural machine translation systems, despite being\ndifferent in architectural skeletons (e.g. recurrence, convolutional), share an\nindispensable feature: the Attention. However, most existing attention methods\nare token-based and ignore the importance of phrasal alignments, the key\ningredient for the success of phrase-based statistical machine translation. In\nthis paper, we propose novel phrase-based attention methods to model n-grams of\ntokens as attention entities. We incorporate our phrase-based attentions into\nthe recently proposed Transformer network, and demonstrate that our approach\nyields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for\nGerman-to-English translation tasks on WMT newstest2014 using WMT'16 training\ndata., write a short concise abstract based on this: \nMost state-of-the-art neural machine translation systems rely on attention mechanisms to improve translation quality. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, which are crucial for phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods that model n-grams of tokens as attention entities, and demonstrate their effectiveness in improving translation quality on English-to-German and German-to-English tasks. Our approach yields improvements of 1.3",
        "title": "Phrase-Based Attentions"
    },
    {
        "abs": "The title is Learning to Represent Edits, start with We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem., write a short concise abstract based on this:  Learning to Represent Edits: A Neural Network Approach to Editing Natural Language and Source Code. In this paper, we introduce a new approach to editing natural language and source code using neural networks. Our approach learns to represent edits in a way that can be applied to new inputs, allowing for more efficient and accurate editing. We evaluate our approach on a variety of edit data and show promising results. This work has the potential to revolutionize the way we edit natural language and source code, making it faster and more",
        "title": "Learning to Represent Edits"
    },
    {
        "abs": "The title is Not-So-Random Features, start with We propose a principled method for kernel learning, which relies on a\nFourier-analytic characterization of translation-invariant or\nrotation-invariant kernels. Our method produces a sequence of feature maps,\niteratively refining the SVM margin. We provide rigorous guarantees for\noptimality and generalization, interpreting our algorithm as online\nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations\non synthetic and real-world datasets demonstrate scalability and consistent\nimprovements over related random features-based methods., write a short concise abstract based on this:  In this paper, we propose a new method for kernel learning that leverages the Fourier transform to produce a sequence of feature maps that iteratively refine the SVM margin. Our approach provides rigorous guarantees for optimality and generalization, and can be interpreted as an online equilibrium-finding dynamics in a two-player min-max game. We evaluate our method on both synthetic and real-world datasets",
        "title": "Not-So-Random Features"
    },
    {
        "abs": "The title is Variational Continual Learning, start with This paper develops variational continual learning (VCL), a simple but\ngeneral framework for continual learning that fuses online variational\ninference (VI) and recent advances in Monte Carlo VI for neural networks. The\nframework can successfully train both deep discriminative models and deep\ngenerative models in complex continual learning settings where existing tasks\nevolve over time and entirely new tasks emerge. Experimental results show that\nVCL outperforms state-of-the-art continual learning methods on a variety of\ntasks, avoiding catastrophic forgetting in a fully automatic way., write a short concise abstract based on this: \nThis paper introduces Variational Continual Learning (VCL), a framework that combines online variational inference and recent advances in Monte Carlo VI for neural networks to successfully train both discriminative and generative models in complex continual learning settings. VCL outperforms state-of-the-art methods in avoiding catastrophic forgetting and can train both deep discriminative models and deep generative models.",
        "title": "Variational Continual Learning"
    },
    {
        "abs": "The title is On reproduction of On the regularization of Wasserstein GANs, start with This report has several purposes. First, our report is written to investigate\nthe reproducibility of the submitted paper On the regularization of Wasserstein\nGANs (2018). Second, among the experiments performed in the submitted paper,\nfive aspects were emphasized and reproduced: learning speed, stability,\nrobustness against hyperparameter, estimating the Wasserstein distance, and\nvarious sampling method. Finally, we identify which parts of the contribution\ncan be reproduced, and at what cost in terms of resources. All source code for\nreproduction is open to the public., write a short concise abstract based on this: \nThis report investigates the reproducibility of the paper \"On the regularization of Wasserstein GANs\" (2018) and reproduces five key experiments from the paper. The report assesses the learning speed, stability, robustness against hyperparameter, estimation of Wasserstein distance, and various sampling methods of the original paper. The authors also identify which parts of the contribution can be reproduced and at what cost in terms of resources. All source code for reproduction is made publicly available",
        "title": "On reproduction of On the regularization of Wasserstein GANs"
    },
    {
        "abs": "The title is Semantic embeddings for program behavior patterns, start with In this paper, we propose a new feature extraction technique for program\nexecution logs. First, we automatically extract complex patterns from a\nprogram's behavior graph. Then, we embed these patterns into a continuous space\nby training an autoencoder. We evaluate the proposed features on a real-world\nmalicious software detection task. We also find that the embedding space\ncaptures interpretable structures in the space of pattern parts., write a short concise abstract based on this: \nIn this paper, we propose a new technique for extracting meaningful features from program execution logs. Our approach involves automatically identifying complex patterns in a program's behavior graph, and then embedding these patterns into a continuous space using an autoencoder. We evaluate the effectiveness of our features on a real-world malicious software detection task, and find that they capture interpretable structures in the space of pattern parts",
        "title": "Semantic embeddings for program behavior patterns"
    },
    {
        "abs": "The title is Variational Autoencoder with Arbitrary Conditioning, start with We propose a single neural probabilistic model based on variational\nautoencoder that can be conditioned on an arbitrary subset of observed features\nand then sample the remaining features in \"one shot\". The features may be both\nreal-valued and categorical. Training of the model is performed by stochastic\nvariational Bayes. The experimental evaluation on synthetic data, as well as\nfeature imputation and image inpainting problems, shows the effectiveness of\nthe proposed approach and diversity of the generated samples., write a short concise abstract based on this: \nA new approach to variational autoencoders is proposed, which can condition on an arbitrary subset of observed features and generate new samples in one shot. The proposed model is based on stochastic variational Bayes and can handle both real-valued and categorical features. Experimental evaluation on synthetic data and real-world problems shows the effectiveness of the proposed approach and its ability to generate diverse samples.",
        "title": "Variational Autoencoder with Arbitrary Conditioning"
    },
    {
        "abs": "The title is Trading Information between Latents in Hierarchical Variational Autoencoders, start with Variational Autoencoders (VAEs) were originally motivated (Kingma & Welling,\n2014) as probabilistic generative models in which one performs approximate\nBayesian inference. The proposal of $\\beta$-VAEs (Higgins et al., 2017) breaks\nthis interpretation and generalizes VAEs to application domains beyond\ngenerative modeling (e.g., representation learning, clustering, or lossy data\ncompression) by introducing an objective function that allows practitioners to\ntrade off between the information content (\"bit rate\") of the latent\nrepresentation and the distortion of reconstructed data (Alemi et al., 2018).\nIn this paper, we reconsider this rate/distortion trade-off in the context of\nhierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We\nidentify a general class of inference models for which one can split the rate\ninto contributions from each layer, which can then be tuned independently. We\nderive theoretical bounds on the performance of downstream tasks as functions\nof the individual layers' rates and verify our theoretical findings in\nlarge-scale experiments. Our results provide guidance for practitioners on\nwhich region in rate-space to target for a given application., write a short concise abstract based on this:  In this paper, we investigate the rate/distortion trade-off in hierarchical Variational Autoencoders (VAEs), which are VAEs with multiple layers of latent variables. We identify a general class of inference models that allow us to split the rate into contributions from each layer, which can be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers' rates and verify our findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.",
        "title": "Trading Information between Latents in Hierarchical Variational Autoencoders"
    },
    {
        "abs": "The title is On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples, start with Understanding and characterizing the subspaces of adversarial examples aid in\nstudying the robustness of deep neural networks (DNNs) to adversarial\nperturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local\nintrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to\nstudy adversarial subspaces. It was demonstrated that LID can be used to\ncharacterize the adversarial subspaces associated with different attack\nmethods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign\nattack.\n  In this paper, we use MNIST and CIFAR-10 to conduct two new sets of\nexperiments that are absent in existing LID analysis and report the limitation\nof LID in characterizing the corresponding adversarial subspaces, which are (i)\noblivious attacks and LID analysis using adversarial examples with different\nconfidence levels; and (ii) black-box transfer attacks. For (i), we find that\nthe performance of LID is very sensitive to the confidence parameter deployed\nby an attack, and the LID learned from ensembles of adversarial examples with\nvarying confidence levels surprisingly gives poor performance. For (ii), we\nfind that when adversarial examples are crafted from another DNN model, LID is\nineffective in characterizing their adversarial subspaces. These two findings\ntogether suggest the limited capability of LID in characterizing the subspaces\nof adversarial examples., write a short concise abstract based on this:  Local Intrinsic Dimensionality (LID) has been proposed as a method to understand and characterize the subspaces of adversarial examples in deep neural networks. However, recent experiments have shown limitations of LID in characterizing these subspaces, particularly when it comes to oblivious attacks and black-box transfer attacks. These findings suggest that LID may not be a reliable method for studying the robustness of deep neural networks to adversarial perturbations.",
        "title": "On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples"
    },
    {
        "abs": "The title is A Variational Inequality Perspective on Generative Adversarial Networks, start with Generative adversarial networks (GANs) form a generative modeling approach\nknown for producing appealing samples, but they are notably difficult to train.\nOne common way to tackle this issue has been to propose new formulations of the\nGAN objective. Yet, surprisingly few studies have looked at optimization\nmethods designed for this adversarial training. In this work, we cast GAN\noptimization problems in the general variational inequality framework. Tapping\ninto the mathematical programming literature, we counter some common\nmisconceptions about the difficulties of saddle point optimization and propose\nto extend techniques designed for variational inequalities to the training of\nGANs. We apply averaging, extrapolation and a computationally cheaper variant\nthat we call extrapolation from the past to the stochastic gradient method\n(SGD) and Adam., write a short concise abstract based on this: \nGenerative adversarial networks (GANs) have shown great promise in generating realistic samples, but their training can be challenging. One approach to addressing this issue is to reformulate the GAN objective using optimization methods designed for variational inequalities. In this work, we leverage the mathematical programming literature to cast GAN optimization problems in the framework of variational inequalities, and propose techniques from this literature to improve the training of GANs. We demonstrate the effectiveness of our approach using several state-of-the-art GAN architectures and show that it can lead to improved training stability and convergence.",
        "title": "A Variational Inequality Perspective on Generative Adversarial Networks"
    },
    {
        "abs": "The title is Predict then Propagate: Graph Neural Networks meet Personalized PageRank, start with Neural message passing algorithms for semi-supervised classification on\ngraphs have recently achieved great success. However, for classifying a node\nthese methods only consider nodes that are a few propagation steps away and the\nsize of this utilized neighborhood is hard to extend. In this paper, we use the\nrelationship between graph convolutional networks (GCN) and PageRank to derive\nan improved propagation scheme based on personalized PageRank. We utilize this\npropagation procedure to construct a simple model, personalized propagation of\nneural predictions (PPNP), and its fast approximation, APPNP. Our model's\ntraining time is on par or faster and its number of parameters on par or lower\nthan previous models. It leverages a large, adjustable neighborhood for\nclassification and can be easily combined with any neural network. We show that\nthis model outperforms several recently proposed methods for semi-supervised\nclassification in the most thorough study done so far for GCN-like models. Our\nimplementation is available online., write a short concise abstract based on this:  In this paper, we propose a new method for semi-supervised classification on graphs called Personalized Propagation of Neural Predictions (PPNP). Our method uses a modified version of PageRank to propagate the predictions of a neural network through the graph, allowing it to consider the relationships between nodes that are further away than previous methods. We show that PPNP outperforms other recent methods for semi-supervised classification on graphs, and is faster and has fewer parameters than many other approaches. Our implementation is available online, making it easy to use and build upon.",
        "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank"
    },
    {
        "abs": "The title is Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples, start with We identify obfuscated gradients, a kind of gradient masking, as a phenomenon\nthat leads to a false sense of security in defenses against adversarial\nexamples. While defenses that cause obfuscated gradients appear to defeat\niterative optimization-based attacks, we find defenses relying on this effect\ncan be circumvented. We describe characteristic behaviors of defenses\nexhibiting the effect, and for each of the three types of obfuscated gradients\nwe discover, we develop attack techniques to overcome it. In a case study,\nexamining non-certified white-box-secure defenses at ICLR 2018, we find\nobfuscated gradients are a common occurrence, with 7 of 9 defenses relying on\nobfuscated gradients. Our new attacks successfully circumvent 6 completely, and\n1 partially, in the original threat model each paper considers., write a short concise abstract based on this: \nObfuscated gradients, a type of gradient masking, can give a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients may appear to defeat iterative optimization-based attacks, they can be circumvented. We identify characteristic behaviors of defenses exhibiting the effect and develop attack techniques to overcome it. In a case study examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on them. Our new attacks successfully circumvent 6 completely and 1 partially in the",
        "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
    },
    {
        "abs": "The title is Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking, start with Methods that learn representations of nodes in a graph play a critical role\nin network analysis since they enable many downstream learning tasks. We\npropose Graph2Gauss - an approach that can efficiently learn versatile node\nembeddings on large scale (attributed) graphs that show strong performance on\ntasks such as link prediction and node classification. Unlike most approaches\nthat represent nodes as point vectors in a low-dimensional continuous space, we\nembed each node as a Gaussian distribution, allowing us to capture uncertainty\nabout the representation. Furthermore, we propose an unsupervised method that\nhandles inductive learning scenarios and is applicable to different types of\ngraphs: plain/attributed, directed/undirected. By leveraging both the network\nstructure and the associated node attributes, we are able to generalize to\nunseen nodes without additional training. To learn the embeddings we adopt a\npersonalized ranking formulation w.r.t. the node distances that exploits the\nnatural ordering of the nodes imposed by the network structure. Experiments on\nreal world networks demonstrate the high performance of our approach,\noutperforming state-of-the-art network embedding methods on several different\ntasks. Additionally, we demonstrate the benefits of modeling uncertainty - by\nanalyzing it we can estimate neighborhood diversity and detect the intrinsic\nlatent dimensionality of a graph., write a short concise abstract based on this:  Deep Gaussian Embedding of Graphs (DGEG) is a novel approach for learning node representations on large-scale graphs. Unlike traditional methods that represent nodes as point vectors, DGEG embeds each node as a Gaussian distribution, capturing uncertainty in the representation. DGEG is an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs. By leveraging both network structure and node attributes, DGEG generalizes to unseen nodes without additional training. Experiments on real-world networks demonstrate the high performance of DGEG, outperforming state-of-the-art methods on several tasks. Additionally, DGEG provides insights into neighborhood diversity and intrinsic dimensionality of a graph.",
        "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking"
    },
    {
        "abs": "The title is Spherical CNNs, start with Convolutional Neural Networks (CNNs) have become the method of choice for\nlearning problems involving 2D planar images. However, a number of problems of\nrecent interest have created a demand for models that can analyze spherical\nimages. Examples include omnidirectional vision for drones, robots, and\nautonomous cars, molecular regression problems, and global weather and climate\nmodelling. A naive application of convolutional networks to a planar projection\nof the spherical signal is destined to fail, because the space-varying\ndistortions introduced by such a projection will make translational weight\nsharing ineffective.\n  In this paper we introduce the building blocks for constructing spherical\nCNNs. We propose a definition for the spherical cross-correlation that is both\nexpressive and rotation-equivariant. The spherical correlation satisfies a\ngeneralized Fourier theorem, which allows us to compute it efficiently using a\ngeneralized (non-commutative) Fast Fourier Transform (FFT) algorithm. We\ndemonstrate the computational efficiency, numerical accuracy, and effectiveness\nof spherical CNNs applied to 3D model recognition and atomization energy\nregression., write a short concise abstract based on this: \nIn this paper, we introduce the concept of spherical convolutional neural networks (S-CNNs) and propose a new definition for the spherical cross-correlation that is both expressive and rotation-equivariant. We demonstrate the efficiency and accuracy of S-CNNs in solving problems involving 3D models and atomization energy regression using a generalized fast Fourier transform (FFT) algorithm. Our results show that S-CNNs can effectively process and analyze spherical data, making them a promising approach for applications in robotics, computer vision, and materials science.",
        "title": "Spherical CNNs"
    },
    {
        "abs": "The title is Learning to SMILE(S), start with This paper shows how one can directly apply natural language processing (NLP)\nmethods to classification problems in cheminformatics. Connection between these\nseemingly separate fields is shown by considering standard textual\nrepresentation of compound, SMILES. The problem of activity prediction against\na target protein is considered, which is a crucial part of computer aided drug\ndesign process. Conducted experiments show that this way one can not only\noutrank state of the art results of hand crafted representations but also gets\ndirect structural insights into the way decisions are made., write a short concise abstract based on this: \nThis paper explores the application of natural language processing (NLP) techniques to classification problems in cheminformatics, specifically in the context of activity prediction against a target protein. The authors show that by using standard textual representation of compounds, SMILES, and NLP methods, they can outrank state-of-the-art results of hand-crafted representations and gain direct structural insights into the decision-making process. The results demonstrate the potential of combining NLP and cheminformatics",
        "title": "Learning to SMILE(S)"
    },
    {
        "abs": "The title is Apple Defect Detection Using Deep Learning Based Object Detection For Better Post Harvest Handling, start with The inclusion of Computer Vision and Deep Learning technologies in\nAgriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects., write a short concise abstract based on this: \nApple defect detection using deep learning-based object detection can improve post-harvest handling and quality evaluation. Recent computer vision and deep learning methods, such as YOLOv3, can help detect healthy apples from those with defects. This technology can increase the quality and productivity of farmers, particularly in the export market.",
        "title": "Apple Defect Detection Using Deep Learning Based Object Detection For Better Post Harvest Handling"
    },
    {
        "abs": "The title is Factorization tricks for LSTM networks, start with We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster to the near state-of the art\nperplexity while using significantly less RNN parameters., write a short concise abstract based on this: \nIn this paper, we present two novel techniques for reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks. The first approach involves factorizing the LSTM matrix into the product of two smaller matrices, while the second approach involves partitioning the LSTM matrix, its inputs, and states into independent groups. Both techniques allow for faster training of large LSTM networks with significantly fewer parameters, bringing them closer to state-",
        "title": "Factorization tricks for LSTM networks"
    },
    {
        "abs": "The title is Fast Reading Comprehension with ConvNets, start with State-of-the-art deep reading comprehension models are dominated by recurrent\nneural nets. Their sequential nature is a natural fit for language, but it also\nprecludes parallelization within an instances and often becomes the bottleneck\nfor deploying such models to latency critical scenarios. This is particularly\nproblematic for longer texts. Here we present a convolutional architecture as\nan alternative to these recurrent architectures. Using simple dilated\nconvolutional units in place of recurrent ones, we achieve results comparable\nto the state of the art on two question answering tasks, while at the same time\nachieving up to two orders of magnitude speedups for question answering., write a short concise abstract based on this: \nDeep learning models have shown great promise in improving reading comprehension, but current state-of-the-art models rely on recurrent neural networks (RNNs) which can be slow and sequential. In this paper, we propose a convolutional architecture as an alternative to RNNs, using dilated convolutional units to achieve comparable results while providing faster processing speeds. We demonstrate the effectiveness of our approach on two question answering tasks, achieving up to two orders of magnitude speedups compared to RNNs.",
        "title": "Fast Reading Comprehension with ConvNets"
    },
    {
        "abs": "The title is The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL, start with In this work, we analyze the reinstatement mechanism introduced by Ritter et\nal. (2018) to reveal two classes of neurons that emerge in the agent's working\nmemory (an epLSTM cell) when trained using episodic meta-RL on an episodic\nvariant of the Harlow visual fixation task. Specifically, Abstract neurons\nencode knowledge shared across tasks, while Episodic neurons carry information\nrelevant for a specific episode's task., write a short concise abstract based on this: \nIn this study, we investigate the emergence of two types of neurons in an agent's working memory when trained using episodic meta-RL on an episodic task. These neurons, called abstract and episodic neurons, encode different types of information. Abstract neurons represent knowledge shared across tasks, while episodic neurons carry information relevant to a specific episode's task. Our findings",
        "title": "The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL"
    },
    {
        "abs": "The title is A coding theorem for the rate-distortion-perception function, start with The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has\nemerged as a useful tool for thinking about realism and distortion of\nreconstructions in lossy compression. Unlike the rate-distortion function,\nhowever, it is unknown whether encoders and decoders exist that achieve the\nrate suggested by the RDPF. Building on results by Li and El Gamal (2018), we\nshow that the RDPF can indeed be achieved using stochastic, variable-length\ncodes. For this class of codes, we also prove that the RDPF lower-bounds the\nachievable rate, write a short concise abstract based on this: \nThe title of the paper is \"A Coding Theorem for the Rate-Distortion-Perception Function\" and it deals with the rate-distortion-perception function (RDPF), which is a tool used to analyze the realism and distortion of reconstructions in lossy compression. The paper shows that the RDPF can be achieved using stochastic, variable-length codes, and proves that the RDPF lower-bounds the achievable rate.",
        "title": "A coding theorem for the rate-distortion-perception function"
    },
    {
        "abs": "The title is Towards Neural Phrase-based Machine Translation, start with In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our\nmethod explicitly models the phrase structures in output sequences using\nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence\nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we\nintroduce a new layer to perform (soft) local reordering of input sequences.\nDifferent from existing neural machine translation (NMT) approaches, NPMT does\nnot use attention-based decoding mechanisms. Instead, it directly outputs\nphrases in a sequential order and can decode in linear time. Our experiments\nshow that NPMT achieves superior performances on IWSLT 2014\nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine\ntranslation tasks compared with strong NMT baselines. We also observe that our\nmethod produces meaningful phrases in output languages., write a short concise abstract based on this:  In this paper, the authors propose a new neural machine translation method called Neural Phrase-based Machine Translation (NPMT). Unlike traditional NMT approaches, NPMT does not use attention-based decoding mechanisms and instead directly outputs phrases in a sequential order. The authors use Sleep-WAke Networks (SWAN) to model the phrase structures in output sequences and introduce a new layer to perform (soft) local reordering of input sequences to mitigate the monotonic alignment requirement of SWAN. The authors show that NPMT achieves superior performances on several machine translation tasks compared to strong NMT baselines and",
        "title": "Towards Neural Phrase-based Machine Translation"
    },
    {
        "abs": "The title is Combating Adversarial Attacks Using Sparse Representations, start with It is by now well-known that small adversarial perturbations can induce\nclassification errors in deep neural networks (DNNs). In this paper, we make\nthe case that sparse representations of the input data are a crucial tool for\ncombating such attacks. For linear classifiers, we show that a sparsifying\nfront end is provably effective against $\\ell_{\\infty}$-bounded attacks,\nreducing output distortion due to the attack by a factor of roughly $K / N$\nwhere $N$ is the data dimension and $K$ is the sparsity level. We then extend\nthis concept to DNNs, showing that a \"locally linear\" model can be used to\ndevelop a theoretical foundation for crafting attacks and defenses.\nExperimental results for the MNIST dataset show the efficacy of the proposed\nsparsifying front end., write a short concise abstract based on this: \nIn this paper, we explore the use of sparse representations to combat adversarial attacks on deep neural networks. We show that a sparsifying front end can significantly reduce the impact of adversarial attacks on linear classifiers, and extend this concept to deep neural networks. Our experimental results on the MNIST dataset demonstrate the effectiveness of the proposed approach.",
        "title": "Combating Adversarial Attacks Using Sparse Representations"
    },
    {
        "abs": "The title is Supervised Policy Update for Deep Reinforcement Learning, start with We propose a new sample-efficient methodology, called Supervised Policy\nUpdate (SPU), for deep reinforcement learning. Starting with data generated by\nthe current policy, SPU formulates and solves a constrained optimization\nproblem in the non-parameterized proximal policy space. Using supervised\nregression, it then converts the optimal non-parameterized policy to a\nparameterized policy, from which it draws new samples. The methodology is\ngeneral in that it applies to both discrete and continuous action spaces, and\ncan handle a wide variety of proximity constraints for the non-parameterized\noptimization problem. We show how the Natural Policy Gradient and Trust Region\nPolicy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization\n(PPO) problem can be addressed by this methodology. The SPU implementation is\nmuch simpler than TRPO. In terms of sample efficiency, our extensive\nexperiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and\noutperforms PPO in Atari video game tasks., write a short concise abstract based on this:  Supervised Policy Update (SPU) is a new method for deep reinforcement learning that uses supervised regression to convert the optimal non-parameterized policy to a parameterized policy, from which new samples are drawn. SPU is general and can handle both discrete and continuous action spaces, as well as a variety of proximity constraints. It can address the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO) problem, and is simpler than TRPO. In experiments, SPU outperforms TRPO in simulated robotic tasks and outperforms PPO in Atari video game tasks.",
        "title": "Supervised Policy Update for Deep Reinforcement Learning"
    },
    {
        "abs": "The title is A Dataset To Evaluate The Representations Learned By Video Prediction Models, start with We present a parameterized synthetic dataset called Moving Symbols to support\nthe objective study of video prediction networks. Using several instantiations\nof the dataset in which variation is explicitly controlled, we highlight issues\nin an existing state-of-the-art approach and propose the use of a performance\nmetric with greater semantic meaning to improve experimental interpretability.\nOur dataset provides canonical test cases that will help the community better\nunderstand, and eventually improve, the representations learned by such\nnetworks in the future. Code is available at\nhttps://github.com/rszeto/moving-symbols ., write a short concise abstract based on this: \nIn this paper, we introduce a new dataset called Moving Symbols to evaluate the representations learned by video prediction models. The dataset provides a parameterized synthetic environment that allows for explicit control of variation, enabling researchers to better understand and improve the performance of these models. We highlight issues with an existing state-of-the-art approach and propose a new performance metric to enhance experimental interpretability. The code for the dataset is available at GitHub.",
        "title": "A Dataset To Evaluate The Representations Learned By Video Prediction Models"
    },
    {
        "abs": "The title is ICLR Reproducibility Challenge Report (Padam : Closing The Generalization Gap Of Adaptive Gradient Methods in Training Deep Neural Networks), start with This work is a part of ICLR Reproducibility Challenge 2019, we try to\nreproduce the results in the conference submission PADAM: Closing The\nGeneralization Gap of Adaptive Gradient Methods In Training Deep Neural\nNetworks. Adaptive gradient methods proposed in past demonstrate a degraded\ngeneralization performance than the stochastic gradient descent (SGD) with\nmomentum. The authors try to address this problem by designing a new\noptimization algorithm that bridges the gap between the space of Adaptive\nGradient algorithms and SGD with momentum. With this method a new tunable\nhyperparameter called partially adaptive parameter p is introduced that varies\nbetween [0, 0.5]. We build the proposed optimizer and use it to mirror the\nexperiments performed by the authors. We review and comment on the empirical\nanalysis performed by the authors. Finally, we also propose a future direction\nfor further study of Padam. Our code is available at:\nhttps://github.com/yashkant/Padam-Tensorflow, write a short concise abstract based on this: \nIn this work, we aim to reproduce the results of the ICLR 2019 paper \"Padam: Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks\". We use TensorFlow to implement the proposed optimizer and compare its performance to the original paper's results. Our findings show that the proposed optimizer achieves similar performance to the original method, closing the generalization gap between Adaptive Gradient methods and Stochastic Gradient Descent with momentum. We also propose a future direction for further study of Padam.",
        "title": "ICLR Reproducibility Challenge Report (Padam : Closing The Generalization Gap Of Adaptive Gradient Methods in Training Deep Neural Networks)"
    },
    {
        "abs": "The title is A comprehensive, application-oriented study of catastrophic forgetting in DNNs, start with We present a large-scale empirical study of catastrophic forgetting (CF) in\nmodern Deep Neural Network (DNN) models that perform sequential (or:\nincremental) learning. A new experimental protocol is proposed that enforces\ntypical constraints encountered in application scenarios. As the investigation\nis empirical, we evaluate CF behavior on the hitherto largest number of visual\nclassification datasets, from each of which we construct a representative\nnumber of Sequential Learning Tasks (SLTs) in close alignment to previous works\non CF. Our results clearly indicate that there is no model that avoids CF for\nall investigated datasets and SLTs under application conditions. We conclude\nwith a discussion of potential solutions and workarounds to CF, notably for the\nEWC and IMM models., write a short concise abstract based on this:  Catastrophic forgetting (CF) is a major challenge in deep neural network (DNN) models that perform sequential learning. In this study, we investigate CF in modern DNN models under application-oriented conditions. We propose a new experimental protocol and evaluate CF behavior on the largest number of visual classification datasets to date. Our results show that no model can avoid CF for all investigated datasets and tasks under application conditions. We discuss potential solutions and workarounds to CF, particularly for the Elastic Weight Consolidation (EWC) and Incremental Matrix Factorization (IMM) models.",
        "title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs"
    },
    {
        "abs": "The title is Adversarial Attacks on Graph Neural Networks via Meta Learning, start with Deep learning models for graphs have advanced the state of the art on many\ntasks. Despite their recent success, little is known about their robustness. We\ninvestigate training time attacks on graph neural networks for node\nclassification that perturb the discrete graph structure. Our core principle is\nto use meta-gradients to solve the bilevel problem underlying training-time\nattacks, essentially treating the graph as a hyperparameter to optimize. Our\nexperiments show that small graph perturbations consistently lead to a strong\ndecrease in performance for graph convolutional networks, and even transfer to\nunsupervised embeddings. Remarkably, the perturbations created by our algorithm\ncan misguide the graph neural networks such that they perform worse than a\nsimple baseline that ignores all relational information. Our attacks do not\nassume any knowledge about or access to the target classifiers., write a short concise abstract based on this: \nAdversarial attacks on graph neural networks (GNNs) have been largely unexplored despite their recent success in various tasks. In this work, we investigate the robustness of GNNs by developing a novel adversarial attack method that manipulates the discrete graph structure during training time. Our approach leverages meta-gradients to solve the bilevel problem underlying training-time attacks, effectively treating the graph as a hyperparameter to optimize. We demonstrate through experiments that small perturbations to the graph structure can significantly degrade the performance of GNNs, even transferring to unsupervised embeddings. Moreover, the perturbations created by our algorithm can misguide the GNNs to perform worse than a simple baseline that ignores",
        "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning"
    },
    {
        "abs": "The title is Multi-Domain Adversarial Learning, start with Multi-domain learning (MDL) aims at obtaining a model with minimal average\nrisk across multiple domains. Our empirical motivation is automated microscopy\ndata, where cultured cells are imaged after being exposed to known and unknown\nchemical perturbations, and each dataset displays significant experimental\nbias. This paper presents a multi-domain adversarial learning approach, MuLANN,\nto leverage multiple datasets with overlapping but distinct class sets, in a\nsemi-supervised setting. Our contributions include: i) a bound on the average-\nand worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss\nto accommodate semi-supervised multi-domain learning and domain adaptation;\niii) the experimental validation of the approach, improving on the state of the\nart on two standard image benchmarks, and a novel bioimage dataset, Cell., write a short concise abstract based on this:  In this paper, we propose a new approach to multi-domain learning called Multi-Domain Adversarial Learning (MuLANN) that leverages multiple datasets with overlapping but distinct class sets in a semi-supervised setting. We provide a bound on the average- and worst-domain risk in MDL and experimentally validate the approach on several benchmarks, achieving state-of-the-art performance. Our approach can be applied to various domains, including image classification, segmentation, and generation.",
        "title": "Multi-Domain Adversarial Learning"
    },
    {
        "abs": "The title is Robust Subspace Recovery Layer for Unsupervised Anomaly Detection, start with We propose a neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall., write a short concise abstract based on this: \nIn this paper, we propose a novel neural network architecture for unsupervised anomaly detection that leverages a robust subspace recovery layer (RSR layer) to identify outliers in the data. The RSR layer is integrated within an autoencoder, which maps the input data to a latent space and then extracts the underlying subspace. The decoder then smoothly maps the subspace back to a \"manifold\" close to the original inliers, allowing for the distinction between inliers and outliers based on their distances. Our extensive experiments with both image and document datasets demonstrate state-of-the-art precision and recall in anomaly detection.",
        "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection"
    },
    {
        "abs": "The title is Hierarchical interpretations for neural network predictions, start with Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise., write a short concise abstract based on this:  Deep neural networks (DNNs) are powerful predictors but their complex relationships are difficult to interpret. We propose a method, agglomerative contextual decomposition (ACD), to explain DNN predictions by producing a hierarchical clustering of input features and their contribution to the final prediction. We demonstrate its effectiveness through examples from sentiment analysis and image classification, showing that ACD can identify incorrect predictions and dataset bias. Additionally, ACD enables users to choose the more accurate of two DNNs and to better trust the outputs. Finally, we show that ACD's hierarchy is robust to adversarial perturbations, indicating that it captures fundamental aspects of the input.",
        "title": "Hierarchical interpretations for neural network predictions"
    },
    {
        "abs": "The title is TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer, start with In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples., write a short concise abstract based on this:  In this paper, we present TimbreTron, a novel approach for musical timbre transfer that leverages the power of WaveNet and CycleGAN. TimbreTron first converts the audio signal into a time-frequency representation using the Constant Q Transform (CQT), and then applies image-domain style transfer to manipulate the timbre of the signal. Finally, a conditional WaveNet synthesizer is used to generate a high-quality waveform that preserves the original musical content. Our experiments show that TimbreTron can effectively transfer the timbre of an audio signal while preserving other musical aspects, and outperform existing methods in terms of perceptual quality.",
        "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer"
    },
    {
        "abs": "The title is Low-dimensional statistical manifold embedding of directed graphs, start with We propose a novel node embedding of directed graphs to statistical\nmanifolds, which is based on a global minimization of pairwise relative entropy\nand graph geodesics in a non-linear way. Each node is encoded with a\nprobability density function over a measurable space. Furthermore, we analyze\nthe connection between the geometrical properties of such embedding and their\nefficient learning procedure. Extensive experiments show that our proposed\nembedding is better in preserving the global geodesic information of graphs, as\nwell as outperforming existing embedding models on directed graphs in a variety\nof evaluation metrics, in an unsupervised setting., write a short concise abstract based on this:  In this paper, we propose a novel method for embedding directed graphs into statistical manifolds, which preserves the global geodesic information of the graph. Our approach is based on minimizing pairwise relative entropy and graph geodesics in a non-linear way, and each node is encoded with a probability density function over a measurable space. We also analyze the connection between the geometric properties of the embedding and the efficient learning procedure. Extensive experiments show that our proposed embedding outperforms existing models on directed graphs in various evaluation metrics, in an unsupervised setting.",
        "title": "Low-dimensional statistical manifold embedding of directed graphs"
    },
    {
        "abs": "The title is Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity, start with The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks., write a short concise abstract based on this: \nDifferentiable neuromodulation of plasticity in artificial neural networks enables self-modifying abilities, improving performance on reinforcement learning and supervised learning tasks. This approach extends previous work on differentiable Hebbian plasticity and offers a powerful new framework for training neural networks.",
        "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity"
    },
    {
        "abs": "The title is Mixed-curvature Variational Autoencoders, start with Euclidean geometry has historically been the typical \"workhorse\" for machine\nlearning applications due to its power and simplicity. However, it has recently\nbeen shown that geometric spaces with constant non-zero curvature improve\nrepresentations and performance on a variety of data types and downstream\ntasks. Consequently, generative models like Variational Autoencoders (VAEs)\nhave been successfully generalized to elliptical and hyperbolic latent spaces.\nWhile these approaches work well on data with particular kinds of biases e.g.\ntree-like data for a hyperbolic VAE, there exists no generic approach unifying\nand leveraging all three models. We develop a Mixed-curvature Variational\nAutoencoder, an efficient way to train a VAE whose latent space is a product of\nconstant curvature Riemannian manifolds, where the per-component curvature is\nfixed or learnable. This generalizes the Euclidean VAE to curved latent spaces\nand recovers it when curvatures of all latent space components go to 0., write a short concise abstract based on this: \nMixed-curvature Variational Autoencoders (MC-VAEs) are a new approach to training generative models that leverages the power of non-Euclidean geometry. By combining the flexibility of hyperbolic and elliptical spaces with the efficiency of Euclidean space, MC-VAEs can generate diverse and structured representations of data. Unlike traditional VAEs, which are limited to Euclidean spaces, MC-VAEs can adapt to different types of data by changing the curvature of the latent space. This makes them a versatile tool for a wide range of applications, including image generation, text-to-image synthesis, and unsupervised learning.",
        "title": "Mixed-curvature Variational Autoencoders"
    },
    {
        "abs": "The title is No Training Required: Exploring Random Encoders for Sentence Classification, start with We explore various methods for computing sentence representations from\npre-trained word embeddings without any training, i.e., using nothing but\nrandom parameterizations. Our aim is to put sentence embeddings on more solid\nfooting by 1) looking at how much modern sentence embeddings gain over random\nmethods---as it turns out, surprisingly little; and by 2) providing the field\nwith more appropriate baselines going forward---which are, as it turns out,\nquite strong. We also make important observations about proper experimental\nprotocol for sentence classification evaluation, together with recommendations\nfor future research., write a short concise abstract based on this: \nIn this paper, we investigate the use of random encoders for sentence classification without any training data. We find that modern sentence embeddings provide little improvement over random methods, and provide strong baselines for future research. Our findings highlight the need for more appropriate evaluation protocols and recommendations for future research in this area.",
        "title": "No Training Required: Exploring Random Encoders for Sentence Classification"
    },
    {
        "abs": "The title is Improving Generalization and Stability of Generative Adversarial Networks, start with Generative Adversarial Networks (GANs) are one of the most popular tools for\nlearning complex high dimensional distributions. However, generalization\nproperties of GANs have not been well understood. In this paper, we analyze the\ngeneralization of GANs in practical settings. We show that discriminators\ntrained on discrete datasets with the original GAN loss have poor\ngeneralization capability and do not approximate the theoretically optimal\ndiscriminator. We propose a zero-centered gradient penalty for improving the\ngeneralization of the discriminator by pushing it toward the optimal\ndiscriminator. The penalty guarantees the generalization and convergence of\nGANs. Experiments on synthetic and large scale datasets verify our theoretical\nanalysis., write a short concise abstract based on this:  Generative Adversarial Networks (GANs) are widely used for learning complex distributions, but their generalization properties are not well understood. This paper analyzes the generalization of GANs in practical settings and shows that the original GAN loss does not lead to a good discriminator. To improve the generalization, a zero-centered gradient penalty is proposed, which guarantees the convergence and generalization of GANs. Experimental results on synthetic and large-scale datasets verify the theoretical analysis.",
        "title": "Improving Generalization and Stability of Generative Adversarial Networks"
    },
    {
        "abs": "The title is Wasserstein Barycenter Model Ensembling, start with In this paper we propose to perform model ensembling in a multiclass or a\nmultilabel learning setting using Wasserstein (W.) barycenters. Optimal\ntransport metrics, such as the Wasserstein distance, allow incorporating\nsemantic side information such as word embeddings. Using W. barycenters to find\nthe consensus between models allows us to balance confidence and semantics in\nfinding the agreement between the models. We show applications of Wasserstein\nensembling in attribute-based classification, multilabel learning and image\ncaptioning generation. These results show that the W. ensembling is a viable\nalternative to the basic geometric or arithmetic mean ensembling., write a short concise abstract based on this:  In this paper, we propose a new method for model ensembling in multiclass or multilabel learning tasks using Wasserstein barycenters. This approach allows incorporating semantic side information, such as word embeddings, to find the consensus between models while balancing confidence and semantics. We demonstrate the effectiveness of Wasserstein ensembling in attribute-based classification, multilabel learning, and image captioning generation tasks, showing that it is a viable alternative to traditional geometric or arithmetic mean ensembling.",
        "title": "Wasserstein Barycenter Model Ensembling"
    },
    {
        "abs": "The title is Stochastic Prediction of Multi-Agent Interactions from Partial Observations, start with We present a method that learns to integrate temporal information, from a\nlearned dynamics model, with ambiguous visual information, from a learned\nvision model, in the context of interacting agents. Our method is based on a\ngraph-structured variational recurrent neural network (Graph-VRNN), which is\ntrained end-to-end to infer the current state of the (partially observed)\nworld, as well as to forecast future states. We show that our method\noutperforms various baselines on two sports datasets, one based on real\nbasketball trajectories, and one generated by a soccer game engine., write a short concise abstract based on this:  In this paper, we propose a novel method for stochastic prediction of multi-agent interactions from partial observations. Our approach combines a learned dynamics model and a learned vision model to integrate temporal information and ambiguous visual information, respectively. We use a graph-structured variational recurrent neural network (Graph-VRNN) to model the interactions between agents and the environment. Our method outperforms various baselines on two sports datasets, demonstrating its effectiveness in predicting the behavior of interacting agents.",
        "title": "Stochastic Prediction of Multi-Agent Interactions from Partial Observations"
    },
    {
        "abs": "The title is Equi-normalization of Neural Networks, start with Modern neural networks are over-parametrized. In particular, each rectified\nlinear hidden unit can be modified by a multiplicative factor by adjusting\ninput and output weights, without changing the rest of the network. Inspired by\nthe Sinkhorn-Knopp algorithm, we introduce a fast iterative method for\nminimizing the L2 norm of the weights, equivalently the weight decay\nregularizer. It provably converges to a unique solution. Interleaving our\nalgorithm with SGD during training improves the test accuracy. For small\nbatches, our approach offers an alternative to batch-and group-normalization on\nCIFAR-10 and ImageNet with a ResNet-18., write a short concise abstract based on this: \nIn this paper, we propose a new method for normalizing the weights of a neural network during training, called equi-normalization. Our approach is inspired by the Sinkhorn-Knopp algorithm and provably converges to a unique solution. We show that interleaving our algorithm with stochastic gradient descent (SGD) during training improves test accuracy for small batches, and offer an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18",
        "title": "Equi-normalization of Neural Networks"
    },
    {
        "abs": "The title is DeepSphere: towards an equivariant graph-based spherical CNN, start with Spherical data is found in many applications. By modeling the discretized\nsphere as a graph, we can accommodate non-uniformly distributed, partial, and\nchanging samplings. Moreover, graph convolutions are computationally more\nefficient than spherical convolutions. As equivariance is desired to exploit\nrotational symmetries, we discuss how to approach rotation equivariance using\nthe graph neural network introduced in Defferrard et al. (2016). Experiments\nshow good performance on rotation-invariant learning problems. Code and\nexamples are available at https://github.com/SwissDataScienceCenter/DeepSphere, write a short concise abstract based on this: \nIn this paper, we propose a novel approach to graph-based convolutional neural networks (CNNs) for spherical data, called DeepSphere. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. To ensure equivariance, we discuss how to approach rotation equivar",
        "title": "DeepSphere: towards an equivariant graph-based spherical CNN"
    },
    {
        "abs": "The title is Graph Wavelet Neural Network, start with We present graph wavelet neural network (GWNN), a novel graph convolutional\nneural network (CNN), leveraging graph wavelet transform to address the\nshortcomings of previous spectral graph CNN methods that depend on graph\nFourier transform. Different from graph Fourier transform, graph wavelet\ntransform can be obtained via a fast algorithm without requiring matrix\neigendecomposition with high computational cost. Moreover, graph wavelets are\nsparse and localized in vertex domain, offering high efficiency and good\ninterpretability for graph convolution. The proposed GWNN significantly\noutperforms previous spectral graph CNNs in the task of graph-based\nsemi-supervised classification on three benchmark datasets: Cora, Citeseer and\nPubmed., write a short concise abstract based on this: \nAbstract: In this paper, we propose a novel graph convolutional neural network (CNN) called Graph Wavelet Neural Network (GWNN) that leverages graph wavelet transform to address the shortcomings of previous spectral graph CNN methods. Unlike graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous",
        "title": "Graph Wavelet Neural Network"
    },
    {
        "abs": "The title is Variational Autoencoder with Arbitrary Conditioning, start with We propose a single neural probabilistic model based on variational\nautoencoder that can be conditioned on an arbitrary subset of observed features\nand then sample the remaining features in \"one shot\". The features may be both\nreal-valued and categorical. Training of the model is performed by stochastic\nvariational Bayes. The experimental evaluation on synthetic data, as well as\nfeature imputation and image inpainting problems, shows the effectiveness of\nthe proposed approach and diversity of the generated samples., write a short concise abstract based on this: \nA new approach to variational autoencoders is proposed, which can condition on an arbitrary subset of observed features and generate new samples in one shot. The proposed model is based on stochastic variational Bayes and can handle both real-valued and categorical features. Experimental evaluation on synthetic data and real-world problems shows the effectiveness of the proposed approach and its ability to generate diverse samples.",
        "title": "Variational Autoencoder with Arbitrary Conditioning"
    },
    {
        "abs": "The title is Learning Programmatically Structured Representations with Perceptor Gradients, start with We present the perceptor gradients algorithm -- a novel approach to learning\nsymbolic representations based on the idea of decomposing an agent's policy\ninto i) a perceptor network extracting symbols from raw observation data and\nii) a task encoding program which maps the input symbols to output actions. We\nshow that the proposed algorithm is able to learn representations that can be\ndirectly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A*\nplanner. Our experimental results confirm that the perceptor gradients\nalgorithm is able to efficiently learn transferable symbolic representations as\nwell as generate new observations according to a semantically meaningful\nspecification., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to learning symbolic representations through the use of perceptor gradients. Our algorithm decomposes an agent's policy into two components: a perceptor network that extracts symbols from raw observation data, and a task encoding program that maps the input symbols to output actions. We demonstrate that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner, and generate new observations according to a semantically meaningful specification. Our experimental results confirm the efficiency and",
        "title": "Learning Programmatically Structured Representations with Perceptor Gradients"
    },
    {
        "abs": "The title is Learning Graph Neural Networks with Noisy Labels, start with We study the robustness to symmetric label noise of GNNs training procedures.\nBy combining the nonlinear neural message-passing models (e.g. Graph\nIsomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present\na noise-tolerant approach for the graph classification task. Our experiments\nshow that test accuracy can be improved under the artificial symmetric noisy\nsetting., write a short concise abstract based on this: \nLearning Graph Neural Networks with Noisy Labels: A Robust Approach\nAbstract: In this paper, we investigate the robustness of graph neural networks (GNNs) to symmetric label noise during training. By combining nonlinear neural message-passing models with loss correction methods, we propose a noise-tolerant approach for the",
        "title": "Learning Graph Neural Networks with Noisy Labels"
    },
    {
        "abs": "The title is Inferring Javascript types using Graph Neural Networks, start with The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy above $90\\%$, which improves on previous\nsimilar work., write a short concise abstract based on this: \nIn this paper, we propose a novel approach for inferring JavaScript types using graph neural networks. Our model achieves an accuracy of above $90\\%$ in predicting token types for JavaScript programs, outperforming previous work in the field. This represents a significant step towards automatic code repair and improving the efficiency of software development.",
        "title": "Inferring Javascript types using Graph Neural Networks"
    },
    {
        "abs": "The title is Dynamics-aware Embeddings, start with In this paper we consider self-supervised representation learning to improve\nsample efficiency in reinforcement learning (RL). We propose a forward\nprediction objective for simultaneously learning embeddings of states and\naction sequences. These embeddings capture the structure of the environment's\ndynamics, enabling efficient policy learning. We demonstrate that our action\nembeddings alone improve the sample efficiency and peak performance of\nmodel-free RL on control from low-dimensional states. By combining state and\naction embeddings, we achieve efficient learning of high-quality policies on\ngoal-conditioned continuous control from pixel observations in only 1-2 million\nenvironment steps., write a short concise abstract based on this:  In this paper, we propose a self-supervised representation learning method to improve sample efficiency in reinforcement learning. By learning embeddings of states and action sequences, we capture the dynamics of the environment and enable efficient policy learning. Our approach achieves significant improvements in sample efficiency and peak performance on control tasks from low-dimensional states, and demonstrates the effectiveness of combining state and action embeddings for goal-conditioned continuous control tasks.",
        "title": "Dynamics-aware Embeddings"
    },
    {
        "abs": "The title is Representation Learning with Multisets, start with We study the problem of learning permutation invariant representations that\ncan capture \"flexible\" notions of containment. We formalize this problem via a\nmeasure theoretic definition of multisets, and obtain a theoretically-motivated\nlearning model. We propose training this model on a novel task: predicting the\nsize of the symmetric difference (or intersection) between pairs of multisets.\nWe demonstrate that our model not only performs very well on predicting\ncontainment relations (and more effectively predicts the sizes of symmetric\ndifferences and intersections than DeepSets-based approaches with unconstrained\nobject representations), but that it also learns meaningful representations., write a short concise abstract based on this: \nIn this paper, we propose a new approach to representation learning called Representation Learning with Multisets (RLM). We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment, and formalize this problem via a measure theoretic definition of multisets. Our approach involves training a model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs well on predicting containment relations,",
        "title": "Representation Learning with Multisets"
    },
    {
        "abs": "The title is GAN-based Generation and Automatic Selection of Explanations for Neural Networks, start with One way to interpret trained deep neural networks (DNNs) is by inspecting\ncharacteristics that neurons in the model respond to, such as by iteratively\noptimising the model input (e.g., an image) to maximally activate specific\nneurons. However, this requires a careful selection of hyper-parameters to\ngenerate interpretable examples for each neuron of interest, and current\nmethods rely on a manual, qualitative evaluation of each setting, which is\nprohibitively slow. We introduce a new metric that uses Fr\\'echet Inception\nDistance (FID) to encourage similarity between model activations for real and\ngenerated data. This provides an efficient way to evaluate a set of generated\nexamples for each setting of hyper-parameters. We also propose a novel\nGAN-based method for generating explanations that enables an efficient search\nthrough the input space and imposes a strong prior favouring realistic outputs.\nWe apply our approach to a classification model trained to predict whether a\nmusic audio recording contains singing voice. Our results suggest that this\nproposed metric successfully selects hyper-parameters leading to interpretable\nexamples, avoiding the need for manual evaluation. Moreover, we see that\nexamples synthesised to maximise or minimise the predicted probability of\nsinging voice presence exhibit vocal or non-vocal characteristics,\nrespectively, suggesting that our approach is able to generate suitable\nexplanations for understanding concepts learned by a neural network., write a short concise abstract based on this: \nA new approach is proposed for generating explanations for deep neural networks (DNNs) using Generative Adversarial Networks (GANs). The method aims to efficiently search through the input space and generate examples that are similar to the real data, while also imposing a strong prior favouring realistic outputs. The proposed metric uses Fr\u00e9chet Inception Distance (FID) to evaluate the similarity between model activations for real and generated data, allowing for an efficient evaluation of generated examples for each setting of hyper-parameters. The approach is applied to a classification model trained to predict whether a music audio recording contains singing voice, and the results show that the proposed metric successfully selects hyper-parameters leading to interpretable examples, avoiding the need for manual evaluation. Additionally, the generated examples exhibit vocal or non-vocal characteristics, suggesting that the approach is able to generate suitable explanations for understanding concepts learned by the DNN.",
        "title": "GAN-based Generation and Automatic Selection of Explanations for Neural Networks"
    },
    {
        "abs": "The title is The Singular Values of Convolutional Layers, start with We characterize the singular values of the linear transformation associated\nwith a standard 2D multi-channel convolutional layer, enabling their efficient\ncomputation. This characterization also leads to an algorithm for projecting a\nconvolutional layer onto an operator-norm ball. We show that this is an\neffective regularizer; for example, it improves the test error of a deep\nresidual network using batch normalization on CIFAR-10 from 6.2\\% to 5.3\\%., write a short concise abstract based on this: \nThe singular values of convolutional layers are crucial for understanding their behavior and designing efficient algorithms for training deep neural networks. In this paper, we characterize the singular values of standard 2D multi-channel convolutional layers and propose an algorithm for projecting them onto an operator-norm ball. We demonstrate the effectiveness of this regularizer by applying it to a deep residual network using batch normalization on",
        "title": "The Singular Values of Convolutional Layers"
    },
    {
        "abs": "The title is Learning to Represent Edits, start with We introduce the problem of learning distributed representations of edits. By\ncombining a \"neural editor\" with an \"edit encoder\", our models learn to\nrepresent the salient information of an edit and can be used to apply edits to\nnew inputs. We experiment on natural language and source code edit data. Our\nevaluation yields promising results that suggest that our neural network models\nlearn to capture the structure and semantics of edits. We hope that this\ninteresting task and data source will inspire other researchers to work further\non this problem., write a short concise abstract based on this:  Learning to Represent Edits: A Neural Network Approach to Editing Natural Language and Source Code. In this paper, we introduce a new approach to editing natural language and source code using neural networks. Our approach learns to represent edits in a way that can be applied to new inputs, allowing for more efficient and accurate editing. We evaluate our approach on a variety of edit data and show promising results. This work has the potential to revolutionize the way we edit natural language and source code, making it faster and more",
        "title": "Learning to Represent Edits"
    },
    {
        "abs": "The title is Symplectic Recurrent Neural Networks, start with We propose Symplectic Recurrent Neural Networks (SRNNs) as learning\nalgorithms that capture the dynamics of physical systems from observed\ntrajectories. An SRNN models the Hamiltonian function of the system by a neural\nnetwork and furthermore leverages symplectic integration, multiple-step\ntraining and initial state optimization to address the challenging numerical\nissues associated with Hamiltonian systems. We show that SRNNs succeed reliably\non complex and noisy Hamiltonian systems. We also show how to augment the SRNN\nintegration scheme in order to handle stiff dynamical systems such as bouncing\nbilliards., write a short concise abstract based on this: \nSymplectic Recurrent Neural Networks (SRNNs) are proposed as learning algorithms to capture the dynamics of physical systems from observed trajectories. SRNNs model the Hamiltonian function of the system using a neural network and leverage symplectic integration, multiple-step training, and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. SRNNs are shown to be reliable on complex and noisy Hamiltonian systems, and can handle stiff dynamical systems such as bouncing",
        "title": "Symplectic Recurrent Neural Networks"
    },
    {
        "abs": "The title is Spectral embedding of regularized block models, start with Spectral embedding is a popular technique for the representation of graph\ndata. Several regularization techniques have been proposed to improve the\nquality of the embedding with respect to downstream tasks like clustering. In\nthis paper, we explain on a simple block model the impact of the complete graph\nregularization, whereby a constant is added to all entries of the adjacency\nmatrix. Specifically, we show that the regularization forces the spectral\nembedding to focus on the largest blocks, making the representation less\nsensitive to noise or outliers. We illustrate these results on both on both\nsynthetic and real data, showing how regularization improves standard\nclustering scores., write a short concise abstract based on this: \nSpectral embedding is a powerful technique for representing graph data, but it can be sensitive to noise or outliers. By adding a regularization term to the spectral embedding algorithm, we can improve its robustness and focus on the largest blocks in the graph. We demonstrate the effectiveness of this approach on both synthetic and real data, showing that regularization leads to better clustering scores.",
        "title": "Spectral embedding of regularized block models"
    },
    {
        "abs": "The title is Locality and compositionality in zero-shot learning, start with In this work we study locality and compositionality in the context of\nlearning representations for Zero Shot Learning (ZSL). In order to well-isolate\nthe importance of these properties in learned representations, we impose the\nadditional constraint that, differently from most recent work in ZSL, no\npre-training on different datasets (e.g. ImageNet) is performed. The results of\nour experiments show how locality, in terms of small parts of the input, and\ncompositionality, i.e. how well can the learned representations be expressed as\na function of a smaller vocabulary, are both deeply related to generalization\nand motivate the focus on more local-aware models in future research directions\nfor representation learning., write a short concise abstract based on this:  In this work, we investigate the role of locality and compositionality in learned representations for zero-shot learning. We find that both properties are crucial for generalization and suggest a focus on more local-aware models in future research.",
        "title": "Locality and compositionality in zero-shot learning"
    },
    {
        "abs": "The title is Training individually fair ML models with Sensitive Subspace Robustness, start with We consider training machine learning models that are fair in the sense that\ntheir performance is invariant under certain sensitive perturbations to the\ninputs. For example, the performance of a resume screening system should be\ninvariant under changes to the gender and/or ethnicity of the applicant. We\nformalize this notion of algorithmic fairness as a variant of individual\nfairness and develop a distributionally robust optimization approach to enforce\nit during training. We also demonstrate the effectiveness of the approach on\ntwo ML tasks that are susceptible to gender and racial biases., write a short concise abstract based on this: \nIn this paper, we propose a new approach to training machine learning models that are fair and robust to sensitive perturbations. Our approach, called Sensitive Subspace Robustness (SSR), formalizes the notion of algorithmic fairness as a variant of individual fairness and uses distributionally robust optimization to enforce it during training. We demonstrate the effectiveness of SSR on two ML tasks that are susceptible to gender and racial biases, showing that it can improve the fairness of the models while maintaining their accuracy. Our",
        "title": "Training individually fair ML models with Sensitive Subspace Robustness"
    },
    {
        "abs": "The title is Predict then Propagate: Graph Neural Networks meet Personalized PageRank, start with Neural message passing algorithms for semi-supervised classification on\ngraphs have recently achieved great success. However, for classifying a node\nthese methods only consider nodes that are a few propagation steps away and the\nsize of this utilized neighborhood is hard to extend. In this paper, we use the\nrelationship between graph convolutional networks (GCN) and PageRank to derive\nan improved propagation scheme based on personalized PageRank. We utilize this\npropagation procedure to construct a simple model, personalized propagation of\nneural predictions (PPNP), and its fast approximation, APPNP. Our model's\ntraining time is on par or faster and its number of parameters on par or lower\nthan previous models. It leverages a large, adjustable neighborhood for\nclassification and can be easily combined with any neural network. We show that\nthis model outperforms several recently proposed methods for semi-supervised\nclassification in the most thorough study done so far for GCN-like models. Our\nimplementation is available online., write a short concise abstract based on this:  In this paper, we propose a new method for semi-supervised classification on graphs called Personalized Propagation of Neural Predictions (PPNP). Our method uses a modified version of PageRank to propagate the predictions of a neural network through the graph, allowing it to consider the relationships between nodes that are further away than previous methods. We show that PPNP outperforms other recent methods for semi-supervised classification on graphs, and is faster and has fewer parameters than many other approaches. Our implementation is available online, making it easy to use and build upon.",
        "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank"
    },
    {
        "abs": "The title is Regularization Matters in Policy Optimization, start with Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg ., write a short concise abstract based on this: \nRegularization techniques have been largely overlooked in deep reinforcement learning (Deep RL) despite their success in traditional machine learning. In this work, we investigate the effectiveness of regularization techniques on policy optimization algorithms for continuous control tasks. Our findings show that conventional regularization techniques can significantly improve performance, especially on harder tasks. We also compare these techniques with entropy regularization and analyze their effectiveness from four perspectives. Our results provide guidance for future practices in regularizing policy optimization algorithms in Deep RL.",
        "title": "Regularization Matters in Policy Optimization"
    },
    {
        "abs": "The title is On the loss landscape of a class of deep neural networks with no bad local valleys, start with We identify a class of over-parameterized deep neural networks with standard\nactivation functions and cross-entropy loss which provably have no bad local\nvalley, in the sense that from any point in parameter space there exists a\ncontinuous path on which the cross-entropy loss is non-increasing and gets\narbitrarily close to zero. This implies that these networks have no sub-optimal\nstrict local minima., write a short concise abstract based on this: \nAbstract: We introduce a class of deep neural networks with standard activation functions and cross-entropy loss that have no bad local valleys. This means that from any point in parameter space, there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This result implies that these networks have no sub-optimal strict local minima, and thus are guaranteed to converge to",
        "title": "On the loss landscape of a class of deep neural networks with no bad local valleys"
    },
    {
        "abs": "The title is A theoretical framework for deep locally connected ReLU network, start with Understanding theoretical properties of deep and locally connected nonlinear\nnetwork, such as deep convolutional neural network (DCNN), is still a hard\nproblem despite its empirical success. In this paper, we propose a novel\ntheoretical framework for such networks with ReLU nonlinearity. The framework\nexplicitly formulates data distribution, favors disentangled representations\nand is compatible with common regularization techniques such as Batch Norm. The\nframework is built upon teacher-student setting, by expanding the student\nforward/backward propagation onto the teacher's computational graph. The\nresulting model does not impose unrealistic assumptions (e.g., Gaussian inputs,\nindependence of activation, etc). Our framework could help facilitate\ntheoretical analysis of many practical issues, e.g. overfitting,\ngeneralization, disentangled representations in deep networks., write a short concise abstract based on this: \nA new theoretical framework for deep and locally connected ReLU networks is proposed, which explicitly formulates the data distribution, favors disentangled representations, and is compatible with common regularization techniques. The framework is built upon the teacher-student setting, expanding the student's forward/backward propagation onto the teacher's computational graph, without imposing unrealistic assumptions. This could help facilitate theoretical analysis of practical issues in deep learning, such as overfitting, generalization, and disentangled representations.",
        "title": "A theoretical framework for deep locally connected ReLU network"
    },
    {
        "abs": "The title is Efficient GAN-Based Anomaly Detection, start with Generative adversarial networks (GANs) are able to model the complex\nhighdimensional distributions of real-world data, which suggests they could be\neffective for anomaly detection. However, few works have explored the use of\nGANs for the anomaly detection task. We leverage recently developed GAN models\nfor anomaly detection, and achieve state-of-the-art performance on image and\nnetwork intrusion datasets, while being several hundred-fold faster at test\ntime than the only published GAN-based method., write a short concise abstract based on this: \nEfficient GAN-Based Anomaly Detection: Leveraging Recent Advances in GANs for State-of-the-Art Performance and Speed.",
        "title": "Efficient GAN-Based Anomaly Detection"
    },
    {
        "abs": "The title is Phrase-Based Attentions, start with Most state-of-the-art neural machine translation systems, despite being\ndifferent in architectural skeletons (e.g. recurrence, convolutional), share an\nindispensable feature: the Attention. However, most existing attention methods\nare token-based and ignore the importance of phrasal alignments, the key\ningredient for the success of phrase-based statistical machine translation. In\nthis paper, we propose novel phrase-based attention methods to model n-grams of\ntokens as attention entities. We incorporate our phrase-based attentions into\nthe recently proposed Transformer network, and demonstrate that our approach\nyields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for\nGerman-to-English translation tasks on WMT newstest2014 using WMT'16 training\ndata., write a short concise abstract based on this: \nMost state-of-the-art neural machine translation systems rely on attention mechanisms to improve translation quality. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, which are crucial for phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods that model n-grams of tokens as attention entities, and demonstrate their effectiveness in improving translation quality on English-to-German and German-to-English tasks. Our approach yields improvements of 1.3",
        "title": "Phrase-Based Attentions"
    },
    {
        "abs": "The title is PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction, start with We propose an algorithm combining calibrated prediction and generalization\nbounds from learning theory to construct confidence sets for deep neural\nnetworks with PAC guarantees---i.e., the confidence set for a given input\ncontains the true label with high probability. We demonstrate how our approach\ncan be used to construct PAC confidence sets on ResNet for ImageNet, a visual\nobject tracking model, and a dynamics model for the half-cheetah reinforcement\nlearning problem., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to constructing confidence sets for deep neural networks with provable guarantees. Our method combines calibrated prediction and generalization bounds from learning theory to create confidence sets that contain the true label with high probability. We demonstrate the effectiveness of our approach on several benchmark datasets, including ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "title": "PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction"
    },
    {
        "abs": "The title is A coding theorem for the rate-distortion-perception function, start with The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has\nemerged as a useful tool for thinking about realism and distortion of\nreconstructions in lossy compression. Unlike the rate-distortion function,\nhowever, it is unknown whether encoders and decoders exist that achieve the\nrate suggested by the RDPF. Building on results by Li and El Gamal (2018), we\nshow that the RDPF can indeed be achieved using stochastic, variable-length\ncodes. For this class of codes, we also prove that the RDPF lower-bounds the\nachievable rate, write a short concise abstract based on this: \nThe title of the paper is \"A Coding Theorem for the Rate-Distortion-Perception Function\" and it deals with the rate-distortion-perception function (RDPF), which is a tool used to analyze the realism and distortion of reconstructions in lossy compression. The paper shows that the RDPF can be achieved using stochastic, variable-length codes, and proves that the RDPF lower-bounds the achievable rate.",
        "title": "A coding theorem for the rate-distortion-perception function"
    },
    {
        "abs": "The title is Variational Recurrent Neural Networks for Graph Classification, start with We address the problem of graph classification based only on structural\ninformation. Inspired by natural language processing techniques (NLP), our\nmodel sequentially embeds information to estimate class membership\nprobabilities. Besides, we experiment with NLP-like variational regularization\ntechniques, making the model predict the next node in the sequence as it reads\nit. We experimentally show that our model achieves state-of-the-art\nclassification results on several standard molecular datasets. Finally, we\nperform a qualitative analysis and give some insights on whether the node\nprediction helps the model better classify graphs., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to graph classification based on sequential information embedding. Our model, Variational Recurrent Neural Networks (VRNN), leverages NLP-like techniques to estimate class membership probabilities and predict the next node in the sequence. We experimentally demonstrate that VRNN achieves state-of-the-art results on several molecular datasets. Additionally, we provide qualitative insights on the effectiveness of node prediction in improving graph classification.",
        "title": "Variational Recurrent Neural Networks for Graph Classification"
    },
    {
        "abs": "The title is The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, start with Neural network pruning techniques can reduce the parameter counts of trained\nnetworks by over 90%, decreasing storage requirements and improving\ncomputational performance of inference without compromising accuracy. However,\ncontemporary experience is that the sparse architectures produced by pruning\nare difficult to train from the start, which would similarly improve training\nperformance.\n  We find that a standard pruning technique naturally uncovers subnetworks\nwhose initializations made them capable of training effectively. Based on these\nresults, we articulate the \"lottery ticket hypothesis:\" dense,\nrandomly-initialized, feed-forward networks contain subnetworks (\"winning\ntickets\") that - when trained in isolation - reach test accuracy comparable to\nthe original network in a similar number of iterations. The winning tickets we\nfind have won the initialization lottery: their connections have initial\nweights that make training particularly effective.\n  We present an algorithm to identify winning tickets and a series of\nexperiments that support the lottery ticket hypothesis and the importance of\nthese fortuitous initializations. We consistently find winning tickets that are\nless than 10-20% of the size of several fully-connected and convolutional\nfeed-forward architectures for MNIST and CIFAR10. Above this size, the winning\ntickets that we find learn faster than the original network and reach higher\ntest accuracy., write a short concise abstract based on this:  A new\nhypothesis, the lottery ticket hypothesis, suggests that randomly initialized neural networks contain subnetworks, or winning tickets, that can be trained to achieve high accuracy without the computational cost of training the entire network. These winning tickets are found to be less than 10-20% of the size of fully connected and convolutional feed-forward architectures for MNIST and CIFAR10, and learn faster than the original network while reaching higher test accuracy.",
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
    },
    {
        "abs": "The title is A Variational Inequality Perspective on Generative Adversarial Networks, start with Generative adversarial networks (GANs) form a generative modeling approach\nknown for producing appealing samples, but they are notably difficult to train.\nOne common way to tackle this issue has been to propose new formulations of the\nGAN objective. Yet, surprisingly few studies have looked at optimization\nmethods designed for this adversarial training. In this work, we cast GAN\noptimization problems in the general variational inequality framework. Tapping\ninto the mathematical programming literature, we counter some common\nmisconceptions about the difficulties of saddle point optimization and propose\nto extend techniques designed for variational inequalities to the training of\nGANs. We apply averaging, extrapolation and a computationally cheaper variant\nthat we call extrapolation from the past to the stochastic gradient method\n(SGD) and Adam., write a short concise abstract based on this: \nGenerative adversarial networks (GANs) have shown great promise in generating realistic samples, but their training can be challenging. One approach to addressing this issue is to reformulate the GAN objective using optimization methods designed for variational inequalities. In this work, we leverage the mathematical programming literature to cast GAN optimization problems in the framework of variational inequalities, and propose techniques from this literature to improve the training of GANs. We demonstrate the effectiveness of our approach using several state-of-the-art GAN architectures and show that it can lead to improved training stability and convergence.",
        "title": "A Variational Inequality Perspective on Generative Adversarial Networks"
    },
    {
        "abs": "The title is Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control, start with In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies., write a short concise abstract based on this:  In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework that can infer the dynamics of a physical system from observed state trajectories. By incorporating appropriate inductive bias in the computation graph, SymODEN learns the underlying dynamics in a physics-informed manner, including Hamiltonian dynamics with control. This allows for more accurate and interpretable models of physical systems, which can be used to synthesize model-based control strategies.",
        "title": "Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control"
    },
    {
        "abs": "The title is GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding, start with Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods., write a short concise abstract based on this:  GraphZoom is a new framework for graph embedding that addresses two major limitations of existing methods: 1) they often fail to incorporate node attribute information during training, and 2) they are computationally expensive and memory-intensive for large graphs. GraphZoom solves these problems by fusing the original graph with node attribute information, coarsening the graph into smaller subgraphs, and progressively refining the embeddings. Experimental results show that GraphZoom significantly improves classification accuracy and accelerates the embedding process for various graph datasets.",
        "title": "GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding"
    },
    {
        "abs": "The title is Anytime MiniBatch: Exploiting Stragglers in Online Distributed Optimization, start with Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance., write a short concise abstract based on this:  In this paper, we propose an online distributed optimization method called Anytime Minibatch, which exploits stragglers in distributed optimization. The method allows nodes to compute gradients of as many data samples as possible within a fixed time, and then averages the minibatch gradients via consensus to update primal variables. The approach prevents stragglers from slowing down the system without wasting their work, and we present a convergence analysis and numerical results showing its effectiveness.",
        "title": "Anytime MiniBatch: Exploiting Stragglers in Online Distributed Optimization"
    },
    {
        "abs": "The title is Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics, start with Scaling end-to-end reinforcement learning to control real robots from vision\npresents a series of challenges, in particular in terms of sample efficiency.\nAgainst end-to-end learning, state representation learning can help learn a\ncompact, efficient and relevant representation of states that speeds up policy\nlearning, reducing the number of samples needed, and that is easier to\ninterpret. We evaluate several state representation learning methods on goal\nbased robotics tasks and propose a new unsupervised model that stacks\nrepresentations and combines strengths of several of these approaches. This\nmethod encodes all the relevant features, performs on par or better than\nend-to-end learning with better sample efficiency, and is robust to\nhyper-parameters change., write a short concise abstract based on this: \nScaling end-to-end reinforcement learning to control real robots from vision poses significant challenges, particularly in terms of sample efficiency. State representation learning can help address these challenges by learning a compact, efficient, and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and is easier to interpret. We evaluate several state representation learning methods on goal-based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust",
        "title": "Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics"
    },
    {
        "abs": "The title is InfoBot: Transfer and Exploration via the Information Bottleneck, start with A central challenge in reinforcement learning is discovering effective\npolicies for tasks where rewards are sparsely distributed. We postulate that in\nthe absence of useful reward signals, an effective exploration strategy should\nseek out {\\it decision states}. These states lie at critical junctions in the\nstate space from where the agent can transition to new, potentially unexplored\nregions. We propose to learn about decision states from prior experience. By\ntraining a goal-conditioned policy with an information bottleneck, we can\nidentify decision states by examining where the model actually leverages the\ngoal state. We find that this simple mechanism effectively identifies decision\nstates, even in partially observed settings. In effect, the model learns the\nsensory cues that correlate with potential subgoals. In new environments, this\nmodel can then identify novel subgoals for further exploration, guiding the\nagent through a sequence of potential decision states and through new regions\nof the state space., write a short concise abstract based on this:  In reinforcement learning, finding effective policies for tasks with sparse rewards is a challenge. Our approach, InfoBot, addresses this by identifying decision states, which are critical points in the state space that can lead to new regions. We train a goal-conditioned policy with an information bottleneck to learn about decision states and identify them by examining where the model leverages the goal state. In new environments, the model can identify novel subgoals for exploration, guiding the agent through potential decision states and new regions of the state space.",
        "title": "InfoBot: Transfer and Exploration via the Information Bottleneck"
    },
    {
        "abs": "The title is Multilingual Neural Machine Translation with Knowledge Distillation, start with Multilingual machine translation, which translates multiple languages with a\nsingle model, has attracted much attention due to its efficiency of offline\ntraining and online serving. However, traditional multilingual translation\nusually yields inferior accuracy compared with the counterpart using individual\nmodels for each language pair, due to language diversity and model capacity\nlimitations. In this paper, we propose a distillation-based approach to boost\nthe accuracy of multilingual machine translation. Specifically, individual\nmodels are first trained and regarded as teachers, and then the multilingual\nmodel is trained to fit the training data and match the outputs of individual\nmodels simultaneously through knowledge distillation. Experiments on IWSLT, WMT\nand Ted talk translation datasets demonstrate the effectiveness of our method.\nParticularly, we show that one model is enough to handle multiple languages (up\nto 44 languages in our experiment), with comparable or even better accuracy\nthan individual models., write a short concise abstract based on this: \nIn this paper, we propose a distillation-based approach to improve the accuracy of multilingual machine translation. By training individual models as teachers and using knowledge distillation to train a single multilingual model, we achieve comparable or even better accuracy than using individual models for each language pair. Our approach allows for handling multiple languages with a single model, making it more efficient and scalable. We demonstrate the effectiveness of our method through experiments on several datasets.",
        "title": "Multilingual Neural Machine Translation with Knowledge Distillation"
    },
    {
        "abs": "The title is Fast Graph Representation Learning with PyTorch Geometric, start with We introduce PyTorch Geometric, a library for deep learning on irregularly\nstructured input data such as graphs, point clouds and manifolds, built upon\nPyTorch. In addition to general graph data structures and processing methods,\nit contains a variety of recently published methods from the domains of\nrelational learning and 3D data processing. PyTorch Geometric achieves high\ndata throughput by leveraging sparse GPU acceleration, by providing dedicated\nCUDA kernels and by introducing efficient mini-batch handling for input\nexamples of different size. In this work, we present the library in detail and\nperform a comprehensive comparative study of the implemented methods in\nhomogeneous evaluation scenarios., write a short concise abstract based on this: \nPyTorch Geometric is a new library for deep learning on irregularly structured input data such as graphs, point clouds, and manifolds. Built upon PyTorch, it provides a variety of recently published methods from the domains of relational learning and 3D data processing, and achieves high data throughput through sparse GPU acceleration, dedicated CUDA kernels, and efficient mini-batch handling. The library is presented in detail and compared to other state-of-the-art methods in homogeneous evaluation scenarios.",
        "title": "Fast Graph Representation Learning with PyTorch Geometric"
    },
    {
        "abs": "The title is Diagnosing and Enhancing VAE Models, start with Although variational autoencoders (VAEs) represent a widely influential deep\ngenerative model, many aspects of the underlying energy function remain poorly\nunderstood. In particular, it is commonly believed that Gaussian\nencoder/decoder assumptions reduce the effectiveness of VAEs in generating\nrealistic samples. In this regard, we rigorously analyze the VAE objective,\ndifferentiating situations where this belief is and is not actually true. We\nthen leverage the corresponding insights to develop a simple VAE enhancement\nthat requires no additional hyperparameters or sensitive tuning.\nQuantitatively, this proposal produces crisp samples and stable FID scores that\nare actually competitive with a variety of GAN models, all while retaining\ndesirable attributes of the original VAE architecture. A shorter version of\nthis work will appear in the ICLR 2019 conference proceedings (Dai and Wipf,\n2019). The code for our model is available at https://github.com/daib13/\nTwoStageVAE., write a short concise abstract based on this: \nAlthough VAEs are widely used deep generative models, their effectiveness in generating realistic samples is limited by the assumption of Gaussian encoder/decoder. We analyze the VAE objective and show that this assumption can be a hindrance in certain situations. We then propose a simple enhancement that improves the quality of generated samples without requiring additional hyperparameters or sensitive tuning. Our approach produces competitive FID scores and retains desirable attributes of the original VAE architecture.",
        "title": "Diagnosing and Enhancing VAE Models"
    },
    {
        "abs": "The title is Bridging Adversarial Robustness and Gradient Interpretability, start with Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nSurprisingly, several studies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. Although this phenomenon is interesting, there are only few\nworks that have offered an explanation. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off., write a short concise abstract based on this: \nAdversarial training is a technique used to improve the robustness of deep neural networks (DNNs) against adversarial attacks. However, recent studies have found that the gradients of adversarially trained DNNs are more interpretable than those of standard DNNs. This paper aims to bridge the gap between these two seemingly unrelated phenomena by showing that adversarial training aligns gradients closer to the image manifold, making them more meaningful. The authors demonstrate that adversarial training causes gradients to be quantitatively meaningful and propose an empirical trade-off between test accuracy and gradient interpretability.",
        "title": "Bridging Adversarial Robustness and Gradient Interpretability"
    },
    {
        "abs": "The title is Proceedings of the ICLR Workshop on Computer Vision for Agriculture (CV4A) 2020, start with This is the proceedings of the Computer Vision for Agriculture (CV4A)\nWorkshop that was held in conjunction with the International Conference on\nLearning Representations (ICLR) 2020.\n  The Computer Vision for Agriculture (CV4A) 2020 workshop was scheduled to be\nheld in Addis Ababa, Ethiopia, on April 26th, 2020. It was held virtually that\nsame day due to the COVID-19 pandemic. The workshop was held in conjunction\nwith the International Conference on Learning Representations (ICLR) 2020., write a short concise abstract based on this:  The Computer Vision for Agriculture (CV4A) 2020 workshop was held in conjunction with the International Conference on Learning Representations (ICLR) 2020, and it was a virtual event due to the COVID-19 pandemic. The workshop focused on the application of computer vision techniques in agriculture, with a goal of improving crop yields, reducing waste, and promoting sustainable agriculture practices. The proceedings",
        "title": "Proceedings of the ICLR Workshop on Computer Vision for Agriculture (CV4A) 2020"
    },
    {
        "abs": "The title is 1st AfricaNLP Workshop Proceedings, 2020, start with Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR\n2020, Virtual Conference, Formerly Addis Ababa Ethiopia., write a short concise abstract based on this: \n\nThe 1st AfricaNLP Workshop was held virtually on April 26, 2020, alongside the International Conference on Learning Representations (IC",
        "title": "1st AfricaNLP Workshop Proceedings, 2020"
    },
    {
        "abs": "The title is Multi-Task Learning in Histo-pathology for Widely Generalizable Model, start with In this work we show preliminary results of deep multi-task learning in the\narea of computational pathology. We combine 11 tasks ranging from patch-wise\noral cancer classification, one of the most prevalent cancers in the developing\nworld, to multi-tissue nuclei instance segmentation and classification., write a short concise abstract based on this: \nIn this work, we explore the use of deep multi-task learning in computational pathology, combining 11 tasks including oral cancer classification, multi-tissue nuclei instance segmentation and classification. Our preliminary results show promising performance in improving the generalizability of the model across different tasks",
        "title": "Multi-Task Learning in Histo-pathology for Widely Generalizable Model"
    },
    {
        "abs": "The title is Compositional Languages Emerge in a Neural Iterated Learning Model, start with The principle of compositionality, which enables natural language to\nrepresent complex concepts via a structured combination of simpler ones, allows\nus to convey an open-ended set of messages using a limited vocabulary. If\ncompositionality is indeed a natural property of language, we may expect it to\nappear in communication protocols that are created by neural agents in language\ngames. In this paper, we propose an effective neural iterated learning (NIL)\nalgorithm that, when applied to interacting neural agents, facilitates the\nemergence of a more structured type of language. Indeed, these languages\nprovide learning speed advantages to neural agents during training, which can\nbe incrementally amplified via NIL. We provide a probabilistic model of NIL and\nan explanation of why the advantage of compositional language exist. Our\nexperiments confirm our analysis, and also demonstrate that the emerged\nlanguages largely improve the generalizing power of the neural agent\ncommunication., write a short concise abstract based on this:  In a recent study, researchers discovered that neural agents in language games can create their own languages through a process called neural iterated learning (NIL). This process allows the agents to communicate more efficiently and effectively, and the resulting languages are more structured and compositional. The study found that these languages provide a significant advantage to the neural agents during training, and the advantage can be amplified through repeated use of NIL. The researchers also developed a probabilistic model of NIL and explained why compositional languages emerge in this process. The findings of this study have important implications for the development of more advanced artificial intelligence systems.",
        "title": "Compositional Languages Emerge in a Neural Iterated Learning Model"
    },
    {
        "abs": "The title is Residual Energy-Based Models for Text Generation, start with Text generation is ubiquitous in many NLP tasks, from summarization, to\ndialogue and machine translation. The dominant parametric approach is based on\nlocally normalized models which predict one word at a time. While these work\nremarkably well, they are plagued by exposure bias due to the greedy nature of\nthe generation process. In this work, we investigate un-normalized energy-based\nmodels (EBMs) which operate not at the token but at the sequence level. In\norder to make training tractable, we first work in the residual of a pretrained\nlocally normalized language model and second we train using noise contrastive\nestimation. Furthermore, since the EBM works at the sequence level, we can\nleverage pretrained bi-directional contextual representations, such as BERT and\nRoBERTa. Our experiments on two large language modeling datasets show that\nresidual EBMs yield lower perplexity compared to locally normalized baselines.\nMoreover, generation via importance sampling is very efficient and of higher\nquality than the baseline models according to human evaluation., write a short concise abstract based on this:  Residual energy-based models (EBMs) are proposed for text generation, which operate at the sequence level and leverage pre-trained bi-directional contextual representations. The EBMs are trained using noise contrastive estimation and show lower perplexity compared to locally normalized baselines. Additionally, generation via importance sampling is efficient and of higher quality than the baseline models.",
        "title": "Residual Energy-Based Models for Text Generation"
    },
    {
        "abs": "The title is Energy-based models for atomic-resolution protein conformations, start with We propose an energy-based model (EBM) of protein conformations that operates\nat atomic scale. The model is trained solely on crystallized protein data. By\ncontrast, existing approaches for scoring conformations use energy functions\nthat incorporate knowledge of physical principles and features that are the\ncomplex product of several decades of research and tuning. To evaluate the\nmodel, we benchmark on the rotamer recovery task, the problem of predicting the\nconformation of a side chain from its context within a protein structure, which\nhas been used to evaluate energy functions for protein design. The model\nachieves performance close to that of the Rosetta energy function, a\nstate-of-the-art method widely used in protein structure prediction and design.\nAn investigation of the model's outputs and hidden representations finds that\nit captures physicochemical properties relevant to protein energy., write a short concise abstract based on this: \nA new energy-based model (EBM) for protein conformations is proposed, trained solely on crystallized protein data. Unlike existing approaches, the model does not incorporate knowledge of physical principles or features, instead relying on the atomic-scale energy landscape to capture protein structure and function. The model is benchmarked on the rotamer recovery task, achieving performance close to that of the widely used Rosetta energy function. Analysis of the model's outputs and hidden representations reveals that it captures relevant physicochemical properties of protein energy.",
        "title": "Energy-based models for atomic-resolution protein conformations"
    },
    {
        "abs": "The title is Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS, start with We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural\ntangent kernel and the Laplace kernel include the same set of functions, when\nboth kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we\nprove that the exponential power kernel with a smaller power (making the kernel\nless smooth) leads to a larger RKHS, when it is restricted to the sphere\n$\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$., write a short concise abstract based on this:  Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS.\nWe prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions, when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger",
        "title": "Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS"
    },
    {
        "abs": "The title is Low-dimensional statistical manifold embedding of directed graphs, start with We propose a novel node embedding of directed graphs to statistical\nmanifolds, which is based on a global minimization of pairwise relative entropy\nand graph geodesics in a non-linear way. Each node is encoded with a\nprobability density function over a measurable space. Furthermore, we analyze\nthe connection between the geometrical properties of such embedding and their\nefficient learning procedure. Extensive experiments show that our proposed\nembedding is better in preserving the global geodesic information of graphs, as\nwell as outperforming existing embedding models on directed graphs in a variety\nof evaluation metrics, in an unsupervised setting., write a short concise abstract based on this:  In this paper, we propose a novel method for embedding directed graphs into statistical manifolds, which preserves the global geodesic information of the graph. Our approach is based on minimizing pairwise relative entropy and graph geodesics in a non-linear way, and each node is encoded with a probability density function over a measurable space. We also analyze the connection between the geometric properties of the embedding and the efficient learning procedure. Extensive experiments show that our proposed embedding outperforms existing models on directed graphs in various evaluation metrics, in an unsupervised setting.",
        "title": "Low-dimensional statistical manifold embedding of directed graphs"
    },
    {
        "abs": "The title is Mixed-curvature Variational Autoencoders, start with Euclidean geometry has historically been the typical \"workhorse\" for machine\nlearning applications due to its power and simplicity. However, it has recently\nbeen shown that geometric spaces with constant non-zero curvature improve\nrepresentations and performance on a variety of data types and downstream\ntasks. Consequently, generative models like Variational Autoencoders (VAEs)\nhave been successfully generalized to elliptical and hyperbolic latent spaces.\nWhile these approaches work well on data with particular kinds of biases e.g.\ntree-like data for a hyperbolic VAE, there exists no generic approach unifying\nand leveraging all three models. We develop a Mixed-curvature Variational\nAutoencoder, an efficient way to train a VAE whose latent space is a product of\nconstant curvature Riemannian manifolds, where the per-component curvature is\nfixed or learnable. This generalizes the Euclidean VAE to curved latent spaces\nand recovers it when curvatures of all latent space components go to 0., write a short concise abstract based on this: \nMixed-curvature Variational Autoencoders (MC-VAEs) are a new approach to training generative models that leverages the power of non-Euclidean geometry. By combining the flexibility of hyperbolic and elliptical spaces with the efficiency of Euclidean space, MC-VAEs can generate diverse and structured representations of data. Unlike traditional VAEs, which are limited to Euclidean spaces, MC-VAEs can adapt to different types of data by changing the curvature of the latent space. This makes them a versatile tool for a wide range of applications, including image generation, text-to-image synthesis, and unsupervised learning.",
        "title": "Mixed-curvature Variational Autoencoders"
    },
    {
        "abs": "The title is Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time, start with We study training of Convolutional Neural Networks (CNNs) with ReLU\nactivations and introduce exact convex optimization formulations with a\npolynomial complexity with respect to the number of data samples, the number of\nneurons, and data dimension. More specifically, we develop a convex analytic\nframework utilizing semi-infinite duality to obtain equivalent convex\noptimization problems for several two- and three-layer CNN architectures. We\nfirst prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm\nregularized convex program. We then show that multi-layer circular CNN training\nproblems with a single ReLU layer are equivalent to an $\\ell_1$ regularized\nconvex program that encourages sparsity in the spectral domain. We also extend\nthese results to three-layer CNNs with two ReLU layers. Furthermore, we present\nextensions of our approach to different pooling methods, which elucidates the\nimplicit architectural bias as convex regularizers., write a short concise abstract based on this: \nIn this paper, we study the training of Convolutional Neural Networks (CNNs) with ReLU activations using convex optimization techniques. We develop a framework that enables the optimization of two- and three-layer CNN architectures with a polynomial complexity in the number of data samples, neurons, and data dimension. Our approach utilizes semi-infinite duality to obtain equivalent convex optimization problems for various CNN architectures. We prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program, and show that multi-layer circular CNNs with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNN",
        "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time"
    },
    {
        "abs": "The title is ReLU Code Space: A Basis for Rating Network Quality Besides Accuracy, start with We propose a new metric space of ReLU activation codes equipped with a\ntruncated Hamming distance which establishes an isometry between its elements\nand polyhedral bodies in the input space which have recently been shown to be\nstrongly related to safety, robustness, and confidence. This isometry allows\nthe efficient computation of adjacency relations between the polyhedral bodies.\nExperiments on MNIST and CIFAR-10 indicate that information besides accuracy\nmight be stored in the code space., write a short concise abstract based on this: \nIn this paper, we propose a new metric space of ReLU activation codes, called ReLU Code Space, which is equipped with a truncated Hamming distance. This distance establishes an isometry between the elements of the code space and polyhedral bodies in the input space, which have been shown to be strongly related to safety, robustness, and confidence. The isometry allows for efficient computation of adjacency relations between the polyhedral bodies, and",
        "title": "ReLU Code Space: A Basis for Rating Network Quality Besides Accuracy"
    },
    {
        "abs": "The title is Satellite-based Prediction of Forage Conditions for Livestock in Northern Kenya, start with This paper introduces the first dataset of satellite images labeled with\nforage quality by on-the-ground experts and provides proof of concept for\napplying computer vision methods to index-based drought insurance. We also\npresent the results of a collaborative benchmark tool used to crowdsource an\naccurate machine learning model on the dataset. Our methods significantly\noutperform the existing technology for an insurance program in Northern Kenya,\nsuggesting that a computer vision-based approach could substantially benefit\npastoralists, whose exposure to droughts is severe and worsening with climate\nchange., write a short concise abstract based on this: \nThis paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts and provides proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts",
        "title": "Satellite-based Prediction of Forage Conditions for Livestock in Northern Kenya"
    },
    {
        "abs": "The title is Robust Subspace Recovery Layer for Unsupervised Anomaly Detection, start with We propose a neural network for unsupervised anomaly detection with a novel\nrobust subspace recovery layer (RSR layer). This layer seeks to extract the\nunderlying subspace from a latent representation of the given data and removes\noutliers that lie away from this subspace. It is used within an autoencoder.\nThe encoder maps the data into a latent space, from which the RSR layer\nextracts the subspace. The decoder then smoothly maps back the underlying\nsubspace to a \"manifold\" close to the original inliers. Inliers and outliers\nare distinguished according to the distances between the original and mapped\npositions (small for inliers and large for outliers). Extensive numerical\nexperiments with both image and document datasets demonstrate state-of-the-art\nprecision and recall., write a short concise abstract based on this: \nIn this paper, we propose a novel neural network architecture for unsupervised anomaly detection that leverages a robust subspace recovery layer (RSR layer) to identify outliers in the data. The RSR layer is integrated within an autoencoder, which maps the input data to a latent space and then extracts the underlying subspace. The decoder then smoothly maps the subspace back to a \"manifold\" close to the original inliers, allowing for the distinction between inliers and outliers based on their distances. Our extensive experiments with both image and document datasets demonstrate state-of-the-art precision and recall in anomaly detection.",
        "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection"
    },
    {
        "abs": "The title is Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity, start with The impressive lifelong learning in animal brains is primarily enabled by\nplastic changes in synaptic connectivity. Importantly, these changes are not\npassive, but are actively controlled by neuromodulation, which is itself under\nthe control of the brain. The resulting self-modifying abilities of the brain\nplay an important role in learning and adaptation, and are a major basis for\nbiological reinforcement learning. Here we show for the first time that\nartificial neural networks with such neuromodulated plasticity can be trained\nwith gradient descent. Extending previous work on differentiable Hebbian\nplasticity, we propose a differentiable formulation for the neuromodulation of\nplasticity. We show that neuromodulated plasticity improves the performance of\nneural networks on both reinforcement learning and supervised learning tasks.\nIn one task, neuromodulated plastic LSTMs with millions of parameters\noutperform standard LSTMs on a benchmark language modeling task (controlling\nfor the number of parameters). We conclude that differentiable neuromodulation\nof plasticity offers a powerful new framework for training neural networks., write a short concise abstract based on this: \nDifferentiable neuromodulation of plasticity in artificial neural networks enables self-modifying abilities, improving performance on reinforcement learning and supervised learning tasks. This approach extends previous work on differentiable Hebbian plasticity and offers a powerful new framework for training neural networks.",
        "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity"
    },
    {
        "abs": "The title is Apple Defect Detection Using Deep Learning Based Object Detection For Better Post Harvest Handling, start with The inclusion of Computer Vision and Deep Learning technologies in\nAgriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects., write a short concise abstract based on this: \nApple defect detection using deep learning-based object detection can improve post-harvest handling and quality evaluation. Recent computer vision and deep learning methods, such as YOLOv3, can help detect healthy apples from those with defects. This technology can increase the quality and productivity of farmers, particularly in the export market.",
        "title": "Apple Defect Detection Using Deep Learning Based Object Detection For Better Post Harvest Handling"
    },
    {
        "abs": "The title is Neural Machine Translation for South Africa's Official Languages, start with Recent advances in neural machine translation (NMT) have led to\nstate-of-the-art results for many European-based translation tasks. However,\ndespite these advances, there is has been little focus in applying these\nmethods to African languages. In this paper, we seek to address this gap by\ncreating an NMT benchmark BLEU score between English and the ten remaining\nofficial languages in South Africa., write a short concise abstract based on this: \nRecent advances in neural machine translation (NMT) have led to state-of-the-art results for many European-based translation tasks. However, despite these advances, there has been little focus in applying these methods to African languages. In this paper, we seek to address this gap by creating an NMT benchmark for South Africa's official languages, with a focus on improving",
        "title": "Neural Machine Translation for South Africa's Official Languages"
    },
    {
        "abs": "The title is PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction, start with We propose an algorithm combining calibrated prediction and generalization\nbounds from learning theory to construct confidence sets for deep neural\nnetworks with PAC guarantees---i.e., the confidence set for a given input\ncontains the true label with high probability. We demonstrate how our approach\ncan be used to construct PAC confidence sets on ResNet for ImageNet, a visual\nobject tracking model, and a dynamics model for the half-cheetah reinforcement\nlearning problem., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to constructing confidence sets for deep neural networks with provable guarantees. Our method combines calibrated prediction and generalization bounds from learning theory to create confidence sets that contain the true label with high probability. We demonstrate the effectiveness of our approach on several benchmark datasets, including ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",
        "title": "PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction"
    },
    {
        "abs": "The title is Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction, start with With the recent success and popularity of pre-trained language models (LMs)\nin natural language processing, there has been a rise in efforts to understand\ntheir inner workings. In line with such interest, we propose a novel method\nthat assists us in investigating the extent to which pre-trained LMs capture\nthe syntactic notion of constituency. Our method provides an effective way of\nextracting constituency trees from the pre-trained LMs without training. In\naddition, we report intriguing findings in the induced trees, including the\nfact that pre-trained LMs outperform other approaches in correctly demarcating\nadverb phrases in sentences., write a short concise abstract based on this: \nPre-trained language models (LMs) have revolutionized natural language processing, but their inner workings remain poorly understood. In this work, we investigate whether these models capture the syntactic notion of constituency by extracting constituency trees from pre-trained LMs without training. Our findings reveal that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences, suggesting they are indeed aware of phrases to some extent. This work provides a simple yet strong baseline for grammar induction and sheds light on the capabilities",
        "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction"
    },
    {
        "abs": "The title is Lookahead: A Far-Sighted Alternative of Magnitude-based Pruning, start with Magnitude-based pruning is one of the simplest methods for pruning neural\nnetworks. Despite its simplicity, magnitude-based pruning and its variants\ndemonstrated remarkable performances for pruning modern architectures. Based on\nthe observation that magnitude-based pruning indeed minimizes the Frobenius\ndistortion of a linear operator corresponding to a single layer, we develop a\nsimple pruning method, coined lookahead pruning, by extending the single layer\noptimization to a multi-layer optimization. Our experimental results\ndemonstrate that the proposed method consistently outperforms magnitude-based\npruning on various networks, including VGG and ResNet, particularly in the\nhigh-sparsity regime. See https://github.com/alinlab/lookahead_pruning for\ncodes., write a short concise abstract based on this: \nAbstract: In this paper, we propose a novel pruning method called Lookahead, which extends the single-layer optimization of magnitude-based pruning to a multi-layer optimization. By doing so, Lookahead can better capture the structural information of a neural network and consistently outperform magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. Our experimental results demonstrate the effectiveness of Lookahead in reducing the Frobenius distortion of a linear operator corresponding to",
        "title": "Lookahead: A Far-Sighted Alternative of Magnitude-based Pruning"
    },
    {
        "abs": "The title is Advancing Renewable Electricity Consumption With Reinforcement Learning, start with As the share of renewable energy sources in the present electric energy mix\nrises, their intermittence proves to be the biggest challenge to carbon free\nelectricity generation. To address this challenge, we propose an electricity\npricing agent, which sends price signals to the customers and contributes to\nshifting the customer demand to periods of high renewable energy generation. We\npropose an implementation of a pricing agent with a reinforcement learning\napproach where the environment is represented by the customers, the electricity\ngeneration utilities and the weather conditions., write a short concise abstract based on this: \nIn this paper, we propose a reinforcement learning-based pricing agent to address the challenge of intermittence in renewable energy sources. The pricing agent sends price signals to customers, shifting their demand to periods of high renewable energy generation, contributing to carbon-free electricity generation. We represent the environment as customers, electricity generation utilities, and weather conditions, and use reinforcement learning to optimize the pricing agent's decision-making process. Our approach can help advance renewable",
        "title": "Advancing Renewable Electricity Consumption With Reinforcement Learning"
    },
    {
        "abs": "The title is Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response, start with We report our experiments in building a domain-specific Tigrinya-to-English\nneural machine translation system. We use transfer learning from other Ge'ez\nscript languages and report an improvement of 1.3 BLEU points over a classic\nneural baseline. We publish our development pipeline as an open-source library\nand also provide a demonstration application., write a short concise abstract based on this: \nIn this paper, we present a domain-specific Tigrinya-to-English neural machine translation system that leverages transfer learning from other Ge'ez script languages. We report an improvement of 1.3 BLEU points over a classic neural baseline, and publish our development pipeline as an open-source library. Additionally, we provide a",
        "title": "Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response"
    },
    {
        "abs": "The title is Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin, start with Nigerian Pidgin is arguably the most widely spoken language in Nigeria.\nVariants of this language are also spoken across West and Central Africa,\nmaking it a very important language. This work aims to establish supervised and\nunsupervised neural machine translation (NMT) baselines between English and\nNigerian Pidgin. We implement and compare NMT models with different\ntokenization methods, creating a solid foundation for future works., write a short concise abstract based on this: \nIn this paper, we establish supervised and unsupervised NMT baselines for Nigerian Pidgin, a widely spoken language in Nigeria and beyond. We explore different tokenization methods and compare NMT models to establish a solid foundation for future works. Our findings contribute to improving machine translation for this important language, with potential applications in various fields such as education, healthcare, and business.",
        "title": "Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin"
    },
    {
        "abs": "The title is Estimating Grape Yield on the Vine from Multiple Images, start with Estimating grape yield prior to harvest is important to commercial vineyard\nproduction as it informs many vineyard and winery decisions. Currently, the\nprocess of yield estimation is time consuming and varies in its accuracy from\n75-90\\% depending on the experience of the viticulturist. This paper proposes a\nmultiple task learning (MTL) convolutional neural network (CNN) approach that\nuses images captured by inexpensive smart phones secured in a simple tripod\narrangement. The CNN models use MTL transfer from autoencoders to achieve 85\\%\naccuracy from image data captured 6 days prior to harvest., write a short concise abstract based on this: \nEstimating grape yield prior to harvest is crucial for commercial vineyard production, yet the current process is time-consuming and unreliable, with accuracy ranging from 75-90%. This paper proposes a novel approach using multiple task learning convolutional neural networks (MTL-CNNs) to estimate grape yield from images captured by inexpensive smartphones. By leveraging transfer learning from autoencoders, the MTL-CNNs achieve an accuracy of 85%",
        "title": "Estimating Grape Yield on the Vine from Multiple Images"
    },
    {
        "abs": "The title is Building Disaster Damage Assessment in Satellite Imagery with Multi-Temporal Fusion, start with Automatic change detection and disaster damage assessment are currently\nprocedures requiring a huge amount of labor and manual work by satellite\nimagery analysts. In the occurrences of natural disasters, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures which are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among top results on the xView2 challenge leaderboard. We release our\ncode used for the competition., write a short concise abstract based on this: \nIn this work, we propose a method for automatic change detection and disaster damage assessment in satellite imagery using multi-temporal fusion. We present a novel approach that leverages the newly released xBD dataset, which provides high-resolution satellite imagery for 100+ disaster-affected areas worldwide. Our method significantly improves over the baseline models, achieving top results on the xView2 challenge leaderboard. We release our code used for the competition, providing a valuable resource for the research community. By automating the process of change",
        "title": "Building Disaster Damage Assessment in Satellite Imagery with Multi-Temporal Fusion"
    },
    {
        "abs": "The title is How Chaotic Are Recurrent Neural Networks?, start with Recurrent neural networks (RNNs) are non-linear dynamic systems. Previous\nwork believes that RNN may suffer from the phenomenon of chaos, where the\nsystem is sensitive to initial states and unpredictable in the long run. In\nthis paper, however, we perform a systematic empirical analysis, showing that a\nvanilla or long short term memory (LSTM) RNN does not exhibit chaotic behavior\nalong the training process in real applications such as text generation. Our\nfindings suggest that future work in this direction should address the other\nside of non-linear dynamics for RNN., write a short concise abstract based on this:  Recurrent neural networks (RNNs) are non-linear dynamic systems, but previous work suggests they may exhibit chaotic behavior. This paper provides a systematic empirical analysis of RNNs during training, showing that they do not exhibit chaotic behavior in real applications such as text generation. These findings suggest that future research should focus on the other side of non-linear dynamics for RNNs.",
        "title": "How Chaotic Are Recurrent Neural Networks?"
    },
    {
        "abs": "The title is BERT Fine-tuning For Arabic Text Summarization, start with Fine-tuning a pretrained BERT model is the state of the art method for\nextractive/abstractive text summarization, in this paper we showcase how this\nfine-tuning method can be applied to the Arabic language to both construct the\nfirst documented model for abstractive Arabic text summarization and show its\nperformance in Arabic extractive summarization. Our model works with\nmultilingual BERT (as Arabic language does not have a pretrained BERT of its\nown). We show its performance in English corpus first before applying it to\nArabic corpora in both extractive and abstractive tasks., write a short concise abstract based on this: \nIn this paper, we explore the application of fine-tuning a pre-trained BERT model for Arabic text summarization. We fine-tune a multilingual BERT model and evaluate its performance on both English and Arabic corpora for extractive and abstractive tasks. Our results show that the fine-tuned BERT model outperforms the original BERT model in Arabic summarization tasks, demonstrating the effectiveness of this approach for Arabic language processing.",
        "title": "BERT Fine-tuning For Arabic Text Summarization"
    },
    {
        "abs": "The title is Using competency questions to select optimal clustering structures for residential energy consumption patterns, start with During cluster analysis domain experts and visual analysis are frequently\nrelied on to identify the optimal clustering structure. This process tends to\nbe adhoc, subjective and difficult to reproduce. This work shows how competency\nquestions can be used to formalise expert knowledge and application\nrequirements for context specific evaluation of a clustering application in the\nresidential energy consumption sector., write a short concise abstract based on this:  Using competency questions to select optimal clustering structures for residential energy consumption patterns. This work formalizes expert knowledge and application requirements for context-specific evaluation of clustering in the residential energy consumption sector. Domain experts and visual analysis are frequently relied on to identify the optimal clustering structure, but this process can be ad-hoc, subjective, and difficult to reproduce. Competency questions are used to",
        "title": "Using competency questions to select optimal clustering structures for residential energy consumption patterns"
    },
    {
        "abs": "The title is Reinforcement Learning with Random Delays, start with Action and observation delays commonly occur in many Reinforcement Learning\napplications, such as remote control scenarios. We study the anatomy of\nrandomly delayed environments, and show that partially resampling trajectory\nfragments in hindsight allows for off-policy multi-step value estimation. We\napply this principle to derive Delay-Correcting Actor-Critic (DCAC), an\nalgorithm based on Soft Actor-Critic with significantly better performance in\nenvironments with delays. This is shown theoretically and also demonstrated\npractically on a delay-augmented version of the MuJoCo continuous control\nbenchmark., write a short concise abstract based on this: \nReinforcement Learning with Random Delays: Action and Observation Delays commonly occur in many Reinforcement Learning applications, such as remote control scenarios. We study the anatomy of randomly delayed environments and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor",
        "title": "Reinforcement Learning with Random Delays"
    },
    {
        "abs": "The title is Differentially Private Learning Needs Better Features (or Much More Data), start with We demonstrate that differentially private machine learning has not yet\nreached its \"AlexNet moment\" on many canonical vision tasks: linear models\ntrained on handcrafted features significantly outperform end-to-end deep neural\nnetworks for moderate privacy budgets. To exceed the performance of handcrafted\nfeatures, we show that private learning requires either much more private data,\nor access to features learned on public data from a similar domain. Our work\nintroduces simple yet strong baselines for differentially private learning that\ncan inform the evaluation of future progress in this area., write a short concise abstract based on this: \nDifferentially private machine learning has not yet reached its full potential, according to a new study. The study shows that linear models trained on handcrafted features outperform end-to-end deep neural networks for moderate privacy budgets. To improve the performance of private learning, the study suggests that either more private data is needed or access to features learned on public data from a similar domain is required. The study introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "title": "Differentially Private Learning Needs Better Features (or Much More Data)"
    },
    {
        "abs": "The title is Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control, start with In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning\nframework which can infer the dynamics of a physical system, given by an\nordinary differential equation (ODE), from observed state trajectories. To\nachieve better generalization with fewer training samples, SymODEN incorporates\nappropriate inductive bias by designing the associated computation graph in a\nphysics-informed manner. In particular, we enforce Hamiltonian dynamics with\ncontrol to learn the underlying dynamics in a transparent way, which can then\nbe leveraged to draw insight about relevant physical aspects of the system,\nsuch as mass and potential energy. In addition, we propose a parametrization\nwhich can enforce this Hamiltonian formalism even when the generalized\ncoordinate data is embedded in a high-dimensional space or we can only access\nvelocity data instead of generalized momentum. This framework, by offering\ninterpretable, physically-consistent models for physical systems, opens up new\npossibilities for synthesizing model-based control strategies., write a short concise abstract based on this:  In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework that can infer the dynamics of a physical system from observed state trajectories. By incorporating appropriate inductive bias in the computation graph, SymODEN learns the underlying dynamics in a physics-informed manner, including Hamiltonian dynamics with control. This allows for more accurate and interpretable models of physical systems, which can be used to synthesize model-based control strategies.",
        "title": "Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control"
    },
    {
        "abs": "The title is Symplectic Recurrent Neural Networks, start with We propose Symplectic Recurrent Neural Networks (SRNNs) as learning\nalgorithms that capture the dynamics of physical systems from observed\ntrajectories. An SRNN models the Hamiltonian function of the system by a neural\nnetwork and furthermore leverages symplectic integration, multiple-step\ntraining and initial state optimization to address the challenging numerical\nissues associated with Hamiltonian systems. We show that SRNNs succeed reliably\non complex and noisy Hamiltonian systems. We also show how to augment the SRNN\nintegration scheme in order to handle stiff dynamical systems such as bouncing\nbilliards., write a short concise abstract based on this: \nSymplectic Recurrent Neural Networks (SRNNs) are proposed as learning algorithms to capture the dynamics of physical systems from observed trajectories. SRNNs model the Hamiltonian function of the system using a neural network and leverage symplectic integration, multiple-step training, and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. SRNNs are shown to be reliable on complex and noisy Hamiltonian systems, and can handle stiff dynamical systems such as bouncing",
        "title": "Symplectic Recurrent Neural Networks"
    },
    {
        "abs": "The title is Classification-Based Anomaly Detection for General Data, start with Anomaly detection, finding patterns that substantially deviate from those\nseen previously, is one of the fundamental problems of artificial intelligence.\nRecently, classification-based methods were shown to achieve superior results\non this task. In this work, we present a unifying view and propose an open-set\nmethod, GOAD, to relax current generalization assumptions. Furthermore, we\nextend the applicability of transformation-based methods to non-image data\nusing random affine transformations. Our method is shown to obtain\nstate-of-the-art accuracy and is applicable to broad data types. The strong\nperformance of our method is extensively validated on multiple datasets from\ndifferent domains., write a short concise abstract based on this: \nAnomaly detection, identifying patterns significantly different from prior observations, is a fundamental AI challenge. Recent advances in classification-based methods have shown superior performance. This work presents a unified approach, GOAD, to relax current generalization assumptions and extend transformation-based methods to non-image data. Our method achieves state-of-the-art accuracy and is applicable to diverse data types, with extensive validation on multiple datasets from different domains.",
        "title": "Classification-Based Anomaly Detection for General Data"
    },
    {
        "abs": "The title is Training individually fair ML models with Sensitive Subspace Robustness, start with We consider training machine learning models that are fair in the sense that\ntheir performance is invariant under certain sensitive perturbations to the\ninputs. For example, the performance of a resume screening system should be\ninvariant under changes to the gender and/or ethnicity of the applicant. We\nformalize this notion of algorithmic fairness as a variant of individual\nfairness and develop a distributionally robust optimization approach to enforce\nit during training. We also demonstrate the effectiveness of the approach on\ntwo ML tasks that are susceptible to gender and racial biases., write a short concise abstract based on this: \nIn this paper, we propose a new approach to training machine learning models that are fair and robust to sensitive perturbations. Our approach, called Sensitive Subspace Robustness (SSR), formalizes the notion of algorithmic fairness as a variant of individual fairness and uses distributionally robust optimization to enforce it during training. We demonstrate the effectiveness of SSR on two ML tasks that are susceptible to gender and racial biases, showing that it can improve the fairness of the models while maintaining their accuracy. Our",
        "title": "Training individually fair ML models with Sensitive Subspace Robustness"
    },
    {
        "abs": "The title is Dynamics-aware Embeddings, start with In this paper we consider self-supervised representation learning to improve\nsample efficiency in reinforcement learning (RL). We propose a forward\nprediction objective for simultaneously learning embeddings of states and\naction sequences. These embeddings capture the structure of the environment's\ndynamics, enabling efficient policy learning. We demonstrate that our action\nembeddings alone improve the sample efficiency and peak performance of\nmodel-free RL on control from low-dimensional states. By combining state and\naction embeddings, we achieve efficient learning of high-quality policies on\ngoal-conditioned continuous control from pixel observations in only 1-2 million\nenvironment steps., write a short concise abstract based on this:  In this paper, we propose a self-supervised representation learning method to improve sample efficiency in reinforcement learning. By learning embeddings of states and action sequences, we capture the dynamics of the environment and enable efficient policy learning. Our approach achieves significant improvements in sample efficiency and peak performance on control tasks from low-dimensional states, and demonstrates the effectiveness of combining state and action embeddings for goal-conditioned continuous control tasks.",
        "title": "Dynamics-aware Embeddings"
    },
    {
        "abs": "The title is SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness, start with In this paper, we cast fair machine learning as invariant machine learning.\nWe first formulate a version of individual fairness that enforces invariance on\ncertain sensitive sets. We then design a transport-based regularizer that\nenforces this version of individual fairness and develop an algorithm to\nminimize the regularizer efficiently. Our theoretical results guarantee the\nproposed approach trains certifiably fair ML models. Finally, in the\nexperimental studies we demonstrate improved fairness metrics in comparison to\nseveral recent fair training procedures on three ML tasks that are susceptible\nto algorithmic bias., write a short concise abstract based on this:  In this paper, we propose a new approach to fair machine learning that focuses on enforcing individual fairness through sensitivity to certain sensitive sets. We formulate a version of individual fairness that is invariant to these sets and design a transport-based regularizer to enforce it. Our approach guarantees certified fairness and outperforms recent fair training methods on three ML tasks.",
        "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness"
    },
    {
        "abs": "The title is Graph-Based Continual Learning, start with Despite significant advances, continual learning models still suffer from\ncatastrophic forgetting when exposed to incrementally available data from\nnon-stationary distributions. Rehearsal approaches alleviate the problem by\nmaintaining and replaying a small episodic memory of previous samples, often\nimplemented as an array of independent memory slots. In this work, we propose\nto augment such an array with a learnable random graph that captures pairwise\nsimilarities between its samples, and use it not only to learn new tasks but\nalso to guard against forgetting. Empirical results on several benchmark\ndatasets show that our model consistently outperforms recently proposed\nbaselines for task-free continual learning., write a short concise abstract based on this:  Despite significant advances in continual learning, models still struggle with catastrophic forgetting when exposed to new data from non-stationary distributions. This work proposes a new approach that augments an episodic memory with a learnable random graph to capture pairwise similarities between samples. The graph is used not only to learn new tasks but also to prevent forgetting of previous knowledge. Empirical results show that the proposed model outperforms recent baselines for task-free continual learning.",
        "title": "Graph-Based Continual Learning"
    },
    {
        "abs": "The title is Group Equivariant Stand-Alone Self-Attention For Vision, start with We provide a general self-attention formulation to impose group equivariance\nto arbitrary symmetry groups. This is achieved by defining positional encodings\nthat are invariant to the action of the group considered. Since the group acts\non the positional encoding directly, group equivariant self-attention networks\n(GSA-Nets) are steerable by nature. Our experiments on vision benchmarks\ndemonstrate consistent improvements of GSA-Nets over non-equivariant\nself-attention networks., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to self-attention in vision tasks, called Group Equivariant Stand-Alone Self-Attention (GSA-Net). Our approach imposes group equivariance on the self-attention mechanism, allowing the network to focus on relevant features while ignoring irrelevant ones. We demonstrate the effectiveness of GSA-Net on several vision",
        "title": "Group Equivariant Stand-Alone Self-Attention For Vision"
    },
    {
        "abs": "The title is Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures, start with We propose to study the problem of few shot graph classification in graph\nneural networks (GNNs) to recognize unseen classes, given limited labeled graph\nexamples. Despite several interesting GNN variants being proposed recently for\nnode and graph classification tasks, when faced with scarce labeled examples in\nthe few shot setting, these GNNs exhibit significant loss in classification\nperformance. Here, we present an approach where a probability measure is\nassigned to each graph based on the spectrum of the graphs normalized\nLaplacian. This enables us to accordingly cluster the graph base labels\nassociated with each graph into super classes, where the Lp Wasserstein\ndistance serves as our underlying distance metric. Subsequently, a super graph\nconstructed based on the super classes is then fed to our proposed GNN\nframework which exploits the latent inter class relationships made explicit by\nthe super graph to achieve better class label separation among the graphs. We\nconduct exhaustive empirical evaluations of our proposed method and show that\nit outperforms both the adaptation of state of the art graph classification\nmethods to few shot scenario and our naive baseline GNNs. Additionally, we also\nextend and study the behavior of our method to semi supervised and active\nlearning scenarios., write a short concise abstract based on this:  In this paper, we propose a novel approach for few-shot learning on graphs, which leverages super-classes based on graph spectral measures to improve classification performance. Our approach assigns a probability measure to each graph based on its spectrum, and clusters the base labels into super classes using the Lp Wasserstein distance. We then use a super graph constructed from the super classes to train a GNN that exploits the latent inter-class relationships to achieve better class label separation. Our experiments show that our proposed method outperforms both state-of-the-art graph classification methods adapted to few-shot learning and naive baseline GNNs, and we also extend and study the behavior of our method in semi-supervised and active learning scenarios.",
        "title": "Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures"
    },
    {
        "abs": "The title is Rethinking Positional Encoding in Language Pre-training, start with In this work, we investigate the positional encoding methods used in language\npre-training (e.g., BERT) and identify several problems in the existing\nformulations. First, we show that in the absolute positional encoding, the\naddition operation applied on positional embeddings and word embeddings brings\nmixed correlations between the two heterogeneous information resources. It may\nbring unnecessary randomness in the attention and further limit the\nexpressiveness of the model. Second, we question whether treating the position\nof the symbol \\texttt{[CLS]} the same as other words is a reasonable design,\nconsidering its special role (the representation of the entire sentence) in the\ndownstream tasks. Motivated from above analysis, we propose a new positional\nencoding method called \\textbf{T}ransformer with \\textbf{U}ntied\n\\textbf{P}ositional \\textbf{E}ncoding (TUPE). In the self-attention module,\nTUPE computes the word contextual correlation and positional correlation\nseparately with different parameterizations and then adds them together. This\ndesign removes the mixed and noisy correlations over heterogeneous embeddings\nand offers more expressiveness by using different projection matrices.\nFurthermore, TUPE unties the \\texttt{[CLS]} symbol from other positions, making\nit easier to capture information from all positions. Extensive experiments and\nablation studies on GLUE benchmark demonstrate the effectiveness of the\nproposed method. Codes and models are released at\nhttps://github.com/guolinke/TUPE., write a short concise abstract based on this:  In this work, we investigate and improve the positional encoding methods used in language pre-training models, such as BERT. We identify problems with the existing formulations and propose a new method called Transformer with Untied Positional Encoding (TUPE). TUPE separates the positional correlation from the word contextual correlation and uses different projection matrices to capture more expressive information. The proposed method also unties the special symbol \\texttt{[CLS]} from other positions, making it easier to capture information from all parts of the input. Experimental results show that TUPE outperforms the existing methods on the GLUE benchmark.",
        "title": "Rethinking Positional Encoding in Language Pre-training"
    },
    {
        "abs": "The title is GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding, start with Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods., write a short concise abstract based on this:  GraphZoom is a new framework for graph embedding that addresses two major limitations of existing methods: 1) they often fail to incorporate node attribute information during training, and 2) they are computationally expensive and memory-intensive for large graphs. GraphZoom solves these problems by fusing the original graph with node attribute information, coarsening the graph into smaller subgraphs, and progressively refining the embeddings. Experimental results show that GraphZoom significantly improves classification accuracy and accelerates the embedding process for various graph datasets.",
        "title": "GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding"
    },
    {
        "abs": "The title is DDPNOpt: Differential Dynamic Programming Neural Optimizer, start with Interpretation of Deep Neural Networks (DNNs) training as an optimal control\nproblem with nonlinear dynamical systems has received considerable attention\nrecently, yet the algorithmic development remains relatively limited. In this\nwork, we make an attempt along this line by reformulating the training\nprocedure from the trajectory optimization perspective. We first show that most\nwidely-used algorithms for training DNNs can be linked to the Differential\nDynamic Programming (DDP), a celebrated second-order method rooted in the\nApproximate Dynamic Programming. In this vein, we propose a new class of\noptimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and\nconvolution networks. DDPNOpt features layer-wise feedback policies which\nimprove convergence and reduce sensitivity to hyper-parameter over existing\nmethods. It outperforms other optimal-control inspired training methods in both\nconvergence and complexity, and is competitive against state-of-the-art first\nand second order methods. We also observe DDPNOpt has surprising benefit in\npreventing gradient vanishing. Our work opens up new avenues for principled\nalgorithmic design built upon the optimal control theory., write a short concise abstract based on this: \nIn this work, we propose a new optimizer for training deep neural networks, DDP Neural Optimizer (DDPNOpt), which formulates the training process as an optimal control problem. By reformulating the training procedure from the trajectory optimization perspective, we show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. Our experiments show that DDPNOpt outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. Additionally, we observe that DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up",
        "title": "DDPNOpt: Differential Dynamic Programming Neural Optimizer"
    },
    {
        "abs": "The title is De-anonymization of authors through arXiv submissions during double-blind review, start with In this paper, we investigate the effects of releasing arXiv preprints of\npapers that are undergoing a double-blind review process. In particular, we ask\nthe following research question: What is the relation between de-anonymization\nof authors through arXiv preprints and acceptance of a research paper at a\n(nominally) double-blind venue? Under two conditions: papers that are released\non arXiv before the review phase and papers that are not, we examine the\ncorrelation between the reputation of their authors with the review scores and\nacceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019\nsubmissions (n=5050), we find statistically significant evidence of positive\ncorrelation between percentage acceptance and papers with high reputation\nreleased on arXiv. In order to understand this observed association better, we\nperform additional analyses based on self-specified confidence scores of\nreviewers and observe that less confident reviewers are more likely to assign\nhigh review scores to papers with well known authors and low review scores to\npapers with less known authors, where reputation is quantified in terms of\nnumber of Google Scholar citations. We emphasize upfront that our results are\npurely correlational and we neither can nor intend to make any causal claims. A\nblog post accompanying the paper and our scraping code will be linked in the\nproject website https://sites.google.com/view/deanon-arxiv/home, write a short concise abstract based on this:  In this paper, we investigate the relationship between the de-anonymization of authors through arXiv preprints and the acceptance of research papers at double-blind venues. We analyze a dataset of ICLR 2020 and ICLR 2019 submissions and find a statistically significant correlation between the reputation of the authors and the acceptance decisions. Specifically, we find that papers with high reputation authors are more likely to be accepted, and less confident reviewers are more likely to assign high review scores to papers with well-known authors. Our results highlight the potential challenges of maintaining double-blind review processes in practice.",
        "title": "De-anonymization of authors through arXiv submissions during double-blind review"
    },
    {
        "abs": "The title is OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning, start with Reinforcement learning (RL) has achieved impressive performance in a variety\nof online settings in which an agent's ability to query the environment for\ntransitions and rewards is effectively unlimited. However, in many practical\napplications, the situation is reversed: an agent may have access to large\namounts of undirected offline experience data, while access to the online\nenvironment is severely limited. In this work, we focus on this offline\nsetting. Our main insight is that, when presented with offline data composed of\na variety of behaviors, an effective way to leverage this data is to extract a\ncontinuous space of recurring and temporally extended primitive behaviors\nbefore using these primitives for downstream task learning. Primitives\nextracted in this way serve two purposes: they delineate the behaviors that are\nsupported by the data from those that are not, making them useful for avoiding\ndistributional shift in offline RL; and they provide a degree of temporal\nabstraction, which reduces the effective horizon yielding better learning in\ntheory, and improved offline RL in practice. In addition to benefiting offline\npolicy optimization, we show that performing offline primitive learning in this\nway can also be leveraged for improving few-shot imitation learning as well as\nexploration and transfer in online RL on a variety of benchmark domains.\nVisualizations are available at https://sites.google.com/view/opal-iclr, write a short concise abstract based on this: \nIn this paper, we propose OPAL, a method for accelerating offline reinforcement learning by extracting a continuous space of recurring and temporally extended primitive behaviors from large amounts of offline data. We show that leveraging these primitives can improve offline policy optimization, few-shot imitation learning, exploration, and transfer in online RL on various benchmark domains. Our approach provides a way to effectively utilize offline data, which is often readily available but difficult to use in RL applications.",
        "title": "OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning"
    },
    {
        "abs": "The title is A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima, start with Stochastic Gradient Descent (SGD) and its variants are mainstream methods for\ntraining deep networks in practice. SGD is known to find a flat minimum that\noften generalizes well. However, it is mathematically unclear how deep learning\ncan select a flat minimum among so many minima. To answer the question\nquantitatively, we develop a density diffusion theory (DDT) to reveal how\nminima selection quantitatively depends on the minima sharpness and the\nhyperparameters. To the best of our knowledge, we are the first to\ntheoretically and empirically prove that, benefited from the Hessian-dependent\ncovariance of stochastic gradient noise, SGD favors flat minima exponentially\nmore than sharp minima, while Gradient Descent (GD) with injected white noise\nfavors flat minima only polynomially more than sharp minima. We also reveal\nthat either a small learning rate or large-batch training requires\nexponentially many iterations to escape from minima in terms of the ratio of\nthe batch size and learning rate. Thus, large-batch training cannot search flat\nminima efficiently in a realistic computational time., write a short concise abstract based on this: \nDeep learning dynamics, particularly Stochastic Gradient Descent (SGD), have been successful in training deep neural networks. However, the mechanism behind SGD's ability to find flat minima that generalize well remains unclear. In this paper, we develop a density diffusion theory (DDT) to quantitatively understand how SGD selects minima. Our results show that SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also find that large-batch training requires exponentially many iterations to escape from minima, making it inefficient for searching flat minima in a realistic computational time. Our results provide a theoretical understanding of SGD's minima selection mechanism and have important implications for deep learning practice.",
        "title": "A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima"
    },
    {
        "abs": "The title is Spectral embedding of regularized block models, start with Spectral embedding is a popular technique for the representation of graph\ndata. Several regularization techniques have been proposed to improve the\nquality of the embedding with respect to downstream tasks like clustering. In\nthis paper, we explain on a simple block model the impact of the complete graph\nregularization, whereby a constant is added to all entries of the adjacency\nmatrix. Specifically, we show that the regularization forces the spectral\nembedding to focus on the largest blocks, making the representation less\nsensitive to noise or outliers. We illustrate these results on both on both\nsynthetic and real data, showing how regularization improves standard\nclustering scores., write a short concise abstract based on this: \nSpectral embedding is a powerful technique for representing graph data, but it can be sensitive to noise or outliers. By adding a regularization term to the spectral embedding algorithm, we can improve its robustness and focus on the largest blocks in the graph. We demonstrate the effectiveness of this approach on both synthetic and real data, showing that regularization leads to better clustering scores.",
        "title": "Spectral embedding of regularized block models"
    },
    {
        "abs": "The title is Locality and compositionality in zero-shot learning, start with In this work we study locality and compositionality in the context of\nlearning representations for Zero Shot Learning (ZSL). In order to well-isolate\nthe importance of these properties in learned representations, we impose the\nadditional constraint that, differently from most recent work in ZSL, no\npre-training on different datasets (e.g. ImageNet) is performed. The results of\nour experiments show how locality, in terms of small parts of the input, and\ncompositionality, i.e. how well can the learned representations be expressed as\na function of a smaller vocabulary, are both deeply related to generalization\nand motivate the focus on more local-aware models in future research directions\nfor representation learning., write a short concise abstract based on this:  In this work, we investigate the role of locality and compositionality in learned representations for zero-shot learning. We find that both properties are crucial for generalization and suggest a focus on more local-aware models in future research.",
        "title": "Locality and compositionality in zero-shot learning"
    },
    {
        "abs": "The title is Representation Learning with Multisets, start with We study the problem of learning permutation invariant representations that\ncan capture \"flexible\" notions of containment. We formalize this problem via a\nmeasure theoretic definition of multisets, and obtain a theoretically-motivated\nlearning model. We propose training this model on a novel task: predicting the\nsize of the symmetric difference (or intersection) between pairs of multisets.\nWe demonstrate that our model not only performs very well on predicting\ncontainment relations (and more effectively predicts the sizes of symmetric\ndifferences and intersections than DeepSets-based approaches with unconstrained\nobject representations), but that it also learns meaningful representations., write a short concise abstract based on this: \nIn this paper, we propose a new approach to representation learning called Representation Learning with Multisets (RLM). We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of containment, and formalize this problem via a measure theoretic definition of multisets. Our approach involves training a model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs well on predicting containment relations,",
        "title": "Representation Learning with Multisets"
    },
    {
        "abs": "The title is Regularization Matters in Policy Optimization, start with Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg ., write a short concise abstract based on this: \nRegularization techniques have been largely overlooked in deep reinforcement learning (Deep RL) despite their success in traditional machine learning. In this work, we investigate the effectiveness of regularization techniques on policy optimization algorithms for continuous control tasks. Our findings show that conventional regularization techniques can significantly improve performance, especially on harder tasks. We also compare these techniques with entropy regularization and analyze their effectiveness from four perspectives. Our results provide guidance for future practices in regularizing policy optimization algorithms in Deep RL.",
        "title": "Regularization Matters in Policy Optimization"
    },
    {
        "abs": "The title is Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification, start with The Receptive Field (RF) size has been one of the most important factors for\nOne Dimensional Convolutional Neural Networks (1D-CNNs) on time series\nclassification tasks. Large efforts have been taken to choose the appropriate\nsize because it has a huge influence on the performance and differs\nsignificantly for each dataset. In this paper, we propose an Omni-Scale block\n(OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and\nuniversal rule. Particularly, it is a set of kernel sizes that can efficiently\ncover the best RF size across different datasets via consisting of multiple\nprime numbers according to the length of the time series. The experiment result\nshows that models with the OS-block can achieve a similar performance as models\nwith the searched optimal RF size and due to the strong optimal RF size capture\nability, simple 1D-CNN models with OS-block achieves the state-of-the-art\nperformance on four time series benchmarks, including both univariate and\nmultivariate data from multiple domains. Comprehensive analysis and discussions\nshed light on why the OS-block can capture optimal RF sizes across different\ndatasets. Code available [https://github.com/Wensi-Tang/OS-CNN], write a short concise abstract based on this: \nIn this paper, we propose an Omni-Scale block (OS-block) for one-dimensional Convolutional Neural Networks (1D-CNNs) that can efficiently cover the best receptive field size across different datasets. The OS-block consists of multiple prime numbers according to the length of the time series, and can achieve similar performance as models with the searched optimal receptive field size. Our experiments on four time series benchmarks show that models with the OS-block achieve state-of-the-art performance, and provide comprehensive analysis and discussions on why the OS-block can capture optimal receptive field sizes across different datasets.",
        "title": "Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification"
    },
    {
        "abs": "The title is Anytime MiniBatch: Exploiting Stragglers in Online Distributed Optimization, start with Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance., write a short concise abstract based on this:  In this paper, we propose an online distributed optimization method called Anytime Minibatch, which exploits stragglers in distributed optimization. The method allows nodes to compute gradients of as many data samples as possible within a fixed time, and then averages the minibatch gradients via consensus to update primal variables. The approach prevents stragglers from slowing down the system without wasting their work, and we present a convergence analysis and numerical results showing its effectiveness.",
        "title": "Anytime MiniBatch: Exploiting Stragglers in Online Distributed Optimization"
    },
    {
        "abs": "The title is Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL), start with Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,\nco-located with ICLR 2021. In this workshop, we want to advance theory, methods\nand tools for allowing experts to express prior coded knowledge for automatic\ndata annotations that can be used to train arbitrary deep neural networks for\nprediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing\nmethods that help modern machine-learning methods to generalize from knowledge\nprovided by experts, in interaction with observable (unlabeled) data. In total,\n15 papers were accepted. All the accepted contributions are listed in these\nProceedings., write a short concise abstract based on this: \nWelcome to the Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL) 2021! This workshop aims to advance the theory, methods, and tools for weakly supervised learning, which enables experts to express prior knowledge for automatic data annotations that can be used to train arbitrary deep neural networks for prediction. The workshop features 15 accepted papers that explore various aspects of weakly supervised learning, including knowledge transfer, data augmentation, and uncertainty estimation. The contributions in these proceedings demonstrate",
        "title": "Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL)"
    },
    {
        "abs": "The title is FFPDG: Fast, Fair and Private Data Generation, start with Generative modeling has been used frequently in synthetic data generation.\nFairness and privacy are two big concerns for synthetic data. Although Recent\nGAN [\\cite{goodfellow2014generative}] based methods show good results in\npreserving privacy, the generated data may be more biased. At the same time,\nthese methods require high computation resources. In this work, we design a\nfast, fair, flexible and private data generation method. We show the\neffectiveness of our method theoretically and empirically. We show that models\ntrained on data generated by the proposed method can perform well (in inference\nstage) on real application scenarios., write a short concise abstract based on this: \nIn this paper, we propose a novel method for fast, fair, flexible, and private data generation using generative adversarial networks (GANs). Our approach combines the strengths of GANs with fairness and privacy considerations, while reducing computational complexity. We demonstrate the effectiveness of our method through theoretical analysis and empirical evaluation on real-world application scenarios. Our results show that models trained on data generated by our method can perform well in inference tasks while ensuring fairness and privacy.",
        "title": "FFPDG: Fast, Fair and Private Data Generation"
    },
    {
        "abs": "The title is Free Lunch for Few-shot Learning: Distribution Calibration, start with Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation., write a short concise abstract based on this:  In few-shot learning, overfitting is a significant challenge due to the limited number of training examples. To address this, we propose a method that calibrates the distribution of few-shot classes by transferring statistics from well-represented classes. By sampling adequate examples from the calibrated distribution, we can expand the inputs to the classifier, improving its accuracy. Our approach is simple and does not require additional parameters, and we demonstrate its effectiveness on two datasets.",
        "title": "Free Lunch for Few-shot Learning: Distribution Calibration"
    },
    {
        "abs": "The title is On the mapping between Hopfield networks and Restricted Boltzmann Machines, start with Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs., write a short concise abstract based on this: \nTitle: On the Mapping between Hopfield Networks and Restricted Boltzmann Machines\nAbstract: We present an exact mapping between Hopfield Networks (HNs) and Restricted Boltzmann Machines (RBMs) for the case of correlated pattern HNs. This mapping allows for the transformation of an HN with $N$ binary variables and $p<N$ arbitrary binary patterns into an RBM with $N$ binary visible variables and $p$ Gaussian hidden variables. We outline the conditions under which the reverse mapping exists and conduct experiments on the MNIST dataset showing the mapping provides a useful initialization to the RBM weights. This correspondence has important implications for the training of RBMs and the understanding of deep architectures that utilize RBMs.",
        "title": "On the mapping between Hopfield networks and Restricted Boltzmann Machines"
    },
    {
        "abs": "The title is Persistent Message Passing, start with Graph neural networks (GNNs) are a powerful inductive bias for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. For many tasks of\ninterest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. We introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states., write a short concise abstract based on this:  Persistent Message Passing (PMP) is a mechanism that enables Graph Neural Networks (GNNs) to efficiently query past states by explicitly persisting them, rather than overwriting node representations. This allows GNNs to support data structure queries dependent on previous states, which is crucial for tasks that require reasoning about the evolution of data structures over time. PMP significantly outperforms GNNs that overwrite states on dynamic temporal range queries, demonstrating its effectiveness in enhancing the inductive bias of GNNs for tasks that require tracking the evolution of data structures.",
        "title": "Persistent Message Passing"
    },
    {
        "abs": "The title is On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers, start with A deep equilibrium model uses implicit layers, which are implicitly defined\nthrough an equilibrium point of an infinite sequence of computation. It avoids\nany explicit computation of the infinite sequence by finding an equilibrium\npoint directly via root-finding and by computing gradients via implicit\ndifferentiation. In this paper, we analyze the gradient dynamics of deep\nequilibrium models with nonlinearity only on weight matrices and non-convex\nobjective functions of weights for regression and classification. Despite\nnon-convexity, convergence to global optimum at a linear rate is guaranteed\nwithout any assumption on the width of the models, allowing the width to be\nsmaller than the output dimension and the number of data points. Moreover, we\nprove a relation between the gradient dynamics of the deep implicit layer and\nthe dynamics of trust region Newton method of a shallow explicit layer. This\nmathematically proven relation along with our numerical observation suggests\nthe importance of understanding implicit bias of implicit layers and an open\nproblem on the topic. Our proofs deal with implicit layers, weight tying and\nnonlinearity on weights, and differ from those in the related literature., write a short concise abstract based on this:  In this paper, we investigate the convergence properties of deep implicit learning models, which use implicit layers to avoid explicit computation of infinite sequences. We prove that these models converge to a global optimum at a linear rate without any assumptions on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. We also establish a relation between the gradient dynamics of the deep implicit layer and the dynamics of a shallow explicit layer, highlighting the importance of understanding implicit bias in these models. Our results provide a theoretical foundation for the use of implicit layers in deep learning.",
        "title": "On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers"
    },
    {
        "abs": "The title is Gradient Projection Memory for Continual Learning, start with The ability to learn continually without forgetting the past tasks is a\ndesired attribute for artificial learning systems. Existing approaches to\nenable such learning in artificial neural networks usually rely on network\ngrowth, importance based weight update or replay of old data from the memory.\nIn contrast, we propose a novel approach where a neural network learns new\ntasks by taking gradient steps in the orthogonal direction to the gradient\nsubspaces deemed important for the past tasks. We find the bases of these\nsubspaces by analyzing network representations (activations) after learning\neach task with Singular Value Decomposition (SVD) in a single shot manner and\nstore them in the memory as Gradient Projection Memory (GPM). With qualitative\nand quantitative analyses, we show that such orthogonal gradient descent\ninduces minimum to no interference with the past tasks, thereby mitigates\nforgetting. We evaluate our algorithm on diverse image classification datasets\nwith short and long sequences of tasks and report better or on-par performance\ncompared to the state-of-the-art approaches., write a short concise abstract based on this: \nLearning continually without forgetting past tasks is crucial for artificial intelligence systems. Existing methods rely on network growth, importance-based weight updates, or replaying old data. We propose a novel approach that learns new tasks by taking gradient steps in orthogonal directions to important subspaces from past tasks. We find these subspaces by analyzing network representations with SVD and store them in Gradient Projection Memory (GPM). Our approach induces minimum interference with past tasks, outperforming state-of-the-art methods on diverse image classification datasets with short and long sequences of tasks.",
        "title": "Gradient Projection Memory for Continual Learning"
    },
    {
        "abs": "The title is Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks, start with In high-dimensional state spaces, the usefulness of Reinforcement Learning\n(RL) is limited by the problem of exploration. This issue has been addressed\nusing potential-based reward shaping (PB-RS) previously. In the present work,\nwe introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the\nstrict optimality guarantees of PB-RS to a guarantee of preserved long-term\nbehavior. Being less restrictive, FV-RS allows for reward shaping functions\nthat are even better suited for improving the sample efficiency of RL\nalgorithms. In particular, we consider settings in which the agent has access\nto an approximate plan. Here, we use examples of simulated robotic manipulation\ntasks to demonstrate that plan-based FV-RS can indeed significantly improve the\nsample efficiency of RL over plan-based PB-RS., write a short concise abstract based on this:  In high-dimensional state spaces, Reinforcement Learning (RL) faces the challenge of exploration. Previous work introduced potential-based reward shaping (PB-RS) to address this issue. However, PB-RS is strict and can limit the performance of RL algorithms. In this work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS), which relaxes the strict optimality guarantees of PB-RS to preserve long-term behavior. FV-RS allows for reward shaping functions that are better suited for improving the sample efficiency of RL algorithms, especially when the agent has access",
        "title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks"
    },
    {
        "abs": "The title is Improving exploration in policy gradient search: Application to symbolic optimization, start with Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. When no labeled\ndata is available, such networks can still be trained using reinforcement\nlearning. However, we demonstrate that this approach can suffer from an early\ncommitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression., write a short concise abstract based on this:  Improving exploration in policy gradient search is crucial for solving complex mathematical optimization problems. While neural networks can learn high-level symbolic patterns, they can suffer from early commitment and initialization bias, limiting exploration. This paper presents two exploration methods to address these issues, improving performance, sample efficiency, and solution complexity for symbolic optimization tasks.",
        "title": "Improving exploration in policy gradient search: Application to symbolic optimization"
    },
    {
        "abs": "The title is Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time, start with We study training of Convolutional Neural Networks (CNNs) with ReLU\nactivations and introduce exact convex optimization formulations with a\npolynomial complexity with respect to the number of data samples, the number of\nneurons, and data dimension. More specifically, we develop a convex analytic\nframework utilizing semi-infinite duality to obtain equivalent convex\noptimization problems for several two- and three-layer CNN architectures. We\nfirst prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm\nregularized convex program. We then show that multi-layer circular CNN training\nproblems with a single ReLU layer are equivalent to an $\\ell_1$ regularized\nconvex program that encourages sparsity in the spectral domain. We also extend\nthese results to three-layer CNNs with two ReLU layers. Furthermore, we present\nextensions of our approach to different pooling methods, which elucidates the\nimplicit architectural bias as convex regularizers., write a short concise abstract based on this: \nIn this paper, we study the training of Convolutional Neural Networks (CNNs) with ReLU activations using convex optimization techniques. We develop a framework that enables the optimization of two- and three-layer CNN architectures with a polynomial complexity in the number of data samples, neurons, and data dimension. Our approach utilizes semi-infinite duality to obtain equivalent convex optimization problems for various CNN architectures. We prove that two-layer CNNs can be globally optimized via an $\\ell_2$ norm regularized convex program, and show that multi-layer circular CNNs with a single ReLU layer are equivalent to an $\\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNN",
        "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time"
    },
    {
        "abs": "The title is The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs, start with We consider the problem of finding the best memoryless stochastic policy for\nan infinite-horizon partially observable Markov decision process (POMDP) with\nfinite state and action spaces with respect to either the discounted or mean\nreward criterion. We show that the (discounted) state-action frequencies and\nthe expected cumulative reward are rational functions of the policy, whereby\nthe degree is determined by the degree of partial observability. We then\ndescribe the optimization problem as a linear optimization problem in the space\nof feasible state-action frequencies subject to polynomial constraints that we\ncharacterize explicitly. This allows us to address the combinatorial and\ngeometric complexity of the optimization problem using recent tools from\npolynomial optimization. In particular, we estimate the number of critical\npoints and use the polynomial programming description of reward maximization to\nsolve a navigation problem in a grid world., write a short concise abstract based on this:  In this paper, we consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces. We show that the state-action frequencies and the expected cumulative reward are rational functions of the policy, and the degree of partial observability determines the degree of complexity. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints, which allows us to address the combinatorial and geometric complexity of the problem using recent tools from polynomial optimization. Finally, we use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.",
        "title": "The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs"
    },
    {
        "abs": "The title is On the advantages of stochastic encoders, start with Stochastic encoders have been used in rate-distortion theory and neural\ncompression because they can be easier to handle. However, in performance\ncomparisons with deterministic encoders they often do worse, suggesting that\nnoise in the encoding process may generally be a bad idea. It is poorly\nunderstood if and when stochastic encoders do better than deterministic\nencoders. In this paper we provide one illustrative example which shows that\nstochastic encoders can significantly outperform the best deterministic\nencoders. Our toy example suggests that stochastic encoders may be particularly\nuseful in the regime of \"perfect perceptual quality\"., write a short concise abstract based on this: \nStochastic encoders have been used in rate-distortion theory and neural compression because they can be easier to handle. However, in performance comparisons with deterministic encoders they often do worse, suggesting that noise in the encoding process may generally be a bad idea. This paper provides an illustrative example showing that stochastic encoders can significantly outperform the best deterministic encoders in the regime of \"perfect perceptual quality\". The findings suggest that stochastic encoders may be particularly useful in this regime",
        "title": "On the advantages of stochastic encoders"
    },
    {
        "abs": "The title is Learned transform compression with optimized entropy encoding, start with We consider the problem of learned transform compression where we learn both,\nthe transform as well as the probability distribution over the discrete codes.\nWe utilize a soft relaxation of the quantization operation to allow for\nback-propagation of gradients and employ vector (rather than scalar)\nquantization of the latent codes. Furthermore, we apply similar relaxation in\nthe code probability assignments enabling direct optimization of the code\nentropy. To the best of our knowledge, this approach is completely novel. We\nconduct a set of proof-of concept experiments confirming the potency of our\napproaches., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to learned transform compression, where both the transform and the probability distribution over the discrete codes are learned. We utilize a soft relaxation of the quantization operation to enable back-propagation of gradients and employ vector quantization of the latent codes. Additionally, we relax the code probability assignments to optimize the code entropy directly. Our approach is novel and we demonstrate its effectiveness through proof-of-concept experiments.",
        "title": "Learned transform compression with optimized entropy encoding"
    },
    {
        "abs": "The title is Improving Simulations with Symmetry Control Neural Networks, start with The dynamics of physical systems is often constrained to lower dimensional\nsub-spaces due to the presence of conserved quantities. Here we propose a\nmethod to learn and exploit such symmetry constraints building upon Hamiltonian\nNeural Networks. By enforcing cyclic coordinates with appropriate loss\nfunctions, we find that we can achieve improved accuracy on simple classical\ndynamics tasks. By fitting analytic formulae to the latent variables in our\nnetwork we recover that our networks are utilizing conserved quantities such as\n(angular) momentum., write a short concise abstract based on this: \nIn this paper, we propose a method to improve simulations of physical systems by leveraging the symmetry constraints of the system. We introduce a new architecture for Hamiltonian Neural Networks that incorporates cyclic coordinates and appropriate loss functions to enforce these constraints. We demonstrate the effectiveness of our approach on simple classical dynamics tasks, showing that it can achieve improved accuracy by utilizing conserved quantities such as momentum. By fitting analytic formulae to the latent variables in our network, we recover that",
        "title": "Improving Simulations with Symmetry Control Neural Networks"
    },
    {
        "abs": "The title is Low-Rank Projections of GCNs Laplacian, start with In this work, we study the behavior of standard models for community\ndetection under spectral manipulations. Through various ablation experiments,\nwe evaluate the impact of bandpass filtering on the performance of a GCN: we\nempirically show that most of the necessary and used information for nodes\nclassification is contained in the low-frequency domain, and thus contrary to\nimages, high frequencies are less crucial to community detection. In\nparticular, it is sometimes possible to obtain accuracies at a state-of-the-art\nlevel with simple classifiers that rely only on a few low frequencies., write a short concise abstract based on this: \nIn this work, we investigate the impact of spectral manipulations on the performance of graph convolutional networks (GCNs) for community detection. Through ablation experiments, we show that most of the necessary information for node classification is contained in the low-frequency domain, and that high frequencies are less crucial for community detection. We also demonstrate that simple classifiers that rely only on a few low frequencies can achieve state-of-the-art accuracy in some cases. Our findings have important implications for the design and",
        "title": "Low-Rank Projections of GCNs Laplacian"
    },
    {
        "abs": "The title is PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning, start with We propose a new framework of synthesizing data using deep generative models\nin a differentially private manner. Within our framework, sensitive data are\nsanitized with rigorous privacy guarantees in a one-shot fashion, such that\ntraining deep generative models is possible without re-using the original data.\nHence, no extra privacy costs or model constraints are incurred, in contrast to\npopular approaches such as Differentially Private Stochastic Gradient Descent\n(DP-SGD), which, among other issues, causes degradation in privacy guarantees\nas the training iteration increases. We demonstrate a realization of our\nframework by making use of the characteristic function and an adversarial\nre-weighting objective, which are of independent interest as well. Our proposal\nhas theoretical guarantees of performance, and empirical evaluations on\nmultiple datasets show that our approach outperforms other methods at\nreasonable levels of privacy., write a short concise abstract based on this:  In this paper, we propose a new framework for synthesizing data in a privacy-preserving manner using deep generative models. Our approach sanitizes sensitive data with rigorous privacy guarantees in a one-shot fashion, allowing for training deep generative models without reusing the original data. Unlike popular methods that degrade privacy guarantees over time, our approach provides theoretical guarantees of performance and outperforms other methods in empirical evaluations on multiple datasets.",
        "title": "PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning"
    },
    {
        "abs": "The title is Understanding Dimensional Collapse in Contrastive Self-supervised Learning, start with Self-supervised visual representation learning aims to learn useful\nrepresentations without relying on human annotations. Joint embedding approach\nbases on maximizing the agreement between embedding vectors from different\nviews of the same image. Various methods have been proposed to solve the\ncollapsing problem where all embedding vectors collapse to a trivial constant\nsolution. Among these methods, contrastive learning prevents collapse via\nnegative sample pairs. It has been shown that non-contrastive methods suffer\nfrom a lesser collapse problem of a different nature: dimensional collapse,\nwhereby the embedding vectors end up spanning a lower-dimensional subspace\ninstead of the entire available embedding space. Here, we show that dimensional\ncollapse also happens in contrastive learning. In this paper, we shed light on\nthe dynamics at play in contrastive learning that leads to dimensional\ncollapse. Inspired by our theory, we propose a novel contrastive learning\nmethod, called DirectCLR, which directly optimizes the representation space\nwithout relying on an explicit trainable projector. Experiments show that\nDirectCLR outperforms SimCLR with a trainable linear projector on ImageNet., write a short concise abstract based on this: \nIn this paper, we investigate the phenomenon of dimensional collapse in contrastive self-supervised learning, a popular approach to visual representation learning. We show that dimensional collapse also occurs in contrastive learning, despite its ability to prevent collapse via negative sample pairs. Inspired by our findings, we propose a novel method called DirectCLR, which optimizes the representation space directly without relying on an explicit trainable projector. Our experiments demonstrate that DirectCLR outperforms existing methods, including SimCLR with a trainable linear projector, on ImageNet.",
        "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning"
    },
    {
        "abs": "The title is Group Equivariant Stand-Alone Self-Attention For Vision, start with We provide a general self-attention formulation to impose group equivariance\nto arbitrary symmetry groups. This is achieved by defining positional encodings\nthat are invariant to the action of the group considered. Since the group acts\non the positional encoding directly, group equivariant self-attention networks\n(GSA-Nets) are steerable by nature. Our experiments on vision benchmarks\ndemonstrate consistent improvements of GSA-Nets over non-equivariant\nself-attention networks., write a short concise abstract based on this: \nIn this paper, we propose a novel approach to self-attention in vision tasks, called Group Equivariant Stand-Alone Self-Attention (GSA-Net). Our approach imposes group equivariance on the self-attention mechanism, allowing the network to focus on relevant features while ignoring irrelevant ones. We demonstrate the effectiveness of GSA-Net on several vision",
        "title": "Group Equivariant Stand-Alone Self-Attention For Vision"
    },
    {
        "abs": "The title is Disambiguating Symbolic Expressions in Informal Documents, start with We propose the task of disambiguating symbolic expressions in informal STEM\ndocuments in the form of LaTeX files - that is, determining their precise\nsemantics and abstract syntax tree - as a neural machine translation task. We\ndiscuss the distinct challenges involved and present a dataset with roughly\n33,000 entries. We evaluated several baseline models on this dataset, which\nfailed to yield even syntactically valid LaTeX before overfitting.\nConsequently, we describe a methodology using a transformer language model\npre-trained on sources obtained from arxiv.org, which yields promising results\ndespite the small size of the dataset. We evaluate our model using a plurality\nof dedicated techniques, taking the syntax and semantics of symbolic\nexpressions into account., write a short concise abstract based on this:  In this paper, we propose a task of disambiguating symbolic expressions in informal STEM documents, specifically LaTeX files. We treat this task as a neural machine translation problem and discuss the challenges involved. We present a dataset of roughly 33,000 entries and evaluate several baseline models, which fail to produce valid LaTeX. We then describe a methodology using a transformer language model pre-trained on sources from arxiv.org, which yields promising results despite the small size of the dataset. We evaluate our model using various techniques, taking into account the syntax and semantics of symbolic expressions.",
        "title": "Disambiguating Symbolic Expressions in Informal Documents"
    },
    {
        "abs": "The title is Fair Mixup: Fairness via Interpolation, start with Training classifiers under fairness constraints such as group fairness,\nregularizes the disparities of predictions between the groups. Nevertheless,\neven though the constraints are satisfied during training, they might not\ngeneralize at evaluation time. To improve the generalizability of fair\nclassifiers, we propose fair mixup, a new data augmentation strategy for\nimposing the fairness constraint. In particular, we show that fairness can be\nachieved by regularizing the models on paths of interpolated samples between\nthe groups. We use mixup, a powerful data augmentation strategy to generate\nthese interpolates. We analyze fair mixup and empirically show that it ensures\na better generalization for both accuracy and fairness measurement in tabular,\nvision, and language benchmarks., write a short concise abstract based on this: \nFair Mixup is a new data augmentation strategy that improves the generalization of fair classifiers by regularizing the models on paths of interpolated samples between different groups. By using mixup, a powerful data augmentation technique, Fair Mixup ensures better accuracy and fairness measurement in various benchmarks.",
        "title": "Fair Mixup: Fairness via Interpolation"
    },
    {
        "abs": "The title is Improved Autoregressive Modeling with Distribution Smoothing, start with While autoregressive models excel at image compression, their sample quality\nis often lacking. Although not realistic, generated images often have high\nlikelihood according to the model, resembling the case of adversarial examples.\nInspired by a successful adversarial defense method, we incorporate randomized\nsmoothing into autoregressive generative modeling. We first model a smoothed\nversion of the data distribution, and then reverse the smoothing process to\nrecover the original data distribution. This procedure drastically improves the\nsample quality of existing autoregressive models on several synthetic and\nreal-world image datasets while obtaining competitive likelihoods on synthetic\ndatasets., write a short concise abstract based on this: \nA new method for improving autoregressive modeling is proposed, which incorporates distribution smoothing to enhance sample quality. By modeling a smoothed version of the data distribution and then reversing the smoothing process, the method significantly improves the sample quality of existing autoregressive models on various synthetic and real-world image datasets while maintaining competitive likelihoods on synthetic datasets.",
        "title": "Improved Autoregressive Modeling with Distribution Smoothing"
    },
    {
        "abs": "The title is Continuous Weight Balancing, start with We propose a simple method by which to choose sample weights for problems\nwith highly imbalanced or skewed traits. Rather than naively discretizing\nregression labels to find binned weights, we take a more principled approach --\nwe derive sample weights from the transfer function between an estimated source\nand specified target distributions. Our method outperforms both unweighted and\ndiscretely-weighted models on both regression and classification tasks. We also\nopen-source our implementation of this method\n(https://github.com/Daniel-Wu/Continuous-Weight-Balancing) to the scientific\ncommunity., write a short concise abstract based on this: \nIn this paper, we propose a novel method for choosing sample weights in\nimbalanced or skewed data problems. Unlike traditional methods that rely on discretizing\nregression labels, we derive sample weights from the transfer function between an\nestimated source and specified target distributions. Our approach outperforms both\nunweighted and discretely-weighted models on both regression and classification tasks. We\nalso open-source our implementation of this",
        "title": "Continuous Weight Balancing"
    },
    {
        "abs": "The title is The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL, start with In this work, we analyze the reinstatement mechanism introduced by Ritter et\nal. (2018) to reveal two classes of neurons that emerge in the agent's working\nmemory (an epLSTM cell) when trained using episodic meta-RL on an episodic\nvariant of the Harlow visual fixation task. Specifically, Abstract neurons\nencode knowledge shared across tasks, while Episodic neurons carry information\nrelevant for a specific episode's task., write a short concise abstract based on this: \nIn this study, we investigate the emergence of two types of neurons in an agent's working memory when trained using episodic meta-RL on an episodic task. These neurons, called abstract and episodic neurons, encode different types of information. Abstract neurons represent knowledge shared across tasks, while episodic neurons carry information relevant to a specific episode's task. Our findings",
        "title": "The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL"
    },
    {
        "abs": "The title is Sparse Coding Frontend for Robust Neural Networks, start with Deep Neural Networks are known to be vulnerable to small, adversarially\ncrafted, perturbations. The current most effective defense methods against\nthese adversarial attacks are variants of adversarial training. In this paper,\nwe introduce a radically different defense trained only on clean images: a\nsparse coding based frontend which significantly attenuates adversarial attacks\nbefore they reach the classifier. We evaluate our defense on CIFAR-10 dataset\nunder a wide range of attack types (including Linf , L2, and L1 bounded\nattacks), demonstrating its promise as a general-purpose approach for defense., write a short concise abstract based on this: \nIn this paper, we propose a novel defense mechanism against adversarial attacks in deep neural networks, called Sparse Coding Frontend (SCF). Unlike existing methods that rely on adversarial training, SCF is trained only on clean images and utilizes sparse coding to significantly attenuate adversarial attacks before they reach the classifier. We evaluate SCF on the CIFAR-10 dataset under various attack types and demonstrate its effectiveness in providing a robust defense against adversarial attacks. Our approach has the potential to",
        "title": "Sparse Coding Frontend for Robust Neural Networks"
    },
    {
        "abs": "The title is A coding theorem for the rate-distortion-perception function, start with The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has\nemerged as a useful tool for thinking about realism and distortion of\nreconstructions in lossy compression. Unlike the rate-distortion function,\nhowever, it is unknown whether encoders and decoders exist that achieve the\nrate suggested by the RDPF. Building on results by Li and El Gamal (2018), we\nshow that the RDPF can indeed be achieved using stochastic, variable-length\ncodes. For this class of codes, we also prove that the RDPF lower-bounds the\nachievable rate, write a short concise abstract based on this: \nThe title of the paper is \"A Coding Theorem for the Rate-Distortion-Perception Function\" and it deals with the rate-distortion-perception function (RDPF), which is a tool used to analyze the realism and distortion of reconstructions in lossy compression. The paper shows that the RDPF can be achieved using stochastic, variable-length codes, and proves that the RDPF lower-bounds the achievable rate.",
        "title": "A coding theorem for the rate-distortion-perception function"
    },
    {
        "abs": "The title is Bermuda Triangles: GNNs Fail to Detect Simple Topological Structures, start with Most graph neural network architectures work by message-passing node vector\nembeddings over the adjacency matrix, and it is assumed that they capture graph\ntopology by doing that. We design two synthetic tasks, focusing purely on\ntopological problems -- triangle detection and clique distance -- on which\ngraph neural networks perform surprisingly badly, failing to detect those\n\"bermuda\" triangles. Datasets and their generation scripts are publicly\navailable on github.com/FujitsuLaboratories/bermudatriangles and\ndataset.labs.fujitsu.com., write a short concise abstract based on this: \nMost graph neural network architectures rely on message-passing node vector embeddings to capture graph topology. However, we show that these models fail to detect simple topological structures, such as triangles, in two synthetic tasks. Despite their reliance on message-passing, these models are unable to capture the underlying topology of the graph, leading to poor performance on tasks that require it. Our findings highlight the limitations of current graph neural",
        "title": "Bermuda Triangles: GNNs Fail to Detect Simple Topological Structures"
    },
    {
        "abs": "The title is Privacy and Integrity Preserving Training Using Trusted Hardware, start with Privacy and security-related concerns are growing as machine learning reaches\ndiverse application domains. The data holders want to train with private data\nwhile exploiting accelerators, such as GPUs, that are hosted in the cloud.\nHowever, Cloud systems are vulnerable to attackers that compromise the privacy\nof data and integrity of computations. This work presents DarKnight, a\nframework for large DNN training while protecting input privacy and computation\nintegrity. DarKnight relies on cooperative execution between trusted execution\nenvironments (TEE) and accelerators, where the TEE provides privacy and\nintegrity verification, while accelerators perform the computation heavy linear\nalgebraic operations., write a short concise abstract based on this: \nPrivacy and security-related concerns are growing as machine learning reaches diverse application domains. However, cloud systems are vulnerable to attackers that compromise the privacy of data and integrity of computations. This work presents DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEE) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation-heavy linear algebraic operations.",
        "title": "Privacy and Integrity Preserving Training Using Trusted Hardware"
    },
    {
        "abs": "The title is Deep Learning Hamiltonian Monte Carlo, start with We generalize the Hamiltonian Monte Carlo algorithm with a stack of neural\nnetwork layers and evaluate its ability to sample from different topologies in\na two dimensional lattice gauge theory. We demonstrate that our model is able\nto successfully mix between modes of different topologies, significantly\nreducing the computational cost required to generated independent gauge field\nconfigurations. Our implementation is available at\nhttps://github.com/saforem2/l2hmc-qcd ., write a short concise abstract based on this: \nDeep Learning Hamiltonian Monte Carlo (DL-HMC) is a novel approach that combines the power of deep neural networks with the efficiency of Hamiltonian Monte Carlo (HMC) to sample from complex target distributions. In this work, we apply DL-HMC to a two-dimensional lattice gauge theory and demonstrate its ability to mix between modes of different topologies, significantly reducing the computational cost required",
        "title": "Deep Learning Hamiltonian Monte Carlo"
    },
    {
        "abs": "The title is Do Concept Bottleneck Models Learn as Intended?, start with Concept bottleneck models map from raw inputs to concepts, and then from\nconcepts to targets. Such models aim to incorporate pre-specified, high-level\nconcepts into the learning procedure, and have been motivated to meet three\ndesiderata: interpretability, predictability, and intervenability. However, we\nfind that concept bottleneck models struggle to meet these goals. Using post\nhoc interpretability methods, we demonstrate that concepts do not correspond to\nanything semantically meaningful in input space, thus calling into question the\nusefulness of concept bottleneck models in their current form., write a short concise abstract based on this: \nConcept bottleneck models are a class of neural network architecture that aim to incorporate pre-specified high-level concepts into the learning process. However, our study finds that these models do not learn as intended, as the concepts do not correspond to anything semantically meaningful in input space. This raises questions about the usefulness of concept bottleneck models in their current form.",
        "title": "Do Concept Bottleneck Models Learn as Intended?"
    },
    {
        "abs": "The title is Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers, start with In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models., write a short concise abstract based on this:  In this paper, we propose a new attack on deep reinforcement learning agents by poisoning them with in-distribution triggers. These triggers are native to the data distribution the model will be trained on and deployed in, and we demonstrate how they can be embedded in the agent following a multi-task learning paradigm. We show the implications of this attack in three common reinforcement learning environments, highlighting the potential security risks of deep learning models.",
        "title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers"
    },
    {
        "abs": "The title is MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders, start with In this paper, we present a novel neuroevolutionary method to identify the\narchitecture and hyperparameters of convolutional autoencoders. Remarkably, we\nused a hypervolume indicator in the context of neural architecture search for\nautoencoders, for the first time to our current knowledge. Results show that\nimages were compressed by a factor of more than 10, while still retaining\nenough information to achieve image classification for the majority of the\ntasks. Thus, this new approach can be used to speed up the AutoML pipeline for\nimage compression., write a short concise abstract based on this: \nIn this paper, we propose a novel neuroevolutionary method for identifying the architecture and hyperparameters of convolutional autoencoders. Unlike previous approaches, we use a hypervolume indicator in the context of neural architecture search for autoencoders, which has not been explored before to our knowledge. Our results show that the proposed method can compress images by a factor of over 10 while still retaining enough information for image classification tasks. This new approach can significantly speed up the Auto",
        "title": "MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders"
    },
    {
        "abs": "The title is Learning Robust Controllers Via Probabilistic Model-Based Policy Search, start with Model-based Reinforcement Learning estimates the true environment through a\nworld model in order to approximate the optimal policy. This family of\nalgorithms usually benefits from better sample efficiency than their model-free\ncounterparts. We investigate whether controllers learned in such a way are\nrobust and able to generalize under small perturbations of the environment. Our\nwork is inspired by the PILCO algorithm, a method for probabilistic policy\nsearch. We show that enforcing a lower bound to the likelihood noise in the\nGaussian Process dynamics model regularizes the policy updates and yields more\nrobust controllers. We demonstrate the empirical benefits of our method in a\nsimulation benchmark., write a short concise abstract based on this: \nLearning robust controllers via probabilistic model-based policy search is proposed. This approach leverages a world model to estimate the true environment and approximate the optimal policy. The algorithm benefits from better sample efficiency than its model-free counterparts. The work investigates the robustness of controllers learned in this way under small perturbations of the environment. The method is inspired by the PILCO algorithm and shows that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. Empirical benefits are demonstrated in a simulation benchmark.",
        "title": "Learning Robust Controllers Via Probabilistic Model-Based Policy Search"
    },
    {
        "abs": "The title is Training and Generating Neural Networks in Compressed Weight Space, start with The inputs and/or outputs of some neural nets are weight matrices of other\nneural nets. Indirect encodings or end-to-end compression of weight matrices\ncould help to scale such approaches. Our goal is to open a discussion on this\ntopic, starting with recurrent neural networks for character-level language\nmodelling whose weight matrices are encoded by the discrete cosine transform.\nOur fast weight version thereof uses a recurrent neural network to parameterise\nthe compressed weights. We present experimental results on the enwik8 dataset., write a short concise abstract based on this: \nIn this paper, we explore the use of compressed weight spaces for training and generating neural networks, particularly for character-level language modeling. We propose a method for encoding and decoding weight matrices using the discrete cosine transform, and demonstrate the effectiveness of our approach on the EnWik8 dataset. Our results show that using compressed weight spaces can significantly reduce the computational requirements for training and generating neural networks, while maintaining their accuracy. We also introduce a fast weight version of our method that uses",
        "title": "Training and Generating Neural Networks in Compressed Weight Space"
    },
    {
        "abs": "The title is ICLR 2021 Challenge for Computational Geometry & Topology: Design and Results, start with This paper presents the computational challenge on differential geometry and\ntopology that happened within the ICLR 2021 workshop \"Geometric and Topological\nRepresentation Learning\". The competition asked participants to provide\ncreative contributions to the fields of computational geometry and topology\nthrough the open-source repositories Geomstats and Giotto-TDA. The challenge\nattracted 16 teams in its two month duration. This paper describes the design\nof the challenge and summarizes its main findings., write a short concise abstract based on this: 16 teams participated in the ICLR 2021 Challenge for Computational Geometry & Topology, which ran for two months. The competition asked participants to create innovative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. This paper describes the challenge's design and summarizes its key findings.",
        "title": "ICLR 2021 Challenge for Computational Geometry & Topology: Design and Results"
    },
    {
        "abs": "The title is Efficient Training Under Limited Resources, start with Training time budget and size of the dataset are among the factors affecting\nthe performance of a Deep Neural Network (DNN). This paper shows that Neural\nArchitecture Search (NAS), Hyper Parameters Optimization (HPO), and Data\nAugmentation help DNNs perform much better while these two factors are limited.\nHowever, searching for an optimal architecture and the best hyperparameter\nvalues besides a good combination of data augmentation techniques under low\nresources requires many experiments. We present our approach to achieving such\na goal in three steps: reducing training epoch time by compressing the model\nwhile maintaining the performance compared to the original model, preventing\nmodel overfitting when the dataset is small, and performing the hyperparameter\ntuning. We used NOMAD, which is a blackbox optimization software based on a\nderivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of\n86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware\nEfficient Training (HAET) Challenge and won second place in the competition.\nThe competition results can be found at haet2021.github.io/challenge and our\nsource code can be found at github.com/DouniaLakhmiri/ICLR\\_HAET2021., write a short concise abstract based on this: \nEfficient Training Under Limited Resources: A Survey of Techniques and Approaches. Deep Neural Networks (DNNs) performance is affected by training time and dataset size. This paper investigates how Neural Architecture Search (NAS), Hyperparameters Optimization (HPO), and Data Augmentation can improve DNN performance while these factors are limited. The authors present their approach to achieving this goal in three steps: reducing training epoch time, preventing overfitting, and performing hyperparameter tuning. They used NOMAD, a blackbox optimization software, to perform NAS and HPO. The work achieved an accuracy of 86.0% on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition.",
        "title": "Efficient Training Under Limited Resources"
    },
    {
        "abs": "The title is SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness, start with In this paper, we cast fair machine learning as invariant machine learning.\nWe first formulate a version of individual fairness that enforces invariance on\ncertain sensitive sets. We then design a transport-based regularizer that\nenforces this version of individual fairness and develop an algorithm to\nminimize the regularizer efficiently. Our theoretical results guarantee the\nproposed approach trains certifiably fair ML models. Finally, in the\nexperimental studies we demonstrate improved fairness metrics in comparison to\nseveral recent fair training procedures on three ML tasks that are susceptible\nto algorithmic bias., write a short concise abstract based on this:  In this paper, we propose a new approach to fair machine learning that focuses on enforcing individual fairness through sensitivity to certain sensitive sets. We formulate a version of individual fairness that is invariant to these sets and design a transport-based regularizer to enforce it. Our approach guarantees certified fairness and outperforms recent fair training methods on three ML tasks.",
        "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness"
    },
    {
        "abs": "The title is Graph-Based Continual Learning, start with Despite significant advances, continual learning models still suffer from\ncatastrophic forgetting when exposed to incrementally available data from\nnon-stationary distributions. Rehearsal approaches alleviate the problem by\nmaintaining and replaying a small episodic memory of previous samples, often\nimplemented as an array of independent memory slots. In this work, we propose\nto augment such an array with a learnable random graph that captures pairwise\nsimilarities between its samples, and use it not only to learn new tasks but\nalso to guard against forgetting. Empirical results on several benchmark\ndatasets show that our model consistently outperforms recently proposed\nbaselines for task-free continual learning., write a short concise abstract based on this:  Despite significant advances in continual learning, models still struggle with catastrophic forgetting when exposed to new data from non-stationary distributions. This work proposes a new approach that augments an episodic memory with a learnable random graph to capture pairwise similarities between samples. The graph is used not only to learn new tasks but also to prevent forgetting of previous knowledge. Empirical results show that the proposed model outperforms recent baselines for task-free continual learning.",
        "title": "Graph-Based Continual Learning"
    },
    {
        "abs": "The title is Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS, start with We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural\ntangent kernel and the Laplace kernel include the same set of functions, when\nboth kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we\nprove that the exponential power kernel with a smaller power (making the kernel\nless smooth) leads to a larger RKHS, when it is restricted to the sphere\n$\\mathbb{S}^{d-1}$ and when it is defined on the entire $\\mathbb{R}^d$., write a short concise abstract based on this:  Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS.\nWe prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions, when both kernels are restricted to the sphere $\\mathbb{S}^{d-1}$. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger",
        "title": "Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS"
    },
    {
        "abs": "The title is Reinforcement Learning with Random Delays, start with Action and observation delays commonly occur in many Reinforcement Learning\napplications, such as remote control scenarios. We study the anatomy of\nrandomly delayed environments, and show that partially resampling trajectory\nfragments in hindsight allows for off-policy multi-step value estimation. We\napply this principle to derive Delay-Correcting Actor-Critic (DCAC), an\nalgorithm based on Soft Actor-Critic with significantly better performance in\nenvironments with delays. This is shown theoretically and also demonstrated\npractically on a delay-augmented version of the MuJoCo continuous control\nbenchmark., write a short concise abstract based on this: \nReinforcement Learning with Random Delays: Action and Observation Delays commonly occur in many Reinforcement Learning applications, such as remote control scenarios. We study the anatomy of randomly delayed environments and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor",
        "title": "Reinforcement Learning with Random Delays"
    },
    {
        "abs": "The title is Differentially Private Learning Needs Better Features (or Much More Data), start with We demonstrate that differentially private machine learning has not yet\nreached its \"AlexNet moment\" on many canonical vision tasks: linear models\ntrained on handcrafted features significantly outperform end-to-end deep neural\nnetworks for moderate privacy budgets. To exceed the performance of handcrafted\nfeatures, we show that private learning requires either much more private data,\nor access to features learned on public data from a similar domain. Our work\nintroduces simple yet strong baselines for differentially private learning that\ncan inform the evaluation of future progress in this area., write a short concise abstract based on this: \nDifferentially private machine learning has not yet reached its full potential, according to a new study. The study shows that linear models trained on handcrafted features outperform end-to-end deep neural networks for moderate privacy budgets. To improve the performance of private learning, the study suggests that either more private data is needed or access to features learned on public data from a similar domain is required. The study introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.",
        "title": "Differentially Private Learning Needs Better Features (or Much More Data)"
    },
    {
        "abs": "The title is Individually Fair Ranking, start with We develop an algorithm to train individually fair learning-to-rank (LTR)\nmodels. The proposed approach ensures items from minority groups appear\nalongside similar items from majority groups. This notion of fair ranking is\nbased on the definition of individual fairness from supervised learning and is\nmore nuanced than prior fair LTR approaches that simply ensure the ranking\nmodel provides underrepresented items with a basic level of exposure. The crux\nof our method is an optimal transport-based regularizer that enforces\nindividual fairness and an efficient algorithm for optimizing the regularizer.\nWe show that our approach leads to certifiably individually fair LTR models and\ndemonstrate the efficacy of our method on ranking tasks subject to demographic\nbiases., write a short concise abstract based on this:  We propose a novel approach to learning-to-rank (LTR) models that ensures individually fair ranking. Our method uses an optimal transport-based regularizer to enforce individual fairness, resulting in certifiably fair models. We demonstrate the effectiveness of our approach on ranking tasks with demographic biases, outperforming existing fair LTR methods.",
        "title": "Individually Fair Ranking"
    },
    {
        "abs": "The title is Individually Fair Gradient Boosting, start with We consider the task of enforcing individual fairness in gradient boosting.\nGradient boosting is a popular method for machine learning from tabular data,\nwhich arise often in applications where algorithmic fairness is a concern. At a\nhigh level, our approach is a functional gradient descent on a\n(distributionally) robust loss function that encodes our intuition of\nalgorithmic fairness for the ML task at hand. Unlike prior approaches to\nindividual fairness that only work with smooth ML models, our approach also\nworks with non-smooth models such as decision trees. We show that our algorithm\nconverges globally and generalizes. We also demonstrate the efficacy of our\nalgorithm on three ML problems susceptible to algorithmic bias., write a short concise abstract based on this: \nIn this paper, we propose a new method for enforcing individual fairness in gradient boosting, a popular machine learning technique for tabular data. Our approach is based on functional gradient descent on a distributionally robust loss function that captures our intuition of fairness for the ML task at hand. Unlike prior methods that only work with smooth models, our approach can handle non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes, and demonstrate its effectiveness on three ML problems prone to algorithmic bias.",
        "title": "Individually Fair Gradient Boosting"
    },
    {
        "abs": "The title is FedPandemic: A Cross-Device Federated Learning Approach Towards Elementary Prognosis of Diseases During a Pandemic, start with The amount of data, manpower and capital required to understand, evaluate and\nagree on a group of symptoms for the elementary prognosis of pandemic diseases\nis enormous. In this paper, we present FedPandemic, a novel noise\nimplementation algorithm integrated with cross-device Federated learning for\nElementary symptom prognosis during a pandemic, taking COVID-19 as a case\nstudy. Our results display consistency and enhance robustness in recovering the\ncommon symptoms displayed by the disease, paving a faster and cheaper path\ntowards symptom retrieval while also preserving the privacy of patient's\nsymptoms via Federated learning., write a short concise abstract based on this: \nA novel approach called FedPandemic combines cross-device Federated learning with a noise implementation algorithm to enhance the accuracy and robustness of symptom prognosis during a pandemic. By leveraging the collective knowledge of numerous devices, FedPandemic can quickly and inexpensively identify common symptoms of the disease, while also protecting patient privacy through Federated learning. Our results demonstrate the effectiveness of FedPandemic in recovering symptoms displayed by COVID-19, paving the way for faster and more cost-effective disease diagnosis.",
        "title": "FedPandemic: A Cross-Device Federated Learning Approach Towards Elementary Prognosis of Diseases During a Pandemic"
    },
    {
        "abs": "The title is Document Structure aware Relational Graph Convolutional Networks for Ontology Population, start with Ontologies comprising of concepts, their attributes, and relationships are\nused in many knowledge based AI systems. While there have been efforts towards\npopulating domain specific ontologies, we examine the role of document\nstructure in learning ontological relationships between concepts in any\ndocument corpus. Inspired by ideas from hypernym discovery and explainability,\nour method performs about 15 points more accurate than a stand-alone R-GCN\nmodel for this task., write a short concise abstract based on this: \nOntologies are used in many knowledge-based AI systems, but existing methods for populating domain-specific ontologies often rely on manual curation. In this work, we explore the role of document structure in learning ontological relationships between concepts in any document corpus. We propose a novel method, Document Structure-aware Relational Graph Convolutional Networks (DS-RGCNs), which leverages both the semantic",
        "title": "Document Structure aware Relational Graph Convolutional Networks for Ontology Population"
    },
    {
        "abs": "The title is Imitation Learning by Reinforcement Learning, start with Imitation learning algorithms learn a policy from demonstrations of expert\nbehavior. We show that, for deterministic experts, imitation learning can be\ndone by reduction to reinforcement learning with a stationary reward. Our\ntheoretical analysis both certifies the recovery of expert reward and bounds\nthe total variation distance between the expert and the imitation learner,\nshowing a link to adversarial imitation learning. We conduct experiments which\nconfirm that our reduction works well in practice for continuous control tasks., write a short concise abstract based on this: \nImitation learning is a popular approach to learning from demonstrations, where an agent learns a policy from expert demonstrations. We show that imitation learning can be reduced to reinforcement learning with a stationary reward, which provides a theoretical guarantee for recovering the expert reward. Our analysis also bounds the total variation distance between the expert and the imitation learner, providing a link to adversarial imitation learning. We demonstrate the effectiveness of our reduction in",
        "title": "Imitation Learning by Reinforcement Learning"
    },
    {
        "abs": "The title is Unifying Likelihood-free Inference with Black-box Optimization and Beyond, start with Black-box optimization formulations for biological sequence design have drawn\nrecent attention due to their promising potential impact on the pharmaceutical\nindustry. In this work, we propose to unify two seemingly distinct worlds:\nlikelihood-free inference and black-box optimization, under one probabilistic\nframework. In tandem, we provide a recipe for constructing various sequence\ndesign methods based on this framework. We show how previous optimization\napproaches can be \"reinvented\" in our framework, and further propose new\nprobabilistic black-box optimization algorithms. Extensive experiments on\nsequence design application illustrate the benefits of the proposed\nmethodology., write a short concise abstract based on this: \nIn this work, we propose a unified framework for likelihood-free inference and black-box optimization, with the goal of bridging two seemingly distinct fields. By combining these approaches under a probabilistic framework, we provide a recipe for constructing various sequence design methods and demonstrate the benefits of our approach through extensive experiments. Our proposed methodology has the potential to significantly impact the pharmaceutical industry by providing a more efficient and effective way to design biological sequences.",
        "title": "Unifying Likelihood-free Inference with Black-box Optimization and Beyond"
    },
    {
        "abs": "The title is Regularization Matters in Policy Optimization, start with Deep Reinforcement Learning (Deep RL) has been receiving increasingly more\nattention thanks to its encouraging performance on a variety of control tasks.\nYet, conventional regularization techniques in training neural networks (e.g.,\n$L_2$ regularization, dropout) have been largely ignored in RL methods,\npossibly because agents are typically trained and evaluated in the same\nenvironment, and because the deep RL community focuses more on high-level\nalgorithm designs. In this work, we present the first comprehensive study of\nregularization techniques with multiple policy optimization algorithms on\ncontinuous control tasks. Interestingly, we find conventional regularization\ntechniques on the policy networks can often bring large improvement, especially\non harder tasks. Our findings are shown to be robust against training\nhyperparameter variations. We also compare these techniques with the more\nwidely used entropy regularization. In addition, we study regularizing\ndifferent components and find that only regularizing the policy network is\ntypically the best. We further analyze why regularization may help\ngeneralization in RL from four perspectives - sample complexity, reward\ndistribution, weight norm, and noise robustness. We hope our study provides\nguidance for future practices in regularizing policy optimization algorithms.\nOur code is available at https://github.com/xuanlinli17/iclr2021_rlreg ., write a short concise abstract based on this: \nRegularization techniques have been largely overlooked in deep reinforcement learning (Deep RL) despite their success in traditional machine learning. In this work, we investigate the effectiveness of regularization techniques on policy optimization algorithms for continuous control tasks. Our findings show that conventional regularization techniques can significantly improve performance, especially on harder tasks. We also compare these techniques with entropy regularization and analyze their effectiveness from four perspectives. Our results provide guidance for future practices in regularizing policy optimization algorithms in Deep RL.",
        "title": "Regularization Matters in Policy Optimization"
    },
    {
        "abs": "The title is Iterated learning for emergent systematicity in VQA, start with Although neural module networks have an architectural bias towards\ncompositionality, they require gold standard layouts to generalize\nsystematically in practice. When instead learning layouts and modules jointly,\ncompositionality does not arise automatically and an explicit pressure is\nnecessary for the emergence of layouts exhibiting the right structure. We\npropose to address this problem using iterated learning, a cognitive science\ntheory of the emergence of compositional languages in nature that has primarily\nbeen applied to simple referential games in machine learning. Considering the\nlayouts of module networks as samples from an emergent language, we use\niterated learning to encourage the development of structure within this\nlanguage. We show that the resulting layouts support systematic generalization\nin neural agents solving the more complex task of visual question-answering.\nOur regularized iterated learning method can outperform baselines without\niterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a\nnew split of the SHAPES dataset we introduce to evaluate systematic\ngeneralization, and on CLOSURE, an extension of CLEVR also designed to test\nsystematic generalization. We demonstrate superior performance in recovering\nground-truth compositional program structure with limited supervision on both\nSHAPES-SyGeT and CLEVR., write a short concise abstract based on this:  Although neural networks have a natural bias towards compositionality, they often fail to generalize systematically in practice. We propose a new method called iterated learning, inspired by cognitive science theories of emergent language, to encourage the development of structure in the layouts of neural networks. We show that this approach leads to better systematic generalization in visual question-answering tasks, outperforming baselines on two new benchmarks. Our method can recover ground-truth compositional program structure with limited supervision, demonstrating its effectiveness in improving the generalization abilities of neural networks.",
        "title": "Iterated learning for emergent systematicity in VQA"
    },
    {
        "abs": "The title is Undistillable: Making A Nasty Teacher That CANNOT teach students, start with Knowledge Distillation (KD) is a widely used technique to transfer knowledge\nfrom pre-trained teacher models to (usually more lightweight) student models.\nHowever, in certain situations, this technique is more of a curse than a\nblessing. For instance, KD poses a potential risk of exposing intellectual\nproperties (IPs): even if a trained machine learning model is released in\n'black boxes' (e.g., as executable software or APIs without open-sourcing\ncode), it can still be replicated by KD through imitating input-output\nbehaviors. To prevent this unwanted effect of KD, this paper introduces and\ninvestigates a concept called Nasty Teacher: a specially trained teacher\nnetwork that yields nearly the same performance as a normal one, but would\nsignificantly degrade the performance of student models learned by imitating\nit. We propose a simple yet effective algorithm to build the nasty teacher,\ncalled self-undermining knowledge distillation. Specifically, we aim to\nmaximize the difference between the output of the nasty teacher and a normal\npre-trained network. Extensive experiments on several datasets demonstrate that\nour method is effective on both standard KD and data-free KD, providing the\ndesirable KD-immunity to model owners for the first time. We hope our\npreliminary study can draw more awareness and interest in this new practical\nproblem of both social and legal importance., write a short concise abstract based on this:  A new technique called \"Nasty Teacher\" is proposed to prevent the unwanted transfer of intellectual properties through knowledge distillation. The Nasty Teacher is a specially trained teacher network that degrades the performance of student models learned by imitating it, providing a desired level of \"KD-immunity\" to model owners. The proposed method, self-undermining knowledge distillation, maximizes the difference between the output of the Nasty Teacher and a normal pre-trained network, and is effective on both standard and data-free knowledge distillation.",
        "title": "Undistillable: Making A Nasty Teacher That CANNOT teach students"
    },
    {
        "abs": "The title is \u03b4-CLUE: Diverse Sets of Explanations for Uncertainty Estimates, start with To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions., write a short concise abstract based on this:  To better interpret uncertainty estimates from probabilistic models, recent work has proposed generating Counterfactual Latent Uncertainty Explanations (CLUEs). However, these approaches may output a single explanation for a given input, leading to a lack of diversity in the explanations. In this work, we propose augmenting the original CLUE approach by generating a set of plausible CLUEs, or diverse inputs that are close to the original input in latent space and lead to confident predictions. This allows for a more comprehensive understanding of the uncertainty estimates and the underlying data.",
        "title": "\u03b4-CLUE: Diverse Sets of Explanations for Uncertainty Estimates"
    }
]