[
    {
        "abs": "This report presents a concise overview of a Theano-based implementation of AlexNet (Krizhevsky et al., 2012) for large-scale visual recognition, including the utilization of multiple GPUs.",
        "title": "Theano-based Large-Scale Visual Recognition with Multiple GPUs"
    },
    {
        "abs": "In this study, we present compelling evidence showing that deep narrow Boltzmann machines possess the ability to approximate universal probability distributions. By analyzing a wide range of probability distributions on a given dataset, we demonstrate the capacity of deep narrow Boltzmann machines to accurately represent and model complex data patterns. Our findings contribute to the understanding of the capabilities of deep narrow Boltzmann machines as powerful tools for probability distribution approximation.",
        "title": "Deep Narrow Boltzmann Machines are Universal Approximators"
    },
    {
        "abs": "In this study, we explore a novel approach to improve the performance of recurrent neural networks (RNNs) by incorporating latent variables through advances in variational inference. We propose a method that leverages these advances to enhance the capabilities of RNNs. By incorporating latent variables, our approach enables the model to capture the underlying complex dependencies and uncertainties present in sequential data. Through experiments, we demonstrate the effectiveness of our method in learning stochastic recurrent networks, showcasing improved performance and enhanced modeling capabilities compared to traditional RNNs.",
        "title": "Learning Stochastic Recurrent Networks"
    },
    {
        "abs": "This paper introduces a general framework for online adaptation of optimization hyperparameters by \"hot swapping\" them. The proposed approach allows for efficient and dynamic adjustments of hyperparameters during runtime, resulting in improved performance and adaptability of optimization algorithms. By exchanging hyperparameters seamlessly, the framework enables automated fine-tuning and adaptation, making it suitable for various applications requiring optimization in online environments. The effectiveness and advantages of the framework are demonstrated through experimental evaluations on different optimization algorithms and benchmark problems.",
        "title": "Hot Swapping for Online Adaptation of Optimization Hyperparameters"
    },
    {
        "abs": "Many modern multiclass and multilabel problems involve large output spaces. As a result, conventional methods for handling these problems become computationally expensive and inefficient. In this study, we propose a new approach called Fast Label Embeddings that tackles the challenge of extremely large output spaces. By leveraging efficient algorithms and data structures, we are able to significantly reduce the computational complexity without sacrificing performance. Our experimental results demonstrate the effectiveness and scalability of Fast Label Embeddings in dealing with the ever-growing output spaces encountered in modern multiclass and multilabel problems.",
        "title": "Fast Label Embeddings for Extremely Large Output Spaces"
    },
    {
        "abs": "Accurate representational learning of both explicit and implicit relationships within data is critical to understand and analyze complex systems. This abstract presents a novel approach called Dynamic Adaptive Network Intelligence (DANI) that aims to enhance the learning capabilities of artificial intelligence systems. DANI leverages advanced neural network architectures and adaptive mechanisms to autonomously adapt its structure and parameters, enabling it to effectively capture and model the intricate relationships present in diverse datasets. By dynamically adapting to changing data patterns, DANI offers a promising framework for improving the performance and versatility of AI algorithms in various application domains.",
        "title": "Dynamic Adaptive Network Intelligence"
    },
    {
        "abs": "Automatic speech recognition systems typically rely on spectral-based features, such as Mel-frequency cepstral coefficients (MFCC) or Perceptual Linear Prediction (PLP). However, these approaches may not capture the underlying linear separability of speech signals effectively. In this paper, we propose a novel approach to learning linearly separable features using convolutional neural networks (CNNs) for speech recognition. Our experimental results show that the learned features by CNNs achieved improved recognition performance compared to traditional spectral-based features.",
        "title": "Learning linearly separable features for speech recognition using convolutional neural networks"
    },
    {
        "abs": "The neural network training framework used in the Kaldi speech recognition toolkit is described. This framework involves parallel training of deep neural networks (DNNs) using a combination of natural gradient and parameter averaging. The abstract would focus on summarizing the key aspects of this framework, highlighting the benefits it offers in training DNNs for speech recognition tasks.",
        "title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging"
    },
    {
        "abs": "In this paper, we propose a novel approach for visualizing and refining the invariances of learned representations. Our method aims to improve the interpretability and performance of these representations by focusing on their geodesics. By analyzing the geometric properties along the geodesics, we can gain insights into the invariances present in the learned representations. Additionally, we introduce a refinement technique that utilizes the information from geodesics to enhance the discriminative power of the representations. Through experiments, we demonstrate the effectiveness of our method in improving both the interpretability and performance of learned representations.",
        "title": "Geodesics of learned representations"
    },
    {
        "abs": "This abstract explores a group theoretic perspective on unsupervised deep learning to understand the reasons behind its success, the nature of representations it captures, and how higher-order representations emerge. Inspired by the principles of group theory, this perspective delves into the underlying structures and symmetries within deep learning models. By illuminating the mechanisms that enable deep learning to learn and extract abstract features, this abstract contributes to a deeper understanding of the fundamentals of unsupervised deep learning.",
        "title": "A Group Theoretic Perspective on Unsupervised Deep Learning"
    },
    {
        "abs": "This paper introduces a novel architecture called Stacked What-Where Auto-encoders (SWWAE) that combines discriminative and generative features. The SWWAE framework integrates both what and where information, enabling effective feature learning. This abstract provides a concise overview of the paper's focus on the SWWAE architecture and its potential contributions to the field.",
        "title": "Stacked What-Where Auto-encoders"
    },
    {
        "abs": "In this study, we investigate the problem of inducing word embeddings that are specifically tailored for predicting the relationship between two words in a bilexical context. We compare different methods experimentally and provide a concise abstract summarizing our findings.",
        "title": "Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison"
    },
    {
        "abs": "This paper proposes a generative model for deep convolutional learning, specifically focused on multi-layered convolutional dictionary learning. A novel probabilistic pooling technique is introduced to enhance the performance of the model. The proposed model provides a promising solution for efficient and accurate deep convolutional learning in various applications.",
        "title": "A Generative Model for Deep Convolutional Learning"
    },
    {
        "abs": "In this paper, we introduce a model that generates realistic images based on textual captions with the aid of attention mechanisms. We are motivated by the recent advancements in generative models and aim to enhance the image generation process. Our proposed model utilizes attention to focus on specific parts of the captions, ensuring accurate image generation. By combining the strengths of generative models and attention mechanisms, we achieve improved results in generating high-quality images from textual descriptions. Through experimental evaluations, we demonstrate the effectiveness of our model in accurately translating captions into visually appealing images.",
        "title": "Generating Images from Captions with Attention"
    },
    {
        "abs": "Convolutional neural networks (CNNs) have proven to be effective on large datasets. However, obtaining labeled data can be challenging. In this study, we propose a Bayesian approach to CNNs using Bernoulli approximate variational inference. This method enables us to make use of unlabeled data by incorporating uncertainty estimation. Experimental results demonstrate the effectiveness of our proposed approach, showing improved performance compared to conventional CNNs.",
        "title": "Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference"
    },
    {
        "abs": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank filters. Our method aims to enhance image classification tasks by reducing the computational cost of CNNs without sacrificing accuracy. By exploiting the low-rank properties of filters, we achieve significant improvements in efficiency while preserving classification performance. Experimental results on various image datasets demonstrate the effectiveness of our approach, showing promising potential in developing more efficient CNN models for image classification tasks.",
        "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification"
    },
    {
        "abs": "This paper introduces a straightforward and effective approach to generate word sense representations. Distributed representations of words have greatly improved the performance of numerous Natural Language Processing (NLP) tasks. However, existing methods for generating word sense representations are often complex and computationally expensive. Our proposed method provides a simplified and efficient solution to this problem, enhancing the accuracy of word sense representations in various NLP applications.",
        "title": "A Simple and Efficient Method To Generate Word Sense Representations"
    },
    {
        "abs": "This study introduces Diverse Embedding Neural Network (DENN), a novel architectural approach for Language Models (LMs). DENN aims to enhance the diversity of word embeddings utilized within LMs. This abstract intends to provide a concise overview of the research topic.",
        "title": "Diverse Embedding Neural Network Language Models"
    },
    {
        "abs": "This paper delves into the domain of cold-start recommendation by focusing on representation learning. It begins by introducing the standard approach of Collaborative Filtering (CF) for predicting user ratings on items. However, this approach heavily relies on existing user-item interactions and often fails to provide accurate recommendations for new users and items. To overcome this limitation, the authors propose a representation learning-based approach for cold-start recommendation. Through the utilization of feature representations, this methodology aims to capture the latent preferences and traits of users and items. This abstract provides a brief overview of the paper's focus on improving the accuracy of CF for cold-start recommendations through representation learning.",
        "title": "Representation Learning for cold-start recommendation"
    },
    {
        "abs": "In this paper, we introduce NICE: Non-linear Independent Components Estimation, a deep learning framework designed for modeling complex high-dimensional densities. Addressing the challenges posed by non-linear dependencies, our proposed framework utilizes non-linear transformations to estimate the independent components of the data. By leveraging the power of deep learning techniques, NICE is capable of accurately capturing the intricate structure of high-dimensional data, enabling advanced modeling and analysis. Through extensive experimentation, we demonstrate the effectiveness of our framework in various real-world datasets, showcasing its potential to significantly contribute to the field of modeling complex densities.",
        "title": "NICE: Non-linear Independent Components Estimation"
    },
    {
        "abs": "We introduce Deep Linear Discriminant Analysis (DeepLDA), a method that learns linearly separable latent representations in an end-to-end manner. DeepLDA utilizes deep neural networks to map input data into a low-dimensional space, where linear classifiers can effectively separate the data. Experimental results demonstrate the superior discriminative power of DeepLDA compared to traditional linear discriminant analysis techniques.",
        "title": "Deep Linear Discriminant Analysis"
    },
    {
        "abs": "The abstract should be short and concise, providing a summary of the main points discussed in the article. It should mention that the article explores the use of Layer-Sequential Unit-Variance (LSUV) initialization as a method for weight initialization in deep network learning.",
        "title": "All you need is a good init"
    },
    {
        "abs": "In this study, we introduce a parametric nonlinear transformation that effectively Gaussianizes data extracted from natural images. The proposed method aims to improve density modeling of images by applying a generalized normalization transformation. By employing this transformation, we can better understand the statistical properties of natural images and enhance image processing tasks.",
        "title": "Density Modeling of Images using a Generalized Normalization Transformation"
    },
    {
        "abs": "This research proposes flattened convolutional neural networks (CNNs) specifically designed to enhance the speed of feedforward execution. These networks aim to reduce redundancy and improve computational efficiency, leading to faster processing of neural network tasks.",
        "title": "Flattened Convolutional Neural Networks for Feedforward Acceleration"
    },
    {
        "abs": "In this paper, we introduce a novel deep learning framework called Purine. Purine is a bi-graph based framework that aims to enhance the efficiency and performance of deep learning models. By utilizing the bi-graph structure, Purine effectively manages and optimizes the computation workflow, leading to improved scalability and reduced memory requirements. We demonstrate the effectiveness of Purine through experiments conducted on various deep learning tasks. Results show that Purine outperforms existing frameworks in terms of both computational efficiency and model performance. This work presents a promising approach towards advancing the field of deep learning and enhancing its practical application in real-world scenarios.",
        "title": "Purine: A bi-graph based deep learning framework"
    },
    {
        "abs": "In this paper, we propose a model that combines the strengths of Recurrent Neural Networks (RNNs) and Stochastic Gradient Variational Bayes (SGVB). The model, called Variational Recurrent Auto-Encoders, aims to improve the expressive power and generative capabilities of traditional RNNs by incorporating a variational inference framework. Through experimental evaluation, we demonstrate the effectiveness of this new model in various tasks such as sequence generation and text prediction. Our results show superior performance compared to traditional RNNs, highlighting the potential of Variational Recurrent Auto-Encoders as a powerful tool in deep learning applications.",
        "title": "Variational Recurrent Auto-Encoders"
    },
    {
        "abs": "This abstract summarizes the concept of word representations through Gaussian embedding. In current work on lexical distributed representations, each word is mapped to a point vector in a lower-dimensional space. The abstract does not provide further details on the specific method or results of this representation process.",
        "title": "Word Representations via Gaussian Embedding"
    },
    {
        "abs": "This abstract discusses the optimization of deep neural networks by training them using low precision multiplications. The focus is on the expensive and power-consuming arithmetic operator, multipliers, in the digital implementation of deep networks. The abstract highlights the potential benefits and effectiveness of employing low precision multiplications in training deep neural networks, aiming to improve their performance while reducing the resource requirements.",
        "title": "Training deep neural networks with low precision multiplications"
    },
    {
        "abs": "Multiple instance learning (MIL) has gained recognition for its ability to reduce the need for costly annotation in various tasks. In this study, we propose a fully convolutional multi-class multiple instance learning approach. By leveraging the power of convolutional neural networks (CNNs) and the concept of multiple instance learning, our method enables accurate classification of instances within a bag, thus minimizing the need for expensive manual annotation. Through experiments on various datasets, we demonstrate the effectiveness and efficiency of our proposed approach compared to traditional methods. Our findings suggest that fully convolutional multi-class multiple instance learning holds promise for reducing annotation costs and improving classification performance in tasks that require extensive labeling.",
        "title": "Fully Convolutional Multi-Class Multiple Instance Learning"
    },
    {
        "abs": "Recently, nested dropout has been proposed as a method for ordering representation units in autoencoders. This paper explores the application of nested dropout to learning compact convolutional neural networks (CNNs). The results demonstrate that incorporating nested dropout into the training process of CNNs leads to improved performance in terms of both model compactness and classification accuracy. The findings suggest that nested dropout can effectively enhance the learning process of CNNs, contributing to the development of more efficient deep learning models.",
        "title": "Learning Compact Convolutional Neural Networks with Nested Dropout"
    },
    {
        "abs": "This paper introduces ADASECANT, a robust and adaptive secant method for stochastic gradient algorithms. Large-scale learning problems have prompted extensive research in stochastic gradient approaches. However, existing algorithms have limitations in efficiency and accuracy. ADASECANT overcomes these challenges by employing a novel adaptive secant method. Experimental results demonstrate the superior performance of ADASECANT in various large-scale learning tasks. This method shows promise in improving the efficiency and accuracy of stochastic gradient algorithms.",
        "title": "ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient"
    },
    {
        "abs": "When a three-dimensional object moves relative to an observer, a change occurs in the observer's visual representation. This abstract explores the transformation properties of learned visual representations, examining how they change when an object undergoes various transformations such as translation, rotation, and scaling. Understanding these transformation properties is crucial for tasks such as object recognition, pose estimation, and activity recognition in computer vision. The abstract provides insights into the effects of different transformations on learned visual representations, highlighting their importance in developing robust and adaptable visual recognition systems.",
        "title": "Transformation Properties of Learned Visual Representations"
    },
    {
        "abs": "Efficient Maximum Inner Product Search (MIPS) is a vital task with broad applicability. In this paper, we propose a clustering approach to enhance the efficiency of approximate MIPS. Our method demonstrates significant gains in search speed without compromising the accuracy of the results. By leveraging the power of clustering, we pave the way for more efficient MIPS techniques, facilitating its implementation in various domains and scenarios.",
        "title": "Clustering is Efficient for Approximate Maximum Inner Product Search"
    },
    {
        "abs": "The variational autoencoder (VAE; Kingma, Welling, 2014) is a recently proposed generative model that combines a stochastic encoder-decoder architecture with variational inference. However, VAEs suffer from the problem of blurry reconstructions and do not always effectively capture the underlying data distribution. In this paper, we introduce importance weighted autoencoders (IWAEs) as an extension to VAEs. IWAEs provide a more accurate approximation to the true posterior distribution by incorporating importance weights, resulting in improved quality of generations and more accurate reconstructions. We demonstrate the effectiveness of IWAEs through experiments on various datasets and provide insights into the advantages and limitations of this approach.",
        "title": "Importance Weighted Autoencoders"
    },
    {
        "abs": "This work investigates the effects of using reduced precision data in Convolutional Neural Networks (CNNs). Specifically, it focuses on the strategies for utilizing bounded memory in deep neural networks. The research aims to understand the impact of reduced-precision data on the performance and efficiency of CNNs.",
        "title": "Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets"
    },
    {
        "abs": "Graph-based semi-supervised algorithms rely heavily on the underlying graph structure of instances for their efficiency. To enhance the efficiency of these algorithms, we propose a metric learning approach for graph-based label propagation. This approach aims to improve the accuracy of label propagation by learning a distance metric that better captures the relationships between instances in the graph. Experimental results demonstrate the effectiveness of our approach in achieving higher labeling accuracy compared to traditional graph-based methods, highlighting the potential for improved performance in various real-world applications.",
        "title": "Metric learning approach for graph-based label propagation"
    },
    {
        "abs": "This abstract proposes that hypernymy, textual entailment, and image captioning can be viewed as special cases of a single concept called order-embeddings of images and language. It suggests that these tasks can be approached using a unified framework, potentially leading to more efficient and effective models for understanding the relationship between images and language.",
        "title": "Order-Embeddings of Images and Language"
    },
    {
        "abs": "We propose local distributional smoothness (LDS) as a novel concept of smoothness for statistical models. This paper introduces the idea of distributional smoothing with virtual adversarial training, which aims to enhance the smoothness of probability distributions at local regions. Our approach utilizes an adversarial attack to perturb the model's input and guides it towards a more robust and smooth distribution. Experimental results demonstrate that our method effectively improves the model's performance and robustness across various tasks and datasets.",
        "title": "Distributional Smoothing with Virtual Adversarial Training"
    },
    {
        "abs": "In recent years, the abundance of large labeled datasets has facilitated significant advancements in Convolutional Network models, resulting in remarkable recognition capabilities. However, these datasets are not immune to label noise, which can adversely impact model performance. This paper explores the challenges associated with training Convolutional Networks using noisy labels and proposes effective strategies to mitigate their negative effects, enabling improved recognition accuracy. The study provides insights into the importance of addressing label noise during the training process, enhancing the robustness and reliability of Convolutional Network models in real-world applications.",
        "title": "Training Convolutional Networks with Noisy Labels"
    },
    {
        "abs": "In this paper, we propose novel guaranteed approaches for training feedforward neural networks with sparse connectivity. By leveraging provable methods, we aim to enhance training efficiency and alleviate the computational burden associated with dense networks. Our findings present promising solutions for improving the training process of neural networks while maintaining sparsity in connectivity.",
        "title": "Provable Methods for Training Neural Networks with Sparse Connectivity"
    },
    {
        "abs": "This study introduces the concept of Entity-Augmented Distributional Semantics, a method for automatically identifying discourse relations in coherent texts. Discourse relations play a significant role in binding smaller linguistic elements together to create meaningful and coherent texts. However, the process of identifying these relations automatically can be challenging. The proposed approach leverages distributional semantics and incorporates entity information to enhance the accuracy of discourse relation identification. The results of the study demonstrate the effectiveness of this method in automatically identifying discourse relations in texts.",
        "title": "Entity-Augmented Distributional Semantics for Discourse Relations"
    },
    {
        "abs": "In this work, we propose a new method to integrate two recent lines of work: predicting relations and factorizing relations, in order to induce semantic representation from text. Our approach enables jointly predicting and factorizing relations, resulting in an enhanced understanding of the underlying semantic structure in textual data. We demonstrate the effectiveness of our method through experiments and highlight its potential implications for various natural language processing tasks.",
        "title": "Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations"
    },
    {
        "abs": "This paper explores the significance of a metric in machine learning problems like classification. It introduces the idea of algorithmic robustness through $(\u03b5, \u03b3, \u03c4)$-good similarity functions, which aim to enhance the quality and efficiency of learning algorithms. By utilizing these functions, the paper aims to demonstrate improved performance and accuracy in various machine learning tasks.",
        "title": "Algorithmic Robustness for Learning via $(\u03b5, \u03b3, \u03c4)$-Good Similarity Functions"
    },
    {
        "abs": "In this paper, we present the multiplicative recurrent neural network as a versatile model for capturing compositional meaning in language. By leveraging the multiplication operation, our proposed model effectively encodes the interaction among multiple input elements, allowing for flexible representation of complex syntactic and semantic structures. Through experimental evaluations on various language tasks, we demonstrate the effectiveness and superior performance of our multiplicative recurrent neural network in capturing compositionality. Our findings suggest that this model holds promising potential for advancing natural language understanding and generation systems.",
        "title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks"
    },
    {
        "abs": "This study delves into the complexities of searching for minima in real-valued non-convex functions within high-dimensional spaces. The abstract will explore various explorations and approaches undertaken to efficiently tackle this challenging task.",
        "title": "Explorations on high dimensional landscapes"
    },
    {
        "abs": "In this study, we introduce a novel statistical model for analyzing photographic images. Our model focuses on capturing the local responses of various regions within the images. We find that natural images exhibit low-dimensionality locally, meaning that the information content within nearby regions is highly correlated and can be efficiently represented. By proposing this new model, we provide insights into the underlying structure of natural images and demonstrate the potential for developing efficient image analysis techniques based on local low-dimensionality.",
        "title": "The local low-dimensionality of natural images"
    },
    {
        "abs": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same complex and intricate architectures. However, recently, there has been a growing interest in developing simpler and more efficient approaches. This paper introduces the All Convolutional Net (ACN), which aims to achieve superior performance while maintaining simplicity. The ACN eliminates the need for fully connected layers and replaces them with global average pooling, resulting in a reduction in both computational complexity and overfitting. Experimental results demonstrate that the ACN can match or even outperform traditional CNN architectures, suggesting that striving for simplicity can lead to remarkable advancements in the field of object recognition.",
        "title": "Striving for Simplicity: The All Convolutional Net"
    },
    {
        "abs": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. However, these activation functions can greatly impact the performance and efficiency of deep neural networks. This paper aims to explore the concept of learning activation functions to improve the performance of deep neural networks. By allowing the activation functions to adapt and optimize themselves, we can potentially enhance the network's ability to learn complex patterns and make more accurate predictions. The research in this paper presents a novel approach to optimizing activation functions and demonstrates its effectiveness in improving the overall performance of deep neural networks.",
        "title": "Learning Activation Functions to Improve Deep Neural Networks"
    },
    {
        "abs": "This paper introduces a greedy parser that utilizes neural networks and leverages a novel compositional approach for word composition. The proposed approach enhances the efficiency and accuracy of parsing tasks by incorporating joint RNN-based techniques.",
        "title": "Joint RNN-Based Greedy Parsing and Word Composition"
    },
    {
        "abs": "This study demonstrates that suitable lateral connections between encoder and decoder enable higher layers of a denoising autoencoder to learn invariant representations of natural images.",
        "title": "Denoising autoencoder with modulated lateral connections learns invariant representations of natural images"
    },
    {
        "abs": "In this study, we propose a novel method to visually analyze and enhance the invariances within learned representations. Our approach aims to uncover the geodesics of these representations, allowing for a more comprehensive understanding of their underlying structure. By leveraging this method, we are able to refine the invariances and improve the overall robustness and generalization capabilities of the learned representations. The results demonstrate the effectiveness of our approach in optimizing the learned representations and advancing the field of representation learning.",
        "title": "Geodesics of learned representations"
    },
    {
        "abs": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms. In this study, we aim to leverage genomic data to predict clinical outcomes in cancer patients. By learning and analyzing genomic representations, we aim to contribute to personalized medicine by identifying potential therapies and prognostic markers. Through the application of machine learning algorithms, we hope to uncover novel associations between genetic variants and clinical outcomes, ultimately improving patient care and treatment strategies.",
        "title": "Learning Genomic Representations to Predict Clinical Outcomes in Cancer"
    },
    {
        "abs": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment or rely on separate networks. In this paper, we propose a novel differentiable transition between additive and multiplicative neurons. Our approach enables flexible and dynamic assignments, allowing neural networks to seamlessly switch between additive and multiplicative operations based on the needs of the task at hand. Experimental results demonstrate the effectiveness and versatility of our proposed method across various domains and tasks. Overall, our work provides a valuable contribution towards enhancing the capabilities and adaptability of neural networks.",
        "title": "A Differentiable Transition Between Additive and Multiplicative Neurons"
    },
    {
        "abs": "This abstract discusses the challenge of improper scaling between layers in training deep neural networks. It highlights the difficulty caused by this issue and emphasizes the importance of scale normalization techniques in mitigating these problems.",
        "title": "Scale Normalization"
    },
    {
        "abs": "In this paper, we extend Stochastic Gradient Variational Bayes (SGVB) to perform posterior inference for the weights of Stick-Breaking. We propose a new method called Stick-Breaking Variational Autoencoders, which provides an efficient and accurate approach for learning the latent structures in Stick-Breaking processes. The proposed method offers improved scalability and flexibility compared to existing approaches, enabling more effective posterior inference in a variety of applications.",
        "title": "Stick-Breaking Variational Autoencoders"
    },
    {
        "abs": "Unsupervised learning on imbalanced data is challenging because the current models struggle to accurately capture the inherent structure within the minority class. This limitation hinders the effectiveness of anomaly detection and minority class identification tasks. In this study, we propose the Structure Consolidation Latent Variable Model (SCLVM) as a solution. By integrating a latent variable framework with a structure consolidation mechanism, SCLVM effectively leverages the intrinsic features of imbalanced data while ensuring the representation of the minority class is preserved. Experimental results demonstrate that SCLVM outperforms existing methods in accurately classifying and detecting minority instances in imbalanced datasets.",
        "title": "Unsupervised Learning with Imbalanced Data via Structure Consolidation Latent Variable Model"
    },
    {
        "abs": "Generative Adversarial Networks (GANs) have emerged as powerful deep generative models in recent years. These models rely on a two-player framework, where a generator network competes against a discriminator network. However, viewing GANs from a density ratio estimation perspective provides a novel understanding of their inner workings. This perspective allows for a deeper analysis of the probabilistic aspects of GANs, leading to improved model performance and better insights into their limitations. In this paper, we explore GANs from a density ratio estimation perspective and provide significant contributions towards the advancement of this field.",
        "title": "Generative Adversarial Nets from a Density Ratio Estimation Perspective"
    },
    {
        "abs": "This paper showcases how natural language processing (NLP) techniques can be directly applied to classification tasks. It explores various methods and their effectiveness in classifying textual data. By utilizing NLP, researchers and practitioners can improve classification accuracy and optimize decision-making processes. The study emphasizes the importance of understanding the underlying principles of NLP for efficient implementation. Overall, this paper provides valuable insights into leveraging NLP methods for classification and highlights their potential impact on enhancing data analysis and decision-making in various domains.",
        "title": "Learning to SMILE(S)"
    },
    {
        "abs": "This abstract presents a neural network architecture and a learning algorithm that are designed to generate factorized symbolic representations, facilitating the understanding of visual concepts.",
        "title": "Understanding Visual Concepts with Continuation Learning"
    },
    {
        "abs": "In this research, we examine the eigenvalues of the Hessian matrix for the loss function in deep learning models. We investigate the changes in eigenvalues before and after certain operations or modifications in order to gain insights into the singularity and beyond of the Hessian. The study sheds light on the behavior and properties of the Hessian matrix, providing valuable information for optimizing deep learning models.",
        "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond"
    },
    {
        "abs": "In this study, we introduce a parametric nonlinear transformation that effectively Gaussianizes data derived from natural images. Our proposed transformation, known as Generalized Normalization Transformation (GNT), demonstrates exceptional applicability in density modeling of images. By applying GNT, we are able to effectively capture the underlying structure and distribute the data in a Gaussian-like fashion. This novel approach significantly contributes to the field of image analysis and paves the way for improved modeling and understanding of image data.",
        "title": "Density Modeling of Images using a Generalized Normalization Transformation"
    },
    {
        "abs": "Approximate variational inference has proven to be a powerful tool for modeling complex unknown probability distributions. In this study, we propose its application in on-line anomaly detection for high-dimensional time series data. By leveraging the advantages of variational inference, our method offers an efficient and scalable solution to detect anomalies in time series data, even in high-dimensional settings. We provide empirical evidence of its effectiveness through experiments on real-world datasets. Overall, our approach shows promise in facilitating the detection of anomalies in high-dimensional time series data, with potential applications in various domains.",
        "title": "Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series"
    },
    {
        "abs": "This concise abstract outlines the development of a general problem setting for training and testing the abilities of information-seeking agents.",
        "title": "Towards Information-Seeking Agents"
    },
    {
        "abs": "In this study, we propose a novel approach to enhance neural network language models by introducing a continuous cache component. Our extension aims to adapt the model's predictions to the given context by incorporating a continuously updated cache mechanism. Through extensive experiments, we demonstrate the effectiveness of our approach in improving the accuracy and coherence of language generation tasks. This research contributes to the advancement of neural language models by providing a more adaptive and context-aware mechanism for predictions.",
        "title": "Improving Neural Language Models with a Continuous Cache"
    },
    {
        "abs": "In this study, we propose a model capable of generating images from captions, taking inspiration from the recent advancements in generative models. Our model incorporates attention mechanisms to enhance the image generation process. Through experiments and evaluations, we showcase the effectiveness of our approach in generating visually coherent images that accurately correspond to the provided captions. This work contributes to the continued progress in the field of generative models and expands the capabilities of image synthesis from textual input.",
        "title": "Generating Images from Captions with Attention"
    },
    {
        "abs": "In this paper, we propose a framework for training multiple neural networks simultaneously in a deep multi-task learning setting. Our framework incorporates a trace norm regularization technique to encourage the shared parameters from all models to be low-rank and therefore generalize better across multiple tasks. By jointly optimizing the parameters of multiple models, our approach improves the overall performance of all tasks. We validate the effectiveness of our framework through extensive experiments on various multi-task learning datasets, demonstrating its superiority over existing methods.",
        "title": "Trace Norm Regularised Deep Multi-Task Learning"
    },
    {
        "abs": "This paper presents a stable and sample-efficient actor-critic deep reinforcement learning agent with experience replay. The agent utilizes experience replay, a technique that enhances learning by reusing past experiences, to improve sample efficiency and stabilize training. The actor-critic architecture allows for separate learning of value functions and policies, enabling the agent to make informed decisions based on the captured knowledge. By combining these techniques, the proposed agent achieves stability and efficiency in learning, making it a promising approach for addressing complex reinforcement learning problems.",
        "title": "Sample Efficient Actor-Critic with Experience Replay"
    },
    {
        "abs": "We present a novel framework for generating pop music called Song From PI. Our model, a hierarchical Recurrent Neural Network, incorporates musical knowledge and structure to generate high-quality pop music compositions. Through extensive training on a large dataset of pop songs, our model learns patterns of music and can generate new compositions that are musically plausible and stylistically similar to popular pop songs. The proposed model shows promising results in terms of melody, harmony, and rhythm, demonstrating its potential to aid in the creative process of songwriters and music producers.",
        "title": "Song From PI: A Musically Plausible Network for Pop Music Generation"
    },
    {
        "abs": "Manymachine learning classifiers are vulnerable to adversarial perturbations, which modify inputs to mislead the classifiers' predictions. This paper focuses on early methods for detecting adversarial images. The abstract provides an overview of the vulnerability of machine learning classifiers to such perturbations and highlights the importance of developing techniques to identify and prevent adversarial attacks.",
        "title": "Early Methods for Detecting Adversarial Images"
    },
    {
        "abs": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank filters. This approach aims to improve image classification performance while reducing computational complexity. Our method introduces a novel way of training CNNs by leveraging the low-rank property of filters, thereby minimizing the number of parameters and operations required during the training and inference stages. Experimental results demonstrate that our approach achieves comparable classification accuracy to traditional CNNs while significantly reducing computational requirements. This approach holds promise for developing efficient image classification systems with reduced computational burden.",
        "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification"
    },
    {
        "abs": "LSUV initialization is a simple and effective method for weight initialization in deep neural network learning. By considering the variance of each layer's output, LSUV ensures that the network's units are initialized to a reasonable scale for optimal learning. This abstract highlights the importance of a good initialization technique and its impact on deep network performance.",
        "title": "All you need is a good init"
    },
    {
        "abs": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in dependency parsing. It introduces the concept of deep biaffine attention, a novel approach that combines multiple layers of attention in order to enhance parsing accuracy. The results demonstrate significant improvements over previous methods, highlighting the effectiveness of deep biaffine attention in neural dependency parsing.",
        "title": "Deep Biaffine Attention for Neural Dependency Parsing"
    },
    {
        "abs": "Accurate representational learning of both the explicit and implicit relationships within data is critical to the development of Dynamic Adaptive Network Intelligence. This abstract emphasizes the significance of precise representation learning in order to enhance the understanding of explicit and implicit correlations within data for the advancement of dynamic adaptive network intelligence.",
        "title": "Dynamic Adaptive Network Intelligence"
    },
    {
        "abs": "Spherical data is found in many applications, necessitating efficient computational models. In this study, we propose DeepSphere, an equivariant graph-based spherical Convolutional Neural Network (CNN). By discretizing the sphere as a graph, DeepSphere enables effective analysis and processing of spherical data. The proposed methodology addresses the challenges posed by rotation and permutation equivariance, ensuring accurate and efficient representation learning on spherical datasets. We demonstrate the viability and performance of DeepSphere through extensive experiments on various tasks, showcasing its potential for advancing research in spherical data analysis.",
        "title": "DeepSphere: towards an equivariant graph-based spherical CNN"
    },
    {
        "abs": "Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision but their high computational complexity limits their widespread usage, particularly on mobile devices. This paper proposes a hardware-oriented approximation approach for implementing CNNs, aiming to reduce their computational requirements while maintaining acceptable accuracy levels. By leveraging hardware optimizations, this approach could enable the efficient deployment of CNNs on mobile devices and pave the way for their broader utilization in real-world applications.",
        "title": "Hardware-oriented Approximation of Convolutional Neural Networks"
    },
    {
        "abs": "This paper presents a learned representation for artistic style, which leverages the diverse range of painting styles as a rich visual vocabulary. By analyzing and synthesizing various artistic styles, the proposed approach enables the construction of a comprehensive representation that captures the intricacies of different artistic expressions. The resulting model provides a powerful tool for understanding and replicating artistic style, offering potential applications in computer graphics, image editing, and virtual environments.",
        "title": "A Learned Representation For Artistic Style"
    },
    {
        "abs": "Sum-Product Networks (SPNs) are a class of hierarchical graphical models that offer a balance between expressiveness and tractability. Learning SPNs has proven to be challenging due to the complexity and scalability issues involved. In this paper, we propose a minimalistic approach to SPN learning that tackles these challenges and aims to make SPNs applicable to real-world scenarios. We present our methodology and demonstrate its effectiveness through experiments on several real applications. Our results show that our approach improves the scalability and efficiency of SPN learning while maintaining its expressive power. This work contributes to the advancement of SPN learning techniques and opens up opportunities for using SPNs in practical domains.",
        "title": "A Minimalistic Approach to Sum-Product Network Learning for Real Applications"
    },
    {
        "abs": "This study presents SqueezeNet, a deep neural network model that achieves comparable accuracy to AlexNet while significantly reducing the number of parameters and model size. Recent research in deep neural networks has primarily focused on improving accuracy. SqueezeNet addresses this challenge by proposing a novel architecture that achieves AlexNet-level accuracy using 50 times fewer parameters and a model size of less than 0.5MB.",
        "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size"
    },
    {
        "abs": "In this paper, we address the problem of question answering with multiple facts by proposing Query-Reduction Networks. These networks aim to improve the efficiency and accuracy of question answering systems by reducing the number of irrelevant or redundant queries. We conduct extensive experiments to demonstrate the effectiveness of our approach. Overall, our research contributes to advancing the field of question answering by enabling more efficient and precise reasoning over multiple facts.",
        "title": "Query-Reduction Networks for Question Answering"
    },
    {
        "abs": "We propose a language-agnostic approach to automatically generate sets of semantically similar clusters of entities. This approach enables the evaluation of distributed representations in a multilingual setting. Our method provides a systematic way to assess the quality and coherence of these representations across different languages. By automatically generating clusters, we eliminate the need for human intervention, making the evaluation process more efficient and scalable. The results of our study demonstrate the effectiveness of this approach in capturing semantic relationships accurately in multiple languages.",
        "title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations"
    },
    {
        "abs": "Recurrent neural networks (RNNs) are extensively employed for predicting temporal data due to their deep feedforward structure. However, despite their effectiveness, RNNs often struggle to handle surprising or unexpected events in the data. This paper proposes a novel approach called surprisal-driven feedback, which aims to enhance RNNs' ability to adapt to such unforeseen events. The authors demonstrate the efficacy of this approach through experimental results on various datasets.",
        "title": "Surprisal-Driven Feedback in Recurrent Networks"
    },
    {
        "abs": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are often prone to mode collapse and lack diversity in generated samples. In this paper, we propose Mode Regularized Generative Adversarial Networks (MR-GANs) to address these limitations. MR-GANs incorporate a penalty term into the GAN objective function, encouraging the generator to generate samples that cover multiple modes of the target distribution. Experimental results demonstrate that MR-GANs can effectively prevent mode collapse and generate more diverse and realistic samples compared to traditional GANs.",
        "title": "Mode Regularized Generative Adversarial Networks"
    },
    {
        "abs": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world applications. In this paper, we propose EPOpt, a method that addresses these challenges by utilizing model ensembles. EPOpt combines multiple models to learn robust neural network policies that can generalize well and ensure safety in diverse contexts. Our experimental results demonstrate the effectiveness of EPOpt in managing sample complexity and achieving safe policy learning.",
        "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles"
    },
    {
        "abs": "Abstract:\nDivnet is a flexible technique introduced for learning networks with diverse neurons, using a method called neural network compression. It models neuronal diversity by employing determinantal point processes. This approach allows for efficient utilization of network resources while maintaining high performance levels. By leveraging the concept of diversity, Divnet offers a promising solution for enhancing neural network compression and enabling more efficient computing systems.",
        "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes"
    },
    {
        "abs": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they operate. In this paper, we propose a metric learning approach for graph-based label propagation. Our method focuses on improving the accuracy and efficiency of graph-based algorithms by learning a distance metric that captures the underlying structure of the data. We demonstrate the effectiveness of our approach through extensive experiments on various datasets, showcasing its ability to enhance the performance of label propagation methods.",
        "title": "Metric learning approach for graph-based label propagation"
    },
    {
        "abs": "The challenge of preventing overfitting in training deep neural networks remains a major concern. Various techniques have been proposed to address this issue, including regularization methods, early stopping, and dropout. However, these approaches often rely on the assumption that the representations learned by the network are uncorrelated. In this paper, we propose a novel approach to reducing overfitting by decorrelating representations in deep networks. Our method introduces a decorrelation loss term during training, encouraging the network to learn more diverse and informative representations. Experimental results demonstrate that our approach effectively mitigates overfitting, leading to improved generalization performance on various deep learning tasks.",
        "title": "Reducing Overfitting in Deep Networks by Decorrelating Representations"
    },
    {
        "abs": "In the field of deep neural networks, training commonly relies on stochastic non-convex optimization procedures. These procedures are driven by the selection of batches of data. This paper proposes an online batch selection method to expedite the training process of neural networks. The proposed method aims to minimize computational costs while maximizing training efficiency. Experimental results demonstrate the effectiveness of the approach in accelerating neural network training and achieving comparable performance to traditional stochastic optimization methods. Overall, our findings highlight the potential of online batch selection for faster and more efficient training of neural networks.",
        "title": "Online Batch Selection for Faster Training of Neural Networks"
    },
    {
        "abs": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on graph convolutional networks. By utilizing both labeled and unlabeled data, our method improves the classification accuracy of graph-structured data using a neural network framework. Our experimental results demonstrate the effectiveness and scalability of our proposed approach in various tasks involving graph-structured data.",
        "title": "Semi-Supervised Classification with Graph Convolutional Networks"
    },
    {
        "abs": "We introduce the Energy-based Generative Adversarial Network (EBGAN) model, which interprets the discriminator as an energy function. This unique approach allows the model to learn the energy landscape of the data distribution, enabling better generation of realistic samples. The EBGAN model shows promise in improving the training stability and generating higher-quality samples compared to traditional GANs.",
        "title": "Energy-based Generative Adversarial Network"
    },
    {
        "abs": "Recent research in the deep learning field has witnessed the emergence of a multitude of novel architectures. This paper aims to explore and discuss the design patterns used in deep convolutional neural networks (CNNs). By analyzing recent developments, this study identifies common design patterns and highlights their importance in improving the performance, efficiency, and interpretability of CNN models. The abstract concludes by emphasizing the significance of understanding these design patterns for researchers, practitioners, and developers working in the field of deep learning.",
        "title": "Deep Convolutional Neural Network Design Patterns"
    },
    {
        "abs": "Machine comprehension (MC) refers to the task of answering a query about a given context paragraph. This task is challenging as it requires modeling complex interactions. In this study, we propose a Bidirectional Attention Flow model that successfully addresses this issue by incorporating bidirectional attention mechanisms. The model achieves state-of-the-art results on various MC datasets, demonstrating its effectiveness in understanding and processing complex textual information.",
        "title": "Bidirectional Attention Flow for Machine Comprehension"
    },
    {
        "abs": "Despite progress in model learning and performing posterior inference, it remains a common challenge for Helmholtz Machines. This paper introduces Joint Stochastic Approximation learning technique for Helmholtz Machines. This technique combines the advantages of stochastic approximation and joint optimization to improve learning efficiency and convergence. Experimental results demonstrate the effectiveness and superiority of the proposed method in achieving accurate modeling and efficient posterior inference in Helmholtz Machines.",
        "title": "Joint Stochastic Approximation learning of Helmholtz Machines"
    },
    {
        "abs": "This paper introduces a novel approach called \"On-the-fly Network Pruning for Object Detection,\" which aims to optimize object detection with deep neural networks. Conventionally, object detection involves processing thousands of candidate objects, resulting in computational inefficiency. To address this issue, our method dynamically prunes the network during inference, reducing the computational load while maintaining high accuracy. Experimental results demonstrate that our approach achieves superior performance compared to existing methods, offering faster and more efficient object detection capabilities.",
        "title": "On-the-fly Network Pruning for Object Detection"
    },
    {
        "abs": "In the vast domain of machine learning, improving the performance of solutions is of utmost importance. One effective approach is modeling interactions between features, which has shown promising results across various domains. This abstract discusses the significance of incorporating feature interactions in machine learning models and highlights its potential to enhance solution outcomes.",
        "title": "Exponential Machines"
    },
    {
        "abs": "We introduce Deep Variational Bayes Filters (DVBF), a novel method for unsupervised learning and identification of state space models directly from raw data.",
        "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data"
    },
    {
        "abs": "Traditional goal-oriented dialogue systems used in domain-specific applications require a significant amount of manual domain-specific handcrafting, which limits their scalability and adaptability. In order to overcome this challenge, this paper focuses on learning end-to-end goal-oriented dialogue systems, which aim to reduce the need for manual domain-specific engineering by leveraging deep learning techniques.",
        "title": "Learning End-to-End Goal-Oriented Dialog"
    },
    {
        "abs": "Adversarial training offers a regularization technique for improving supervised learning algorithms by creating virtual adversaries. Specifically, virtual adversarial training focuses on the regularization of semi-supervised text classification models.",
        "title": "Adversarial Training Methods for Semi-Supervised Text Classification"
    },
    {
        "abs": "Unsupervised learning of probabilistic models poses a significant challenge in the field of machine learning. One such approach to tackle this challenge is density estimation using Real NVP (Non-Volume Preserving) methods. Real NVP provides a framework to estimate the probability density function of an input dataset by transforming it using invertible neural networks. This abstract explores the utilization and advantages of Real NVP in density estimation, highlighting its potential applications in various domains such as anomaly detection, generative modeling, and data synthesis.",
        "title": "Density estimation using Real NVP"
    },
    {
        "abs": "This paper is focused on studying the view-manifold structure in the feature spaces implied by Convolutional Neural Networks (CNNs). We aim to investigate and explore how CNNs achieve view invariance, a crucial property for robust object recognition. By examining the layers of CNNs, we aim to uncover the underlying mechanisms that enable these networks to extract invariant features across different viewpoints. This research provides valuable insights into the inner workings of CNNs and contributes to our understanding of their effectiveness in achieving view invariance.",
        "title": "Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance"
    },
    {
        "abs": "In this paper, we propose the use of Hadamard Product for low-rank bilinear pooling in order to enhance the representation capabilities of bilinear models compared to linear models. Bilinear models have shown promising results in various applications, and our approach aims to further improve their performance. Our experiments demonstrate the effectiveness of the Hadamard Product for low-rank bilinear pooling in generating richer representations.",
        "title": "Hadamard Product for Low-rank Bilinear Pooling"
    },
    {
        "abs": "In this paper, we propose a new interpretation of importance-weighted autoencoders, challenging the standard understanding. Traditionally, these autoencoders aim to maximize a tighter lower bound on some objective function. Our research aims to redefine the concept of importance-weighted autoencoders and presents a novel perspective on their significance.",
        "title": "Reinterpreting Importance-Weighted Autoencoders"
    },
    {
        "abs": "In this paper, we propose a new generalization bound for feedforward neural networks, which is formulated in terms of the product of the spectral normalization of weight matrices and margin bounds. We present a PAC-Bayesian approach to derive these margin bounds for neural networks, providing a theoretical guarantee for their generalization performance. Our approach offers a concise yet robust framework for understanding and analyzing the generalization properties of neural networks.",
        "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"
    },
    {
        "abs": "In this paper, we propose to equip Generative Adversarial Networks (GANs) with the ability to produce high-quality samples by calibrating the energy-based framework. By incorporating a regularization term and optimizing the discriminator with a hybrid algorithm, we demonstrate the improved performance of our calibrated GAN model on various benchmark datasets. The experimental results show that our approach achieves superior image generation quality, surpassing state-of-the-art GAN models. Our contributions provide a promising direction for enhancing the training stability and output quality of GANs.",
        "title": "Calibrating Energy-based Generative Adversarial Networks"
    },
    {
        "abs": "In this work, we propose a novel approach for outlier detection based on ensembles of neural networks obtained by variational Bayesian methods. Our method combines the benefits of both neural network ensembles and variational Bayesian techniques to efficiently identify outliers in datasets. Through extensive experiments on various datasets, we demonstrate the superior performance of our approach compared to state-of-the-art methods. Our efficient variational Bayesian neural network ensembles offer a promising solution for accurate and reliable outlier detection in a wide range of applications.",
        "title": "Efficient variational Bayesian neural network ensembles for outlier detection"
    },
    {
        "abs": "This article discusses two effective strategies for reducing the number of parameters and speeding up the training process in LSTM networks. By implementing specific factorization tricks, the authors demonstrate how these techniques can optimize the performance of LSTM models without compromising their accuracy. This abstract provides a brief overview of the main concepts and goals covered in the article.",
        "title": "Factorization tricks for LSTM networks"
    },
    {
        "abs": "This article presents new observations and discussions on unexplored phenomena encountered during the training of residual networks. The focus is on exploring loss function topology with the use of cyclical learning rates.",
        "title": "Exploring loss function topology with cyclical learning rates"
    },
    {
        "abs": "Machine learning models are frequently utilized at test-time, but they often face constraints and trade-offs that were not present during training. This paper addresses the challenge of changing model behavior at test-time using reinforcement learning. By leveraging this approach, models can adapt their behaviors based on the specific constraints and trade-offs encountered during testing, thereby enhancing their performance and flexibility in real-world scenarios.",
        "title": "Changing Model Behavior at Test-Time Using Reinforcement Learning"
    },
    {
        "abs": "This paper investigates the existence of adversarial attacks on deep learning architectures by providing examples. Various deep learning architectures are analyzed, highlighting their vulnerability to adversarial attacks. The findings of this study contribute to a deeper understanding of the potential risks associated with deep policies and the need for developing robust defense mechanisms.",
        "title": "Delving into adversarial attacks on deep policies"
    },
    {
        "abs": "This paper introduces Variational Continual Learning (VCL), a straightforward yet versatile framework for addressing the challenge of continual learning. VCL aims to alleviate catastrophic forgetting by leveraging probabilistic generative models and variational inference techniques. By incorporating a Bayesian perspective, VCL demonstrates promising results in mitigating forgetting while maintaining computational efficiency. The framework provides a foundation for further research and development in developing more robust and flexible continual learning algorithms.",
        "title": "Variational Continual Learning"
    },
    {
        "abs": "In this study, we propose a nonparametric approach for automatically determining the optimal size of a neural network for a given task without prior knowledge. Traditional methods often rely on heuristics or assumptions about the underlying data distribution, which may limit their effectiveness in certain scenarios. Our nonparametric neural network framework leverages a data-driven approach, allowing the network to adapt its complexity based on the available training samples. By eliminating the need for manual selection of network size, our method offers a more efficient and accurate solution for neural network configuration. Experimental results on various tasks illustrate the effectiveness of our approach in achieving superior performance compared to existing methods.",
        "title": "Nonparametric Neural Networks"
    },
    {
        "abs": "Natural Language Inference (NLI) is a task that involves an agent determining the logical relationship between two given statements. In this paper, we explore NLI over the interaction space, where the agent must make inferences based on conversations or interactions between individuals. We address the challenge of representing and understanding the dynamics of human interactions, and propose a framework that leverages contextual information to improve NLI performance. Through experiments, we demonstrate the effectiveness of our approach in capturing the nuanced reasoning required for accurate inference in interactive scenarios. Our findings pave the way for further advancements in NLI research and its applications in natural language understanding.",
        "title": "Natural Language Inference over Interaction Space"
    },
    {
        "abs": "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the susceptibility of these models to adversarial attacks. Adversarial examples, carefully constructed inputs designed to deceive neural networks, can compromise the reliability and integrity of these systems. In this paper, we propose a novel approach called Provably Minimally-Distorted Adversarial Examples (PMAD) to address this issue. PMAD provides a method to generate adversarial examples that achieve minimal distortion while still being effective in fooling the neural network. We demonstrate the effectiveness of our approach through extensive experiments and show that PMAD can significantly enhance the robustness of neural networks in safety-critical applications.",
        "title": "Provably Minimally-Distorted Adversarial Examples"
    },
    {
        "abs": "This paper presents a novel approach called Stick-Breaking Variational Autoencoders (SB-VAE) for performing posterior inference of weights in stochastic gradient variational Bayes. The proposed method extends the existing framework and enhances its capabilities, enabling more efficient and accurate estimation of the weights. The SB-VAE method leverages the stick-breaking process to model the distribution of the weights, leading to improved performance in various applications. Experimental results demonstrate the effectiveness of SB-VAE in achieving better posterior inference for weight estimation compared to traditional methods.",
        "title": "Stick-Breaking Variational Autoencoders"
    },
    {
        "abs": "We propose a framework for training multiple neural networks simultaneously in the context of deep multi-task learning. Our framework incorporates the trace norm regularization technique to encourage shared features among the models. By jointly optimizing the parameters from all the models, we aim to improve performance on multiple tasks while benefiting from shared knowledge. The results demonstrate the effectiveness of our approach in achieving better overall performance compared to training individual models separately.",
        "title": "Trace Norm Regularised Deep Multi-Task Learning"
    },
    {
        "abs": "This paper presents a stable actor-critic deep reinforcement learning agent with experience replay that is sample efficient. By employing experience replay, the agent effectively learns from past experiences, reducing the amount of training samples required. The actor-critic architecture enables the agent to simultaneously learn a policy and value function, enhancing its decision-making capabilities. Experimental results demonstrate the sample efficiency and stability of the proposed approach compared to traditional reinforcement learning methods. Overall, this work contributes to the advancement of efficient and stable deep reinforcement learning algorithms.",
        "title": "Sample Efficient Actor-Critic with Experience Replay"
    },
    {
        "abs": "Many machine learning classifiers are vulnerable to adversarial perturbations, which are modifications made to an input to mislead the classifier's prediction. This vulnerability poses significant challenges in the security and reliability of machine learning systems. This paper explores early methods for detecting adversarial images, which aim to identify these perturbations and improve the resilience of classifiers against adversarial attacks. The research focuses on investigating the effectiveness of various detection techniques and provides insights into their limitations and potential solutions.",
        "title": "Early Methods for Detecting Adversarial Images"
    },
    {
        "abs": "The abstract proposes a principled method for kernel learning using a Fourier-analytic characterization of not-so-random features. This method aims to improve the randomness of feature selection in kernel learning algorithms. The authors provide theoretical insights and experimental results demonstrating the effectiveness of this approach.",
        "title": "Not-So-Random Features"
    },
    {
        "abs": "This study delves into the field of fast reading comprehension using Convolutional Neural Networks (ConvNets). The current state-of-the-art models employed for deep reading comprehension primarily rely on Recurrent Neural Networks (RNNs), which operate in a sequential manner. The aim of this research is to explore how ConvNets can be leveraged to enhance the speed and accuracy of reading comprehension tasks.",
        "title": "Fast Reading Comprehension with ConvNets"
    },
    {
        "abs": "This report aims to investigate the reproducibility of the regularization techniques employed in Wasserstein GANs. By replicating the results of the original study, we assess the consistency of the regularization methods and their impact on the overall performance of the Wasserstein GAN model.",
        "title": "On reproduction of On the regularization of Wasserstein GANs"
    },
    {
        "abs": "Variational Autoencoders (VAEs) were originally introduced as probabilistic generative models in 2014. In recent years, there has been increased interest in hierarchical VAEs, which allow for the learning of more complex latent representations. This paper explores the concept of trading information between different levels of latents in hierarchical VAEs. By improving the information exchange process, the overall performance and interpretability of hierarchical VAEs can be enhanced. The experimental results demonstrate the effectiveness of the proposed approach in improving the quality of generated samples and the disentanglement of latent factors.",
        "title": "Trading Information between Latents in Hierarchical Variational Autoencoders"
    },
    {
        "abs": "This abstract discusses the crucial role of methods that learn representations of nodes in a graph in network analysis. Specifically, it focuses on the deep Gaussian embedding technique, which enables unsupervised inductive learning through ranking. The abstract emphasizes the significance of this approach in capturing the underlying structure and patterns present in graphs, leading to improved performance in various network-based applications.",
        "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking"
    },
    {
        "abs": "This paper investigates the potential of self-ensembling in addressing visual domain adaptation challenges. Our approach focuses on leveraging self-ensembling to improve the adaptation process. By utilizing this technique, we aim to enhance the performance of visual domain adaptation methods.",
        "title": "Self-ensembling for visual domain adaptation"
    },
    {
        "abs": "Most machine learning classifiers, including deep neural networks, are susceptible to adversarial examples which are inputs deliberately designed to deceive the classifier. These adversarial examples can have subtle perturbations that are imperceptible to humans but can significantly affect the classifier's output, leading to incorrect predictions. This paper proposes a theoretical framework for enhancing the robustness of classifiers against adversarial examples, aiming to develop methods that can improve their resilience and accuracy in the presence of such inputs.",
        "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples"
    },
    {
        "abs": "In this study, we establish a comprehensive problem setting for training and evaluating the capabilities of information-seeking agents. By doing so, we aim to enhance the understanding and performance of these agents in searching and extracting relevant information. Through our research, we hope to contribute to the development of more efficient and effective information-seeking systems.",
        "title": "Towards Information-Seeking Agents"
    },
    {
        "abs": "In this study, we propose an extension to neural network language models that aims to improve their prediction accuracy through the integration of a continuous cache. Our approach allows the model to adapt its predictions based on the context provided by the cache, thereby enhancing its ability to generate coherent and contextually appropriate language. Through various experiments, we demonstrate the efficacy of our proposed extension in improving the performance of neural language models across diverse text generation tasks.",
        "title": "Improving Neural Language Models with a Continuous Cache"
    },
    {
        "abs": "This study delves into the concept of Generative Adversarial Networks (GANs) as powerful deep generative models. GANs adopt a two-player framework, where a generator network attempts to produce realistic samples, while a discriminator network tries to differentiate real samples from generated ones. This research provides a fresh perspective by viewing GANs as density ratio estimators, shedding light on their underlying principles and implications. By analyzing GANs through this lens, we gain a deeper understanding of their capabilities and potential applications.",
        "title": "Generative Adversarial Nets from a Density Ratio Estimation Perspective"
    },
    {
        "abs": "In this study, we present an innovative framework called Song From PI for generating pop music. Our model, a hierarchical Recurrent Neural Network, utilizes a hierarchical structure to capture various levels of musical context and generate coherent and musically plausible compositions. Through extensive experimentation and evaluation, we demonstrate the effectiveness of our approach in generating pop music that exhibits stylistic coherence and captures the essence of this popular genre.",
        "title": "Song From PI: A Musically Plausible Network for Pop Music Generation"
    },
    {
        "abs": "This study examines the eigenvalues of the Hessian matrix of a loss function in deep learning. Specifically, we investigate the changes in eigenvalues before and after certain operations. By analyzing the singularity and other properties of the Hessian, we gain insights into the behavior of the loss function and its optimization algorithm. Our findings contribute to enhancing the understanding of deep learning models and their optimization processes.",
        "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond"
    },
    {
        "abs": "This paper introduces a novel technique for extracting features from program execution logs, termed semantic embeddings for program behavior patterns. By leveraging the power of semantic embeddings, we capture the inherent relationships and meanings within program behavior patterns. The proposed technique aims to provide a more comprehensive representation of program execution logs, facilitating enhanced analysis and understanding of program behaviors. The effectiveness and utility of the technique are demonstrated through experiments on real-world program execution logs. Overall, our approach presents a promising avenue for advancing program analysis and optimizing software development processes.",
        "title": "Semantic embeddings for program behavior patterns"
    },
    {
        "abs": "In this study, we examined the efficiency of a novel insect-inspired sparse neural network called the FlyHash model for vision-based route following. The FlyHash model, developed by Dasgupta and colleagues, was evaluated in terms of its ability to accurately navigate a given route using visual input. The findings of this study provide insights into the potential of insect-inspired neural networks for solving complex visual tasks and highlights the FlyHash model as a promising approach in the field of route following and navigation.",
        "title": "Vision-based route following by an embodied insect-inspired sparse neural network"
    },
    {
        "abs": "In peer review, reviewers are usually asked to provide scores for the papers. These scores serve as a quantitative measure of the paper's quality and are used to determine its acceptance or rejection. However, the process of assigning scores can vary among reviewers and lacks consistency. To address this issue, integrating rankings into quantized scores is proposed, aiming to enhance the objectivity and reliability of the peer review system. By incorporating rankings alongside scores, a more comprehensive evaluation can be achieved, ensuring fair assessments and improving the overall quality of the review process.",
        "title": "Integrating Rankings into Quantized Scores in Peer Review"
    },
    {
        "abs": "Many recent studies have probed status bias in the peer-review process of academic journals and the potential influence of author metadata on acceptance rates. In this feature-rich, matched observational study, we investigate the association between author metadata and acceptance in the context of a corpus of ICLR submissions between 2017-2022. By examining various characteristics such as the affiliations, academic rank, gender, and publication history of the authors, we aim to shed light on potential biases that may exist in the review process. Our findings provide insights into the role of author metadata in acceptance rates and contribute to the ongoing efforts to promote fairness and transparency in the peer-review process.",
        "title": "Association between author metadata and acceptance: A feature-rich, matched observational study of a corpus of ICLR submissions between 2017-2022"
    },
    {
        "abs": "In this study, we present a novel approach to approximate Tishby et al.'s information bottleneck (IB) framework. Our method is based on a deep variational approach, which allows for more accurate and efficient analysis of high-dimensional data. By incorporating deep neural networks in the IB framework, we are able to learn meaningful representations that capture both relevant features and discard irrelevant information. We demonstrate the effectiveness of our approach through experiments on various datasets, showcasing its improved performance over traditional IB methods. Our findings highlight the potential of deep variational information bottlenecks for various applications in machine learning and data analysis.",
        "title": "Deep Variational Information Bottleneck"
    },
    {
        "abs": "Structured Attention Networks (SANs) have proven to be an effective approach for embedding categorical inference within various machine learning tasks. By leveraging attention mechanisms, SANs can effectively allocate importance to specific parts of the input, enabling improved performance in tasks such as natural language processing, computer vision, and recommendation systems. This abstract provides an overview of the significance and potential applications of SANs in the field of machine learning.",
        "title": "Structured Attention Networks"
    },
    {
        "abs": "In this paper, we propose the use of an ensemble of diverse specialists, where speciality is defined according to their expertise in detecting and defending against adversarial examples. Through this ensemble approach, we aim to enhance the robustness of machine learning models against such malicious attacks. By leveraging the collective knowledge and distinct perspectives of the specialists, we expect to achieve improved performance and generalization in the face of adversarial examples. Our findings demonstrate the potential of ensemble-based defenses in strengthening the security of machine learning systems.",
        "title": "Robustness to Adversarial Examples through an Ensemble of Specialists"
    },
    {
        "abs": "This paper introduces Neural Phrase-based Machine Translation (NPMT), a method that explicitly models the translation process by focusing on phrases. The proposed approach aims to improve the quality of machine translation output by effectively capturing the linguistic characteristics of the source and target languages.",
        "title": "Towards Neural Phrase-based Machine Translation"
    },
    {
        "abs": "In this paper, we present a novel adversarial image generation model called LR-GAN. Unlike traditional GANs, LR-GAN incorporates scene structure and context into the generation process, resulting in more coherent and visually pleasing images. By leveraging layered recursive networks, LR-GAN is able to generate images that capture intricate details at different scales. Experimental results demonstrate the effectiveness of LR-GAN in generating high-quality and diverse images across various datasets.",
        "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation"
    },
    {
        "abs": "This article presents a simple scheme that enables an agent to learn about its environment through intrinsic motivation and automatic curricula via asymmetric self-play. The scheme provides a framework for the agent to autonomously explore its surroundings, learn from its experiences, and develop adaptive strategies. The approach leverages intrinsic motivation as a driving force for exploration and uses asymmetric self-play to generate diverse training scenarios. The findings demonstrate the effectiveness of this scheme in promoting autonomous learning and skill acquisition without the need for external guidance or supervision.",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
    },
    {
        "abs": "In this paper, we explore the concept of maximum entropy modeling as a versatile and widely employed framework for constructing statistical models using incomplete information. We delve into the fundamentals of this approach, highlighting its flexibility and popularity in various fields. Through comprehensive analysis, we aim to provide a clear understanding of maximum entropy flow networks and their significance in formulating statistical models in the presence of partial information.",
        "title": "Maximum Entropy Flow Networks"
    },
    {
        "abs": "The abstract of the article titled \"CommAI: Evaluating the first steps towards a useful general AI\" focuses on the successful application of machine learning in solving new complex problems on a daily basis and highlights the initial progress made towards developing a practical general AI.",
        "title": "CommAI: Evaluating the first steps towards a useful general AI"
    },
    {
        "abs": "Neural networks that compute over graph structures are a natural fit for problems in areas such as social network analysis, recommendation systems, and natural language processing. These dynamic computation graphs allow for flexible and efficient processing of complex data, enabling deep learning models to capture intricate relationships and make accurate predictions. This paper explores the concepts and techniques employed in deep learning with dynamic computation graphs, highlighting their potential impact in various domains.",
        "title": "Deep Learning with Dynamic Computation Graphs"
    },
    {
        "abs": "Although deep learning models have proven effective at solving problems in natural language processing, the complexity and lack of interpretability of these models pose challenges in understanding how they arrive at their predictions. In this study, we propose a method for automatically extracting rules from Long Short-Term Memory (LSTM) networks, a type of deep learning model widely used for processing sequential data. By extracting rules from the learned representations of LSTM networks, we aim to provide a more interpretable and transparent understanding of their decision-making process. Experimental results demonstrate the effectiveness of our method in accurately extracting meaningful rules from LSTM networks, and its potential applications in improving model interpretability and opening the black box of deep learning models.",
        "title": "Automatic Rule Extraction from Long Short Term Memory Networks"
    },
    {
        "abs": "Deep reinforcement learning has achieved impressive results in recent years. However, tasks with sparse rewards pose a challenge to traditional approaches. To address this issue, stochastic neural networks offer a promising solution for hierarchical reinforcement learning. These networks enable agents to learn complex tasks by providing a framework for multiple levels of abstraction. Through the use of stochasticity, these networks can explore and exploit the environment more effectively, thus improving learning efficiency. This paper explores the potential of stochastic neural networks in hierarchical reinforcement learning and discusses their advantages and limitations. Experimental results demonstrate the effectiveness of these networks in solving tasks with sparse rewards. Overall, stochastic neural networks have the potential to revolutionize hierarchical reinforcement learning by providing a robust and efficient framework for complex tasks.",
        "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"
    },
    {
        "abs": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are two widely used architectures in this field. GANs, through a adversarial game between a generator and a discriminator, achieve high-quality image generation. VAEs, on the other hand, focus on learning latent representations and enable efficient sampling of new data points. In this paper, we propose a novel framework for unifying these two models. By combining the strengths of GANs and VAEs, we aim to create a more powerful and flexible generative model that can capture complex data distributions and generate high-quality samples. Our experimental results demonstrate the effectiveness and superior performance of the proposed unified deep generative model in various tasks, including image generation, data reconstruction, and latent space interpolation.",
        "title": "On Unifying Deep Generative Models"
    },
    {
        "abs": "In this paper, we address the challenge of identifying out-of-distribution (OOD) images in neural networks. We propose a novel solution, called ODIN, which enhances the reliability of OOD image detection. Utilizing temperature scaling and input preprocessing techniques, ODIN effectively separates in-distribution and OOD samples. Through extensive evaluations on various datasets, we demonstrate that ODIN significantly outperforms existing methods in detecting OOD images, contributing to improved reliability and robustness of neural networks.",
        "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks"
    },
    {
        "abs": "This paper introduces an information-theoretic framework for unsupervised learning using neural population infomax. The proposed framework aims to generate robust and efficient representations at a large scale. By leveraging the infomax principle, the framework can achieve fast and accurate learning without the need for labeled data. The effectiveness of the framework is demonstrated through various experiments and comparisons with existing unsupervised learning methods.",
        "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax"
    },
    {
        "abs": "Recurrent Neural Networks (RNNs) have demonstrated remarkable performance in sequence modeling tasks. However, the training of RNNs can be computationally expensive due to the update of states at each time step. In this paper, we propose a method called Skip RNN that learns to skip state updates in order to reduce computational costs during training. By selectively updating states only when necessary, Skip RNN achieves comparable performance to traditional RNNs while significantly reducing the training time. Experimental results on various sequence modeling tasks validate the effectiveness of Skip RNN in terms of computational efficiency without sacrificing model accuracy.",
        "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks"
    },
    {
        "abs": "The abstract: \nRestart techniques are widely used in gradient-free optimization to address the challenges posed by multimodal functions. This paper introduces a new technique called Stochastic Gradient Descent with Warm Restarts (SGDR) that utilizes partial warm restarts. These partial warm restarts enhance the performance of the optimization process by periodically resetting the learning rate schedule. The effectiveness of SGDR is demonstrated through experimental results on various benchmarks, showcasing its ability to tackle the multimodality of complex functions.",
        "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"
    },
    {
        "abs": "In recent years, policy gradient methods have shown major advances in solving complex reinforcement learning problems. However, these methods often suffer from high variance, leading to slow convergence. This paper proposes a novel approach to address this issue by introducing action-dependent control variates based on Stein's identity. The use of control variates significantly reduces the variance of policy gradient estimates, resulting in faster and more stable convergence. Experimental results demonstrate the effectiveness of the proposed method in improving the performance of policy optimization algorithms. Overall, this research contributes to enhancing policy gradient methods and their applicability in challenging reinforcement learning tasks.",
        "title": "Action-depedent Control Variates for Policy Optimization via Stein's Identity"
    },
    {
        "abs": "Skip connections have revolutionized the training of deep neural networks by eliminating the occurrence of singularities, making them an integral component in modern network architectures. This abstract highlights the significance of skip connections in enabling the successful training of very deep networks and emphasizes their indispensability in contemporary deep learning practices.",
        "title": "Skip Connections Eliminate Singularities"
    },
    {
        "abs": "We have attempted to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" for the ICLR 2018 Reproducibility Report.",
        "title": "Natural Language Inference over Interaction Space: ICLR 2018 Reproducibility Report"
    },
    {
        "abs": "The \"Learn to Pay Attention\" model implemented in convolutional neural networks has been successfully applied in various domains. This report presents our findings and analyses on the effectiveness of this attention mechanism in improving the performance of image recognition tasks. Through rigorous experimentation, we demonstrate the model's ability to learn attention weights and highlight significant spatial regions in images, leading to enhanced feature extraction and better classification accuracy. The results of this study provide valuable insights into the potential applications and benefits of attention mechanisms in deep learning models.",
        "title": "Reproduction Report on \"Learn to Pay Attention\""
    },
    {
        "abs": "The abstract focuses on the task of computing universal representations of sentences in natural language processing. It introduces SufiSent, a new approach that utilizes suffix encodings for generating these representations. SufiSent aims to capture and encode the overall meaning of diverse sentences, enabling more effective sentence-level understanding and analysis in various NLP tasks.",
        "title": "SufiSent - Universal Sentence Representations Using Suffix Encodings"
    },
    {
        "abs": "In many neural models, polynomial features are commonly employed as a means of enhancing representation matching. These features are implemented as polynomial functions of pre-existing ones. This paper explores the scaling of polynomial features and its impact on representation matching in neural models. By delving into the relationship between polynomial features and existing ones, we aim to provide insights into the effectiveness of scaling in improving representation matching tasks.",
        "title": "On the scaling of polynomial features for representation matching"
    },
    {
        "abs": "This paper presents a novel PAC-Bayesian approach to establish spectrally-normalized margin bounds for feedforward neural networks. The generalization bound proposed in this study is formulated based on the product of certain terms, outlining an efficient strategy for assessing the performance and reliability of neural networks in terms of their spectral properties. This work contributes to the broader understanding and advancement of theoretical foundations for neural network generalization.",
        "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks"
    },
    {
        "abs": "In this work, we delve into the Batch Normalization technique and put forth its probabilistic interpretation. Our proposal aims to illustrate a novel approach to uncertainty estimation through Stochastic Batch Normalization.",
        "title": "Uncertainty Estimation via Stochastic Batch Normalization"
    },
    {
        "abs": "Abstract: \n\nDeep convolutional networks have achieved remarkable success in a wide range of computer vision tasks by progressively extracting complex hierarchical features from input images. However, their effectiveness heavily relies on the availability of large-scale labeled datasets for training. In this paper, we propose i-RevNet, a novel deep invertible network architecture that is capable of learning deep representations while being able to reconstruct the original input. By imposing an invertibility constraint on network operations, i-RevNet enables bidirectional information flow and facilitates efficient generation of unlabeled data for training. Experimental results demonstrate the superior performance and generalization ability of i-RevNet compared to traditional deep convolutional networks.",
        "title": "i-RevNet: Deep Invertible Networks"
    },
    {
        "abs": "In this paper, we explore the effectiveness of deep latent variable models for representation learning. Specifically, we adopt a novel approach called the Deep Copula Information Bottleneck, which aims to learn sparse latent representations. Through experimental evaluations, we demonstrate the superior performance of our approach in capturing complex data dependencies and generating efficient representations. This work contributes to the advancement of representation learning techniques, particularly in the context of deep latent variable models.",
        "title": "Learning Sparse Latent Representations with the Deep Copula Information Bottleneck"
    },
    {
        "abs": "We present a variant of the MAC model introduced by Hudson and Manning in 2018 for transfer learning. This variant utilizes a unique approach by incorporating a specific method (denoted as a) within the MAC model framework. The effectiveness of this variant is demonstrated through comparative evaluations against previous models. Our findings suggest that the proposed variant outperforms existing models in various tasks, showcasing its potential for efficient transfer learning.",
        "title": "On transfer learning using a MAC model variant"
    },
    {
        "abs": "The Adaptive Computation Time for Recurrent Neural Networks (ACT) is emerging as a highly promising architectural approach. This paper aims to compare the ACT with fixed computation time models, evaluating their performance in various tasks. By exploring the advantages and limitations of each approach, this study sheds light on the potential benefits of using adaptive computation time in recurrent neural networks.",
        "title": "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks"
    },
    {
        "abs": "This paper explores the application of Generative Adversarial Networks (GANs) in anomaly detection. GANs have demonstrated their capability to model complex high-dimensional distributions found in real-world data. By leveraging the adversarial training framework, we propose an efficient GAN-based approach for identifying anomalies. Our method combines the power of generator and discriminator networks to accurately identify anomalous instances while maintaining computational efficiency. Extensive experimental results on benchmark datasets demonstrate the superiority of our proposed approach in terms of both detection accuracy and efficiency.",
        "title": "Efficient GAN-Based Anomaly Detection"
    },
    {
        "abs": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between two given sentences. This abstract focuses on the significance of NLI in the context of interaction space. The aim is to explore how NLI can enhance the understanding of language during interactions, allowing agents to effectively comprehend and respond to user queries or commands. By leveraging NLI, interactive systems can make accurate inference about the user's intended meaning and provide more contextually appropriate and informed responses, leading to improved user experience and increased system performance.",
        "title": "Natural Language Inference over Interaction Space"
    },
    {
        "abs": "Abstract: The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the potential presence of adversarial examples, which can cause significant distortions and compromise the integrity of the system. This paper proposes a novel approach called Provably Minimally-Distorted Adversarial Examples (PMD-AEs) that addresses this limitation by guaranteeing minimal distortion while adversarially attacking the system. Experimental results demonstrate the effectiveness and robustness of the proposed method, providing a promising solution for enhancing the security and reliability of neural networks in safety-critical applications.",
        "title": "Provably Minimally-Distorted Adversarial Examples"
    },
    {
        "abs": "Abstract: \n\nDeep neural networks (DNNs) have demonstrated remarkable predictive performance by harnessing their inherent ability to learn from data. This paper explores the concept of hierarchical interpretations for neural network predictions, facilitating a better understanding of the underlying reasoning process. By unraveling the hierarchical nature of these predictions, researchers can gain deeper insights into the decision-making mechanisms of DNNs and improve their interpretability. The proposed framework holds promise for enhancing the trustworthiness and explainability of neural network predictions, thereby enabling their wider adoption in critical applications.",
        "title": "Hierarchical interpretations for neural network predictions"
    },
    {
        "abs": "This work addresses the problem of musical timbre transfer, where the goal is to create a pipeline that combines WaveNet, CycleGAN, and Constant-Q Transform (CQT) to effectively transform the timbre of a musical composition.",
        "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer"
    },
    {
        "abs": "In this study, we explore the combination of hidden-states-based approaches for word-level language modeling. More specifically, we delve into the task of meta-learning a dynamical language model.",
        "title": "Meta-Learning a Dynamical Language Model"
    },
    {
        "abs": "This paper introduces the concept of using Generative Adversarial Networks (GANs) for semi-supervised learning by revisiting the idea of manifold regularization. GANs are powerful generative models that can effectively model the intricate structure of natural images. Leveraging this capability, the authors propose a new approach for semi-supervised learning that utilizes GANs to learn and exploit the underlying manifold structure of the data. The experimental results demonstrate the effectiveness of this method in improving semi-supervised learning performance compared to other state-of-the-art approaches.",
        "title": "Semi-Supervised Learning with GANs: Revisiting Manifold Regularization"
    },
    {
        "abs": "In this study, we investigate a class of over-parameterized deep neural networks with standard activation functions and cross-entropy. We analyze the loss landscape of these networks and identify that there are no bad local valleys. This finding suggests that the optimization process for these networks is more efficient and reliable compared to networks with bad local valleys. Understanding the properties of loss landscapes in deep neural networks enables us to develop improved optimization algorithms and further enhance the performance and training of these networks.",
        "title": "On the loss landscape of a class of deep neural networks with no bad local valleys"
    },
    {
        "abs": "Visual Question Answering (VQA) models have historically encountered difficulties in accurately counting objects within natural images. In this study, we address the challenge of counting objects by proposing a method that leverages neural networks to learn this skill. Through extensive experimentation, we demonstrate the effectiveness of our approach in accurately counting objects in natural images, thereby improving the overall performance of VQA models.",
        "title": "Learning to Count Objects in Natural Images for Visual Question Answering"
    },
    {
        "abs": "One of the challenges in the study of generative adversarial networks (GANs) is the instability of their training process, which often leads to mode collapse and poor image quality. In this paper, we propose a novel method called Spectral Normalization to stabilize the training of GANs. By normalizing the spectral norm of the discriminator's weight matrices, we ensure Lipschitz continuity in the network, reducing the chances of mode collapse and improving the quality of generated images. We experimentally validate our approach on various datasets and demonstrate its effectiveness in improving the stability and performance of GANs. Our results suggest that Spectral Normalization is a promising technique to address the instability issue in GAN training.",
        "title": "Spectral Normalization for Generative Adversarial Networks"
    },
    {
        "abs": "This study investigates the relationship between node centralities and classification performance in characterizing node embedding algorithms. By embedding graph nodes into a vector space, machine learning techniques can be applied to analyze their characteristics. The abstract provides a concise overview of the purpose and focus of the research.",
        "title": "Node Centralities and Classification Performance for Characterizing Node Embedding Algorithms"
    },
    {
        "abs": "We introduce a new dataset of logical entailments to measure models' ability to understand logical relationships. This dataset aims to assess the performance of neural networks in accurately inferring logical entailments.",
        "title": "Can Neural Networks Understand Logical Entailment?"
    },
    {
        "abs": "Neural network pruning techniques have the potential to significantly reduce the parameter count of trained networks by more than 90%. This paper explores the Lottery Ticket Hypothesis, which focuses on finding sparse, trainable neural networks. By identifying and training these \"winning tickets\" in the initial network, we demonstrate that it is possible to achieve high performance while significantly reducing the computational burden. Our findings contribute to the growing body of research on network optimization and offer promising insights for efficient neural network design.",
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
    },
    {
        "abs": "In this study, we characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer. By analyzing the singular values, we aim to gain insights into the behavior and performance of convolutional layers in deep learning models. Our findings provide a better understanding of the information preservation and dimensionality reduction capabilities of convolutional layers, which can inform the design and optimization of deep neural networks.",
        "title": "The Singular Values of Convolutional Layers"
    },
    {
        "abs": "This paper presents a theoretical framework for understanding the properties of deep and locally connected non-linear networks, particularly focusing on deep convolutional neural networks. By exploring the theoretical foundations of these networks, this research aims to shed light on the underlying mechanisms and provide insights into the behavior and performance of deep non-linear networks. The proposed framework strives to enhance the understanding of deep locally connected ReLU networks and their applications in various fields.",
        "title": "A theoretical framework for deep locally connected ReLU network"
    },
    {
        "abs": "Neural Program Search is an algorithm designed to generate programs by interpreting natural language descriptions. This algorithm tackles the challenge of solving programming tasks solely based on written explanations and examples. By leveraging the power of neural networks, it offers a promising approach to automate program generation and advance the field of natural language programming.",
        "title": "Neural Program Search: Solving Programming Tasks from Description and Examples"
    },
    {
        "abs": "Most state-of-the-art neural machine translation systems employ phrase-based attentions, despite having different architectural skeletons such as recurrent or convolutional models. This abstract aims to provide a concise overview of the topic.",
        "title": "Phrase-Based Attentions"
    },
    {
        "abs": "We propose a novel approach for learning distributed representations of edits by using a \"neural editor\". This study addresses the problem of representing edits in a distributed manner and introduces a neural network framework that combines various techniques to achieve this. The proposed approach provides a concise and efficient solution for representing edits, allowing for more effective editing tasks and improving the overall editing process.",
        "title": "Learning to Represent Edits"
    },
    {
        "abs": "In this paper, we propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of not-so-random features. By exploring the mathematical properties of these features, we develop a robust framework for efficiently learning kernels. Our proposed method aims to improve the accuracy and effectiveness of kernel-based machine learning algorithms.",
        "title": "Not-So-Random Features"
    },
    {
        "abs": "This paper introduces Variational Continual Learning (VCL), a straightforward yet versatile framework for achieving continual learning. VCL addresses the challenge of learning multiple tasks sequentially without forgetting previous knowledge. By utilizing variational inference techniques, VCL effectively maintains a balance between exploiting new information and retaining old knowledge. The proposed approach demonstrates promising results in various scenarios, showcasing its potential in addressing the continual learning problem.",
        "title": "Variational Continual Learning"
    },
    {
        "abs": "This report aims to investigate the reproducibility of the regularization techniques employed in Wasserstein Generative Adversarial Networks (GANs). We examine the effectiveness of these regularization methods in improving the stability and convergence of Wasserstein GANs. Through extensive experimentation and analysis, we assess the extent to which the published results can be reproduced and highlight any challenges or discrepancies encountered during the reproduction process.",
        "title": "On reproduction of On the regularization of Wasserstein GANs"
    },
    {
        "abs": "In this paper, we propose a novel technique for extracting features from program execution logs, called semantic embeddings. By leveraging the power of deep learning and natural language processing, we aim to capture the underlying behavior patterns exhibited by programs during their execution. Our approach offers a more semantically rich representation of program logs, enabling better analysis and understanding of software behavior. We present experimental results that demonstrate the effectiveness of our technique in improving program analysis tasks.",
        "title": "Semantic embeddings for program behavior patterns"
    },
    {
        "abs": "In this study, we propose a novel approach for a neural probabilistic model called Variational Autoencoder with Arbitrary Conditioning. Our model utilizes a Variational Autoencoder framework and introduces the ability to condition the generated outputs on arbitrary input information. This enables the model to learn and generate data in a more controlled manner while still maintaining the benefits of unsupervised learning. Through extensive experimentation, we demonstrate the effectiveness of our approach in various application domains, showcasing its potential for improved generative modeling tasks.",
        "title": "Variational Autoencoder with Arbitrary Conditioning"
    },
    {
        "abs": "Variational Autoencoders (VAEs) were originally introduced as probabilistic generative models by Kingma and Welling in 2014. These models have gained significant popularity due to their ability to learn compact latent representations of input data. However, in the hierarchical setting, where multiple levels of latent variables are used, there exist challenges in effectively trading information between different levels of the hierarchy. This paper investigates the problem of trading information between latents in Hierarchical Variational Autoencoders, aiming to improve the overall performance and representation learning capabilities of the model.",
        "title": "Trading Information between Latents in Hierarchical Variational Autoencoders"
    },
    {
        "abs": "This study explores the robustness of deep learning models by understanding and characterizing the subspaces of adversarial examples. The limitations of local intrinsic dimensionality in accurately characterizing these subspaces are investigated. This research sheds light on the challenges associated with analyzing and defending against adversarial attacks in deep learning systems.",
        "title": "On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples"
    },
    {
        "abs": "This paper presents a variational inequality perspective on generative adversarial networks (GANs), a popular approach for generative modeling in the field of machine learning. GANs are known for their ability to produce visually appealing samples. By viewing GANs from a variational inequality standpoint, we aim to provide new insights into their underlying principles and mechanisms. Through this perspective, we explore the theoretical foundations of GANs and discuss their potential applications and limitations. Our findings contribute to a deeper understanding of GANs and pave the way for future improvements and advancements in this area of research.",
        "title": "A Variational Inequality Perspective on Generative Adversarial Networks"
    },
    {
        "abs": "In recent years, neural message passing algorithms have shown remarkable success in semi-supervised classification on graphs. However, these approaches often lack the ability to effectively propagate information through the graph structure. To address this limitation, we propose a novel framework that combines graph neural networks with personalized PageRank. Our approach, called Predict then Propagate, leverages the strengths of both techniques to enhance the accuracy and robustness of graph-based classification tasks. Experimental results on various datasets demonstrate the superior performance of our method compared to existing state-of-the-art algorithms.",
        "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank"
    },
    {
        "abs": "In this paper, we investigate the phenomenon of obfuscated gradients, a form of gradient masking, which deceives defenses against adversarial examples. We analyze how obfuscated gradients provide a false sense of security in the robustness of machine learning models. By circumventing defenses, adversaries exploit this vulnerability to create adversarial examples that are misclassified by the targeted model. Our findings highlight the need for improved defenses against obfuscated gradients to enhance the security and reliability of machine learning systems.",
        "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
    },
    {
        "abs": "This abstract highlights the importance of node representation learning in network graphs and introduces the concept of Deep Gaussian Embedding as a method for unsupervised inductive learning via ranking.",
        "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking"
    },
    {
        "abs": "In recent years, Convolutional Neural Networks (CNNs) have emerged as the preferred method for tackling learning problems that involve 2D data. However, as more applications arise in fields such as computer vision, it has become apparent that CNNs are not well-suited for processing spherical data. This limitation inspired the development of Spherical CNNs, which aim to extend the capabilities of CNNs to handle spherical data. This paper explores the challenges faced by CNNs in spherical domains and introduces the concept of Spherical CNNs as a promising solution to address these limitations.",
        "title": "Spherical CNNs"
    },
    {
        "abs": "This paper discusses how natural language processing (NLP) methods can be directly applied to classification. It explores the application of NLP techniques in the field of classification and highlights the importance of understanding the underlying principles of NLP for effective classification. The paper aims to provide insights and guidance for researchers and practitioners interested in utilizing NLP for classification tasks.",
        "title": "Learning to SMILE(S)"
    },
    {
        "abs": "This abstract discusses the use of computer vision and deep learning technologies in agriculture, specifically focusing on their application in apple defect detection. By employing object detection techniques, a more efficient and accurate post-harvest handling process can be achieved, leading to improved fruit quality and reduced waste.",
        "title": "Apple Defect Detection Using Deep Learning Based Object Detection For Better Post Harvest Handling"
    },
    {
        "abs": "In this paper, we present two simple and effective factorization tricks for enhancing the performance of LSTM networks. These methods focus on reducing the number of parameters and speeding up the training process. By implementing these techniques, we demonstrate significant improvements in both efficiency and accuracy of LSTM networks.",
        "title": "Factorization tricks for LSTM networks"
    },
    {
        "abs": "This abstract discusses the dominance of recurrent neural networks in state-of-the-art deep reading comprehension models. It highlights the sequential nature of these models and implies that there might be alternatives to explore for achieving faster reading comprehension.",
        "title": "Fast Reading Comprehension with ConvNets"
    },
    {
        "abs": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to understand the emergence of abstract and episodic neurons in episodic meta-reinforcement learning (meta-RL). The reinstatement mechanism plays a crucial role in enabling higher-level cognitive processes, such as generalization and episodic memory, in deep reinforcement learning agents. We provide a comprehensive examination of this mechanism and its implications for improving meta-RL algorithms. Overall, our analysis sheds light on the theoretical foundations and practical implications of abstract and episodic neurons in meta-RL.",
        "title": "The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL"
    },
    {
        "abs": "The rate-distortion-perception function (RDPF) proposed by Blau and Michaeli in 2019 has been widely recognized as a valuable tool in coding. This abstract provides a brief overview of the RDPF and its significance in various applications.",
        "title": "A coding theorem for the rate-distortion-perception function"
    },
    {
        "abs": "In this paper, we introduce Neural Phrase-based Machine Translation (NPMT). Our approach effectively represents and translates phrases by explicitly modeling them within the machine translation system. We demonstrate the effectiveness and potential of NPMT in improving translation quality.",
        "title": "Towards Neural Phrase-based Machine Translation"
    },
    {
        "abs": "In recent years, it has become widely recognized that even small perturbations introduced to deep neural networks can lead to erroneous classifications. This phenomenon, known as adversarial attacks, poses a significant threat to the reliability and security of deep learning systems. In this study, we propose a novel approach for combating adversarial attacks by leveraging sparse representations. By incorporating sparse representations into deep learning models, we aim to enhance their robustness against adversarial perturbations. Our experimental results demonstrate the effectiveness of this approach in mitigating the impact of adversarial attacks and improving the overall resilience of deep learning models.",
        "title": "Combating Adversarial Attacks Using Sparse Representations"
    },
    {
        "abs": "In this study, we introduce Supervised Policy Update (SPU), a novel and efficient methodology for deep reinforcement learning. SPU aims to improve the sample efficiency of policy updates in deep reinforcement learning by leveraging supervised learning techniques. By merging the benefits of supervised learning and reinforcement learning, SPU demonstrates promising results in enhancing the learning process and achieving better performance. Through extensive experimentation and evaluation, our findings support the effectiveness and potential of SPU as a valuable addition to deep reinforcement learning methodologies.",
        "title": "Supervised Policy Update for Deep Reinforcement Learning"
    },
    {
        "abs": "We present a parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction models. This dataset enables the evaluation of representations learned by these models. With Moving Symbols, various scenarios can be generated, allowing for comprehensive analysis and comparison of different video prediction algorithms. The dataset serves as a valuable resource for researchers working on video prediction models and contributes to the advancement of this field.",
        "title": "A Dataset To Evaluate The Representations Learned By Video Prediction Models"
    },
    {
        "abs": "This work is a part of ICLR Reproducibility Challenge 2019, where we aim to reproduce the findings of the paper titled \"Closing The Generalization Gap Of Adaptive Gradient Methods in Training Deep Neural Networks.\" We focus on reproducing the results presented in the paper, which proposes Padam, a novel technique to improve generalization performance of adaptive gradient methods. Through our reproduction efforts, we provide an evaluation of the authored claims and insights into the practicality and effectiveness of Padam in reducing the generalization gap in deep neural network training.",
        "title": "ICLR Reproducibility Challenge Report (Padam : Closing The Generalization Gap Of Adaptive Gradient Methods in Training Deep Neural Networks)"
    },
    {
        "abs": "In this study, we present a large-scale empirical investigation into catastrophic forgetting (CF) in modern Deep Neural Networks (DNNs). We aim to provide a comprehensive and application-oriented analysis of CF, which refers to the phenomenon where DNNs forget prior learned information when trained on new tasks. Through our study, we aim to shed light on the underlying causes of CF and explore potential mitigation strategies. By conducting extensive experiments on various benchmark datasets, we examine the impact of CF on DNN performance and determine the factors that exacerbate or alleviate this issue. Our findings contribute valuable insights to the field of deep learning, allowing for more informed decision-making and improved training methods for DNNs.",
        "title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs"
    },
    {
        "abs": "Graph neural networks (GNNs) have significantly improved the performance on various tasks in the field of deep learning. However, their vulnerability to adversarial attacks remains a critical concern. In this study, we propose a meta-learning approach for adversarial attacks on GNNs. Our method aims to find effective attack strategies by leveraging knowledge from previous attack experiences. Through extensive experiments, we demonstrate the effectiveness and efficiency of our approach compared to traditional attack methods. Our findings highlight the importance of developing robust GNN models that can withstand adversarial attacks in real-world applications.",
        "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning"
    },
    {
        "abs": "Multi-Domain Adversarial Learning (MDL) is a technique that focuses on achieving a model with the least average risk across various domains. By utilizing adversarial learning, MDL minimizes the discrepancies between different domains, enabling the development of a robust and generalized model. This abstract provides a brief overview of MDL and its objective of attaining optimal performance in the presence of multiple domains.",
        "title": "Multi-Domain Adversarial Learning"
    },
    {
        "abs": "In this study, we propose a neural network framework for unsupervised anomaly detection, incorporating a novel robust subspace recovery layer. Our approach aims to effectively identify anomalies in complex datasets by leveraging the inherent structure of the data. The proposed robust subspace recovery layer enhances the network's ability to separate normal and abnormal data points, leading to improved anomaly detection performance. Experimental results demonstrate the efficacy of our approach in various real-world scenarios, highlighting its potential for practical applications in anomaly detection tasks.",
        "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection"
    },
    {
        "abs": "Deep neural networks (DNNs) have made significant strides in predictive performance, mainly attributed to their ability to learn. However, understanding the hierarchical interpretations of DNN predictions remains a challenge. In this study, we explore hierarchical interpretations for neural network predictions, aiming to gain insights into the inner workings of DNNs and enhance their interpretability.",
        "title": "Hierarchical interpretations for neural network predictions"
    },
    {
        "abs": "In this work, we present TimbreTron, a novel pipeline for musical timbre transfer. The goal of our research is to accurately transform the timbre of a given piece of music while maintaining its musical characteristics. To achieve this, we introduce a combination of WaveNet, CycleGAN, and Constant-Q Transform (CQT) techniques. Our results demonstrate the effectiveness of TimbreTron in achieving high-quality timbre transfer, opening possibilities for creative music production and exploration.",
        "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer"
    },
    {
        "abs": "In this paper, we propose a new approach for node embedding of directed graphs. Our method aims to map nodes onto low-dimensional statistical manifolds. This technique is based on novel algorithms and offers a unique perspective for analyzing directed graphs. Through experimentation and analysis, we demonstrate the effectiveness of our approach in capturing the underlying statistical properties of the graph structure. This research provides valuable insights into the low-dimensional representation of directed graphs and has promising applications in various domains, such as network analysis and data visualization.",
        "title": "Low-dimensional statistical manifold embedding of directed graphs"
    },
    {
        "abs": "This study explores the impressive lifelong learning capabilities observed in animal brains, primarily attributed to plastic changes occurring at the synaptic level. Specifically, it introduces a novel approach called Backpropamine, which leverages differentiable neuromodulated plasticity to train self-modifying neural networks. Through this mechanism, the research aims to enhance our understanding of how animals continually learn, adapt, and acquire new skills throughout their lifetime.",
        "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity"
    },
    {
        "abs": "Euclidean geometry has been widely used in machine learning applications due to its simplicity and computational efficiency. However, many real-world problems require a more flexible and expressive framework to capture the intrinsic structure of complex data. In this paper, we introduce mixed-curvature variational autoencoders, a novel approach that combines Euclidean geometry with alternative curvature spaces. This method enables the modeling of nonlinear geometric structures, improving the representation and generation capabilities of autoencoders. Experimental results demonstrate the superiority of mixed-curvature variational autoencoders in capturing and reconstructing complex data distributions compared to traditional Euclidean-based approaches.",
        "title": "Mixed-curvature Variational Autoencoders"
    },
    {
        "abs": "In this study, we investigate different techniques for generating sentence representations using pre-trained word embeddings without the need for any additional training. By exploring various methods, we aim to find efficient ways of computing sentence embeddings for sentence classification tasks.",
        "title": "No Training Required: Exploring Random Encoders for Sentence Classification"
    },
    {
        "abs": "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high-dimensional data distributions. However, GANs often suffer from two major challenges: poor generalization and instability during training. This paper aims to address these issues by proposing novel techniques to improve the generalization capability and stability of GANs. Through experimental evaluations, our proposed methods demonstrate significant improvements, leading to more reliable and consistent results in generating realistic and diverse samples.",
        "title": "Improving Generalization and Stability of Generative Adversarial Networks"
    },
    {
        "abs": "In this paper, we propose a Wasserstein Barycenter Model Ensembling technique for multiclass or multilabel classification. Our method aims to enhance the performance of ensemble learning by utilizing the Wasserstein barycenter concept. We demonstrate the effectiveness of our approach through extensive experiments and showcase its potential in improving classification accuracy and robustness in various real-world scenarios.",
        "title": "Wasserstein Barycenter Model Ensembling"
    },
    {
        "abs": "Our method introduces a stochastic prediction framework that effectively integrates temporal information utilizing a learned dynamics model. By utilizing partial observations, we are able to predict multi-agent interactions accurately. This approach allows for robust predictions even in complex scenarios where complete information is not available.",
        "title": "Stochastic Prediction of Multi-Agent Interactions from Partial Observations"
    },
    {
        "abs": "Modern neural networks are often over-parametrized due to the flexibility of rectified linear hidden units, which can be easily modified. This leads to inefficient and redundant models. To address this issue, equi-normalization is proposed as a solution to effectively constrain the parameters of neural networks. By normalizing the weights and biases of the network layers, equi-normalization enables more efficient training and improves the generalization capability of the model. Experimental results show that equi-normalization is able to significantly reduce the parameter size of neural networks without sacrificing performance.",
        "title": "Equi-normalization of Neural Networks"
    },
    {
        "abs": "This paper explores the use of spherical data in various applications and proposes a graph-based spherical convolutional neural network (CNN) called DeepSphere. By representing the discretized sphere as a graph, DeepSphere aims to achieve equivariance and tackle challenges peculiar to spherical data processing. The network shows promising potential in handling spherical data in diverse applications.",
        "title": "DeepSphere: towards an equivariant graph-based spherical CNN"
    },
    {
        "abs": "In this paper, we present Graph Wavelet Neural Network (GWNN), a novel graph convolutional neural network (CNN) that leverages the power of wavelet analysis to enhance graph-based learning. GWNN is designed to effectively address the limitations of traditional graph CNNs by incorporating wavelet-based filters that capture both local and global structural information of the graph. Experimental results demonstrate that GWNN significantly outperforms state-of-the-art graph CNNs on various real-world graph-based tasks, demonstrating its potential for advancing the field of graph representation learning and analysis.",
        "title": "Graph Wavelet Neural Network"
    },
    {
        "abs": "We propose a single neural probabilistic model based on Variational Autoencoder (VAE) that can be conditioned on arbitrary inputs. This model enables flexible incorporation of external information into the latent space of VAEs, allowing for customized generation and manipulation of data. By conditioning the VAE on different inputs, such as class labels or fixed feature vectors, the proposed model enhances its capability to generate diverse and contextually relevant samples. Our experiments demonstrate the effectiveness of the model in various tasks, showcasing its potential applications in domains like image synthesis, text generation, and data augmentation.",
        "title": "Variational Autoencoder with Arbitrary Conditioning"
    },
    {
        "abs": "We present the Perceptor Gradients algorithm, a novel approach to learning symbolic representations based on programming techniques. This algorithm aims to learn programmatically structured representations by leveraging the power of gradients. Through experiments and evaluations, we demonstrate the effectiveness of our proposed method in acquiring interpretable and robust symbolic representations. Our findings provide insights into the potential of incorporating programming techniques in learning symbolic representations.",
        "title": "Learning Programmatically Structured Representations with Perceptor Gradients"
    },
    {
        "abs": "We study the robustness to symmetric label noise of Graph Neural Network (GNN) training procedures. By combining existing approaches to handle noisy labels in the non-graph setting with modifications tailored for GNNs, we propose an algorithm called G-Noisy. G-Noisy enhances the robustness of GNNs to label noise by learning cleaner representations of the data. We conduct experiments on various benchmark datasets and demonstrate that G-Noisy outperforms baseline methods in terms of classification accuracy under different noise levels. Additionally, we show that G-Noisy effectively learns from noisy labels even when the noise rate is as high as 50%. Our findings highlight the importance of considering label noise in GNN training and provide a practical solution for improving robustness in real-world applications.",
        "title": "Learning Graph Neural Networks with Noisy Labels"
    },
    {
        "abs": "In recent years, the use of 'BigCode' and state-of-the-art deep learning methods has shown promising avenues for inferring JavaScript types. This paper explores the application of Graph Neural Networks (GNNs) in inferring JavaScript types. Our study demonstrates that GNNs can effectively capture the complex relationships between code elements and infer accurate JavaScript types. We present experimental results showcasing the superior performance of GNNs compared to traditional methods in JavaScript type inference. This work opens up new possibilities for improving program comprehension and enhancing software development tools.",
        "title": "Inferring Javascript types using Graph Neural Networks"
    },
    {
        "abs": "In this paper, we explore the use of self-supervised representation learning techniques to enhance sample efficiency in reinforcement learning. By incorporating dynamics-aware embeddings, we aim to improve the efficiency of learning policies in complex and dynamic environments. Our findings suggest that utilizing these embeddings can greatly enhance sample efficiency and accelerate the learning process, paving the way for more effective and efficient reinforcement learning algorithms.",
        "title": "Dynamics-aware Embeddings"
    },
    {
        "abs": "This study focuses on the problem of learning permutation invariant representations that can capture flexible notions within multisets. By examining the intricacies of representation learning, we strive to uncover methods that can effectively capture and represent the varying elements of a multiset, regardless of their order or arrangement. This abstract summarizes our research efforts in exploring innovative techniques for achieving permutation invariant representations and their potential applications in various domains.",
        "title": "Representation Learning with Multisets"
    },
    {
        "abs": "This abstract presents a novel approach for interpreting trained deep neural networks (DNNs) by inspecting characteristic features of neurons. The proposed method leverages Generative Adversarial Networks (GANs) to generate explanations for DNN predictions and automatically select the most salient ones. This framework enables a more comprehensive understanding of DNN decision-making processes and facilitates the interpretability of complex models.",
        "title": "GAN-based Generation and Automatic Selection of Explanations for Neural Networks"
    },
    {
        "abs": "In this study, we investigate and characterize the singular values of a linear transformation associated with a standard 2D multi-channel convolutional layer. By analyzing the properties of the singular values, we aim to gain insights into the information flow and representational capacity of convolutional layers in deep learning models. Our findings provide valuable understanding on the behavior and limitations of these layers, which can contribute to improving and designing more efficient convolutional neural networks.",
        "title": "The Singular Values of Convolutional Layers"
    },
    {
        "abs": "In this study, we address the challenge of learning distributed representations of edits. To tackle this problem, we propose a novel approach that combines a \"neural editor\" framework. By utilizing this framework, we aim to enable the generation of concise and informative representations for different types of edits. This research aims to enhance the understanding and analysis of edits, ultimately contributing to more advanced natural language processing applications.",
        "title": "Learning to Represent Edits"
    },
    {
        "abs": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that effectively capture the dynamics of complex systems. By integrating symplectic integration schemes into recurrent neural networks, SRNNs offer enhanced precision and stability in modeling the temporal evolution of dynamical systems. Experimental results demonstrate the superior performance of SRNNs in various domains, including physics simulations, control systems, and time series prediction. Overall, SRNNs present a promising approach for accurately modeling and understanding the dynamics of complex systems.",
        "title": "Symplectic Recurrent Neural Networks"
    },
    {
        "abs": "Spectral embedding is a widely used technique for representing graph data. In this study, we focus on regularized block models and explore various regularization techniques. By incorporating regularization, we aim to improve the accuracy and stability of the spectral embedding process. Our findings shed light on the effectiveness of different regularization methods in capturing latent structures within the graph data.",
        "title": "Spectral embedding of regularized block models"
    },
    {
        "abs": "In this work, we investigate the concepts of locality and compositionality in the context of learning representations for zero-shot learning. We analyze the influence of local information and the ability to compose different attributes or concepts to enhance the performance of zero-shot learning algorithms. Through our experiments, we demonstrate the importance of incorporating these factors into the learning process for improved zero-shot learning performance. Our findings contribute to a better understanding of how locality and compositionality impact the representation learning process in zero-shot learning tasks.",
        "title": "Locality and compositionality in zero-shot learning"
    },
    {
        "abs": "This study explores the concept of training machine learning models that are fair, meaning they exhibit equal performance across different groups. To achieve this, the authors propose a technique called Sensitive Subspace Robustness, which aims to ensure that the model remains unbiased even when sensitive features are present. The research investigates various approaches to train fair ML models and evaluate their performance. Results indicate that the proposed method successfully trains fair models, providing insights for future developments in this field.",
        "title": "Training individually fair ML models with Sensitive Subspace Robustness"
    },
    {
        "abs": "In recent times, neural message passing algorithms have achieved remarkable success in semi-supervised classification on graphs. However, these algorithms often struggle with effectively propagating information throughout the entire graph. In this study, we introduce a novel approach by incorporating personalized PageRank into graph neural networks. Our method, titled \"Predict then Propagate,\" improves the information propagation capabilities of graph neural networks, leading to enhanced performance in semi-supervised classification tasks on graphs. Through extensive experiments, we demonstrate the effectiveness of our approach and its superiority over existing techniques. Overall, our work presents a compelling fusion of graph neural networks and personalized PageRank, offering promising advancements in graph-based learning tasks.",
        "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank"
    },
    {
        "abs": "Deep Reinforcement Learning (DeepRL) has been receiving increasingly more attention thanks to its encouraging results in various domains. One critical aspect that has been identified as vital for the success of DeepRL is regularization. Regularization techniques play a crucial role in policy optimization by preventing overfitting and improving generalization capabilities. This paper explores the importance of regularization in DeepRL and highlights its impact on policy optimization. Through experiments and empirical analysis, we demonstrate that regularization techniques significantly enhance the learning process and help achieve better performance in DeepRL tasks. Our findings emphasize the need to incorporate effective regularization methods in DeepRL algorithms to effectively address the challenges associated with training deep neural networks for reinforcement learning.",
        "title": "Regularization Matters in Policy Optimization"
    },
    {
        "abs": "In this study, we investigate a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss. We analyze the loss landscape of these networks and demonstrate the absence of bad local valleys. This finding suggests that gradient-based optimization methods can effectively escape poor local optima and converge to a good solution. Our results provide insights into the optimization behavior of deep neural networks and have implications for improving training algorithms in this field.",
        "title": "On the loss landscape of a class of deep neural networks with no bad local valleys"
    },
    {
        "abs": "This paper proposes a theoretical framework for analyzing the properties of deep and locally connected non-linear networks, specifically focusing on ReLU networks. The study aims to better understand the theoretical aspects of these networks, particularly deep convolutional neural networks. By establishing a comprehensive theoretical framework, researchers can gain insights into the behavior and capabilities of these networks, ultimately facilitating their optimal design and application in various domains.",
        "title": "A theoretical framework for deep locally connected ReLU network"
    },
    {
        "abs": "Generative adversarial networks (GANs) have proven to be capable of modeling complex high-dimensional distributions of real-world data. In this paper, we propose an efficient anomaly detection approach based on GANs. By utilizing the generative and discriminative capabilities of GANs, our method is able to accurately distinguish anomalous instances from the normal data distribution. Our experimental results demonstrate the superior performance of the proposed approach compared to existing methods on various benchmark datasets. Furthermore, we also present insights into the interpretability and efficiency aspects of our approach, making it a promising solution for anomaly detection tasks.",
        "title": "Efficient GAN-Based Anomaly Detection"
    },
    {
        "abs": "This short concise abstract describes the commonality between most state-of-the-art neural machine translation systems despite their different architectural skeletons (e.g., recurrence, convolutional). Specifically, it focuses on phrase-based attentions, which are essential components that enable efficient and accurate translation results. By highlighting the significance of phrase-based attentions, this abstract provides insights into the unifying factor among diverse neural machine translation approaches.",
        "title": "Phrase-Based Attentions"
    },
    {
        "abs": "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct Confidence Sets for Deep Neural Networks, referred to as PAC Confidence Sets. This algorithm aims to provide accurate and reliable uncertainty estimates for the predictions of deep neural networks. By incorporating calibrated prediction and generalization bounds, our method enables the construction of confidence sets that are both statistically valid and interpretable. Experimental results demonstrate the efficacy of our approach in producing well-calibrated confidence sets for deep neural networks.",
        "title": "PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction"
    },
    {
        "abs": "The rate-distortion-perception function (RDPF) has gained recognition as a valuable tool in various applications. In this paper, we explore a coding theorem for the RDPF, proposed by Blau and Michaeli in 2019. By examining the relationship between rate, distortion, and perception, we aim to provide a clear understanding of the coding capabilities of the RDPF. Through our analysis, we intend to establish the significance and applicability of this theorem in practical scenarios.",
        "title": "A coding theorem for the rate-distortion-perception function"
    },
    {
        "abs": "In this paper, we propose Variational Recurrent Neural Networks (VRNNs) as a solution to the problem of graph classification using solely structural information. Taking inspiration from natural processes, VRNNs offer a novel approach for effectively classifying graphs by leveraging the inherent structural characteristics. Through our experiments, we demonstrate the superior performance of VRNNs compared to existing methods, highlighting their potential for applications in various domains requiring graph classification.",
        "title": "Variational Recurrent Neural Networks for Graph Classification"
    },
    {
        "abs": "This abstract discusses the Lottery Ticket Hypothesis, which proposes that neural network pruning techniques can significantly reduce the parameter count of trained networks by over 90%. Pruning involves removing unnecessary connections or neurons from the network, resulting in a sparser and more efficient model. This hypothesis sheds light on the potential for finding trainable neural networks with fewer parameters, thus enabling faster training and improved performance.",
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
    },
    {
        "abs": "Generative Adversarial Networks (GANs) have gained recognition for their ability to generate visually appealing samples. This article explores GANs from a variational inequality perspective, highlighting their potential for generative modeling. By understanding the underlying principles and potential applications of GANs, researchers can further enhance their effectiveness in producing high-quality samples.",
        "title": "A Variational Inequality Perspective on Generative Adversarial Networks"
    },
    {
        "abs": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework capable of inferring and learning Hamiltonian dynamics with control. SymODEN utilizes symplectic integration schemes and neural networks to model and simulate Hamiltonian systems accurately. Our framework offers the ability to learn and predict the behavior of complex physical systems efficiently and accurately, enabling applications in various domains such as robotics, physics simulations, and control engineering.",
        "title": "Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control"
    },
    {
        "abs": "Graph embedding techniques have gained much attention in various applications that require accurate and scalable representations of graphs. In this paper, we propose GraphZoom, a novel multi-level spectral approach for graph embedding. GraphZoom leverages the spectral clustering algorithm to recursively partition the input graph into multiple levels of subgraphs. With these subgraphs, GraphZoom employs a graph spectral transformation technique to embed the graph into a low-dimensional space while preserving its structural information. Experimental results demonstrate that GraphZoom achieves superior performance in terms of accuracy and scalability compared to existing methods.",
        "title": "GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding"
    },
    {
        "abs": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization algorithms is the presence of stragglers, which are workers that perform much slower than other workers. In this paper, we propose Anytime MiniBatch, a novel approach that exploits the presence of stragglers in online distributed optimization. We demonstrate through theoretical analyses and empirical evaluations that Anytime MiniBatch can significantly reduce the overall training time and achieve competitive convergence rates compared to existing algorithms, making it a promising technique for improving the efficiency of distributed optimization in machine learning.",
        "title": "Anytime MiniBatch: Exploiting Stragglers in Online Distributed Optimization"
    },
    {
        "abs": "This study explores the challenges of scaling end-to-end reinforcement learning for controlling real robots from visual input. The focus is on decoupling feature extraction from policy learning and assessing the benefits of state representation learning in goal-based robotics. By addressing these challenges, this research aims to improve the effectiveness and applicability of reinforcement learning methods in real-world robotic systems.",
        "title": "Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics"
    },
    {
        "abs": "Abstract: \nThe central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparse or delayed. In this paper, we propose a novel approach called InfoBot, which tackles this challenge by incorporating an information bottleneck into the learning process. InfoBot learns to transfer knowledge from a set of auxiliary tasks to a target task, by compressing the input information to retain only the relevant features. Our experiments demonstrate that InfoBot outperforms traditional reinforcement learning methods on various challenging tasks, showing its potential for effective policy discovery in domains with sparse rewards.",
        "title": "InfoBot: Transfer and Exploration via the Information Bottleneck"
    },
    {
        "abs": "Multilingual machine translation refers to the process of translating multiple languages using a single neural network model. This approach has gained significant interest in recent years. In this study, we investigate the effectiveness of knowledge distillation in improving the performance of multilingual machine translation systems. Our findings suggest that incorporating knowledge distillation techniques can enhance the accuracy and efficiency of multilingual translation models.",
        "title": "Multilingual Neural Machine Translation with Knowledge Distillation"
    },
    {
        "abs": "This article introduces PyTorch Geometric, a powerful library designed for deep learning on irregularly structured input data. PyTorch Geometric enables fast graph representation learning, making it ideal for scenarios where the input data is in the form of graphs. With this library, efficient and accurate graph-based models can be built using PyTorch, opening up possibilities for a wide range of applications.",
        "title": "Fast Graph Representation Learning with PyTorch Geometric"
    },
    {
        "abs": "This paper explores the diagnostics and improvements of variational autoencoder (VAE) models, which are widely used deep generative models. Despite their extensive utilization, several aspects of VAE models remain unclear, prompting the need for a comprehensive analysis. By examining the various challenges faced by VAEs, this study aims to enhance their performance and provide insights into their underlying mechanisms.",
        "title": "Diagnosing and Enhancing VAE Models"
    },
    {
        "abs": "Adversarial training is a training scheme specifically developed to mitigate adversarial attacks through the augmentation of the training process.",
        "title": "Bridging Adversarial Robustness and Gradient Interpretability"
    },
    {
        "abs": "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held in 2020. The workshop focused on the application of computer vision techniques in the field of agriculture. This abstract provides a concise overview of the workshop and its objectives.",
        "title": "Proceedings of the ICLR Workshop on Computer Vision for Agriculture (CV4A) 2020"
    },
    {
        "abs": "The 1st AfricaNLP Workshop Proceedings showcase the research and findings presented at the workshop held on April 26th, 2020, in conjunction with the virtual conference of ICLR 2020. This compilation of papers highlights the latest advancements and discussions in the field of Natural Language Processing (NLP) specific to the African context. Researchers, practitioners, and enthusiasts can benefit from this concise collection of abstracts, exploring the diverse applications, challenges, and solutions in African NLP.",
        "title": "1st AfricaNLP Workshop Proceedings, 2020"
    },
    {
        "abs": "This work showcases the initial outcomes of employing deep multi-task learning techniques in the field of histo-pathology, aiming to develop a widely generalizable model. Through this approach, we demonstrate the potential to concurrently tackle multiple histology-related tasks while creating a model that can be applied across diverse domains.",
        "title": "Multi-Task Learning in Histo-pathology for Widely Generalizable Model"
    },
    {
        "abs": "This study explores the emergence of compositional languages in a neural iterated learning model. The principle of compositionality, known for allowing natural language to represent complex concepts via a structured framework, is investigated. A concise abstract summarizing the findings and implications of this research is presented.",
        "title": "Compositional Languages Emerge in a Neural Iterated Learning Model"
    },
    {
        "abs": "Text generation is a fundamental component in various natural language processing tasks, ranging from summarization to dialogue and machine translation. Residual energy-based models have emerged as a promising approach for achieving effective text generation. This paper explores the ubiquity of text generation in NLP tasks and highlights the significance of utilizing residual energy-based models in order to enhance the quality and fluency of generated text.",
        "title": "Residual Energy-Based Models for Text Generation"
    },
    {
        "abs": "In this study, we propose an energy-based model (EBM) for protein conformations that operates at the atomic scale. The EBM utilizes energy principles to describe the structural arrangement of protein molecules. By considering the interactions between atoms, we aim to accurately predict the 3D shape and dynamics of proteins. This approach holds promise in deciphering the intricate folding pathways and functional properties of proteins, providing valuable insights into their biological roles.",
        "title": "Energy-based models for atomic-resolution protein conformations"
    },
    {
        "abs": "In this study, we prove that the Reproducing Kernel Hilbert Spaces (RKHS) of a Deep Neural Tangent Kernel (DNTK) and Laplace Kernel are the same. By establishing this equivalence, we provide a deeper understanding of the inner workings of deep neural networks and their relationship to other kernel methods. This result holds significant implications for the theoretical analysis and practical application of deep learning techniques.",
        "title": "Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS"
    },
    {
        "abs": "In this study, we propose a novel approach to embed directed graphs into low-dimensional statistical manifolds. Our method builds upon existing techniques, adapting them to effectively capture the complex structure and dependencies inherent in directed graphs. By mapping nodes to statistical manifolds, we can better analyze and interpret their relationships. Our experimental results demonstrate the efficacy of our approach in uncovering meaningful patterns and improving graph analysis tasks. This work opens new avenues for studying directed graphs and their underlying statistical properties.",
        "title": "Low-dimensional statistical manifold embedding of directed graphs"
    },
    {
        "abs": "This paper introduces Mixed-curvature Variational Autoencoders, a new approach in machine learning applications that deviates from the traditionally employed Euclidean geometry. Euclidean geometry has long been recognized as the dominant paradigm in machine learning; however, this research aims to explore alternative geometries to enhance the performance of Variational Autoencoders. The proposed algorithm introduces mixed-curvature spaces and demonstrates their effectiveness in optimizing variational inference and latent space exploration. Through this novel approach, the paper seeks to address the limitations of Euclidean geometry and pave the way for improved machine learning applications.",
        "title": "Mixed-curvature Variational Autoencoders"
    },
    {
        "abs": "In this study, we focus on the training of Convolutional Neural Networks (CNNs) with Rectified Linear Unit (ReLU) activations. We propose a novel approach that introduces exact convex regularizers for CNN architecture optimization. Specifically, we investigate the optimization of two- and three-layer networks using convex techniques, allowing us to achieve polynomial time complexity. Our findings demonstrate the effectiveness of implicit convex regularizers in improving CNN performance, providing valuable insights for future CNN design and training.",
        "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time"
    },
    {
        "abs": "We propose a new metric space of ReLU activation code equipped with a truncated Hamming distance for evaluating network quality beyond accuracy. This metric space provides a basis for measuring the performance and robustness of neural networks by quantifying the similarity between their activation codes. By incorporating the truncated Hamming distance, our approach provides a concise and efficient way to assess network quality, allowing for more comprehensive evaluations beyond traditional accuracy measurements.",
        "title": "ReLU Code Space: A Basis for Rating Network Quality Besides Accuracy"
    },
    {
        "abs": "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground assessments for livestock in Northern Kenya. Through satellite-based prediction, this study aims to provide a reliable tool for predicting and monitoring forage conditions, which are crucial for sustainable livestock management in the region.",
        "title": "Satellite-based Prediction of Forage Conditions for Livestock in Northern Kenya"
    },
    {
        "abs": "In this study, we propose a neural network approach for unsupervised anomaly detection using a novel robust subspace recovery layer. Our method aims to accurately identify anomalies in data by leveraging the power of neural networks and the efficiency of subspace recovery techniques. Through extensive experiments, we demonstrate the effectiveness and robustness of our approach in detecting anomalies across various datasets. Our proposed method offers a promising solution for anomaly detection in real-world applications where labeled data for training is limited or unavailable.",
        "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection"
    },
    {
        "abs": "The impressive lifelong learning observed in animal brains is primarily attributed to the plastic changes occurring at the synaptic level. These changes facilitate the modification of neural connections, enabling animals to adapt to new experiences and acquire new skills. In order to simulate this capability in artificial neural networks, a novel approach called Backpropamine is proposed. Backpropamine combines traditional backpropagation with differentiable neuromodulated plasticity, allowing for self-modification of neural networks during training. This innovation holds promise in advancing the field of artificial intelligence towards achieving truly adaptable and lifelong learning systems.",
        "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity"
    },
    {
        "abs": "The inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the efficiency and accuracy of various farming processes. One such application is the detection of apple defects using deep learning-based object detection algorithms. This abstract explores the potential benefits of deploying such technology in post-harvest handling of apples, leading to improved crop quality, reduced wastage, and enhanced profitability for farmers.",
        "title": "Apple Defect Detection Using Deep Learning Based Object Detection For Better Post Harvest Handling"
    },
    {
        "abs": "Recent advances in neural machine translation (NMT) have achieved state-of-the-art results for many European languages. However, the application of NMT to South Africa's official languages is an underexplored area. This paper aims to explore the potential of NMT for translating the official languages of South Africa by examining current research and identifying the challenges and opportunities in adapting NMT models for these languages. The findings of this study will contribute to the development of language technology and enable effective communication across South Africa's diverse linguistic landscape.",
        "title": "Neural Machine Translation for South Africa's Official Languages"
    },
    {
        "abs": "In this study, we propose an algorithm that combines calibrated prediction and generalization bounds from learning theory to construct PAC confidence sets for deep neural networks. By leveraging both calibrated prediction and generalization bounds, our algorithm provides a reliable estimation of the predictive uncertainty in deep neural networks. This approach enhances the interpretability and reliability of deep neural network predictions, making them more suitable for real-world applications where confidence and reliability are crucial factors.",
        "title": "PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction"
    },
    {
        "abs": "With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there is a need to investigate the extent to which these models are aware of phrases. This study proposes simple yet strong baselines for grammar induction to evaluate the phrase awareness capabilities of pre-trained LMs. By examining their performance on this task, we aim to gain insights into the underlying linguistic knowledge encoded by these models.",
        "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction"
    },
    {
        "abs": "Magnitude-based pruning is a straightforward technique commonly used for reducing the complexity of neural networks. While its simplicity is advantageous, this method lacks the ability to consider future weight magnitudes during the pruning process. In this paper, we propose a novel alternative called Lookahead, which incorporates a far-sighted approach to pruning. Lookahead aims to improve upon magnitude-based pruning by considering the potential impact of weight magnitudes in the future, thereby enhancing the overall efficiency and performance of neural network pruning.",
        "title": "Lookahead: A Far-Sighted Alternative of Magnitude-based Pruning"
    },
    {
        "abs": "As the share of renewable energy sources in the present electric energy mix rises, there is a growing need to optimize and enhance renewable electricity consumption. This study proposes a novel approach utilizing reinforcement learning to advance the integration of renewable energy sources into the current grid infrastructure. Through the utilization of intelligent algorithms, this research aims to develop effective strategies to maximize renewable electricity consumption, ultimately contributing to a sustainable and reliable future energy system.",
        "title": "Advancing Renewable Electricity Consumption With Reinforcement Learning"
    },
    {
        "abs": "In this study, we present our findings on the development of a domain-specific Tigrinya-to-English neural machine translation system. By utilizing transfer learning techniques, we report our experiments and results in addressing the challenge of language translation for humanitarian response efforts. This paper provides a concise overview of our methodology and outcomes, highlighting the potential of neural machine translation in facilitating effective communication and aiding humanitarian initiatives.",
        "title": "Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian Response"
    },
    {
        "abs": "Nigerian Pidgin is a widely spoken language in Nigeria, with various dialects. This study aims to develop baseline models for Neural Machine Translation (NMT) systems in both supervised and unsupervised settings for Nigerian Pidgin. These baselines will serve as a foundation for future research to improve translation quality and aid in preserving the linguistic diversity of the language.",
        "title": "Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin"
    },
    {
        "abs": "Estimating grape yield prior to harvest is crucial for commercial vineyard production as it provides valuable information for planning and decision-making. In this study, we propose a novel approach for estimating grape yield on the vine using multiple images. By analyzing these images and applying advanced image processing techniques, our method accurately predicts grape yield, enabling vineyard owners to make informed decisions regarding crop management and production strategies. This innovative approach has the potential to revolutionize the grape industry by improving efficiency, reducing costs, and optimizing yields.",
        "title": "Estimating Grape Yield on the Vine from Multiple Images"
    },
    {
        "abs": "Automatic change detection and disaster damage assessment are procedures requiring a huge amount of time and resources. In this paper, we propose a novel approach to building disaster damage assessment in satellite imagery using multi-temporal fusion. Our method utilizes advanced machine learning techniques to automatically detect changes in satellite imagery over time and assess the extent of damage caused by disasters. Experimental results demonstrate the efficacy of our approach in accurately identifying and quantifying building damage, significantly reducing the time and resources required for manual assessment. This research has the potential to revolutionize the field of disaster response and recovery, providing timely and accurate information to aid decision-making processes.",
        "title": "Building Disaster Damage Assessment in Satellite Imagery with Multi-Temporal Fusion"
    },
    {
        "abs": "Previous work suggests that recurrent neural networks (RNNs), which are non-linear dynamic systems, may exhibit chaotic behavior. This abstract explores the extent to which RNNs can be considered chaotic based on existing research.",
        "title": "How Chaotic Are Recurrent Neural Networks?"
    },
    {
        "abs": "Fine-tuning a pretrained BERT model is currently considered the most advanced technique for extracting or abstracting text. In this study, we focus on applying this approach specifically for Arabic text summarization.",
        "title": "BERT Fine-tuning For Arabic Text Summarization"
    },
    {
        "abs": "In the field of cluster analysis for residential energy consumption patterns, domain experts and visual analysis have traditionally been used to identify optimal clustering structures. However, these methods may be subjective and time-consuming. In this study, we propose an alternative approach of utilizing competency questions to select the most appropriate clustering structures. This method aims to provide an objective and efficient means for identifying energy consumption patterns, leading to more effective energy management strategies.",
        "title": "Using competency questions to select optimal clustering structures for residential energy consumption patterns"
    },
    {
        "abs": "This paper focuses on the occurrence of action and observation delays in various Reinforcement Learning applications, particularly in remote control scenarios. The study aims to address the challenge of random delays encountered in these systems. By exploring different approaches and techniques, the research aims to improve the performance and effectiveness of Reinforcement Learning algorithms in dealing with delays.",
        "title": "Reinforcement Learning with Random Delays"
    },
    {
        "abs": "This abstract highlights that differentially private machine learning techniques have not yet achieved a significant breakthrough comparable to the impact made by AlexNet. It suggests that in order to reach such a moment, the approach requires better features or a substantial increase in available data.",
        "title": "Differentially Private Learning Needs Better Features (or Much More Data)"
    },
    {
        "abs": "In this paper, we present Symplectic ODE-Net (SymODEN), a deep learning framework that has the ability to infer and learn Hamiltonian dynamics with control. SymODEN utilizes symplectic integration schemes to ensure numerical stability, accuracy, and conservation of phase-space volume. By leveraging its ability to handle control inputs, SymODEN offers a powerful approach for modeling and exploring complex dynamical systems with Hamiltonian structures. We demonstrate the effectiveness and versatility of SymODEN through experimental results on various simulated systems.",
        "title": "Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control"
    },
    {
        "abs": "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of complex systems. By incorporating symplectic integration methods, SRNNs can preserve the symplectic structure of the underlying physical systems and ensure accurate long-term predictions. Through experiments on various dynamical systems, we demonstrate that SRNNs outperform traditional recurrent neural networks in terms of stability and accuracy, making them a promising tool for modeling and understanding complex dynamics.",
        "title": "Symplectic Recurrent Neural Networks"
    },
    {
        "abs": "Anomaly detection, the task of identifying patterns that significantly deviate from previously observed data, is crucial in various domains. This paper proposes a classification-based approach for detecting anomalies in general data. By harnessing machine learning techniques, our method effectively classifies data instances as normal or anomalous, enabling accurate identification of deviations. Through experimentation, our approach demonstrates high performance and versatility in different application scenarios, highlighting its potential for robust anomaly detection in various domains.",
        "title": "Classification-Based Anomaly Detection for General Data"
    },
    {
        "abs": "In machine learning, it is crucial to ensure fairness to avoid biased decision-making. This paper proposes a novel approach called Sensitive Subspace Robustness, which focuses on training individual fair ML models. By considering the performance of these models in a fair manner, we aim to mitigate biases and promote equitable outcomes. This study explores the implications and benefits of this approach, highlighting its potential to address fairness concerns in machine learning.",
        "title": "Training individually fair ML models with Sensitive Subspace Robustness"
    },
    {
        "abs": "In this paper, we explore the use of self-supervised representation learning to enhance the efficiency of sample utilization in reinforcement learning. We propose a method called Dynamics-aware Embeddings, which aims to capture the underlying dynamics of the environment during the learning process. By leveraging these embeddings, our approach enables more effective and efficient exploration and planning in reinforcement learning tasks. We conduct experiments to demonstrate the effectiveness of our method, showcasing its ability to achieve improved sample efficiency and better performance compared to existing approaches.",
        "title": "Dynamics-aware Embeddings"
    },
    {
        "abs": "In this paper, we introduce SenSeI, a novel approach for achieving individual fairness in machine learning models. We present a formulation that casts fair machine learning as invariant machine learning. Our proposed method ensures sensitivity to individual characteristics by enforcing set invariance, resulting in fair treatment regardless of an individual's sensitive attributes. By addressing the issue of individual fairness, we aim to contribute to the development of unbiased and equitable machine learning practices.",
        "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness"
    },
    {
        "abs": "Despite significant advances, continua learning models still suffer from catastrophic forgetting when exposed to incrementally.",
        "title": "Graph-Based Continual Learning"
    },
    {
        "abs": "In this paper, we propose a general self-attention formulation to enforce group equivariance for arbitrary symmetry groups in the context of vision tasks. We demonstrate the effectiveness of our approach through experimental evaluations on various visual recognition benchmarks. Our formulation allows for improved performance in capturing global contextual dependencies while preserving local details, resulting in state-of-the-art results in several vision tasks. These findings highlight the importance of incorporating group equivariance in self-attention mechanisms for enhanced vision models.",
        "title": "Group Equivariant Stand-Alone Self-Attention For Vision"
    },
    {
        "abs": "We propose studying the problem of few-shot graph classification in graph neural networks. To address the challenge of limited labeled data, we introduce a novel approach based on super-classes that leverage graph spectral measures. Our work aims to enhance the performance of few-shot learning tasks on graphs by providing a more robust and effective classification framework.",
        "title": "Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures"
    },
    {
        "abs": "In this work, we investigate the positional encoding methods used in language pre-training, such as BERT. We critically analyze the existing approaches and propose a rethinking of positional encoding to enhance language understanding. By exploring alternative ways to encode positional information, we aim to improve the effectiveness and efficiency of language pre-training models.",
        "title": "Rethinking Positional Encoding in Language Pre-training"
    },
    {
        "abs": "Graph embedding techniques have been widely utilized in various applications that involve complex data structures such as social networks, citation networks, and recommendation systems. However, existing embedding methods often struggle with accuracy and scalability, especially when dealing with large-scale graphs. In this paper, we propose GraphZoom, a novel multi-level spectral approach for graph embedding. By leveraging the power of multi-level decomposition and spectral clustering, GraphZoom achieves both high embedding accuracy and scalability. Experimental results demonstrate the effectiveness and efficiency of our proposed method compared to state-of-the-art approaches.",
        "title": "GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding"
    },
    {
        "abs": "This article proposes a novel approach called DDPNOpt, which interprets the training of Deep Neural Networks (DNNs) as an optimal control problem with nonlinear dynamics. By leveraging Differential Dynamic Programming, DDPNOpt optimizes the DNNs more efficiently and effectively than traditional methods. The abstract briefly introduces the concept and significance of DDPNOpt in training DNNs as an optimal control problem with nonlinear dynamics.",
        "title": "DDPNOpt: Differential Dynamic Programming Neural Optimizer"
    },
    {
        "abs": "In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing double-blind review. Specifically, we focus on the de-anonymization of authors through these preprints. We analyze the potential consequences of such releases and discuss the implications for the review process. By examining the impact on author anonymity and reviewing bias, our findings shed light on the benefits and drawbacks of arXiv preprint dissemination during double-blind review.",
        "title": "De-anonymization of authors through arXiv submissions during double-blind review"
    },
    {
        "abs": "Reinforcement learning (RL) has demonstrated remarkable achievements in various online settings. However, the high sample complexity and potential risks associated with online learning pose challenges. To address these limitations, our study proposes OPAL, a novel approach that leverages offline primitive discovery to accelerate RL in an offline setting. OPAL intelligently explores the environment by mining offline data and discovering valuable primitives, allowing for more efficient learning and reducing the reliance on online interactions. Through experiments, we demonstrate the effectiveness of OPAL in accelerating offline RL while maintaining performance levels comparable to traditional online methods.",
        "title": "OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning"
    },
    {
        "abs": "The Stochastic Gradient Descent (SGD) and its variants are widely used methods for training deep neural networks. This paper proposes a diffusion theory for understanding the dynamics of SGD in deep learning. The theory suggests that SGD exponentially favors flat minima during the learning process. This finding has implications for the optimization landscape of deep learning models and can potentially lead to improvements in training algorithms.",
        "title": "A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima"
    },
    {
        "abs": "Spectral embedding is a widely used technique for representing graph data. Regularization techniques are often employed to improve the quality of the embeddings. This study explores the spectral embedding of regularized block models, aiming to enhance the accuracy and reliability of graph representations. The research highlights the significance of regularization in improving the performance of spectral embeddings and discusses the potential applications of this approach in various domains.",
        "title": "Spectral embedding of regularized block models"
    },
    {
        "abs": "In this work, we investigate the effects of locality and compositionality on learning representations for zero-shot learning. Our study focuses on understanding the impact of these factors in improving the performance and generalization capabilities of models in the context of zero-shot learning tasks. By analyzing the relationship between locality, which captures the spatial relationships between features, and compositionality, which explores the semantic interactions between different components, we aim to provide insights into developing more effective zero-shot learning approaches.",
        "title": "Locality and compositionality in zero-shot learning"
    },
    {
        "abs": "In this study, we address the problem of learning permutation invariant representations that can effectively capture flexible concepts. Specifically, we explore the concept of representation learning with multisets. By focusing on capturing the inherent flexibility in the data, our approach aims to overcome the limitations of traditional permutation invariant methods. This abstract provides a concise overview of our research endeavor.",
        "title": "Representation Learning with Multisets"
    },
    {
        "abs": "Deep Reinforcement Learning (DeepRL) has been receiving increasingly more attention thanks to its encouraging results in solving challenging problems. However, in policy optimization, the issue of overfitting poses a significant challenge. This paper explores the importance of regularization techniques in DeepRL and demonstrates their effectiveness in improving the generalization and robustness of learned policies. Through empirical evaluations, various regularization methods are examined, including L1/L2 regularization, dropout, and batch normalization. The results show that regularization plays a crucial role in addressing overfitting and enhancing the performance of DeepRL algorithms. This work highlights the necessity of considering regularization techniques when applying DeepRL in policy optimization tasks.",
        "title": "Regularization Matters in Policy Optimization"
    },
    {
        "abs": "In time series classification, the receptive field (RF) size has long been a critical factor in determining the performance of convolutional neural networks (CNNs). This paper introduces Omni-Scale CNNs, a novel approach that achieves simple and effective configuration of kernel sizes for time series classification tasks. By incorporating multiple kernel sizes within a single network, Omni-Scale CNNs are capable of capturing features at different scales and achieve improved classification accuracy. Experimental results on various benchmark datasets demonstrate the effectiveness and efficiency of Omni-Scale CNNs compared to traditional approaches. Overall, this work presents a valuable contribution to the field of time series classification by addressing the important aspect of RF size in CNNs.",
        "title": "Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification"
    },
    {
        "abs": "Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization is the presence of stragglers, which significantly slow down the overall convergence of the algorithm. In this study, we propose the Anytime MiniBatch algorithm, a novel approach that exploits the presence of stragglers in online distributed optimization. Our algorithm leverages the asynchronous nature of distributed systems to achieve anytime convergence and improve overall optimization efficiency. Through extensive experiments, we demonstrate the effectiveness of our approach in reducing the impact of stragglers and improving the convergence time of distributed optimization algorithms.",
        "title": "Anytime MiniBatch: Exploiting Stragglers in Online Distributed Optimization"
    },
    {
        "abs": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning, co-located with ICLR 2021. This workshop aims to bring together researchers and practitioners working on weakly supervised learning, a challenging but promising field that involves training models with limited or noisy labels. By providing a platform for discussion and collaboration, WeaSuL seeks to share recent advancements and explore novel solutions to the problem of weak supervision. Join us at this workshop to uncover new insights, exchange ideas, and foster advancements towards more effective and practical weakly supervised learning techniques.",
        "title": "Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL)"
    },
    {
        "abs": "FFPDG: Fast, Fair and Private Data Generation\n\nThis paper introduces FFPDG, a novel approach to synthetic data generation that combines generative modeling with a focus on fairness and privacy. Despite the common utilization of generative modeling for synthetic data generation, the concepts of fairness and privacy have often been overlooked. FFPDG aims to bridge this gap by providing a fast and efficient method for generating data that adheres to both fairness and privacy principles. The proposed approach holds promise in enabling researchers and data scientists to generate diverse synthetic datasets that are representative, unbiased, and maintain individual privacy, thus advancing the field towards more ethical and inclusive data generation practices.",
        "title": "FFPDG: Fast, Fair and Private Data Generation"
    },
    {
        "abs": "Few-shot learning refers to the task of training a model with a limited number of samples. However, this is challenging as the model can easily overfit to the small dataset. In this study, we introduce a novel approach called distribution calibration, which aims to mitigate the overfitting issue in few-shot learning. Our results demonstrate that distribution calibration effectively improves the performance of the learned model, providing a promising avenue for further advancements in few-shot learning.",
        "title": "Free Lunch for Few-shot Learning: Distribution Calibration"
    },
    {
        "abs": "This paper explores the mapping between Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs), two influential models commonly used in the field of artificial intelligence. The abstract provides a brief overview of the significance and relationship between these models.",
        "title": "On the mapping between Hopfield networks and Restricted Boltzmann Machines"
    },
    {
        "abs": "Graph neural networks (GNNs) have emerged as a potent tool for modeling algorithmic reasoning procedures. With their strong inductive bias, GNNs offer an effective means of capturing and analyzing complex relationships among entities represented as graphs. This abstract provides a concise overview of how GNNs facilitate algorithmic reasoning and their potential applications in various domains.",
        "title": "Persistent Message Passing"
    },
    {
        "abs": "This abstract introduces the theory of implicit deep learning, specifically focusing on a novel approach called Adeepequilibriummodel. The model utilizes implicit layers that are implicitly defined through an equilibrium point. The abstract briefly outlines the concept, highlighting its relevance in achieving global convergence.",
        "title": "On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers"
    },
    {
        "abs": "The continual learning capability without forgetting past tasks is a desirable attribute for intelligent systems. In this paper, we propose a novel method called Gradient Projection Memory (GPM) to address the issue of catastrophic forgetting in the context of continual learning. By utilizing a memory module, GPM preserves the knowledge of previous tasks and selectively updates the weights to adapt to new tasks. Experimental results demonstrate that GPM outperforms state-of-the-art methods in terms of both accuracy and memory efficiency, showcasing its potential for enabling lifelong learning in intelligent systems.",
        "title": "Gradient Projection Memory for Continual Learning"
    },
    {
        "abs": "Abstract: In high-dimensional state spaces, the efficacy of Reinforcement Learning (RL) is constrained by the problem of sparse rewards. This limitation makes it difficult to achieve successful goal-directed tasks. To overcome this challenge, we propose a novel approach called Plan-Based Relaxed Reward Shaping. This method combines the advantages of both RL and planning, allowing for the creation of intermediate goals that guide the learning process and provide more frequent rewards. By shaping the reward function, we enable RL algorithms to effectively navigate complex environments, leading to improved performance in goal-oriented tasks. Our experimental results demonstrate the effectiveness of our approach on various challenging problems in high-dimensional state spaces.",
        "title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks"
    },
    {
        "abs": "In this research, we propose a novel approach to enhance exploration in policy gradient search methods by incorporating neural networks. Specifically, we focus on automating mathematical tasks through the utilization of neural networks for large-scale symbolic optimization. Our study aims to improve the efficiency and effectiveness of the search process by developing new strategies within the framework of policy gradient optimization. By leveraging neural networks, we aim to achieve better exploration capabilities and ultimately enhance the overall performance of automated mathematical tasks.",
        "title": "Improving exploration in policy gradient search: Application to symbolic optimization"
    },
    {
        "abs": "This study focuses on the training of Convolutional Neural Networks (CNNs) with Rectified Linear Unit (ReLU) activations. We introduce exact convex regularizers for CNN architectures and propose an efficient convex optimization method for training two- and three-layer networks. By minimizing the proposed regularizers, we achieve improved performance on CNNs.",
        "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time"
    },
    {
        "abs": "In this paper, we address the problem of finding the optimal memoryless stochastic policy for infinite-horizon partially observable Markov decision processes (POMDPs). We propose a geometric approach to memoryless stochastic policy optimization, where we exploit the inherent structure of the POMDP to find the best policy. Our approach aims to maximize long-term rewards by considering the uncertainty in the system and making informed decisions under partial observability. Through our analysis and experiments, we demonstrate the effectiveness of our method in solving infinite-horizon POMDPs.",
        "title": "The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs"
    },
    {
        "abs": "Stochastic encoders, owing to their ability to be used in rate-distortion theory and neural compression, have become increasingly popular in various fields. This abstract aims to highlight the advantages of stochastic encoders and their potential applications in data compression and neural networks.",
        "title": "On the advantages of stochastic encoders"
    },
    {
        "abs": "This study focuses on the problem of learned transform compression, where the goal is to learn both the transform and the optimized entropy encoding for efficient data compression.",
        "title": "Learned transform compression with optimized entropy encoding"
    },
    {
        "abs": "This study focuses on improving simulations using Symmetry Control Neural Networks (SCNN), as the dynamics of physical systems are often limited to lower-dimensional sub-spaces. By leveraging SCNN, this research proposes a novel approach to enhance simulation accuracy and efficiency. The application of SCNN enables the identification and utilization of underlying symmetries in physical systems, leading to significant improvements in simulation outcomes. Through extensive experimentation, this study demonstrates the effectiveness of SCNN in enhancing simulation fidelity, thereby providing a promising avenue for advancing simulation methodologies.",
        "title": "Improving Simulations with Symmetry Control Neural Networks"
    },
    {
        "abs": "In this work, we investigate the performance of standard models used for community detection under spectral methods. Specifically, we focus on low-rank projections of Graph Convolutional Networks (GCNs) Laplacian. Our study aims to understand and evaluate the effectiveness of these models in identifying communities within complex networks. By examining the behavior of such models, our findings contribute to the advancement of community detection techniques and their applications in various domains.",
        "title": "Low-Rank Projections of GCNs Laplacian"
    },
    {
        "abs": "We propose a new framework for synthesizing data using deep generative models in a differentially private manner. Our method, called PEARL, leverages private embeddings and adversarial reconstruction learning to protect sensitive information while preserving the underlying data's statistical properties. Through extensive experiments, we demonstrate the effectiveness of PEARL in generating synthetic data that closely resembles the original data distribution while maintaining privacy guarantees. Overall, our work provides a powerful solution for data synthesis in a privacy-preserving manner.",
        "title": "PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning"
    },
    {
        "abs": "This paper focuses on understanding the phenomenon of dimensional collapse in contrastive self-supervised learning. Self-supervised visual representation learning involves learning valuable representations without depending on human annotations. The abstract aims to provide a brief overview of the research topic.",
        "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning"
    },
    {
        "abs": "In this paper, we propose a self-attention formulation that enables the imposition of group equivariance to arbitrary symmetry groups in vision tasks. Our approach provides a general solution for incorporating group equivariant properties into stand-alone self-attention mechanisms. By leveraging this formulation, we aim to improve the modeling capability of self-attention in capturing and handling symmetries present in visual data.",
        "title": "Group Equivariant Stand-Alone Self-Attention For Vision"
    },
    {
        "abs": "We propose the task of disambiguating symbolic expressions in informal STEM documents in the form. Our objective is to develop a system that can accurately identify and clarify the meaning of symbolic expressions used in these documents, which often lack precise definitions or context. Through the use of natural language processing techniques and machine learning algorithms, we aim to improve the comprehension and interpretation of these expressions, contributing to the overall understanding of informal STEM materials.",
        "title": "Disambiguating Symbolic Expressions in Informal Documents"
    },
    {
        "abs": "This study explores the concept of fairness in training classifiers by applying group fairness constraints and regularizing prediction disparities between different groups. The proposed approach, Fair Mixup, leverages interpolation techniques to mitigate biases and achieve fair predictions. Through experiments and evaluations, the effectiveness of Fair Mixup in promoting fairness is demonstrated.",
        "title": "Fair Mixup: Fairness via Interpolation"
    },
    {
        "abs": "This article presents a novel approach called \"Improved Autoregressive Modeling with Distribution Smoothing\" to address the issue of low sample quality in autoregressive models used for image compression. The proposed method aims to enhance the performance of these models by implementing distribution smoothing techniques. By doing so, the article provides a solution for improving the sample quality in autoregressive models, thereby enhancing their effectiveness in image compression applications.",
        "title": "Improved Autoregressive Modeling with Distribution Smoothing"
    },
    {
        "abs": "Abstract: \nThis study presents an innovative approach for selecting sample weights in problems that involve continuous weight balancing. The proposed method aims to address the challenges posed by highly complex problems, providing a user-friendly and efficient solution. Through rigorous analysis and experimentation, we demonstrate the effectiveness and reliability of our approach in achieving optimal weight balancing outcomes. This research contributes to the field of continuous weight balancing and offers valuable insights for practitioners and researchers seeking to enhance the performance of weight distribution in various applications.",
        "title": "Continuous Weight Balancing"
    },
    {
        "abs": "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to investigate the emergence of abstract and episodic neurons in episodic meta-RL. We aim to understand the underlying mechanisms behind the formation of these types of neurons in deep reinforcement learning systems. By studying the reinstatement mechanism, we gain insights into how the representation of abstract concepts and episodic memories can be learned and utilized in the meta reinforcement learning framework. Our findings contribute to a deeper understanding of the cognitive processes involved in episodic meta-RL and have implications for the development of more advanced artificial intelligence systems.",
        "title": "The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL"
    },
    {
        "abs": "Deep neural networks are known to be vulnerable to small, adversarially crafted perturbations. The current approach to addressing this vulnerability involves training robust neural networks using sparse coding. This paper proposes a sparse coding frontend for robust neural networks, which enhances the network's ability to resist adversarial attacks. Experimental results demonstrate the effectiveness of the proposed approach in achieving robustness against adversarial perturbations.",
        "title": "Sparse Coding Frontend for Robust Neural Networks"
    },
    {
        "abs": "The abstract aims at summarizing the main content of the article in a concise manner. Based on the given title, we can write a short abstract as follows:\n\nThe Rate-Distortion-Perception (RDP) function developed by Blau and Michaeli in 2019 has proven to be a valuable tool in the field of coding. This theoretical framework allows for efficient encoding and decoding of information while considering the perception and quality of the output. By incorporating human perception into the encoding process, the RDP function offers improved performance compared to traditional rate-distortion methods. This article explores the key concepts of the RDP function and its potential applications in various domains.",
        "title": "A coding theorem for the rate-distortion-perception function"
    },
    {
        "abs": "This study investigates the limitations of graph neural network (GNN) architectures in detecting simple topological structures within the Bermuda Triangle region. Most GNNs rely on message-passing node vector embeddings over the adjacency matrix to capture graph information. However, our findings demonstrate that these architectures fail to effectively recognize and characterize basic topological structures associated with this mysterious area. The results shed light on the challenges of utilizing GNNs for anomaly detection in complex environments and highlight the need for more sophisticated approaches in analyzing intricate graph structures.",
        "title": "Bermuda Triangles: GNNs Fail to Detect Simple Topological Structures"
    },
    {
        "abs": "As machine learning continues to expand into diverse application domains, concerns related to privacy and security are becoming increasingly important. This paper focuses on the concept of privacy and integrity preserving training using trusted hardware. With the growing demand for sensitive data analysis, ensuring the protection of data privacy and maintaining the integrity of training processes has become critical. This abstract aims to provide an overview of the challenges and approaches associated with privacy and integrity preservation in machine learning training using trusted hardware.",
        "title": "Privacy and Integrity Preserving Training Using Trusted Hardware"
    },
    {
        "abs": "In this study, we propose a novel approach for enhancing the performance of the Hamiltonian Monte Carlo algorithm. By leveraging the capabilities of deep learning, we generalize the algorithm by incorporating a stack of neural network layers. This innovative approach aims to overcome the limitations of traditional Hamiltonian Monte Carlo in sampling complex posterior distributions. By augmenting the algorithm with neural networks, we demonstrate improved sampling efficiency and accuracy, making it a promising technique for various scientific and machine learning applications.",
        "title": "Deep Learning Hamiltonian Monte Carlo"
    },
    {
        "abs": "This study examines the effectiveness of concept bottleneck models in learning as intended. Concept bottleneck models are designed to map raw inputs to intermediate concepts, and then map these concepts to target outputs. Through an analysis of these models, this research aims to determine if they successfully learn and represent concepts as intended.",
        "title": "Do Concept Bottleneck Models Learn as Intended?"
    },
    {
        "abs": "In this paper, we propose a new data poisoning attack for deep reinforcement learning agents and apply it successfully. By strategically injecting in-distribution triggers into the training data, we examine the impact on the agent's decision-making process. Through our experiments, we demonstrate the susceptibility of deep reinforcement learning agents to this attack and emphasize the importance of developing robust defense mechanisms to prevent malicious exploitation.",
        "title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers"
    },
    {
        "abs": "In this paper, we propose a novel neuroevolutionary method called MONCAE (Multi-Objective Neuroevolution of Convolutional Autoencoders) to effectively identify the architecture and hyperparameters for convolutional autoencoders. By leveraging evolutionary algorithms, our approach aims to optimize multiple objectives simultaneously, including reconstruction error and sparsity, to obtain superior autoencoder models. We conduct thorough experiments to demonstrate the effectiveness and efficiency of our proposed method compared to traditional approaches. With its ability to automatically discover optimal structures and hyperparameters, MONCAE offers promising opportunities for the advancement of convolutional autoencoder research.",
        "title": "MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders"
    },
    {
        "abs": "Model-based reinforcement learning is a powerful approach that utilizes a world model to estimate the true environment and make approximations. In this study, we propose a probabilistic model-based policy search algorithm for learning robust controllers. Our approach leverages the estimated environment model to generate policy updates, thereby improving controller performance and adaptability. Through extensive experiments, we demonstrate the effectiveness of our algorithm in several challenging control tasks. Overall, our research contributes to the advancement of robust controller learning using probabilistic model-based reinforcement learning.",
        "title": "Learning Robust Controllers Via Probabilistic Model-Based Policy Search"
    },
    {
        "abs": "This paper focuses on the training and generating of neural networks using weight matrices as inputs and/or outputs. By operating in a compressed weight space, the efficiency and effectiveness of neural network modeling can be significantly improved. The proposed approach offers potential advancements in various fields, such as artificial intelligence and machine learning.",
        "title": "Training and Generating Neural Networks in Compressed Weight Space"
    },
    {
        "abs": "This paper presents the computational challenge on differential geometry and topology that took place within the scope of the ICLR 2021 conference. The challenge aimed to explore innovative techniques and approaches to tackle complex problems in computational geometry and topology. In this abstract, we summarize the design of the challenge and highlight the results obtained by participants, showcasing their advancements in the field.",
        "title": "ICLR 2021 Challenge for Computational Geometry & Topology: Design and Results"
    },
    {
        "abs": "The abstract for the topic \"Efficient Training Under Limited Resources\" can be:\n\nTraining time budget and size of the dataset are among the factors that significantly affect the performance of a training process. In scenarios where resources are limited, it is essential to devise efficient methodologies that optimize the training process. This paper aims to explore strategies for achieving higher performance with time and dataset constraints, presenting approaches to minimize resource consumption while maintaining or even enhancing training effectiveness.",
        "title": "Efficient Training Under Limited Resources"
    },
    {
        "abs": "In this paper, we propose SenSeI, a novel framework for achieving individual fairness in machine learning models. Our approach focuses on enforcing sensitive set invariance, which ensures that similar individuals from different sensitive groups are treated equally. By formulating fair machine learning as invariant machine learning, we address the challenge of discrimination by minimizing disparate treatment based on sensitive attributes. Through empirical evaluations, we demonstrate the effectiveness of SenSeI in attaining individual fairness while maintaining competitive model performance and generalizability. Our findings highlight the importance of considering sensitive set invariance for creating fair and unbiased machine learning systems.",
        "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness"
    },
    {
        "abs": "Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally learning new tasks. In order to address this challenge, graph-based approaches have emerged as a promising solution. By utilizing the relationships between different tasks as edges in a graph, these methods aim to mitigate catastrophic forgetting by leveraging knowledge transfer. This abstract provides an overview of the graph-based continual learning framework, discussing its advantages, challenges, and recent developments. Additionally, we highlight the potential applications and future directions for further improving the performance of graph-based continual learning models.",
        "title": "Graph-Based Continual Learning"
    },
    {
        "abs": "In this study, we examine the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and a Laplace kernel. Through rigorous analysis, we demonstrate that these two kernel functions have the same RKHS. This finding has significant implications for the use of deep neural networks and Laplace kernels in various machine learning applications.",
        "title": "Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS"
    },
    {
        "abs": "This paper explores the challenges posed by action and observation delays in reinforcement learning applications, specifically in the context of remote control scenarios. The authors discuss the common occurrence of delays and its impact on learning efficiency. They propose using random delays as a means to improve and optimize reinforcement learning algorithms. Through empirical evaluations, they demonstrate the effectiveness and potential benefits of incorporating random delays in the learning process. Overall, this research highlights the importance of addressing delays in reinforcement learning and presents a promising approach to enhance performance in real-world applications.",
        "title": "Reinforcement Learning with Random Delays"
    },
    {
        "abs": "In this study, we assess the current state of differentially private machine learning and argue that it has not yet reached its \"AlexNet moment.\" We demonstrate the need for better features or significantly more data to effectively apply differential privacy in machine learning tasks. This abstract highlights the ongoing challenges and suggests avenues for further research in improving the effectiveness of differentially private learning techniques.",
        "title": "Differentially Private Learning Needs Better Features (or Much More Data)"
    },
    {
        "abs": "This paper presents a novel algorithm for training individually fair learning-to-rank (LTR) models. The proposed approach prioritizes fairness at the individual level, aiming to provide fair rankings for each user. By considering a set of fairness constraints, our algorithm ensures that the ranking system is fair and unbiased, while still maintaining high relevance and performance. Experimental results demonstrate the effectiveness and efficiency of our approach in achieving individually fair rankings. Overall, our work contributes to the advancement of fair ranking methods and has implications for various applications such as search engines and recommender systems.",
        "title": "Individually Fair Ranking"
    },
    {
        "abs": "This article discusses the concept of individually fair gradient boosting, which aims to enforce fairness in machine learning algorithms. The focus is on gradient boosting, a powerful technique used in predictive modeling. The authors explore various approaches to achieve individual fairness and provide insights into mitigating bias in this specific context.",
        "title": "Individually Fair Gradient Boosting"
    },
    {
        "abs": "Abstract: The amount of data, manpower, and capital required to understand, evaluate, and agree on the elementary prognosis of diseases during a pandemic is immense. To address this challenge, we propose FedPandemic, a cross-device federated learning approach. FedPandemic leverages distributed computational power and data privacy preservation to enable collaborative learning and real-time disease prognosis based on elementary indicators. By collectively training models on decentralized devices, FedPandemic provides a scalable and efficient solution for early disease detection and prediction, reducing the burden on centralized resources.",
        "title": "FedPandemic: A Cross-Device Federated Learning Approach Towards Elementary Prognosis of Diseases During a Pandemic"
    },
    {
        "abs": "In knowledge-based AI systems, ontologies play a crucial role as they encompass concepts, attributes, and relationships. However, effectively populating ontologies poses a challenge. In this research, we propose a novel approach called Document Structure aware Relational Graph Convolutional Networks (DSR-GCN) for ontology population. Our method takes into consideration the document structure in order to enhance the extraction and representation of valuable information for ontology construction. Experimental results demonstrate the effectiveness and superiority of DSR-GCN in accurately populating ontologies.",
        "title": "Document Structure aware Relational Graph Convolutional Networks for Ontology Population"
    },
    {
        "abs": "Imitation learning algorithms aim to learn a policy by observing and replicating expert behavior through demonstration. In this study, we demonstrate that these algorithms can provide effective results.",
        "title": "Imitation Learning by Reinforcement Learning"
    },
    {
        "abs": "In recent years, black-box optimization formulations for biological sequence design have gained significant attention due to their promising applications. This abstract explores the unification of likelihood-free inference with black-box optimization, discussing how it can enhance and expand the potential of these optimization methods. The aim is to uncover novel approaches that go beyond traditional black-box optimization frameworks, opening new avenues for effective biological sequence design.",
        "title": "Unifying Likelihood-free Inference with Black-box Optimization and Beyond"
    },
    {
        "abs": "Deep reinforcement learning (DeepRL) has been receiving increasingly more attention thanks to its encouraging results in various domains. However, it is crucial to understand the role of regularization in policy optimization. Regularization techniques play a vital role in stabilizing the training process, preventing overfitting, and improving the generalization capability of DeepRL algorithms. In this paper, we highlight the importance of regularization in policy optimization and discuss different regularization methods commonly used in DeepRL. Through experiments and analysis, we demonstrate the impact of regularization on the performance and convergence of DeepRL algorithms, emphasizing the need for careful consideration and integration of regularization techniques in policy optimization.",
        "title": "Regularization Matters in Policy Optimization"
    },
    {
        "abs": "This abstract discusses the limitations of neural module networks (NMNs) in understanding and generating systematic patterns in Visual Question Answering (VQA) tasks. Although NMNs show a bias towards compositionality in their architectural design, they heavily rely on gold standard layouts, which restricts their flexibility and generalizability. To address this challenge, the concept of iterated learning is proposed as a potential solution to enable emergent systematicity in VQA. By iteratively training and transferring knowledge across generations of NMNs, it is anticipated that they will be able to learn and adapt to systematic patterns in VQA tasks without explicitly relying on gold standard layouts.",
        "title": "Iterated learning for emergent systematicity in VQA"
    },
    {
        "abs": "This abstract discusses the concept of Knowledge Distillation (KD) as a commonly utilized technique to transfer knowledge from pre-trained teacher models. It explores the challenges of applying KD to create a teacher that deliberately fails at teaching students. This article aims to highlight the potential consequences of undistillable knowledge, particularly when used to develop a nasty teacher incapable of effectively imparting knowledge to students.",
        "title": "Undistillable: Making A Nasty Teacher That CANNOT teach students"
    },
    {
        "abs": "This abstract summarizes recent advancements in interpreting uncertainty estimates from differentiable probabilistic models. Specifically, there has been a proposal to generate Counterfactual Latent Uncertainty Explanations (CLUE) to provide diverse sets of explanations for uncertainty estimates. The CLUE approach offers a novel perspective for understanding and interpreting uncertainty in probabilistic models, contributing to improved decision-making and transparency in various domains.",
        "title": "\u03b4-CLUE: Diverse Sets of Explanations for Uncertainty Estimates"
    }
]